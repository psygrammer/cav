Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2016-02-03 09:49:56.909836: step 0, loss = 4.67 (2.0 examples/sec; 65.123 sec/batch)
2016-02-03 09:50:02.208112: step 10, loss = 4.66 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 09:50:06.935291: step 20, loss = 4.63 (309.4 examples/sec; 0.414 sec/batch)
2016-02-03 09:50:11.677843: step 30, loss = 4.61 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 09:50:16.492321: step 40, loss = 4.59 (261.3 examples/sec; 0.490 sec/batch)
2016-02-03 09:50:21.212418: step 50, loss = 4.57 (286.6 examples/sec; 0.447 sec/batch)
2016-02-03 09:50:25.903630: step 60, loss = 4.56 (289.0 examples/sec; 0.443 sec/batch)
2016-02-03 09:50:30.640395: step 70, loss = 4.54 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 09:50:35.276647: step 80, loss = 4.52 (287.4 examples/sec; 0.445 sec/batch)
2016-02-03 09:50:39.996685: step 90, loss = 4.50 (262.0 examples/sec; 0.489 sec/batch)
2016-02-03 09:50:44.778525: step 100, loss = 4.49 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 09:50:50.061908: step 110, loss = 4.47 (251.3 examples/sec; 0.509 sec/batch)
2016-02-03 09:50:54.755692: step 120, loss = 4.45 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 09:50:59.488314: step 130, loss = 4.44 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 09:51:04.253789: step 140, loss = 4.42 (261.3 examples/sec; 0.490 sec/batch)
2016-02-03 09:51:08.937261: step 150, loss = 4.40 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 09:51:13.708941: step 160, loss = 4.38 (249.0 examples/sec; 0.514 sec/batch)
2016-02-03 09:51:18.442474: step 170, loss = 4.37 (258.0 examples/sec; 0.496 sec/batch)
2016-02-03 09:51:23.275588: step 180, loss = 4.35 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 09:51:28.006162: step 190, loss = 4.33 (285.9 examples/sec; 0.448 sec/batch)
2016-02-03 09:51:32.802344: step 200, loss = 4.32 (256.5 examples/sec; 0.499 sec/batch)
2016-02-03 09:51:38.099826: step 210, loss = 4.30 (280.5 examples/sec; 0.456 sec/batch)
2016-02-03 09:51:42.990568: step 220, loss = 4.28 (263.1 examples/sec; 0.486 sec/batch)
2016-02-03 09:51:47.820105: step 230, loss = 4.27 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 09:51:52.601138: step 240, loss = 4.26 (264.9 examples/sec; 0.483 sec/batch)
2016-02-03 09:51:57.459115: step 250, loss = 4.24 (268.1 examples/sec; 0.478 sec/batch)
2016-02-03 09:52:02.282625: step 260, loss = 4.23 (307.4 examples/sec; 0.416 sec/batch)
2016-02-03 09:52:07.126495: step 270, loss = 4.21 (257.8 examples/sec; 0.497 sec/batch)
2016-02-03 09:52:11.859513: step 280, loss = 4.20 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 09:52:16.667588: step 290, loss = 4.18 (248.7 examples/sec; 0.515 sec/batch)
2016-02-03 09:52:21.574492: step 300, loss = 4.16 (247.8 examples/sec; 0.517 sec/batch)
2016-02-03 09:52:26.942723: step 310, loss = 4.15 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 09:52:31.706298: step 320, loss = 4.13 (259.0 examples/sec; 0.494 sec/batch)
2016-02-03 09:52:36.519624: step 330, loss = 4.12 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 09:52:41.310205: step 340, loss = 4.11 (251.1 examples/sec; 0.510 sec/batch)
2016-02-03 09:52:46.121710: step 350, loss = 4.08 (254.0 examples/sec; 0.504 sec/batch)
2016-02-03 09:52:50.893370: step 360, loss = 4.08 (260.9 examples/sec; 0.491 sec/batch)
2016-02-03 09:52:55.716475: step 370, loss = 4.06 (244.4 examples/sec; 0.524 sec/batch)
2016-02-03 09:53:00.545472: step 380, loss = 4.04 (251.5 examples/sec; 0.509 sec/batch)
2016-02-03 09:53:05.449742: step 390, loss = 4.04 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 09:53:10.233977: step 400, loss = 4.02 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 09:53:15.588405: step 410, loss = 4.01 (246.4 examples/sec; 0.519 sec/batch)
2016-02-03 09:53:20.445322: step 420, loss = 4.00 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 09:53:25.282819: step 430, loss = 3.98 (250.3 examples/sec; 0.511 sec/batch)
2016-02-03 09:53:30.082220: step 440, loss = 3.96 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 09:53:34.905969: step 450, loss = 3.95 (285.9 examples/sec; 0.448 sec/batch)
2016-02-03 09:53:39.758820: step 460, loss = 3.94 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 09:53:44.546440: step 470, loss = 3.93 (281.4 examples/sec; 0.455 sec/batch)
2016-02-03 09:53:49.392301: step 480, loss = 3.92 (251.8 examples/sec; 0.508 sec/batch)
2016-02-03 09:53:54.179395: step 490, loss = 3.90 (284.7 examples/sec; 0.450 sec/batch)
2016-02-03 09:53:58.956554: step 500, loss = 3.89 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 09:54:04.152573: step 510, loss = 3.87 (282.9 examples/sec; 0.452 sec/batch)
2016-02-03 09:54:08.893021: step 520, loss = 3.86 (270.5 examples/sec; 0.473 sec/batch)
2016-02-03 09:54:13.620350: step 530, loss = 3.85 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 09:54:18.472832: step 540, loss = 3.84 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 09:54:23.318772: step 550, loss = 3.82 (260.1 examples/sec; 0.492 sec/batch)
2016-02-03 09:54:28.077973: step 560, loss = 3.82 (272.7 examples/sec; 0.469 sec/batch)
2016-02-03 09:54:32.838433: step 570, loss = 3.80 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 09:54:37.579458: step 580, loss = 3.80 (298.8 examples/sec; 0.428 sec/batch)
2016-02-03 09:54:42.267521: step 590, loss = 3.78 (281.6 examples/sec; 0.454 sec/batch)
2016-02-03 09:54:46.998875: step 600, loss = 3.76 (285.3 examples/sec; 0.449 sec/batch)
2016-02-03 09:54:52.349499: step 610, loss = 3.75 (251.0 examples/sec; 0.510 sec/batch)
2016-02-03 09:54:57.117696: step 620, loss = 3.75 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 09:55:01.947883: step 630, loss = 3.73 (249.6 examples/sec; 0.513 sec/batch)
2016-02-03 09:55:06.590015: step 640, loss = 3.72 (251.4 examples/sec; 0.509 sec/batch)
2016-02-03 09:55:11.225140: step 650, loss = 3.70 (294.4 examples/sec; 0.435 sec/batch)
2016-02-03 09:55:15.999187: step 660, loss = 3.70 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 09:55:20.675653: step 670, loss = 3.69 (285.5 examples/sec; 0.448 sec/batch)
2016-02-03 09:55:25.383666: step 680, loss = 3.67 (319.0 examples/sec; 0.401 sec/batch)
2016-02-03 09:55:29.966803: step 690, loss = 3.67 (290.5 examples/sec; 0.441 sec/batch)
2016-02-03 09:55:34.662815: step 700, loss = 3.65 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 09:55:39.906437: step 710, loss = 3.64 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 09:55:44.648481: step 720, loss = 3.63 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 09:55:49.386806: step 730, loss = 3.62 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 09:55:54.058373: step 740, loss = 3.61 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 09:55:58.732669: step 750, loss = 3.60 (290.5 examples/sec; 0.441 sec/batch)
2016-02-03 09:56:03.347907: step 760, loss = 3.58 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 09:56:08.083218: step 770, loss = 3.58 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 09:56:12.804385: step 780, loss = 3.57 (263.6 examples/sec; 0.486 sec/batch)
2016-02-03 09:56:17.543298: step 790, loss = 3.56 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 09:56:22.259675: step 800, loss = 3.55 (239.7 examples/sec; 0.534 sec/batch)
2016-02-03 09:56:27.435155: step 810, loss = 3.54 (291.6 examples/sec; 0.439 sec/batch)
2016-02-03 09:56:32.113861: step 820, loss = 3.53 (283.5 examples/sec; 0.451 sec/batch)
2016-02-03 09:56:36.878087: step 830, loss = 3.52 (256.4 examples/sec; 0.499 sec/batch)
2016-02-03 09:56:41.568165: step 840, loss = 3.51 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 09:56:46.213250: step 850, loss = 3.50 (289.9 examples/sec; 0.442 sec/batch)
2016-02-03 09:56:50.940440: step 860, loss = 3.49 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 09:56:55.670567: step 870, loss = 3.48 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 09:57:00.455797: step 880, loss = 3.47 (249.0 examples/sec; 0.514 sec/batch)
2016-02-03 09:57:05.057999: step 890, loss = 3.46 (295.9 examples/sec; 0.433 sec/batch)
2016-02-03 09:57:09.828558: step 900, loss = 3.44 (249.7 examples/sec; 0.513 sec/batch)
2016-02-03 09:57:15.039994: step 910, loss = 3.43 (256.1 examples/sec; 0.500 sec/batch)
2016-02-03 09:57:19.699621: step 920, loss = 3.41 (284.7 examples/sec; 0.450 sec/batch)
2016-02-03 09:57:24.464563: step 930, loss = 3.35 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 09:57:29.130309: step 940, loss = 3.28 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 09:57:33.832738: step 950, loss = 3.20 (264.7 examples/sec; 0.484 sec/batch)
2016-02-03 09:57:38.500380: step 960, loss = 3.20 (302.3 examples/sec; 0.423 sec/batch)
2016-02-03 09:57:43.074502: step 970, loss = 3.16 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 09:57:47.653396: step 980, loss = 3.03 (292.1 examples/sec; 0.438 sec/batch)
2016-02-03 09:57:52.252620: step 990, loss = 3.07 (264.3 examples/sec; 0.484 sec/batch)
2016-02-03 09:57:56.866623: step 1000, loss = 3.12 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 09:58:02.055261: step 1010, loss = 3.03 (249.1 examples/sec; 0.514 sec/batch)
2016-02-03 09:58:06.621439: step 1020, loss = 3.16 (300.2 examples/sec; 0.426 sec/batch)
2016-02-03 09:58:11.142865: step 1030, loss = 3.16 (306.1 examples/sec; 0.418 sec/batch)
2016-02-03 09:58:15.730002: step 1040, loss = 3.06 (300.9 examples/sec; 0.425 sec/batch)
2016-02-03 09:58:20.242422: step 1050, loss = 3.08 (288.1 examples/sec; 0.444 sec/batch)
2016-02-03 09:58:24.856113: step 1060, loss = 3.05 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 09:58:29.561201: step 1070, loss = 2.99 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 09:58:34.252499: step 1080, loss = 3.07 (284.6 examples/sec; 0.450 sec/batch)
2016-02-03 09:58:38.836673: step 1090, loss = 2.92 (259.7 examples/sec; 0.493 sec/batch)
2016-02-03 09:58:43.394880: step 1100, loss = 2.99 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 09:58:48.502035: step 1110, loss = 2.96 (265.8 examples/sec; 0.482 sec/batch)
2016-02-03 09:58:53.088116: step 1120, loss = 2.88 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 09:58:57.757029: step 1130, loss = 2.91 (257.3 examples/sec; 0.497 sec/batch)
2016-02-03 09:59:02.308031: step 1140, loss = 2.88 (296.9 examples/sec; 0.431 sec/batch)
2016-02-03 09:59:06.891765: step 1150, loss = 2.96 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 09:59:11.439638: step 1160, loss = 2.88 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 09:59:16.072937: step 1170, loss = 2.82 (286.7 examples/sec; 0.446 sec/batch)
2016-02-03 09:59:20.694562: step 1180, loss = 2.77 (288.5 examples/sec; 0.444 sec/batch)
2016-02-03 09:59:25.284772: step 1190, loss = 2.82 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 09:59:29.955493: step 1200, loss = 2.89 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 09:59:35.181732: step 1210, loss = 2.79 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 09:59:39.919704: step 1220, loss = 2.59 (254.9 examples/sec; 0.502 sec/batch)
2016-02-03 09:59:44.554423: step 1230, loss = 2.96 (257.8 examples/sec; 0.496 sec/batch)
2016-02-03 09:59:49.233962: step 1240, loss = 2.76 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 09:59:53.985479: step 1250, loss = 2.73 (255.2 examples/sec; 0.502 sec/batch)
2016-02-03 09:59:58.708267: step 1260, loss = 2.77 (292.4 examples/sec; 0.438 sec/batch)
2016-02-03 10:00:03.471932: step 1270, loss = 2.78 (255.1 examples/sec; 0.502 sec/batch)
2016-02-03 10:00:08.105340: step 1280, loss = 2.70 (274.4 examples/sec; 0.467 sec/batch)
2016-02-03 10:00:12.893615: step 1290, loss = 2.61 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 10:00:17.695204: step 1300, loss = 2.64 (277.9 examples/sec; 0.461 sec/batch)
2016-02-03 10:00:22.959502: step 1310, loss = 2.62 (280.3 examples/sec; 0.457 sec/batch)
2016-02-03 10:00:27.667136: step 1320, loss = 2.70 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 10:00:32.353197: step 1330, loss = 2.64 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 10:00:36.992806: step 1340, loss = 2.75 (286.3 examples/sec; 0.447 sec/batch)
2016-02-03 10:00:41.758818: step 1350, loss = 2.57 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 10:00:46.460635: step 1360, loss = 2.70 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 10:00:51.100659: step 1370, loss = 2.75 (280.5 examples/sec; 0.456 sec/batch)
2016-02-03 10:00:55.805326: step 1380, loss = 2.65 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 10:01:00.427050: step 1390, loss = 2.74 (280.6 examples/sec; 0.456 sec/batch)
2016-02-03 10:01:05.036037: step 1400, loss = 2.51 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 10:01:10.145855: step 1410, loss = 2.54 (285.4 examples/sec; 0.449 sec/batch)
2016-02-03 10:01:14.756217: step 1420, loss = 2.52 (291.6 examples/sec; 0.439 sec/batch)
2016-02-03 10:01:19.267398: step 1430, loss = 2.51 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 10:01:23.937273: step 1440, loss = 2.47 (258.7 examples/sec; 0.495 sec/batch)
2016-02-03 10:01:28.509351: step 1450, loss = 2.47 (289.5 examples/sec; 0.442 sec/batch)
2016-02-03 10:01:33.086397: step 1460, loss = 2.37 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 10:01:37.598150: step 1470, loss = 2.48 (296.6 examples/sec; 0.432 sec/batch)
2016-02-03 10:01:42.197901: step 1480, loss = 2.40 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 10:01:46.810585: step 1490, loss = 2.29 (256.9 examples/sec; 0.498 sec/batch)
2016-02-03 10:01:51.368084: step 1500, loss = 2.45 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 10:01:56.481817: step 1510, loss = 2.38 (256.7 examples/sec; 0.499 sec/batch)
2016-02-03 10:02:01.188752: step 1520, loss = 2.52 (309.7 examples/sec; 0.413 sec/batch)
2016-02-03 10:02:05.816501: step 1530, loss = 2.49 (293.8 examples/sec; 0.436 sec/batch)
2016-02-03 10:02:10.458606: step 1540, loss = 2.46 (267.5 examples/sec; 0.478 sec/batch)
2016-02-03 10:02:15.025001: step 1550, loss = 2.36 (293.2 examples/sec; 0.437 sec/batch)
2016-02-03 10:02:19.713682: step 1560, loss = 2.53 (257.8 examples/sec; 0.497 sec/batch)
2016-02-03 10:02:24.436597: step 1570, loss = 2.39 (256.4 examples/sec; 0.499 sec/batch)
2016-02-03 10:02:28.952167: step 1580, loss = 2.31 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 10:02:33.665025: step 1590, loss = 2.47 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 10:02:38.361389: step 1600, loss = 2.39 (273.2 examples/sec; 0.468 sec/batch)
2016-02-03 10:02:43.536731: step 1610, loss = 2.46 (285.1 examples/sec; 0.449 sec/batch)
2016-02-03 10:02:48.103126: step 1620, loss = 2.37 (322.7 examples/sec; 0.397 sec/batch)
2016-02-03 10:02:52.698382: step 1630, loss = 2.17 (269.3 examples/sec; 0.475 sec/batch)
2016-02-03 10:02:57.384800: step 1640, loss = 2.44 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 10:03:02.054602: step 1650, loss = 2.23 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 10:03:06.773882: step 1660, loss = 2.22 (261.7 examples/sec; 0.489 sec/batch)
2016-02-03 10:03:11.476106: step 1670, loss = 2.20 (254.5 examples/sec; 0.503 sec/batch)
2016-02-03 10:03:16.018714: step 1680, loss = 2.29 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 10:03:20.631391: step 1690, loss = 2.24 (265.8 examples/sec; 0.482 sec/batch)
2016-02-03 10:03:25.334493: step 1700, loss = 2.26 (289.3 examples/sec; 0.442 sec/batch)
2016-02-03 10:03:30.550612: step 1710, loss = 2.28 (258.8 examples/sec; 0.495 sec/batch)
2016-02-03 10:03:35.181712: step 1720, loss = 2.27 (282.4 examples/sec; 0.453 sec/batch)
2016-02-03 10:03:39.837453: step 1730, loss = 2.36 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 10:03:44.483367: step 1740, loss = 2.21 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 10:03:49.180652: step 1750, loss = 2.34 (280.9 examples/sec; 0.456 sec/batch)
2016-02-03 10:03:53.797443: step 1760, loss = 2.25 (259.3 examples/sec; 0.494 sec/batch)
2016-02-03 10:03:58.422052: step 1770, loss = 2.13 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 10:04:03.149995: step 1780, loss = 2.06 (296.2 examples/sec; 0.432 sec/batch)
2016-02-03 10:04:07.837971: step 1790, loss = 2.06 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 10:04:12.560925: step 1800, loss = 2.03 (286.2 examples/sec; 0.447 sec/batch)
2016-02-03 10:04:17.843234: step 1810, loss = 2.24 (277.9 examples/sec; 0.461 sec/batch)
2016-02-03 10:04:22.544698: step 1820, loss = 2.17 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 10:04:27.269438: step 1830, loss = 2.05 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 10:04:32.012637: step 1840, loss = 2.21 (255.2 examples/sec; 0.502 sec/batch)
2016-02-03 10:04:36.711605: step 1850, loss = 2.05 (260.4 examples/sec; 0.492 sec/batch)
2016-02-03 10:04:41.418951: step 1860, loss = 2.16 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 10:04:46.157413: step 1870, loss = 1.90 (293.6 examples/sec; 0.436 sec/batch)
2016-02-03 10:04:50.876534: step 1880, loss = 1.96 (262.1 examples/sec; 0.488 sec/batch)
2016-02-03 10:04:55.605609: step 1890, loss = 2.08 (283.5 examples/sec; 0.452 sec/batch)
2016-02-03 10:05:00.309350: step 1900, loss = 2.04 (257.1 examples/sec; 0.498 sec/batch)
2016-02-03 10:05:05.515828: step 1910, loss = 2.17 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 10:05:10.222088: step 1920, loss = 2.14 (260.3 examples/sec; 0.492 sec/batch)
2016-02-03 10:05:14.916085: step 1930, loss = 1.99 (254.8 examples/sec; 0.502 sec/batch)
2016-02-03 10:05:19.606093: step 1940, loss = 2.02 (284.7 examples/sec; 0.450 sec/batch)
2016-02-03 10:05:24.360173: step 1950, loss = 2.14 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 10:05:29.132136: step 1960, loss = 2.08 (253.8 examples/sec; 0.504 sec/batch)
2016-02-03 10:05:33.909634: step 1970, loss = 2.22 (280.1 examples/sec; 0.457 sec/batch)
2016-02-03 10:05:38.682485: step 1980, loss = 2.13 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 10:05:43.333930: step 1990, loss = 1.90 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 10:05:48.007993: step 2000, loss = 1.97 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 10:05:53.233911: step 2010, loss = 1.97 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 10:05:57.915079: step 2020, loss = 1.96 (293.2 examples/sec; 0.437 sec/batch)
2016-02-03 10:06:02.677189: step 2030, loss = 1.94 (290.7 examples/sec; 0.440 sec/batch)
2016-02-03 10:06:07.459594: step 2040, loss = 2.20 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 10:06:12.251118: step 2050, loss = 1.86 (243.8 examples/sec; 0.525 sec/batch)
2016-02-03 10:06:17.053835: step 2060, loss = 2.11 (237.8 examples/sec; 0.538 sec/batch)
2016-02-03 10:06:21.777178: step 2070, loss = 2.11 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 10:06:26.463164: step 2080, loss = 1.93 (300.8 examples/sec; 0.426 sec/batch)
2016-02-03 10:06:31.126616: step 2090, loss = 2.03 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 10:06:35.820979: step 2100, loss = 1.89 (263.6 examples/sec; 0.486 sec/batch)
2016-02-03 10:06:41.173947: step 2110, loss = 1.90 (245.9 examples/sec; 0.521 sec/batch)
2016-02-03 10:06:45.927128: step 2120, loss = 1.86 (275.0 examples/sec; 0.465 sec/batch)
2016-02-03 10:06:50.630451: step 2130, loss = 2.16 (283.5 examples/sec; 0.451 sec/batch)
2016-02-03 10:06:55.365825: step 2140, loss = 2.01 (291.0 examples/sec; 0.440 sec/batch)
2016-02-03 10:07:00.006053: step 2150, loss = 1.98 (298.2 examples/sec; 0.429 sec/batch)
2016-02-03 10:07:04.666041: step 2160, loss = 2.09 (282.4 examples/sec; 0.453 sec/batch)
2016-02-03 10:07:09.444359: step 2170, loss = 1.99 (247.8 examples/sec; 0.516 sec/batch)
2016-02-03 10:07:14.102595: step 2180, loss = 1.92 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 10:07:18.867658: step 2190, loss = 2.04 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 10:07:23.611138: step 2200, loss = 2.11 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 10:07:28.799536: step 2210, loss = 1.81 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 10:07:33.566099: step 2220, loss = 1.86 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 10:07:38.361186: step 2230, loss = 1.78 (265.3 examples/sec; 0.482 sec/batch)
2016-02-03 10:07:43.069122: step 2240, loss = 1.82 (249.8 examples/sec; 0.512 sec/batch)
2016-02-03 10:07:47.788839: step 2250, loss = 1.84 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 10:07:52.456943: step 2260, loss = 1.79 (285.9 examples/sec; 0.448 sec/batch)
2016-02-03 10:07:57.184856: step 2270, loss = 1.85 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 10:08:01.984814: step 2280, loss = 2.00 (250.8 examples/sec; 0.510 sec/batch)
2016-02-03 10:08:06.613670: step 2290, loss = 1.62 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 10:08:11.355370: step 2300, loss = 1.73 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 10:08:16.521497: step 2310, loss = 1.63 (289.2 examples/sec; 0.443 sec/batch)
2016-02-03 10:08:21.202581: step 2320, loss = 1.74 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 10:08:25.959011: step 2330, loss = 1.93 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 10:08:30.696893: step 2340, loss = 1.71 (250.2 examples/sec; 0.512 sec/batch)
2016-02-03 10:08:35.415232: step 2350, loss = 1.89 (255.8 examples/sec; 0.500 sec/batch)
2016-02-03 10:08:40.209963: step 2360, loss = 1.61 (254.8 examples/sec; 0.502 sec/batch)
2016-02-03 10:08:44.896120: step 2370, loss = 1.83 (255.9 examples/sec; 0.500 sec/batch)
2016-02-03 10:08:49.588296: step 2380, loss = 1.79 (269.2 examples/sec; 0.475 sec/batch)
2016-02-03 10:08:54.404537: step 2390, loss = 1.70 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 10:08:59.029902: step 2400, loss = 1.68 (280.9 examples/sec; 0.456 sec/batch)
2016-02-03 10:09:04.177586: step 2410, loss = 1.85 (265.2 examples/sec; 0.483 sec/batch)
2016-02-03 10:09:08.889390: step 2420, loss = 1.81 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 10:09:13.538956: step 2430, loss = 1.83 (297.7 examples/sec; 0.430 sec/batch)
2016-02-03 10:09:18.154341: step 2440, loss = 1.87 (288.8 examples/sec; 0.443 sec/batch)
2016-02-03 10:09:22.691994: step 2450, loss = 1.75 (303.9 examples/sec; 0.421 sec/batch)
2016-02-03 10:09:27.441423: step 2460, loss = 1.68 (249.5 examples/sec; 0.513 sec/batch)
2016-02-03 10:09:32.023803: step 2470, loss = 1.68 (291.7 examples/sec; 0.439 sec/batch)
2016-02-03 10:09:36.721205: step 2480, loss = 1.66 (240.9 examples/sec; 0.531 sec/batch)
2016-02-03 10:09:41.324493: step 2490, loss = 1.74 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 10:09:45.920249: step 2500, loss = 1.82 (331.9 examples/sec; 0.386 sec/batch)
2016-02-03 10:09:51.074115: step 2510, loss = 1.69 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 10:09:55.836087: step 2520, loss = 1.62 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 10:10:00.535547: step 2530, loss = 1.63 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 10:10:05.324718: step 2540, loss = 1.74 (252.7 examples/sec; 0.506 sec/batch)
2016-02-03 10:10:09.957753: step 2550, loss = 1.78 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 10:10:14.582499: step 2560, loss = 1.56 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 10:10:19.286220: step 2570, loss = 1.80 (289.4 examples/sec; 0.442 sec/batch)
2016-02-03 10:10:24.012567: step 2580, loss = 1.51 (292.1 examples/sec; 0.438 sec/batch)
2016-02-03 10:10:28.701805: step 2590, loss = 1.58 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 10:10:33.383206: step 2600, loss = 1.55 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 10:10:38.541669: step 2610, loss = 1.64 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 10:10:43.166211: step 2620, loss = 1.67 (281.5 examples/sec; 0.455 sec/batch)
2016-02-03 10:10:47.814903: step 2630, loss = 1.52 (282.8 examples/sec; 0.453 sec/batch)
2016-02-03 10:10:52.374538: step 2640, loss = 1.55 (260.6 examples/sec; 0.491 sec/batch)
2016-02-03 10:10:57.019126: step 2650, loss = 1.53 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 10:11:01.656565: step 2660, loss = 1.62 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 10:11:06.337386: step 2670, loss = 1.62 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 10:11:11.000147: step 2680, loss = 1.59 (272.6 examples/sec; 0.470 sec/batch)
2016-02-03 10:11:15.680864: step 2690, loss = 1.48 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 10:11:20.440422: step 2700, loss = 1.63 (255.8 examples/sec; 0.500 sec/batch)
2016-02-03 10:11:25.599062: step 2710, loss = 1.77 (283.8 examples/sec; 0.451 sec/batch)
2016-02-03 10:11:30.193606: step 2720, loss = 1.74 (307.0 examples/sec; 0.417 sec/batch)
2016-02-03 10:11:34.829473: step 2730, loss = 1.68 (284.8 examples/sec; 0.449 sec/batch)
2016-02-03 10:11:39.525339: step 2740, loss = 1.80 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 10:11:44.242532: step 2750, loss = 1.43 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 10:11:48.974878: step 2760, loss = 1.56 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 10:11:53.706339: step 2770, loss = 1.53 (279.4 examples/sec; 0.458 sec/batch)
2016-02-03 10:11:58.423579: step 2780, loss = 1.79 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 10:12:03.080809: step 2790, loss = 1.62 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 10:12:07.773721: step 2800, loss = 1.44 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 10:12:13.004368: step 2810, loss = 1.62 (258.3 examples/sec; 0.496 sec/batch)
2016-02-03 10:12:17.653223: step 2820, loss = 1.80 (282.4 examples/sec; 0.453 sec/batch)
2016-02-03 10:12:22.339461: step 2830, loss = 1.73 (292.9 examples/sec; 0.437 sec/batch)
2016-02-03 10:12:27.034974: step 2840, loss = 1.53 (281.4 examples/sec; 0.455 sec/batch)
2016-02-03 10:12:31.706469: step 2850, loss = 1.60 (281.5 examples/sec; 0.455 sec/batch)
2016-02-03 10:12:36.352825: step 2860, loss = 1.59 (281.0 examples/sec; 0.456 sec/batch)
2016-02-03 10:12:41.039426: step 2870, loss = 1.55 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 10:12:45.674365: step 2880, loss = 1.55 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 10:12:50.294521: step 2890, loss = 1.55 (287.6 examples/sec; 0.445 sec/batch)
2016-02-03 10:12:54.972551: step 2900, loss = 1.52 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 10:13:00.118119: step 2910, loss = 1.54 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 10:13:04.790219: step 2920, loss = 1.61 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 10:13:09.379806: step 2930, loss = 1.54 (311.1 examples/sec; 0.411 sec/batch)
2016-02-03 10:13:13.918551: step 2940, loss = 1.50 (276.9 examples/sec; 0.462 sec/batch)
2016-02-03 10:13:18.530786: step 2950, loss = 1.41 (293.2 examples/sec; 0.437 sec/batch)
2016-02-03 10:13:23.218608: step 2960, loss = 1.49 (249.3 examples/sec; 0.514 sec/batch)
2016-02-03 10:13:27.754698: step 2970, loss = 1.41 (294.1 examples/sec; 0.435 sec/batch)
2016-02-03 10:13:32.326164: step 2980, loss = 1.65 (283.0 examples/sec; 0.452 sec/batch)
2016-02-03 10:13:37.031765: step 2990, loss = 1.48 (264.3 examples/sec; 0.484 sec/batch)
2016-02-03 10:13:41.662362: step 3000, loss = 1.37 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 10:13:46.815539: step 3010, loss = 1.57 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 10:13:51.436007: step 3020, loss = 1.45 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 10:13:56.069625: step 3030, loss = 1.49 (279.7 examples/sec; 0.458 sec/batch)
2016-02-03 10:14:00.739423: step 3040, loss = 1.53 (294.4 examples/sec; 0.435 sec/batch)
2016-02-03 10:14:05.352327: step 3050, loss = 1.56 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 10:14:10.040135: step 3060, loss = 1.64 (249.6 examples/sec; 0.513 sec/batch)
2016-02-03 10:14:14.671896: step 3070, loss = 1.42 (312.1 examples/sec; 0.410 sec/batch)
2016-02-03 10:14:19.365910: step 3080, loss = 1.71 (295.9 examples/sec; 0.433 sec/batch)
2016-02-03 10:14:24.106364: step 3090, loss = 1.49 (258.8 examples/sec; 0.494 sec/batch)
2016-02-03 10:14:28.846000: step 3100, loss = 1.30 (255.0 examples/sec; 0.502 sec/batch)
2016-02-03 10:14:34.045151: step 3110, loss = 1.56 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 10:14:38.735058: step 3120, loss = 1.33 (289.5 examples/sec; 0.442 sec/batch)
2016-02-03 10:14:43.494310: step 3130, loss = 1.28 (257.5 examples/sec; 0.497 sec/batch)
2016-02-03 10:14:48.231118: step 3140, loss = 1.34 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 10:14:53.027313: step 3150, loss = 1.38 (264.9 examples/sec; 0.483 sec/batch)
2016-02-03 10:14:57.714515: step 3160, loss = 1.48 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 10:15:02.427035: step 3170, loss = 1.62 (260.5 examples/sec; 0.491 sec/batch)
2016-02-03 10:15:07.063583: step 3180, loss = 1.41 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 10:15:11.758973: step 3190, loss = 1.45 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 10:15:16.556254: step 3200, loss = 1.20 (261.0 examples/sec; 0.490 sec/batch)
2016-02-03 10:15:21.668004: step 3210, loss = 1.45 (286.3 examples/sec; 0.447 sec/batch)
2016-02-03 10:15:26.357658: step 3220, loss = 1.34 (290.6 examples/sec; 0.440 sec/batch)
2016-02-03 10:15:31.034462: step 3230, loss = 1.70 (257.2 examples/sec; 0.498 sec/batch)
2016-02-03 10:15:35.643854: step 3240, loss = 1.30 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 10:15:40.316304: step 3250, loss = 1.39 (286.6 examples/sec; 0.447 sec/batch)
2016-02-03 10:15:44.992362: step 3260, loss = 1.37 (304.6 examples/sec; 0.420 sec/batch)
2016-02-03 10:15:49.651687: step 3270, loss = 1.16 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 10:15:54.396588: step 3280, loss = 1.69 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 10:15:59.134255: step 3290, loss = 1.34 (249.8 examples/sec; 0.512 sec/batch)
2016-02-03 10:16:03.861606: step 3300, loss = 1.46 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 10:16:08.972619: step 3310, loss = 1.35 (302.3 examples/sec; 0.423 sec/batch)
2016-02-03 10:16:13.611569: step 3320, loss = 1.78 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 10:16:18.292505: step 3330, loss = 1.45 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 10:16:23.056311: step 3340, loss = 1.36 (241.3 examples/sec; 0.530 sec/batch)
2016-02-03 10:16:27.678709: step 3350, loss = 1.19 (253.0 examples/sec; 0.506 sec/batch)
2016-02-03 10:16:32.311409: step 3360, loss = 1.45 (259.9 examples/sec; 0.493 sec/batch)
2016-02-03 10:16:37.055089: step 3370, loss = 1.61 (263.6 examples/sec; 0.486 sec/batch)
2016-02-03 10:16:41.815568: step 3380, loss = 1.43 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 10:16:46.491748: step 3390, loss = 1.23 (285.2 examples/sec; 0.449 sec/batch)
2016-02-03 10:16:51.281401: step 3400, loss = 1.24 (257.2 examples/sec; 0.498 sec/batch)
2016-02-03 10:16:56.530904: step 3410, loss = 1.50 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 10:17:01.287291: step 3420, loss = 1.43 (256.5 examples/sec; 0.499 sec/batch)
2016-02-03 10:17:06.062792: step 3430, loss = 1.35 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 10:17:10.869172: step 3440, loss = 1.23 (259.9 examples/sec; 0.493 sec/batch)
2016-02-03 10:17:15.572628: step 3450, loss = 1.49 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 10:17:20.376478: step 3460, loss = 1.51 (256.9 examples/sec; 0.498 sec/batch)
2016-02-03 10:17:25.056235: step 3470, loss = 1.55 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 10:17:29.806371: step 3480, loss = 1.34 (270.5 examples/sec; 0.473 sec/batch)
2016-02-03 10:17:34.545675: step 3490, loss = 1.46 (291.1 examples/sec; 0.440 sec/batch)
2016-02-03 10:17:39.308287: step 3500, loss = 1.21 (258.4 examples/sec; 0.495 sec/batch)
2016-02-03 10:17:44.587551: step 3510, loss = 1.34 (255.9 examples/sec; 0.500 sec/batch)
2016-02-03 10:17:49.304267: step 3520, loss = 1.47 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 10:17:54.078053: step 3530, loss = 1.60 (248.3 examples/sec; 0.516 sec/batch)
2016-02-03 10:17:58.808105: step 3540, loss = 1.34 (261.7 examples/sec; 0.489 sec/batch)
2016-02-03 10:18:03.480159: step 3550, loss = 1.26 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 10:18:08.239235: step 3560, loss = 1.39 (260.4 examples/sec; 0.492 sec/batch)
2016-02-03 10:18:12.950338: step 3570, loss = 1.36 (264.7 examples/sec; 0.484 sec/batch)
2016-02-03 10:18:17.617968: step 3580, loss = 1.28 (313.1 examples/sec; 0.409 sec/batch)
2016-02-03 10:18:22.372271: step 3590, loss = 1.34 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 10:18:27.088948: step 3600, loss = 1.30 (266.4 examples/sec; 0.480 sec/batch)
2016-02-03 10:18:32.274551: step 3610, loss = 1.37 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 10:18:37.009365: step 3620, loss = 1.35 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 10:18:41.699788: step 3630, loss = 1.13 (261.9 examples/sec; 0.489 sec/batch)
2016-02-03 10:18:46.410365: step 3640, loss = 1.31 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 10:18:51.139954: step 3650, loss = 1.49 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 10:18:55.780017: step 3660, loss = 1.22 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 10:19:00.438784: step 3670, loss = 1.29 (287.6 examples/sec; 0.445 sec/batch)
2016-02-03 10:19:05.126208: step 3680, loss = 1.20 (295.0 examples/sec; 0.434 sec/batch)
2016-02-03 10:19:09.806728: step 3690, loss = 1.46 (307.1 examples/sec; 0.417 sec/batch)
2016-02-03 10:19:14.484806: step 3700, loss = 1.22 (284.8 examples/sec; 0.449 sec/batch)
2016-02-03 10:19:19.602725: step 3710, loss = 1.42 (267.0 examples/sec; 0.479 sec/batch)
2016-02-03 10:19:24.217412: step 3720, loss = 1.13 (281.7 examples/sec; 0.454 sec/batch)
2016-02-03 10:19:28.891751: step 3730, loss = 1.38 (253.6 examples/sec; 0.505 sec/batch)
2016-02-03 10:19:33.636910: step 3740, loss = 1.28 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 10:19:38.346585: step 3750, loss = 1.37 (250.8 examples/sec; 0.510 sec/batch)
2016-02-03 10:19:42.851732: step 3760, loss = 1.63 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 10:19:47.433763: step 3770, loss = 1.26 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 10:19:52.137406: step 3780, loss = 1.15 (260.6 examples/sec; 0.491 sec/batch)
2016-02-03 10:19:56.823599: step 3790, loss = 1.40 (277.7 examples/sec; 0.461 sec/batch)
2016-02-03 10:20:01.441535: step 3800, loss = 1.37 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 10:20:06.480875: step 3810, loss = 1.25 (289.7 examples/sec; 0.442 sec/batch)
2016-02-03 10:20:11.199941: step 3820, loss = 1.32 (275.6 examples/sec; 0.464 sec/batch)
2016-02-03 10:20:15.898179: step 3830, loss = 1.21 (252.1 examples/sec; 0.508 sec/batch)
2016-02-03 10:20:20.575205: step 3840, loss = 1.22 (262.6 examples/sec; 0.487 sec/batch)
2016-02-03 10:20:25.286969: step 3850, loss = 1.17 (289.4 examples/sec; 0.442 sec/batch)
2016-02-03 10:20:30.032903: step 3860, loss = 1.22 (264.2 examples/sec; 0.484 sec/batch)
2016-02-03 10:20:34.792136: step 3870, loss = 1.02 (283.3 examples/sec; 0.452 sec/batch)
2016-02-03 10:20:39.487235: step 3880, loss = 1.20 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 10:20:44.101460: step 3890, loss = 1.41 (264.2 examples/sec; 0.484 sec/batch)
2016-02-03 10:20:48.744472: step 3900, loss = 1.13 (261.0 examples/sec; 0.490 sec/batch)
2016-02-03 10:20:53.956025: step 3910, loss = 1.63 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 10:20:58.675314: step 3920, loss = 1.37 (251.5 examples/sec; 0.509 sec/batch)
2016-02-03 10:21:03.357854: step 3930, loss = 1.36 (268.2 examples/sec; 0.477 sec/batch)
2016-02-03 10:21:07.981876: step 3940, loss = 1.26 (286.8 examples/sec; 0.446 sec/batch)
2016-02-03 10:21:12.731233: step 3950, loss = 1.19 (259.7 examples/sec; 0.493 sec/batch)
2016-02-03 10:21:17.416439: step 3960, loss = 1.17 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 10:21:22.160951: step 3970, loss = 1.22 (266.9 examples/sec; 0.480 sec/batch)
2016-02-03 10:21:26.762977: step 3980, loss = 1.29 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 10:21:31.465694: step 3990, loss = 1.21 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 10:21:36.098498: step 4000, loss = 1.11 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 10:21:41.269553: step 4010, loss = 1.36 (301.7 examples/sec; 0.424 sec/batch)
2016-02-03 10:21:45.859296: step 4020, loss = 1.11 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 10:21:50.497815: step 4030, loss = 1.16 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 10:21:55.217350: step 4040, loss = 1.29 (266.3 examples/sec; 0.481 sec/batch)
2016-02-03 10:21:59.848690: step 4050, loss = 1.23 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 10:22:04.520269: step 4060, loss = 1.36 (262.0 examples/sec; 0.488 sec/batch)
2016-02-03 10:22:09.167302: step 4070, loss = 1.29 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 10:22:13.898530: step 4080, loss = 1.07 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 10:22:18.500237: step 4090, loss = 1.19 (295.2 examples/sec; 0.434 sec/batch)
2016-02-03 10:22:23.249139: step 4100, loss = 1.30 (287.3 examples/sec; 0.445 sec/batch)
2016-02-03 10:22:28.577981: step 4110, loss = 1.25 (259.4 examples/sec; 0.493 sec/batch)
2016-02-03 10:22:33.243377: step 4120, loss = 1.20 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 10:22:37.962723: step 4130, loss = 1.18 (261.4 examples/sec; 0.490 sec/batch)
2016-02-03 10:22:42.664086: step 4140, loss = 1.31 (309.0 examples/sec; 0.414 sec/batch)
2016-02-03 10:22:47.405159: step 4150, loss = 1.07 (254.7 examples/sec; 0.503 sec/batch)
2016-02-03 10:22:52.058676: step 4160, loss = 1.27 (281.7 examples/sec; 0.454 sec/batch)
2016-02-03 10:22:56.761351: step 4170, loss = 1.17 (301.4 examples/sec; 0.425 sec/batch)
2016-02-03 10:23:01.441610: step 4180, loss = 1.22 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 10:23:06.065704: step 4190, loss = 1.13 (291.5 examples/sec; 0.439 sec/batch)
2016-02-03 10:23:10.714472: step 4200, loss = 1.11 (279.2 examples/sec; 0.458 sec/batch)
2016-02-03 10:23:15.883359: step 4210, loss = 1.16 (299.3 examples/sec; 0.428 sec/batch)
2016-02-03 10:23:20.639991: step 4220, loss = 1.25 (254.6 examples/sec; 0.503 sec/batch)
2016-02-03 10:23:25.344344: step 4230, loss = 1.11 (283.6 examples/sec; 0.451 sec/batch)
2016-02-03 10:23:29.993223: step 4240, loss = 1.03 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 10:23:34.709574: step 4250, loss = 1.38 (287.8 examples/sec; 0.445 sec/batch)
2016-02-03 10:23:39.423488: step 4260, loss = 1.23 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 10:23:44.156256: step 4270, loss = 1.23 (260.0 examples/sec; 0.492 sec/batch)
2016-02-03 10:23:48.907029: step 4280, loss = 1.17 (262.2 examples/sec; 0.488 sec/batch)
2016-02-03 10:23:53.547231: step 4290, loss = 1.19 (242.2 examples/sec; 0.528 sec/batch)
2016-02-03 10:23:58.227333: step 4300, loss = 1.19 (289.7 examples/sec; 0.442 sec/batch)
2016-02-03 10:24:03.304774: step 4310, loss = 1.18 (292.5 examples/sec; 0.438 sec/batch)
2016-02-03 10:24:08.038285: step 4320, loss = 1.13 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 10:24:12.739211: step 4330, loss = 1.24 (279.2 examples/sec; 0.459 sec/batch)
2016-02-03 10:24:17.479204: step 4340, loss = 1.46 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 10:24:22.114669: step 4350, loss = 1.26 (288.2 examples/sec; 0.444 sec/batch)
2016-02-03 10:24:26.761113: step 4360, loss = 1.21 (296.0 examples/sec; 0.432 sec/batch)
2016-02-03 10:24:31.539209: step 4370, loss = 1.33 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 10:24:36.218915: step 4380, loss = 1.02 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 10:24:40.985935: step 4390, loss = 1.17 (258.1 examples/sec; 0.496 sec/batch)
2016-02-03 10:24:45.675010: step 4400, loss = 1.20 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 10:24:50.772094: step 4410, loss = 1.17 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 10:24:55.554878: step 4420, loss = 1.23 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 10:25:00.251463: step 4430, loss = 1.04 (257.3 examples/sec; 0.498 sec/batch)
2016-02-03 10:25:04.959093: step 4440, loss = 1.16 (255.9 examples/sec; 0.500 sec/batch)
2016-02-03 10:25:09.720873: step 4450, loss = 1.27 (254.3 examples/sec; 0.503 sec/batch)
2016-02-03 10:25:14.293702: step 4460, loss = 1.24 (294.2 examples/sec; 0.435 sec/batch)
2016-02-03 10:25:18.911568: step 4470, loss = 1.12 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 10:25:23.625778: step 4480, loss = 1.22 (284.1 examples/sec; 0.451 sec/batch)
2016-02-03 10:25:28.323065: step 4490, loss = 1.24 (287.2 examples/sec; 0.446 sec/batch)
2016-02-03 10:25:32.900034: step 4500, loss = 1.30 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 10:25:38.027489: step 4510, loss = 1.03 (274.4 examples/sec; 0.466 sec/batch)
2016-02-03 10:25:42.683276: step 4520, loss = 1.25 (284.0 examples/sec; 0.451 sec/batch)
2016-02-03 10:25:47.285170: step 4530, loss = 1.05 (281.1 examples/sec; 0.455 sec/batch)
2016-02-03 10:25:51.896710: step 4540, loss = 1.51 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 10:25:56.532106: step 4550, loss = 1.30 (275.0 examples/sec; 0.466 sec/batch)
2016-02-03 10:26:01.169438: step 4560, loss = 1.15 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 10:26:05.783039: step 4570, loss = 1.29 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 10:26:10.467657: step 4580, loss = 1.01 (267.5 examples/sec; 0.478 sec/batch)
2016-02-03 10:26:15.173100: step 4590, loss = 0.97 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 10:26:19.865584: step 4600, loss = 1.09 (264.2 examples/sec; 0.484 sec/batch)
2016-02-03 10:26:25.050480: step 4610, loss = 1.23 (278.5 examples/sec; 0.460 sec/batch)
2016-02-03 10:26:29.679247: step 4620, loss = 1.16 (312.1 examples/sec; 0.410 sec/batch)
2016-02-03 10:26:34.350185: step 4630, loss = 1.24 (261.1 examples/sec; 0.490 sec/batch)
2016-02-03 10:26:39.046421: step 4640, loss = 1.06 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 10:26:43.687763: step 4650, loss = 1.03 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 10:26:48.309426: step 4660, loss = 1.16 (305.2 examples/sec; 0.419 sec/batch)
2016-02-03 10:26:52.904043: step 4670, loss = 1.01 (286.7 examples/sec; 0.446 sec/batch)
2016-02-03 10:26:57.519271: step 4680, loss = 1.02 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 10:27:02.253471: step 4690, loss = 1.06 (256.4 examples/sec; 0.499 sec/batch)
2016-02-03 10:27:06.987252: step 4700, loss = 1.05 (252.5 examples/sec; 0.507 sec/batch)
2016-02-03 10:27:12.187125: step 4710, loss = 1.38 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 10:27:16.874172: step 4720, loss = 1.10 (283.9 examples/sec; 0.451 sec/batch)
2016-02-03 10:27:21.531938: step 4730, loss = 1.01 (260.0 examples/sec; 0.492 sec/batch)
2016-02-03 10:27:26.160980: step 4740, loss = 1.22 (293.5 examples/sec; 0.436 sec/batch)
2016-02-03 10:27:30.830761: step 4750, loss = 1.20 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 10:27:35.547740: step 4760, loss = 1.17 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 10:27:40.252755: step 4770, loss = 1.25 (279.7 examples/sec; 0.458 sec/batch)
2016-02-03 10:27:44.872893: step 4780, loss = 1.01 (283.4 examples/sec; 0.452 sec/batch)
2016-02-03 10:27:49.484807: step 4790, loss = 1.20 (296.9 examples/sec; 0.431 sec/batch)
2016-02-03 10:27:54.155853: step 4800, loss = 0.93 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 10:27:59.416229: step 4810, loss = 1.32 (253.2 examples/sec; 0.506 sec/batch)
2016-02-03 10:28:04.033060: step 4820, loss = 1.17 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 10:28:08.769533: step 4830, loss = 1.23 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 10:28:13.422272: step 4840, loss = 1.26 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 10:28:18.126812: step 4850, loss = 1.07 (289.1 examples/sec; 0.443 sec/batch)
2016-02-03 10:28:22.847778: step 4860, loss = 1.23 (288.2 examples/sec; 0.444 sec/batch)
2016-02-03 10:28:27.620991: step 4870, loss = 1.03 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 10:28:32.222969: step 4880, loss = 1.13 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 10:28:36.826314: step 4890, loss = 1.33 (305.3 examples/sec; 0.419 sec/batch)
2016-02-03 10:28:41.489390: step 4900, loss = 1.36 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 10:28:46.709123: step 4910, loss = 1.18 (267.0 examples/sec; 0.479 sec/batch)
2016-02-03 10:28:51.394187: step 4920, loss = 1.21 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 10:28:56.110058: step 4930, loss = 1.09 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 10:29:00.806988: step 4940, loss = 1.04 (276.3 examples/sec; 0.463 sec/batch)
2016-02-03 10:29:05.567137: step 4950, loss = 1.02 (256.5 examples/sec; 0.499 sec/batch)
2016-02-03 10:29:10.361144: step 4960, loss = 0.91 (261.9 examples/sec; 0.489 sec/batch)
2016-02-03 10:29:15.002393: step 4970, loss = 1.05 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 10:29:19.782249: step 4980, loss = 0.88 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 10:29:24.485197: step 4990, loss = 1.54 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 10:29:29.289089: step 5000, loss = 1.10 (266.4 examples/sec; 0.480 sec/batch)
2016-02-03 10:29:34.521250: step 5010, loss = 1.11 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 10:29:39.137155: step 5020, loss = 1.15 (303.0 examples/sec; 0.422 sec/batch)
2016-02-03 10:29:43.881353: step 5030, loss = 1.16 (263.2 examples/sec; 0.486 sec/batch)
2016-02-03 10:29:48.549513: step 5040, loss = 1.10 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 10:29:53.301008: step 5050, loss = 1.03 (260.5 examples/sec; 0.491 sec/batch)
2016-02-03 10:29:57.955835: step 5060, loss = 1.14 (253.6 examples/sec; 0.505 sec/batch)
2016-02-03 10:30:02.602590: step 5070, loss = 1.16 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 10:30:07.244946: step 5080, loss = 1.27 (266.4 examples/sec; 0.480 sec/batch)
2016-02-03 10:30:11.901842: step 5090, loss = 1.09 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 10:30:16.619711: step 5100, loss = 1.02 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 10:30:21.883184: step 5110, loss = 0.94 (272.6 examples/sec; 0.470 sec/batch)
2016-02-03 10:30:26.490677: step 5120, loss = 0.99 (283.6 examples/sec; 0.451 sec/batch)
2016-02-03 10:30:31.094888: step 5130, loss = 1.09 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 10:30:35.782507: step 5140, loss = 1.15 (286.2 examples/sec; 0.447 sec/batch)
2016-02-03 10:30:40.441594: step 5150, loss = 1.17 (292.4 examples/sec; 0.438 sec/batch)
2016-02-03 10:30:45.081998: step 5160, loss = 1.09 (258.9 examples/sec; 0.494 sec/batch)
2016-02-03 10:30:49.796836: step 5170, loss = 1.26 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 10:30:54.504565: step 5180, loss = 0.99 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 10:30:59.203342: step 5190, loss = 1.20 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 10:31:03.879965: step 5200, loss = 1.03 (287.4 examples/sec; 0.445 sec/batch)
2016-02-03 10:31:09.057061: step 5210, loss = 0.94 (287.4 examples/sec; 0.445 sec/batch)
2016-02-03 10:31:13.816843: step 5220, loss = 1.05 (302.4 examples/sec; 0.423 sec/batch)
2016-02-03 10:31:18.449495: step 5230, loss = 1.17 (304.6 examples/sec; 0.420 sec/batch)
2016-02-03 10:31:23.203917: step 5240, loss = 1.07 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 10:31:27.916327: step 5250, loss = 1.10 (286.3 examples/sec; 0.447 sec/batch)
2016-02-03 10:31:32.590341: step 5260, loss = 1.15 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 10:31:37.203342: step 5270, loss = 1.08 (295.5 examples/sec; 0.433 sec/batch)
2016-02-03 10:31:41.941441: step 5280, loss = 1.16 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 10:31:46.597949: step 5290, loss = 1.03 (287.2 examples/sec; 0.446 sec/batch)
2016-02-03 10:31:51.303148: step 5300, loss = 1.05 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 10:31:56.542877: step 5310, loss = 1.06 (247.4 examples/sec; 0.517 sec/batch)
2016-02-03 10:32:01.245775: step 5320, loss = 0.97 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 10:32:05.963526: step 5330, loss = 0.89 (294.5 examples/sec; 0.435 sec/batch)
2016-02-03 10:32:10.691623: step 5340, loss = 1.34 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 10:32:15.371796: step 5350, loss = 0.99 (293.5 examples/sec; 0.436 sec/batch)
2016-02-03 10:32:20.069743: step 5360, loss = 1.39 (278.6 examples/sec; 0.460 sec/batch)
2016-02-03 10:32:24.758452: step 5370, loss = 1.26 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 10:32:29.398463: step 5380, loss = 0.92 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 10:32:34.127759: step 5390, loss = 1.14 (266.9 examples/sec; 0.479 sec/batch)
2016-02-03 10:32:38.802425: step 5400, loss = 0.88 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 10:32:44.082859: step 5410, loss = 0.83 (258.9 examples/sec; 0.494 sec/batch)
2016-02-03 10:32:48.794234: step 5420, loss = 1.04 (255.4 examples/sec; 0.501 sec/batch)
2016-02-03 10:32:53.520644: step 5430, loss = 1.11 (284.7 examples/sec; 0.450 sec/batch)
2016-02-03 10:32:58.272104: step 5440, loss = 0.97 (267.2 examples/sec; 0.479 sec/batch)
2016-02-03 10:33:02.930894: step 5450, loss = 1.26 (277.8 examples/sec; 0.461 sec/batch)
2016-02-03 10:33:07.584190: step 5460, loss = 0.93 (264.1 examples/sec; 0.485 sec/batch)
2016-02-03 10:33:12.280201: step 5470, loss = 1.01 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 10:33:16.936908: step 5480, loss = 1.18 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 10:33:21.626883: step 5490, loss = 1.11 (282.8 examples/sec; 0.453 sec/batch)
2016-02-03 10:33:26.369760: step 5500, loss = 1.04 (268.2 examples/sec; 0.477 sec/batch)
2016-02-03 10:33:31.580281: step 5510, loss = 1.16 (258.5 examples/sec; 0.495 sec/batch)
2016-02-03 10:33:36.290541: step 5520, loss = 1.17 (285.8 examples/sec; 0.448 sec/batch)
2016-02-03 10:33:41.124191: step 5530, loss = 1.08 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 10:33:45.763937: step 5540, loss = 1.18 (274.4 examples/sec; 0.466 sec/batch)
2016-02-03 10:33:50.476161: step 5550, loss = 1.06 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 10:33:55.205536: step 5560, loss = 1.05 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 10:33:59.952292: step 5570, loss = 1.08 (264.9 examples/sec; 0.483 sec/batch)
2016-02-03 10:34:04.709359: step 5580, loss = 1.11 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 10:34:09.412325: step 5590, loss = 1.03 (282.8 examples/sec; 0.453 sec/batch)
2016-02-03 10:34:14.223609: step 5600, loss = 1.12 (289.5 examples/sec; 0.442 sec/batch)
2016-02-03 10:34:19.480306: step 5610, loss = 0.86 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 10:34:24.241654: step 5620, loss = 1.10 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 10:34:28.932038: step 5630, loss = 1.19 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 10:34:33.695201: step 5640, loss = 1.09 (248.6 examples/sec; 0.515 sec/batch)
2016-02-03 10:34:38.380510: step 5650, loss = 1.17 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 10:34:43.082936: step 5660, loss = 1.17 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 10:34:47.737058: step 5670, loss = 0.94 (313.7 examples/sec; 0.408 sec/batch)
2016-02-03 10:34:52.516914: step 5680, loss = 1.05 (263.2 examples/sec; 0.486 sec/batch)
2016-02-03 10:34:57.316174: step 5690, loss = 1.09 (238.9 examples/sec; 0.536 sec/batch)
2016-02-03 10:35:02.088925: step 5700, loss = 1.09 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 10:35:07.304733: step 5710, loss = 1.01 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 10:35:12.075034: step 5720, loss = 0.92 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 10:35:16.783533: step 5730, loss = 1.20 (253.3 examples/sec; 0.505 sec/batch)
2016-02-03 10:35:21.463623: step 5740, loss = 1.11 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 10:35:26.111409: step 5750, loss = 0.99 (285.7 examples/sec; 0.448 sec/batch)
2016-02-03 10:35:30.894332: step 5760, loss = 1.12 (276.9 examples/sec; 0.462 sec/batch)
2016-02-03 10:35:35.596170: step 5770, loss = 1.04 (270.9 examples/sec; 0.472 sec/batch)
2016-02-03 10:35:40.285788: step 5780, loss = 1.03 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 10:35:45.024450: step 5790, loss = 1.03 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 10:35:49.721644: step 5800, loss = 0.98 (268.2 examples/sec; 0.477 sec/batch)
2016-02-03 10:35:54.927382: step 5810, loss = 1.00 (265.3 examples/sec; 0.483 sec/batch)
2016-02-03 10:35:59.639857: step 5820, loss = 1.06 (256.6 examples/sec; 0.499 sec/batch)
2016-02-03 10:36:04.410960: step 5830, loss = 0.94 (286.9 examples/sec; 0.446 sec/batch)
2016-02-03 10:36:09.170363: step 5840, loss = 1.18 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 10:36:13.796979: step 5850, loss = 0.99 (290.6 examples/sec; 0.440 sec/batch)
2016-02-03 10:36:18.564119: step 5860, loss = 1.09 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 10:36:23.311110: step 5870, loss = 0.96 (286.5 examples/sec; 0.447 sec/batch)
2016-02-03 10:36:27.930250: step 5880, loss = 0.92 (293.5 examples/sec; 0.436 sec/batch)
2016-02-03 10:36:32.664335: step 5890, loss = 1.12 (257.8 examples/sec; 0.497 sec/batch)
2016-02-03 10:36:37.437796: step 5900, loss = 1.12 (264.7 examples/sec; 0.484 sec/batch)
2016-02-03 10:36:42.682612: step 5910, loss = 1.04 (273.2 examples/sec; 0.469 sec/batch)
2016-02-03 10:36:47.431867: step 5920, loss = 0.88 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 10:36:52.121344: step 5930, loss = 1.16 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 10:36:56.834879: step 5940, loss = 1.01 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 10:37:01.522467: step 5950, loss = 1.01 (252.3 examples/sec; 0.507 sec/batch)
2016-02-03 10:37:06.221417: step 5960, loss = 1.00 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 10:37:10.979769: step 5970, loss = 1.23 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 10:37:15.793777: step 5980, loss = 1.05 (274.4 examples/sec; 0.466 sec/batch)
2016-02-03 10:37:20.558644: step 5990, loss = 1.18 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 10:37:25.323265: step 6000, loss = 1.05 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 10:37:30.492639: step 6010, loss = 1.08 (292.3 examples/sec; 0.438 sec/batch)
2016-02-03 10:37:35.226755: step 6020, loss = 1.09 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 10:37:39.922095: step 6030, loss = 1.15 (263.2 examples/sec; 0.486 sec/batch)
2016-02-03 10:37:44.671653: step 6040, loss = 1.24 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 10:37:49.446413: step 6050, loss = 0.93 (243.3 examples/sec; 0.526 sec/batch)
2016-02-03 10:37:54.106953: step 6060, loss = 1.08 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 10:37:58.783939: step 6070, loss = 0.91 (286.1 examples/sec; 0.447 sec/batch)
2016-02-03 10:38:03.550660: step 6080, loss = 0.97 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 10:38:08.251165: step 6090, loss = 0.96 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 10:38:13.000070: step 6100, loss = 1.23 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 10:38:18.292769: step 6110, loss = 0.99 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 10:38:23.051748: step 6120, loss = 0.87 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 10:38:27.732113: step 6130, loss = 1.01 (264.7 examples/sec; 0.483 sec/batch)
2016-02-03 10:38:32.466286: step 6140, loss = 0.99 (253.6 examples/sec; 0.505 sec/batch)
2016-02-03 10:38:37.212169: step 6150, loss = 0.87 (286.4 examples/sec; 0.447 sec/batch)
2016-02-03 10:38:41.966394: step 6160, loss = 0.94 (290.6 examples/sec; 0.440 sec/batch)
2016-02-03 10:38:46.759040: step 6170, loss = 0.80 (297.0 examples/sec; 0.431 sec/batch)
2016-02-03 10:38:51.357108: step 6180, loss = 0.99 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 10:38:56.083884: step 6190, loss = 1.04 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 10:39:00.832186: step 6200, loss = 1.05 (282.6 examples/sec; 0.453 sec/batch)
2016-02-03 10:39:06.080686: step 6210, loss = 1.00 (284.4 examples/sec; 0.450 sec/batch)
2016-02-03 10:39:10.792520: step 6220, loss = 1.15 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 10:39:15.573222: step 6230, loss = 0.94 (248.2 examples/sec; 0.516 sec/batch)
2016-02-03 10:39:20.371025: step 6240, loss = 0.84 (284.0 examples/sec; 0.451 sec/batch)
2016-02-03 10:39:25.055664: step 6250, loss = 0.99 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 10:39:29.718738: step 6260, loss = 1.13 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 10:39:34.438449: step 6270, loss = 1.10 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 10:39:39.140555: step 6280, loss = 1.11 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 10:39:43.863832: step 6290, loss = 1.12 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 10:39:48.528913: step 6300, loss = 0.90 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 10:39:53.762582: step 6310, loss = 0.86 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 10:39:58.415426: step 6320, loss = 1.04 (270.9 examples/sec; 0.473 sec/batch)
2016-02-03 10:40:03.120848: step 6330, loss = 1.03 (258.4 examples/sec; 0.495 sec/batch)
2016-02-03 10:40:07.816289: step 6340, loss = 1.05 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 10:40:12.645286: step 6350, loss = 1.00 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 10:40:17.326445: step 6360, loss = 1.14 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 10:40:21.952563: step 6370, loss = 0.98 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 10:40:26.578682: step 6380, loss = 1.02 (315.4 examples/sec; 0.406 sec/batch)
2016-02-03 10:40:31.243394: step 6390, loss = 0.98 (276.4 examples/sec; 0.463 sec/batch)
2016-02-03 10:40:36.004329: step 6400, loss = 1.01 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 10:40:41.153829: step 6410, loss = 1.02 (281.2 examples/sec; 0.455 sec/batch)
2016-02-03 10:40:45.845798: step 6420, loss = 0.97 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 10:40:50.527455: step 6430, loss = 1.00 (290.2 examples/sec; 0.441 sec/batch)
2016-02-03 10:40:55.204335: step 6440, loss = 1.11 (281.5 examples/sec; 0.455 sec/batch)
2016-02-03 10:40:59.904098: step 6450, loss = 1.05 (268.6 examples/sec; 0.477 sec/batch)
2016-02-03 10:41:04.611215: step 6460, loss = 1.11 (309.1 examples/sec; 0.414 sec/batch)
2016-02-03 10:41:09.318215: step 6470, loss = 0.95 (241.6 examples/sec; 0.530 sec/batch)
2016-02-03 10:41:13.888350: step 6480, loss = 0.87 (266.4 examples/sec; 0.480 sec/batch)
2016-02-03 10:41:18.632122: step 6490, loss = 0.93 (250.9 examples/sec; 0.510 sec/batch)
2016-02-03 10:41:23.344300: step 6500, loss = 1.02 (261.3 examples/sec; 0.490 sec/batch)
2016-02-03 10:41:28.592605: step 6510, loss = 1.07 (258.3 examples/sec; 0.495 sec/batch)
2016-02-03 10:41:33.270147: step 6520, loss = 1.03 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 10:41:37.921712: step 6530, loss = 0.94 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 10:41:42.625788: step 6540, loss = 1.10 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 10:41:47.313687: step 6550, loss = 1.07 (276.7 examples/sec; 0.463 sec/batch)
2016-02-03 10:41:51.986669: step 6560, loss = 0.91 (253.1 examples/sec; 0.506 sec/batch)
2016-02-03 10:41:56.641974: step 6570, loss = 1.03 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 10:42:01.357570: step 6580, loss = 1.09 (293.9 examples/sec; 0.436 sec/batch)
2016-02-03 10:42:06.050175: step 6590, loss = 1.16 (291.6 examples/sec; 0.439 sec/batch)
2016-02-03 10:42:10.784208: step 6600, loss = 1.03 (257.9 examples/sec; 0.496 sec/batch)
2016-02-03 10:42:15.944804: step 6610, loss = 1.03 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 10:42:20.651168: step 6620, loss = 1.00 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 10:42:25.231921: step 6630, loss = 0.92 (294.6 examples/sec; 0.435 sec/batch)
2016-02-03 10:42:29.973513: step 6640, loss = 1.09 (257.7 examples/sec; 0.497 sec/batch)
2016-02-03 10:42:34.590893: step 6650, loss = 1.09 (290.2 examples/sec; 0.441 sec/batch)
2016-02-03 10:42:39.350752: step 6660, loss = 0.74 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 10:42:43.979782: step 6670, loss = 1.04 (285.4 examples/sec; 0.448 sec/batch)
2016-02-03 10:42:48.645114: step 6680, loss = 1.28 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 10:42:53.374482: step 6690, loss = 0.97 (292.9 examples/sec; 0.437 sec/batch)
2016-02-03 10:42:58.089018: step 6700, loss = 1.00 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 10:43:03.370032: step 6710, loss = 1.16 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 10:43:08.087500: step 6720, loss = 0.99 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 10:43:12.854306: step 6730, loss = 1.07 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 10:43:17.551387: step 6740, loss = 0.97 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 10:43:22.348994: step 6750, loss = 1.10 (240.2 examples/sec; 0.533 sec/batch)
2016-02-03 10:43:27.025647: step 6760, loss = 1.00 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 10:43:31.766633: step 6770, loss = 1.10 (253.9 examples/sec; 0.504 sec/batch)
2016-02-03 10:43:36.450490: step 6780, loss = 1.02 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 10:43:41.148590: step 6790, loss = 0.95 (268.6 examples/sec; 0.477 sec/batch)
2016-02-03 10:43:45.867088: step 6800, loss = 0.97 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 10:43:51.095040: step 6810, loss = 0.87 (262.0 examples/sec; 0.489 sec/batch)
2016-02-03 10:43:55.868311: step 6820, loss = 1.01 (286.9 examples/sec; 0.446 sec/batch)
2016-02-03 10:44:00.594073: step 6830, loss = 1.01 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 10:44:05.354996: step 6840, loss = 1.11 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 10:44:10.110167: step 6850, loss = 1.27 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 10:44:14.866985: step 6860, loss = 0.84 (277.7 examples/sec; 0.461 sec/batch)
2016-02-03 10:44:19.507928: step 6870, loss = 0.99 (284.2 examples/sec; 0.450 sec/batch)
2016-02-03 10:44:24.269135: step 6880, loss = 0.88 (246.5 examples/sec; 0.519 sec/batch)
2016-02-03 10:44:29.040529: step 6890, loss = 1.02 (252.6 examples/sec; 0.507 sec/batch)
2016-02-03 10:44:33.827978: step 6900, loss = 0.98 (247.1 examples/sec; 0.518 sec/batch)
2016-02-03 10:44:38.978648: step 6910, loss = 1.07 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 10:44:43.727599: step 6920, loss = 0.96 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 10:44:48.444849: step 6930, loss = 0.97 (297.5 examples/sec; 0.430 sec/batch)
2016-02-03 10:44:53.221636: step 6940, loss = 0.99 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 10:44:57.952229: step 6950, loss = 1.19 (261.3 examples/sec; 0.490 sec/batch)
2016-02-03 10:45:02.678589: step 6960, loss = 0.97 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 10:45:07.361664: step 6970, loss = 0.85 (267.2 examples/sec; 0.479 sec/batch)
2016-02-03 10:45:11.987304: step 6980, loss = 1.01 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 10:45:16.730894: step 6990, loss = 1.12 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 10:45:21.432263: step 7000, loss = 1.04 (279.4 examples/sec; 0.458 sec/batch)
2016-02-03 10:45:26.663901: step 7010, loss = 0.86 (270.5 examples/sec; 0.473 sec/batch)
2016-02-03 10:45:31.350882: step 7020, loss = 0.90 (270.9 examples/sec; 0.473 sec/batch)
2016-02-03 10:45:36.056088: step 7030, loss = 1.10 (263.0 examples/sec; 0.487 sec/batch)
2016-02-03 10:45:40.791736: step 7040, loss = 0.94 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 10:45:45.437851: step 7050, loss = 0.81 (252.9 examples/sec; 0.506 sec/batch)
2016-02-03 10:45:50.090802: step 7060, loss = 1.07 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 10:45:54.752156: step 7070, loss = 0.89 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 10:45:59.409895: step 7080, loss = 1.10 (282.2 examples/sec; 0.454 sec/batch)
2016-02-03 10:46:04.140679: step 7090, loss = 1.10 (295.9 examples/sec; 0.433 sec/batch)
2016-02-03 10:46:08.903534: step 7100, loss = 0.79 (254.1 examples/sec; 0.504 sec/batch)
2016-02-03 10:46:14.188886: step 7110, loss = 0.87 (289.0 examples/sec; 0.443 sec/batch)
2016-02-03 10:46:18.938416: step 7120, loss = 1.07 (262.0 examples/sec; 0.489 sec/batch)
2016-02-03 10:46:23.646127: step 7130, loss = 1.05 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 10:46:28.348465: step 7140, loss = 0.93 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 10:46:33.031797: step 7150, loss = 1.04 (290.3 examples/sec; 0.441 sec/batch)
2016-02-03 10:46:37.669379: step 7160, loss = 0.97 (260.8 examples/sec; 0.491 sec/batch)
2016-02-03 10:46:42.400527: step 7170, loss = 0.99 (295.3 examples/sec; 0.433 sec/batch)
2016-02-03 10:46:47.131799: step 7180, loss = 1.07 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 10:46:51.827546: step 7190, loss = 0.99 (276.6 examples/sec; 0.463 sec/batch)
2016-02-03 10:46:56.403564: step 7200, loss = 0.91 (298.8 examples/sec; 0.428 sec/batch)
2016-02-03 10:47:01.620334: step 7210, loss = 0.87 (311.1 examples/sec; 0.411 sec/batch)
2016-02-03 10:47:06.397993: step 7220, loss = 0.89 (284.1 examples/sec; 0.451 sec/batch)
2016-02-03 10:47:11.109526: step 7230, loss = 1.08 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 10:47:15.716150: step 7240, loss = 1.04 (285.3 examples/sec; 0.449 sec/batch)
2016-02-03 10:47:20.352402: step 7250, loss = 0.87 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 10:47:25.022794: step 7260, loss = 1.07 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 10:47:29.692191: step 7270, loss = 1.19 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 10:47:34.380081: step 7280, loss = 1.06 (280.4 examples/sec; 0.457 sec/batch)
2016-02-03 10:47:39.057815: step 7290, loss = 1.01 (263.0 examples/sec; 0.487 sec/batch)
2016-02-03 10:47:43.754577: step 7300, loss = 0.97 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 10:47:48.975133: step 7310, loss = 1.03 (282.2 examples/sec; 0.454 sec/batch)
2016-02-03 10:47:53.683704: step 7320, loss = 0.92 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 10:47:58.483238: step 7330, loss = 1.21 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 10:48:03.125227: step 7340, loss = 1.04 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 10:48:07.795177: step 7350, loss = 0.93 (279.8 examples/sec; 0.457 sec/batch)
2016-02-03 10:48:12.528491: step 7360, loss = 0.95 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 10:48:17.213906: step 7370, loss = 0.84 (310.5 examples/sec; 0.412 sec/batch)
2016-02-03 10:48:21.935333: step 7380, loss = 0.95 (267.3 examples/sec; 0.479 sec/batch)
2016-02-03 10:48:26.621319: step 7390, loss = 0.80 (264.2 examples/sec; 0.484 sec/batch)
2016-02-03 10:48:31.356734: step 7400, loss = 1.08 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 10:48:36.627508: step 7410, loss = 0.99 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 10:48:41.374362: step 7420, loss = 0.94 (274.4 examples/sec; 0.466 sec/batch)
2016-02-03 10:48:46.088665: step 7430, loss = 0.98 (253.1 examples/sec; 0.506 sec/batch)
2016-02-03 10:48:50.808989: step 7440, loss = 0.98 (266.9 examples/sec; 0.480 sec/batch)
2016-02-03 10:48:55.547298: step 7450, loss = 0.97 (257.5 examples/sec; 0.497 sec/batch)
2016-02-03 10:49:00.205269: step 7460, loss = 0.95 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 10:49:04.919258: step 7470, loss = 1.01 (284.9 examples/sec; 0.449 sec/batch)
2016-02-03 10:49:09.702409: step 7480, loss = 1.02 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 10:49:14.444198: step 7490, loss = 0.88 (284.0 examples/sec; 0.451 sec/batch)
2016-02-03 10:49:19.179706: step 7500, loss = 1.04 (281.0 examples/sec; 0.456 sec/batch)
2016-02-03 10:49:24.426714: step 7510, loss = 0.89 (289.4 examples/sec; 0.442 sec/batch)
2016-02-03 10:49:29.163901: step 7520, loss = 1.11 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 10:49:33.683775: step 7530, loss = 0.98 (308.3 examples/sec; 0.415 sec/batch)
2016-02-03 10:49:38.361786: step 7540, loss = 1.16 (293.5 examples/sec; 0.436 sec/batch)
2016-02-03 10:49:42.906885: step 7550, loss = 0.76 (295.1 examples/sec; 0.434 sec/batch)
2016-02-03 10:49:47.593653: step 7560, loss = 0.99 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 10:49:52.383031: step 7570, loss = 0.92 (243.1 examples/sec; 0.527 sec/batch)
2016-02-03 10:49:57.052361: step 7580, loss = 0.75 (281.1 examples/sec; 0.455 sec/batch)
2016-02-03 10:50:01.792899: step 7590, loss = 1.03 (258.3 examples/sec; 0.496 sec/batch)
2016-02-03 10:50:06.441380: step 7600, loss = 0.82 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 10:50:11.726528: step 7610, loss = 0.97 (262.6 examples/sec; 0.487 sec/batch)
2016-02-03 10:50:16.445888: step 7620, loss = 1.16 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 10:50:21.119938: step 7630, loss = 0.91 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 10:50:25.866930: step 7640, loss = 0.93 (257.6 examples/sec; 0.497 sec/batch)
2016-02-03 10:50:30.543857: step 7650, loss = 1.00 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 10:50:35.192248: step 7660, loss = 1.09 (267.2 examples/sec; 0.479 sec/batch)
2016-02-03 10:50:39.964621: step 7670, loss = 0.98 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 10:50:44.781746: step 7680, loss = 0.91 (257.0 examples/sec; 0.498 sec/batch)
2016-02-03 10:50:49.473012: step 7690, loss = 1.08 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 10:50:54.132189: step 7700, loss = 1.32 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 10:50:59.377926: step 7710, loss = 0.89 (263.0 examples/sec; 0.487 sec/batch)
2016-02-03 10:51:04.086946: step 7720, loss = 0.95 (300.8 examples/sec; 0.426 sec/batch)
2016-02-03 10:51:08.857095: step 7730, loss = 0.88 (279.2 examples/sec; 0.458 sec/batch)
2016-02-03 10:51:13.596699: step 7740, loss = 1.09 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 10:51:18.303193: step 7750, loss = 0.93 (258.1 examples/sec; 0.496 sec/batch)
2016-02-03 10:51:23.048546: step 7760, loss = 0.96 (253.6 examples/sec; 0.505 sec/batch)
2016-02-03 10:51:27.807052: step 7770, loss = 1.00 (245.7 examples/sec; 0.521 sec/batch)
2016-02-03 10:51:32.521522: step 7780, loss = 0.91 (260.7 examples/sec; 0.491 sec/batch)
2016-02-03 10:51:37.169810: step 7790, loss = 1.10 (276.9 examples/sec; 0.462 sec/batch)
2016-02-03 10:51:41.976406: step 7800, loss = 1.01 (254.3 examples/sec; 0.503 sec/batch)
2016-02-03 10:51:47.242510: step 7810, loss = 0.87 (250.8 examples/sec; 0.510 sec/batch)
2016-02-03 10:51:51.918893: step 7820, loss = 0.88 (300.2 examples/sec; 0.426 sec/batch)
2016-02-03 10:51:56.684575: step 7830, loss = 1.16 (251.9 examples/sec; 0.508 sec/batch)
2016-02-03 10:52:01.388976: step 7840, loss = 0.85 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 10:52:06.112351: step 7850, loss = 1.25 (291.6 examples/sec; 0.439 sec/batch)
2016-02-03 10:52:10.747567: step 7860, loss = 0.88 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 10:52:15.492945: step 7870, loss = 0.91 (252.3 examples/sec; 0.507 sec/batch)
2016-02-03 10:52:20.178204: step 7880, loss = 1.19 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 10:52:24.960141: step 7890, loss = 0.97 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 10:52:29.701901: step 7900, loss = 0.81 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 10:52:34.858429: step 7910, loss = 0.83 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 10:52:39.577470: step 7920, loss = 0.99 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 10:52:44.232237: step 7930, loss = 0.98 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 10:52:48.992487: step 7940, loss = 0.92 (253.5 examples/sec; 0.505 sec/batch)
2016-02-03 10:52:53.631994: step 7950, loss = 0.84 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 10:52:58.314595: step 7960, loss = 0.96 (280.9 examples/sec; 0.456 sec/batch)
2016-02-03 10:53:03.034387: step 7970, loss = 0.95 (298.5 examples/sec; 0.429 sec/batch)
2016-02-03 10:53:07.722260: step 7980, loss = 0.88 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 10:53:12.443271: step 7990, loss = 1.14 (285.2 examples/sec; 0.449 sec/batch)
2016-02-03 10:53:17.285411: step 8000, loss = 0.93 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 10:53:22.466043: step 8010, loss = 1.02 (288.5 examples/sec; 0.444 sec/batch)
2016-02-03 10:53:27.214648: step 8020, loss = 0.91 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 10:53:31.862298: step 8030, loss = 0.89 (277.2 examples/sec; 0.462 sec/batch)
2016-02-03 10:53:36.596055: step 8040, loss = 0.96 (244.3 examples/sec; 0.524 sec/batch)
2016-02-03 10:53:41.279861: step 8050, loss = 0.89 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 10:53:45.949327: step 8060, loss = 0.86 (294.1 examples/sec; 0.435 sec/batch)
2016-02-03 10:53:50.608472: step 8070, loss = 1.14 (276.6 examples/sec; 0.463 sec/batch)
2016-02-03 10:53:55.318977: step 8080, loss = 0.94 (247.8 examples/sec; 0.517 sec/batch)
2016-02-03 10:54:00.049797: step 8090, loss = 1.14 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 10:54:04.785268: step 8100, loss = 0.80 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 10:54:10.050616: step 8110, loss = 0.96 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 10:54:14.823327: step 8120, loss = 0.90 (276.4 examples/sec; 0.463 sec/batch)
2016-02-03 10:54:19.576233: step 8130, loss = 0.82 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 10:54:24.241080: step 8140, loss = 1.02 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 10:54:28.966144: step 8150, loss = 0.90 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 10:54:33.741790: step 8160, loss = 1.04 (259.3 examples/sec; 0.494 sec/batch)
2016-02-03 10:54:38.292322: step 8170, loss = 0.92 (285.7 examples/sec; 0.448 sec/batch)
2016-02-03 10:54:42.914454: step 8180, loss = 0.99 (284.2 examples/sec; 0.450 sec/batch)
2016-02-03 10:54:47.549500: step 8190, loss = 0.94 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 10:54:52.286471: step 8200, loss = 0.78 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 10:54:57.428923: step 8210, loss = 0.79 (284.7 examples/sec; 0.450 sec/batch)
2016-02-03 10:55:02.149795: step 8220, loss = 1.01 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 10:55:06.827284: step 8230, loss = 1.07 (268.2 examples/sec; 0.477 sec/batch)
2016-02-03 10:55:11.430958: step 8240, loss = 1.00 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 10:55:16.095278: step 8250, loss = 0.88 (258.2 examples/sec; 0.496 sec/batch)
2016-02-03 10:55:20.782334: step 8260, loss = 1.01 (294.7 examples/sec; 0.434 sec/batch)
2016-02-03 10:55:25.491138: step 8270, loss = 0.68 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 10:55:30.148379: step 8280, loss = 0.96 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 10:55:34.841807: step 8290, loss = 1.04 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 10:55:39.557256: step 8300, loss = 0.93 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 10:55:44.639531: step 8310, loss = 1.01 (272.6 examples/sec; 0.470 sec/batch)
2016-02-03 10:55:49.360405: step 8320, loss = 0.94 (252.6 examples/sec; 0.507 sec/batch)
2016-02-03 10:55:53.960649: step 8330, loss = 0.99 (301.4 examples/sec; 0.425 sec/batch)
2016-02-03 10:55:58.691471: step 8340, loss = 0.77 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 10:56:03.379553: step 8350, loss = 0.96 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 10:56:08.075301: step 8360, loss = 0.95 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 10:56:12.772769: step 8370, loss = 1.09 (254.3 examples/sec; 0.503 sec/batch)
2016-02-03 10:56:17.408859: step 8380, loss = 0.91 (291.2 examples/sec; 0.440 sec/batch)
2016-02-03 10:56:22.102877: step 8390, loss = 0.87 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 10:56:26.775955: step 8400, loss = 1.02 (285.0 examples/sec; 0.449 sec/batch)
2016-02-03 10:56:32.027280: step 8410, loss = 0.81 (263.9 examples/sec; 0.485 sec/batch)
2016-02-03 10:56:36.703839: step 8420, loss = 0.99 (291.2 examples/sec; 0.439 sec/batch)
2016-02-03 10:56:41.401360: step 8430, loss = 1.21 (287.6 examples/sec; 0.445 sec/batch)
2016-02-03 10:56:46.142933: step 8440, loss = 0.86 (252.1 examples/sec; 0.508 sec/batch)
2016-02-03 10:56:50.825269: step 8450, loss = 0.98 (292.6 examples/sec; 0.437 sec/batch)
2016-02-03 10:56:55.598630: step 8460, loss = 0.81 (257.6 examples/sec; 0.497 sec/batch)
2016-02-03 10:57:00.279654: step 8470, loss = 1.02 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 10:57:04.943871: step 8480, loss = 1.02 (285.9 examples/sec; 0.448 sec/batch)
2016-02-03 10:57:09.715157: step 8490, loss = 0.81 (247.6 examples/sec; 0.517 sec/batch)
2016-02-03 10:57:14.359362: step 8500, loss = 0.96 (295.2 examples/sec; 0.434 sec/batch)
2016-02-03 10:57:19.551991: step 8510, loss = 1.05 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 10:57:24.214688: step 8520, loss = 0.92 (312.7 examples/sec; 0.409 sec/batch)
2016-02-03 10:57:29.017680: step 8530, loss = 0.91 (250.6 examples/sec; 0.511 sec/batch)
2016-02-03 10:57:33.748260: step 8540, loss = 0.84 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 10:57:38.478615: step 8550, loss = 0.96 (273.2 examples/sec; 0.468 sec/batch)
2016-02-03 10:57:43.215412: step 8560, loss = 0.88 (261.0 examples/sec; 0.490 sec/batch)
2016-02-03 10:57:47.898636: step 8570, loss = 1.02 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 10:57:52.561912: step 8580, loss = 0.99 (293.1 examples/sec; 0.437 sec/batch)
2016-02-03 10:57:57.226712: step 8590, loss = 0.93 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 10:58:01.904282: step 8600, loss = 0.82 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 10:58:07.195522: step 8610, loss = 1.09 (248.6 examples/sec; 0.515 sec/batch)
2016-02-03 10:58:11.926533: step 8620, loss = 1.07 (277.7 examples/sec; 0.461 sec/batch)
2016-02-03 10:58:16.628870: step 8630, loss = 0.94 (295.7 examples/sec; 0.433 sec/batch)
2016-02-03 10:58:21.370923: step 8640, loss = 1.02 (254.9 examples/sec; 0.502 sec/batch)
2016-02-03 10:58:26.018968: step 8650, loss = 0.87 (289.2 examples/sec; 0.443 sec/batch)
2016-02-03 10:58:30.722618: step 8660, loss = 0.98 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 10:58:35.436082: step 8670, loss = 0.79 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 10:58:40.249826: step 8680, loss = 1.05 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 10:58:44.952843: step 8690, loss = 0.88 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 10:58:49.716257: step 8700, loss = 1.11 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 10:58:54.952844: step 8710, loss = 0.95 (264.2 examples/sec; 0.485 sec/batch)
2016-02-03 10:58:59.669941: step 8720, loss = 1.07 (260.9 examples/sec; 0.491 sec/batch)
2016-02-03 10:59:04.382825: step 8730, loss = 0.83 (277.9 examples/sec; 0.461 sec/batch)
2016-02-03 10:59:09.027761: step 8740, loss = 1.00 (250.5 examples/sec; 0.511 sec/batch)
2016-02-03 10:59:13.757287: step 8750, loss = 0.93 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 10:59:18.495712: step 8760, loss = 1.05 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 10:59:23.182919: step 8770, loss = 0.84 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 10:59:27.941951: step 8780, loss = 1.06 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 10:59:32.610854: step 8790, loss = 0.84 (259.1 examples/sec; 0.494 sec/batch)
2016-02-03 10:59:37.290125: step 8800, loss = 0.97 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 10:59:42.460701: step 8810, loss = 0.91 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 10:59:47.130293: step 8820, loss = 0.99 (294.2 examples/sec; 0.435 sec/batch)
2016-02-03 10:59:51.906074: step 8830, loss = 0.94 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 10:59:56.646523: step 8840, loss = 1.06 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 11:00:01.469047: step 8850, loss = 0.96 (264.5 examples/sec; 0.484 sec/batch)
2016-02-03 11:00:06.164129: step 8860, loss = 0.81 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 11:00:10.828002: step 8870, loss = 0.76 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 11:00:15.614663: step 8880, loss = 1.03 (258.5 examples/sec; 0.495 sec/batch)
2016-02-03 11:00:20.295282: step 8890, loss = 1.06 (260.4 examples/sec; 0.492 sec/batch)
2016-02-03 11:00:25.020934: step 8900, loss = 1.11 (257.5 examples/sec; 0.497 sec/batch)
2016-02-03 11:00:30.221621: step 8910, loss = 1.01 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 11:00:34.858372: step 8920, loss = 1.05 (303.2 examples/sec; 0.422 sec/batch)
2016-02-03 11:00:39.598151: step 8930, loss = 0.96 (257.6 examples/sec; 0.497 sec/batch)
2016-02-03 11:00:44.254663: step 8940, loss = 1.08 (248.7 examples/sec; 0.515 sec/batch)
2016-02-03 11:00:49.000997: step 8950, loss = 0.97 (282.9 examples/sec; 0.452 sec/batch)
2016-02-03 11:00:53.778942: step 8960, loss = 0.95 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 11:00:58.447425: step 8970, loss = 0.88 (255.6 examples/sec; 0.501 sec/batch)
2016-02-03 11:01:03.187067: step 8980, loss = 0.89 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 11:01:07.970759: step 8990, loss = 0.82 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 11:01:12.655723: step 9000, loss = 0.93 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 11:01:17.857299: step 9010, loss = 1.03 (268.6 examples/sec; 0.477 sec/batch)
2016-02-03 11:01:22.513218: step 9020, loss = 0.97 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 11:01:27.281962: step 9030, loss = 0.90 (256.9 examples/sec; 0.498 sec/batch)
2016-02-03 11:01:32.009072: step 9040, loss = 1.04 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 11:01:36.687800: step 9050, loss = 0.91 (275.7 examples/sec; 0.464 sec/batch)
2016-02-03 11:01:41.401810: step 9060, loss = 1.03 (256.9 examples/sec; 0.498 sec/batch)
2016-02-03 11:01:46.044429: step 9070, loss = 0.76 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 11:01:50.700371: step 9080, loss = 0.87 (297.1 examples/sec; 0.431 sec/batch)
2016-02-03 11:01:55.436954: step 9090, loss = 0.93 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 11:02:00.166939: step 9100, loss = 0.84 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 11:02:05.405235: step 9110, loss = 0.82 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 11:02:10.168032: step 9120, loss = 0.90 (249.2 examples/sec; 0.514 sec/batch)
2016-02-03 11:02:14.862849: step 9130, loss = 0.94 (284.9 examples/sec; 0.449 sec/batch)
2016-02-03 11:02:19.570314: step 9140, loss = 1.01 (299.5 examples/sec; 0.427 sec/batch)
2016-02-03 11:02:24.280058: step 9150, loss = 0.92 (255.2 examples/sec; 0.502 sec/batch)
2016-02-03 11:02:28.968010: step 9160, loss = 0.93 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 11:02:33.734971: step 9170, loss = 1.02 (243.8 examples/sec; 0.525 sec/batch)
2016-02-03 11:02:38.391711: step 9180, loss = 0.95 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 11:02:43.083640: step 9190, loss = 0.92 (288.0 examples/sec; 0.444 sec/batch)
2016-02-03 11:02:47.796024: step 9200, loss = 1.23 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 11:02:53.085382: step 9210, loss = 0.97 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 11:02:57.809217: step 9220, loss = 0.91 (261.0 examples/sec; 0.490 sec/batch)
2016-02-03 11:03:02.420191: step 9230, loss = 0.84 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 11:03:07.017207: step 9240, loss = 0.96 (291.9 examples/sec; 0.438 sec/batch)
2016-02-03 11:03:11.716971: step 9250, loss = 0.99 (253.7 examples/sec; 0.504 sec/batch)
2016-02-03 11:03:16.406607: step 9260, loss = 0.97 (255.9 examples/sec; 0.500 sec/batch)
2016-02-03 11:03:21.103182: step 9270, loss = 0.93 (251.2 examples/sec; 0.510 sec/batch)
2016-02-03 11:03:25.833250: step 9280, loss = 0.78 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 11:03:30.578825: step 9290, loss = 0.99 (262.2 examples/sec; 0.488 sec/batch)
2016-02-03 11:03:35.321951: step 9300, loss = 1.04 (257.8 examples/sec; 0.497 sec/batch)
2016-02-03 11:03:40.491389: step 9310, loss = 0.78 (259.3 examples/sec; 0.494 sec/batch)
2016-02-03 11:03:45.195080: step 9320, loss = 0.88 (293.0 examples/sec; 0.437 sec/batch)
2016-02-03 11:03:49.937276: step 9330, loss = 0.92 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 11:03:54.671911: step 9340, loss = 0.96 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 11:03:59.420653: step 9350, loss = 0.89 (260.3 examples/sec; 0.492 sec/batch)
2016-02-03 11:04:04.031436: step 9360, loss = 0.83 (289.8 examples/sec; 0.442 sec/batch)
2016-02-03 11:04:08.799530: step 9370, loss = 0.88 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 11:04:13.569955: step 9380, loss = 0.80 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 11:04:18.289882: step 9390, loss = 1.17 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 11:04:22.919230: step 9400, loss = 0.77 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 11:04:28.166328: step 9410, loss = 0.90 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 11:04:32.969741: step 9420, loss = 0.99 (233.2 examples/sec; 0.549 sec/batch)
2016-02-03 11:04:37.675675: step 9430, loss = 0.86 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 11:04:42.447542: step 9440, loss = 0.94 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 11:04:47.126094: step 9450, loss = 0.88 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 11:04:51.849369: step 9460, loss = 1.00 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 11:04:56.642647: step 9470, loss = 0.83 (248.6 examples/sec; 0.515 sec/batch)
2016-02-03 11:05:01.344448: step 9480, loss = 0.74 (268.2 examples/sec; 0.477 sec/batch)
2016-02-03 11:05:06.085981: step 9490, loss = 1.06 (249.3 examples/sec; 0.513 sec/batch)
2016-02-03 11:05:10.817800: step 9500, loss = 0.97 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 11:05:16.125849: step 9510, loss = 0.89 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 11:05:20.866653: step 9520, loss = 0.95 (247.8 examples/sec; 0.517 sec/batch)
2016-02-03 11:05:25.558127: step 9530, loss = 0.92 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 11:05:30.304941: step 9540, loss = 0.97 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 11:05:34.989150: step 9550, loss = 0.90 (285.7 examples/sec; 0.448 sec/batch)
2016-02-03 11:05:39.691404: step 9560, loss = 0.86 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 11:05:44.396972: step 9570, loss = 0.93 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 11:05:49.035945: step 9580, loss = 0.90 (283.5 examples/sec; 0.451 sec/batch)
2016-02-03 11:05:53.786216: step 9590, loss = 0.86 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 11:05:58.462853: step 9600, loss = 0.95 (289.5 examples/sec; 0.442 sec/batch)
2016-02-03 11:06:03.633223: step 9610, loss = 0.94 (294.1 examples/sec; 0.435 sec/batch)
2016-02-03 11:06:08.351396: step 9620, loss = 0.96 (259.9 examples/sec; 0.492 sec/batch)
2016-02-03 11:06:13.000388: step 9630, loss = 0.96 (309.5 examples/sec; 0.414 sec/batch)
2016-02-03 11:06:17.779480: step 9640, loss = 0.99 (240.0 examples/sec; 0.533 sec/batch)
2016-02-03 11:06:22.455453: step 9650, loss = 0.75 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 11:06:27.081407: step 9660, loss = 0.90 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 11:06:31.876276: step 9670, loss = 0.98 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 11:06:36.566344: step 9680, loss = 0.80 (293.1 examples/sec; 0.437 sec/batch)
2016-02-03 11:06:41.269374: step 9690, loss = 1.04 (288.4 examples/sec; 0.444 sec/batch)
2016-02-03 11:06:45.944892: step 9700, loss = 0.93 (291.7 examples/sec; 0.439 sec/batch)
2016-02-03 11:06:51.257953: step 9710, loss = 0.94 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 11:06:55.977879: step 9720, loss = 0.96 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 11:07:00.632814: step 9730, loss = 0.96 (306.1 examples/sec; 0.418 sec/batch)
2016-02-03 11:07:05.306930: step 9740, loss = 0.97 (251.3 examples/sec; 0.509 sec/batch)
2016-02-03 11:07:10.072504: step 9750, loss = 0.93 (251.0 examples/sec; 0.510 sec/batch)
2016-02-03 11:07:14.750440: step 9760, loss = 0.87 (275.6 examples/sec; 0.464 sec/batch)
2016-02-03 11:07:19.557954: step 9770, loss = 0.79 (262.6 examples/sec; 0.487 sec/batch)
2016-02-03 11:07:24.186494: step 9780, loss = 0.94 (283.0 examples/sec; 0.452 sec/batch)
2016-02-03 11:07:28.880347: step 9790, loss = 1.02 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 11:07:33.651474: step 9800, loss = 0.86 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 11:07:38.686354: step 9810, loss = 0.83 (296.8 examples/sec; 0.431 sec/batch)
2016-02-03 11:07:43.321073: step 9820, loss = 0.97 (255.2 examples/sec; 0.501 sec/batch)
2016-02-03 11:07:48.035624: step 9830, loss = 0.84 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 11:07:52.774365: step 9840, loss = 1.05 (268.0 examples/sec; 0.478 sec/batch)
2016-02-03 11:07:57.497937: step 9850, loss = 0.98 (253.5 examples/sec; 0.505 sec/batch)
2016-02-03 11:08:02.220816: step 9860, loss = 0.95 (293.3 examples/sec; 0.436 sec/batch)
2016-02-03 11:08:06.927204: step 9870, loss = 0.80 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 11:08:11.623912: step 9880, loss = 0.80 (247.5 examples/sec; 0.517 sec/batch)
2016-02-03 11:08:16.399189: step 9890, loss = 0.91 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 11:08:21.120482: step 9900, loss = 0.95 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 11:08:26.431114: step 9910, loss = 0.93 (282.8 examples/sec; 0.453 sec/batch)
2016-02-03 11:08:31.105021: step 9920, loss = 0.88 (290.4 examples/sec; 0.441 sec/batch)
2016-02-03 11:08:35.853552: step 9930, loss = 1.05 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 11:08:40.549832: step 9940, loss = 0.78 (260.3 examples/sec; 0.492 sec/batch)
2016-02-03 11:08:45.245334: step 9950, loss = 0.82 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 11:08:49.942981: step 9960, loss = 0.90 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 11:08:54.616179: step 9970, loss = 1.15 (283.1 examples/sec; 0.452 sec/batch)
2016-02-03 11:08:59.362244: step 9980, loss = 0.99 (258.8 examples/sec; 0.495 sec/batch)
2016-02-03 11:09:03.966612: step 9990, loss = 0.74 (294.6 examples/sec; 0.434 sec/batch)
2016-02-03 11:09:08.689195: step 10000, loss = 0.94 (304.5 examples/sec; 0.420 sec/batch)
2016-02-03 11:09:13.923919: step 10010, loss = 0.99 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 11:09:18.608373: step 10020, loss = 0.89 (316.6 examples/sec; 0.404 sec/batch)
2016-02-03 11:09:23.375485: step 10030, loss = 0.86 (267.2 examples/sec; 0.479 sec/batch)
2016-02-03 11:09:28.093861: step 10040, loss = 0.97 (250.1 examples/sec; 0.512 sec/batch)
2016-02-03 11:09:32.733184: step 10050, loss = 1.01 (260.5 examples/sec; 0.491 sec/batch)
2016-02-03 11:09:37.433811: step 10060, loss = 1.08 (249.5 examples/sec; 0.513 sec/batch)
2016-02-03 11:09:42.138332: step 10070, loss = 1.01 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 11:09:46.851245: step 10080, loss = 0.97 (253.8 examples/sec; 0.504 sec/batch)
2016-02-03 11:09:51.521071: step 10090, loss = 0.92 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 11:09:56.151304: step 10100, loss = 0.82 (301.1 examples/sec; 0.425 sec/batch)
2016-02-03 11:10:01.497682: step 10110, loss = 1.13 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 11:10:06.159649: step 10120, loss = 0.86 (283.4 examples/sec; 0.452 sec/batch)
2016-02-03 11:10:10.852707: step 10130, loss = 0.92 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 11:10:15.505564: step 10140, loss = 0.81 (261.1 examples/sec; 0.490 sec/batch)
2016-02-03 11:10:20.142962: step 10150, loss = 0.97 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 11:10:24.795946: step 10160, loss = 0.88 (256.2 examples/sec; 0.500 sec/batch)
2016-02-03 11:10:29.411594: step 10170, loss = 0.94 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 11:10:34.182238: step 10180, loss = 0.86 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 11:10:38.889852: step 10190, loss = 0.94 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 11:10:43.614868: step 10200, loss = 0.93 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 11:10:48.836388: step 10210, loss = 0.88 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 11:10:53.506810: step 10220, loss = 0.93 (261.5 examples/sec; 0.490 sec/batch)
2016-02-03 11:10:58.256843: step 10230, loss = 1.02 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 11:11:02.896276: step 10240, loss = 1.11 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 11:11:07.520069: step 10250, loss = 0.86 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 11:11:12.146770: step 10260, loss = 0.93 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 11:11:16.835074: step 10270, loss = 0.92 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 11:11:21.498508: step 10280, loss = 0.89 (290.9 examples/sec; 0.440 sec/batch)
2016-02-03 11:11:26.171365: step 10290, loss = 0.91 (279.2 examples/sec; 0.458 sec/batch)
2016-02-03 11:11:30.856600: step 10300, loss = 0.84 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 11:11:36.063373: step 10310, loss = 1.00 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 11:11:40.683405: step 10320, loss = 1.01 (285.9 examples/sec; 0.448 sec/batch)
2016-02-03 11:11:45.377895: step 10330, loss = 0.85 (268.0 examples/sec; 0.478 sec/batch)
2016-02-03 11:11:50.153153: step 10340, loss = 0.87 (266.3 examples/sec; 0.481 sec/batch)
2016-02-03 11:11:54.703618: step 10350, loss = 1.01 (299.6 examples/sec; 0.427 sec/batch)
2016-02-03 11:11:59.444803: step 10360, loss = 1.00 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 11:12:04.187230: step 10370, loss = 0.84 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 11:12:08.957325: step 10380, loss = 0.75 (259.5 examples/sec; 0.493 sec/batch)
2016-02-03 11:12:13.669304: step 10390, loss = 0.66 (277.6 examples/sec; 0.461 sec/batch)
2016-02-03 11:12:18.391378: step 10400, loss = 0.89 (258.1 examples/sec; 0.496 sec/batch)
2016-02-03 11:12:23.639466: step 10410, loss = 0.76 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 11:12:28.375804: step 10420, loss = 0.93 (283.6 examples/sec; 0.451 sec/batch)
2016-02-03 11:12:33.050429: step 10430, loss = 0.74 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 11:12:37.715805: step 10440, loss = 0.77 (261.4 examples/sec; 0.490 sec/batch)
2016-02-03 11:12:42.344026: step 10450, loss = 0.88 (279.2 examples/sec; 0.458 sec/batch)
2016-02-03 11:12:47.035771: step 10460, loss = 0.86 (290.8 examples/sec; 0.440 sec/batch)
2016-02-03 11:12:51.686371: step 10470, loss = 0.83 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 11:12:56.394148: step 10480, loss = 0.92 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 11:13:01.064034: step 10490, loss = 0.91 (251.5 examples/sec; 0.509 sec/batch)
2016-02-03 11:13:05.716742: step 10500, loss = 0.96 (291.5 examples/sec; 0.439 sec/batch)
2016-02-03 11:13:10.944590: step 10510, loss = 0.88 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 11:13:15.629166: step 10520, loss = 0.80 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 11:13:20.341026: step 10530, loss = 0.93 (255.9 examples/sec; 0.500 sec/batch)
2016-02-03 11:13:25.060281: step 10540, loss = 0.96 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 11:13:29.775248: step 10550, loss = 1.02 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 11:13:34.414633: step 10560, loss = 0.89 (316.5 examples/sec; 0.404 sec/batch)
2016-02-03 11:13:39.176051: step 10570, loss = 0.89 (256.6 examples/sec; 0.499 sec/batch)
2016-02-03 11:13:43.917057: step 10580, loss = 0.88 (257.0 examples/sec; 0.498 sec/batch)
2016-02-03 11:13:48.621999: step 10590, loss = 0.90 (242.8 examples/sec; 0.527 sec/batch)
2016-02-03 11:13:53.252436: step 10600, loss = 0.83 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 11:13:58.360252: step 10610, loss = 1.00 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 11:14:03.040565: step 10620, loss = 0.85 (300.1 examples/sec; 0.426 sec/batch)
2016-02-03 11:14:07.760121: step 10630, loss = 1.03 (272.6 examples/sec; 0.470 sec/batch)
2016-02-03 11:14:12.347466: step 10640, loss = 1.02 (261.5 examples/sec; 0.489 sec/batch)
2016-02-03 11:14:16.970459: step 10650, loss = 0.93 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 11:14:21.621039: step 10660, loss = 1.00 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 11:14:26.228810: step 10670, loss = 0.97 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 11:14:30.870029: step 10680, loss = 0.81 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 11:14:35.489802: step 10690, loss = 0.91 (300.6 examples/sec; 0.426 sec/batch)
2016-02-03 11:14:40.202567: step 10700, loss = 0.87 (252.8 examples/sec; 0.506 sec/batch)
2016-02-03 11:14:45.417985: step 10710, loss = 0.83 (255.5 examples/sec; 0.501 sec/batch)
2016-02-03 11:14:50.164441: step 10720, loss = 0.94 (266.9 examples/sec; 0.480 sec/batch)
2016-02-03 11:14:54.879920: step 10730, loss = 0.90 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 11:14:59.564045: step 10740, loss = 0.93 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 11:15:04.266412: step 10750, loss = 0.87 (252.0 examples/sec; 0.508 sec/batch)
2016-02-03 11:15:08.949681: step 10760, loss = 0.85 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 11:15:13.681839: step 10770, loss = 0.93 (263.0 examples/sec; 0.487 sec/batch)
2016-02-03 11:15:18.330359: step 10780, loss = 0.91 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 11:15:22.958497: step 10790, loss = 0.94 (295.3 examples/sec; 0.433 sec/batch)
2016-02-03 11:15:27.635438: step 10800, loss = 0.92 (286.0 examples/sec; 0.448 sec/batch)
2016-02-03 11:15:32.825469: step 10810, loss = 1.00 (286.1 examples/sec; 0.447 sec/batch)
2016-02-03 11:15:37.618544: step 10820, loss = 0.69 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 11:15:42.258622: step 10830, loss = 0.74 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 11:15:46.917111: step 10840, loss = 0.78 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 11:15:51.601246: step 10850, loss = 0.76 (282.6 examples/sec; 0.453 sec/batch)
2016-02-03 11:15:56.360459: step 10860, loss = 1.01 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 11:16:01.096143: step 10870, loss = 0.97 (281.7 examples/sec; 0.454 sec/batch)
2016-02-03 11:16:05.782218: step 10880, loss = 0.98 (252.0 examples/sec; 0.508 sec/batch)
2016-02-03 11:16:10.482926: step 10890, loss = 0.91 (301.2 examples/sec; 0.425 sec/batch)
2016-02-03 11:16:15.128665: step 10900, loss = 0.81 (253.3 examples/sec; 0.505 sec/batch)
2016-02-03 11:16:20.324369: step 10910, loss = 0.83 (270.5 examples/sec; 0.473 sec/batch)
2016-02-03 11:16:25.008769: step 10920, loss = 0.94 (264.9 examples/sec; 0.483 sec/batch)
2016-02-03 11:16:29.673814: step 10930, loss = 0.81 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 11:16:34.362403: step 10940, loss = 1.02 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 11:16:39.016283: step 10950, loss = 0.86 (289.5 examples/sec; 0.442 sec/batch)
2016-02-03 11:16:43.717545: step 10960, loss = 0.82 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 11:16:48.380596: step 10970, loss = 0.94 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 11:16:53.027944: step 10980, loss = 0.94 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 11:16:57.674612: step 10990, loss = 0.95 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 11:17:02.363104: step 11000, loss = 0.65 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 11:17:07.458252: step 11010, loss = 0.86 (283.4 examples/sec; 0.452 sec/batch)
2016-02-03 11:17:12.150697: step 11020, loss = 0.88 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 11:17:16.795823: step 11030, loss = 0.85 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 11:17:21.429912: step 11040, loss = 0.89 (279.2 examples/sec; 0.458 sec/batch)
2016-02-03 11:17:26.087518: step 11050, loss = 0.89 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 11:17:30.781253: step 11060, loss = 0.93 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 11:17:35.498070: step 11070, loss = 0.81 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 11:17:40.254547: step 11080, loss = 0.84 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 11:17:44.961034: step 11090, loss = 0.87 (286.8 examples/sec; 0.446 sec/batch)
2016-02-03 11:17:49.659548: step 11100, loss = 0.92 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 11:17:54.949085: step 11110, loss = 1.03 (260.4 examples/sec; 0.492 sec/batch)
2016-02-03 11:17:59.736168: step 11120, loss = 0.95 (277.6 examples/sec; 0.461 sec/batch)
2016-02-03 11:18:04.449757: step 11130, loss = 0.91 (258.0 examples/sec; 0.496 sec/batch)
2016-02-03 11:18:09.038744: step 11140, loss = 0.93 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 11:18:13.768903: step 11150, loss = 1.01 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 11:18:18.519923: step 11160, loss = 0.88 (269.2 examples/sec; 0.476 sec/batch)
2016-02-03 11:18:23.167984: step 11170, loss = 0.91 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 11:18:27.952319: step 11180, loss = 0.91 (249.4 examples/sec; 0.513 sec/batch)
2016-02-03 11:18:32.631288: step 11190, loss = 0.95 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 11:18:37.336307: step 11200, loss = 0.85 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 11:18:42.640415: step 11210, loss = 0.93 (258.5 examples/sec; 0.495 sec/batch)
2016-02-03 11:18:47.336993: step 11220, loss = 0.84 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 11:18:51.969523: step 11230, loss = 0.90 (300.5 examples/sec; 0.426 sec/batch)
2016-02-03 11:18:56.646809: step 11240, loss = 0.89 (275.7 examples/sec; 0.464 sec/batch)
2016-02-03 11:19:01.353509: step 11250, loss = 0.67 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 11:19:06.067775: step 11260, loss = 1.08 (309.1 examples/sec; 0.414 sec/batch)
2016-02-03 11:19:10.829686: step 11270, loss = 0.83 (260.0 examples/sec; 0.492 sec/batch)
2016-02-03 11:19:15.477165: step 11280, loss = 0.78 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 11:19:20.174492: step 11290, loss = 0.95 (258.9 examples/sec; 0.494 sec/batch)
2016-02-03 11:19:24.847139: step 11300, loss = 0.79 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 11:19:30.155403: step 11310, loss = 0.80 (256.4 examples/sec; 0.499 sec/batch)
2016-02-03 11:19:34.854711: step 11320, loss = 0.89 (272.7 examples/sec; 0.469 sec/batch)
2016-02-03 11:19:39.617963: step 11330, loss = 0.96 (256.9 examples/sec; 0.498 sec/batch)
2016-02-03 11:19:44.300619: step 11340, loss = 0.83 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 11:19:49.023892: step 11350, loss = 0.92 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 11:19:53.756756: step 11360, loss = 0.81 (284.6 examples/sec; 0.450 sec/batch)
2016-02-03 11:19:58.471906: step 11370, loss = 0.89 (292.9 examples/sec; 0.437 sec/batch)
2016-02-03 11:20:03.147307: step 11380, loss = 0.88 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 11:20:07.853622: step 11390, loss = 1.01 (265.0 examples/sec; 0.483 sec/batch)
2016-02-03 11:20:12.540335: step 11400, loss = 0.91 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 11:20:17.835327: step 11410, loss = 0.94 (269.2 examples/sec; 0.475 sec/batch)
2016-02-03 11:20:22.389210: step 11420, loss = 0.94 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 11:20:27.154669: step 11430, loss = 0.92 (248.0 examples/sec; 0.516 sec/batch)
2016-02-03 11:20:31.860100: step 11440, loss = 1.02 (295.2 examples/sec; 0.434 sec/batch)
2016-02-03 11:20:36.617016: step 11450, loss = 0.75 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 11:20:41.356540: step 11460, loss = 1.24 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 11:20:46.085447: step 11470, loss = 0.97 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 11:20:50.810275: step 11480, loss = 0.85 (268.1 examples/sec; 0.478 sec/batch)
2016-02-03 11:20:55.502721: step 11490, loss = 0.89 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 11:21:00.195530: step 11500, loss = 0.84 (277.8 examples/sec; 0.461 sec/batch)
2016-02-03 11:21:05.412844: step 11510, loss = 0.80 (262.6 examples/sec; 0.487 sec/batch)
2016-02-03 11:21:10.112640: step 11520, loss = 0.95 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 11:21:14.752839: step 11530, loss = 0.98 (257.4 examples/sec; 0.497 sec/batch)
2016-02-03 11:21:19.434055: step 11540, loss = 0.87 (252.7 examples/sec; 0.507 sec/batch)
2016-02-03 11:21:24.126300: step 11550, loss = 0.76 (288.7 examples/sec; 0.443 sec/batch)
2016-02-03 11:21:28.834060: step 11560, loss = 0.74 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 11:21:33.490769: step 11570, loss = 0.94 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 11:21:38.181691: step 11580, loss = 0.87 (293.3 examples/sec; 0.436 sec/batch)
2016-02-03 11:21:42.880923: step 11590, loss = 1.06 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 11:21:47.681691: step 11600, loss = 0.94 (263.0 examples/sec; 0.487 sec/batch)
2016-02-03 11:21:52.863007: step 11610, loss = 0.85 (282.4 examples/sec; 0.453 sec/batch)
2016-02-03 11:21:57.568009: step 11620, loss = 0.91 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 11:22:02.328228: step 11630, loss = 1.05 (291.3 examples/sec; 0.439 sec/batch)
2016-02-03 11:22:07.064229: step 11640, loss = 0.78 (290.0 examples/sec; 0.441 sec/batch)
2016-02-03 11:22:11.796329: step 11650, loss = 0.77 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 11:22:16.517005: step 11660, loss = 0.97 (289.6 examples/sec; 0.442 sec/batch)
2016-02-03 11:22:21.179084: step 11670, loss = 0.88 (267.0 examples/sec; 0.479 sec/batch)
2016-02-03 11:22:25.892235: step 11680, loss = 0.91 (307.1 examples/sec; 0.417 sec/batch)
2016-02-03 11:22:30.526744: step 11690, loss = 0.83 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 11:22:35.300622: step 11700, loss = 0.89 (256.8 examples/sec; 0.498 sec/batch)
2016-02-03 11:22:40.554604: step 11710, loss = 0.89 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 11:22:45.260525: step 11720, loss = 0.80 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 11:22:49.955254: step 11730, loss = 0.85 (292.0 examples/sec; 0.438 sec/batch)
2016-02-03 11:22:54.715072: step 11740, loss = 0.81 (249.6 examples/sec; 0.513 sec/batch)
2016-02-03 11:22:59.409806: step 11750, loss = 0.91 (258.4 examples/sec; 0.495 sec/batch)
2016-02-03 11:23:04.098639: step 11760, loss = 1.00 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 11:23:08.824038: step 11770, loss = 0.88 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 11:23:13.504502: step 11780, loss = 1.12 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 11:23:18.190851: step 11790, loss = 0.80 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 11:23:22.827454: step 11800, loss = 0.95 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 11:23:28.064799: step 11810, loss = 0.87 (256.5 examples/sec; 0.499 sec/batch)
2016-02-03 11:23:32.753275: step 11820, loss = 0.75 (257.6 examples/sec; 0.497 sec/batch)
2016-02-03 11:23:37.299587: step 11830, loss = 1.01 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 11:23:41.981274: step 11840, loss = 0.80 (261.0 examples/sec; 0.490 sec/batch)
2016-02-03 11:23:46.608120: step 11850, loss = 1.08 (277.9 examples/sec; 0.461 sec/batch)
2016-02-03 11:23:51.232438: step 11860, loss = 0.97 (294.2 examples/sec; 0.435 sec/batch)
2016-02-03 11:23:55.984508: step 11870, loss = 0.87 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 11:24:00.662917: step 11880, loss = 0.95 (279.4 examples/sec; 0.458 sec/batch)
2016-02-03 11:24:05.326532: step 11890, loss = 0.93 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 11:24:09.996820: step 11900, loss = 0.69 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 11:24:15.208787: step 11910, loss = 1.00 (246.6 examples/sec; 0.519 sec/batch)
2016-02-03 11:24:19.870810: step 11920, loss = 0.87 (262.6 examples/sec; 0.487 sec/batch)
2016-02-03 11:24:24.614256: step 11930, loss = 1.09 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 11:24:29.330615: step 11940, loss = 1.07 (281.4 examples/sec; 0.455 sec/batch)
2016-02-03 11:24:34.071148: step 11950, loss = 0.88 (284.0 examples/sec; 0.451 sec/batch)
2016-02-03 11:24:38.826403: step 11960, loss = 0.93 (280.5 examples/sec; 0.456 sec/batch)
2016-02-03 11:24:43.530146: step 11970, loss = 0.91 (258.6 examples/sec; 0.495 sec/batch)
2016-02-03 11:24:48.198370: step 11980, loss = 1.05 (287.3 examples/sec; 0.445 sec/batch)
2016-02-03 11:24:52.919951: step 11990, loss = 0.91 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 11:24:57.646513: step 12000, loss = 0.90 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 11:25:02.922669: step 12010, loss = 0.94 (273.8 examples/sec; 0.468 sec/batch)
2016-02-03 11:25:07.657786: step 12020, loss = 0.88 (272.6 examples/sec; 0.470 sec/batch)
2016-02-03 11:25:12.377257: step 12030, loss = 0.83 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 11:25:17.100146: step 12040, loss = 0.82 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 11:25:21.824311: step 12050, loss = 0.83 (294.6 examples/sec; 0.434 sec/batch)
2016-02-03 11:25:26.546351: step 12060, loss = 1.02 (287.3 examples/sec; 0.445 sec/batch)
2016-02-03 11:25:31.296001: step 12070, loss = 0.90 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 11:25:35.997685: step 12080, loss = 1.06 (279.2 examples/sec; 0.458 sec/batch)
2016-02-03 11:25:40.702300: step 12090, loss = 0.80 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 11:25:45.395377: step 12100, loss = 0.87 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 11:25:50.592968: step 12110, loss = 0.79 (268.6 examples/sec; 0.476 sec/batch)
2016-02-03 11:25:55.292661: step 12120, loss = 0.85 (294.7 examples/sec; 0.434 sec/batch)
2016-02-03 11:26:00.023863: step 12130, loss = 0.94 (248.9 examples/sec; 0.514 sec/batch)
2016-02-03 11:26:04.703565: step 12140, loss = 0.90 (266.3 examples/sec; 0.481 sec/batch)
2016-02-03 11:26:09.451631: step 12150, loss = 1.10 (263.1 examples/sec; 0.486 sec/batch)
2016-02-03 11:26:14.218469: step 12160, loss = 0.79 (290.9 examples/sec; 0.440 sec/batch)
2016-02-03 11:26:18.901966: step 12170, loss = 0.85 (261.3 examples/sec; 0.490 sec/batch)
2016-02-03 11:26:23.598231: step 12180, loss = 0.93 (265.3 examples/sec; 0.482 sec/batch)
2016-02-03 11:26:28.252917: step 12190, loss = 0.91 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 11:26:32.928638: step 12200, loss = 0.92 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 11:26:38.125035: step 12210, loss = 0.99 (291.7 examples/sec; 0.439 sec/batch)
2016-02-03 11:26:42.803585: step 12220, loss = 0.85 (260.4 examples/sec; 0.492 sec/batch)
2016-02-03 11:26:47.520290: step 12230, loss = 0.66 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 11:26:52.200750: step 12240, loss = 0.84 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 11:26:56.906707: step 12250, loss = 0.81 (282.5 examples/sec; 0.453 sec/batch)
2016-02-03 11:27:01.676661: step 12260, loss = 0.95 (283.7 examples/sec; 0.451 sec/batch)
2016-02-03 11:27:06.369919: step 12270, loss = 1.06 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 11:27:11.045814: step 12280, loss = 1.02 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 11:27:15.761790: step 12290, loss = 1.03 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 11:27:20.496043: step 12300, loss = 0.63 (283.0 examples/sec; 0.452 sec/batch)
2016-02-03 11:27:25.802012: step 12310, loss = 1.03 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 11:27:30.488490: step 12320, loss = 0.82 (281.4 examples/sec; 0.455 sec/batch)
2016-02-03 11:27:35.205163: step 12330, loss = 0.81 (270.9 examples/sec; 0.472 sec/batch)
2016-02-03 11:27:39.913196: step 12340, loss = 0.92 (255.2 examples/sec; 0.502 sec/batch)
2016-02-03 11:27:44.561726: step 12350, loss = 0.79 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 11:27:49.241239: step 12360, loss = 0.94 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 11:27:54.018623: step 12370, loss = 0.78 (287.9 examples/sec; 0.445 sec/batch)
2016-02-03 11:27:58.694279: step 12380, loss = 0.86 (287.3 examples/sec; 0.445 sec/batch)
2016-02-03 11:28:03.457487: step 12390, loss = 0.83 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 11:28:08.160347: step 12400, loss = 1.19 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 11:28:13.427804: step 12410, loss = 0.83 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 11:28:18.171772: step 12420, loss = 1.00 (291.5 examples/sec; 0.439 sec/batch)
2016-02-03 11:28:22.916470: step 12430, loss = 0.87 (294.2 examples/sec; 0.435 sec/batch)
2016-02-03 11:28:27.676414: step 12440, loss = 0.99 (260.5 examples/sec; 0.491 sec/batch)
2016-02-03 11:28:32.429816: step 12450, loss = 0.90 (270.9 examples/sec; 0.472 sec/batch)
2016-02-03 11:28:37.252514: step 12460, loss = 0.80 (286.2 examples/sec; 0.447 sec/batch)
2016-02-03 11:28:41.895414: step 12470, loss = 0.72 (265.0 examples/sec; 0.483 sec/batch)
2016-02-03 11:28:46.571029: step 12480, loss = 1.02 (293.5 examples/sec; 0.436 sec/batch)
2016-02-03 11:28:51.305906: step 12490, loss = 0.79 (286.5 examples/sec; 0.447 sec/batch)
2016-02-03 11:28:56.089910: step 12500, loss = 0.81 (258.8 examples/sec; 0.495 sec/batch)
2016-02-03 11:29:01.363215: step 12510, loss = 0.89 (277.9 examples/sec; 0.461 sec/batch)
2016-02-03 11:29:06.063400: step 12520, loss = 1.08 (292.3 examples/sec; 0.438 sec/batch)
2016-02-03 11:29:10.744741: step 12530, loss = 0.92 (257.7 examples/sec; 0.497 sec/batch)
2016-02-03 11:29:15.491894: step 12540, loss = 1.01 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 11:29:20.237399: step 12550, loss = 0.95 (253.0 examples/sec; 0.506 sec/batch)
2016-02-03 11:29:24.968858: step 12560, loss = 0.70 (286.2 examples/sec; 0.447 sec/batch)
2016-02-03 11:29:29.709392: step 12570, loss = 0.84 (290.7 examples/sec; 0.440 sec/batch)
2016-02-03 11:29:34.394419: step 12580, loss = 0.90 (320.9 examples/sec; 0.399 sec/batch)
2016-02-03 11:29:39.159502: step 12590, loss = 0.86 (279.6 examples/sec; 0.458 sec/batch)
2016-02-03 11:29:43.954953: step 12600, loss = 0.98 (276.3 examples/sec; 0.463 sec/batch)
2016-02-03 11:29:49.214931: step 12610, loss = 0.94 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 11:29:53.895982: step 12620, loss = 0.85 (287.0 examples/sec; 0.446 sec/batch)
2016-02-03 11:29:58.570957: step 12630, loss = 0.95 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 11:30:03.232378: step 12640, loss = 0.90 (285.1 examples/sec; 0.449 sec/batch)
2016-02-03 11:30:07.970646: step 12650, loss = 0.75 (275.0 examples/sec; 0.465 sec/batch)
2016-02-03 11:30:12.650456: step 12660, loss = 0.82 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 11:30:17.380364: step 12670, loss = 0.79 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 11:30:22.075357: step 12680, loss = 0.87 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 11:30:26.795657: step 12690, loss = 1.05 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 11:30:31.480321: step 12700, loss = 0.76 (300.9 examples/sec; 0.425 sec/batch)
2016-02-03 11:30:36.744496: step 12710, loss = 0.89 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 11:30:41.389697: step 12720, loss = 1.01 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 11:30:46.123421: step 12730, loss = 0.97 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 11:30:50.838005: step 12740, loss = 0.78 (303.5 examples/sec; 0.422 sec/batch)
2016-02-03 11:30:55.651270: step 12750, loss = 0.89 (252.7 examples/sec; 0.506 sec/batch)
2016-02-03 11:31:00.424984: step 12760, loss = 0.92 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 11:31:05.057813: step 12770, loss = 0.79 (253.1 examples/sec; 0.506 sec/batch)
2016-02-03 11:31:09.751347: step 12780, loss = 0.89 (280.5 examples/sec; 0.456 sec/batch)
2016-02-03 11:31:14.416877: step 12790, loss = 0.82 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 11:31:19.141087: step 12800, loss = 0.85 (292.4 examples/sec; 0.438 sec/batch)
2016-02-03 11:31:24.437341: step 12810, loss = 0.91 (258.2 examples/sec; 0.496 sec/batch)
2016-02-03 11:31:29.119158: step 12820, loss = 0.72 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 11:31:33.835643: step 12830, loss = 1.00 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 11:31:38.527098: step 12840, loss = 0.82 (296.1 examples/sec; 0.432 sec/batch)
2016-02-03 11:31:43.285129: step 12850, loss = 0.86 (251.9 examples/sec; 0.508 sec/batch)
2016-02-03 11:31:47.931058: step 12860, loss = 0.85 (298.5 examples/sec; 0.429 sec/batch)
2016-02-03 11:31:52.723859: step 12870, loss = 0.77 (255.7 examples/sec; 0.501 sec/batch)
2016-02-03 11:31:57.365153: step 12880, loss = 0.93 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 11:32:02.093084: step 12890, loss = 0.77 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 11:32:06.794304: step 12900, loss = 0.84 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 11:32:12.007127: step 12910, loss = 0.82 (265.2 examples/sec; 0.483 sec/batch)
2016-02-03 11:32:16.724487: step 12920, loss = 0.82 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 11:32:21.427351: step 12930, loss = 0.97 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 11:32:26.097944: step 12940, loss = 0.83 (257.8 examples/sec; 0.497 sec/batch)
2016-02-03 11:32:30.779827: step 12950, loss = 0.97 (306.5 examples/sec; 0.418 sec/batch)
2016-02-03 11:32:35.449689: step 12960, loss = 0.91 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 11:32:40.179766: step 12970, loss = 0.79 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 11:32:44.954527: step 12980, loss = 0.80 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 11:32:49.677376: step 12990, loss = 0.76 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 11:32:54.438765: step 13000, loss = 0.86 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 11:32:59.653385: step 13010, loss = 0.83 (256.0 examples/sec; 0.500 sec/batch)
2016-02-03 11:33:04.404362: step 13020, loss = 0.94 (291.0 examples/sec; 0.440 sec/batch)
2016-02-03 11:33:09.197077: step 13030, loss = 0.89 (249.8 examples/sec; 0.512 sec/batch)
2016-02-03 11:33:13.892882: step 13040, loss = 1.07 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 11:33:18.521661: step 13050, loss = 0.89 (334.1 examples/sec; 0.383 sec/batch)
2016-02-03 11:33:23.350276: step 13060, loss = 0.77 (260.8 examples/sec; 0.491 sec/batch)
2016-02-03 11:33:28.073921: step 13070, loss = 0.98 (283.0 examples/sec; 0.452 sec/batch)
2016-02-03 11:33:32.787733: step 13080, loss = 0.93 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 11:33:37.470854: step 13090, loss = 0.87 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 11:33:42.144593: step 13100, loss = 0.90 (258.8 examples/sec; 0.495 sec/batch)
2016-02-03 11:33:47.330537: step 13110, loss = 0.96 (280.4 examples/sec; 0.456 sec/batch)
2016-02-03 11:33:52.027779: step 13120, loss = 0.78 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 11:33:56.722862: step 13130, loss = 1.01 (254.9 examples/sec; 0.502 sec/batch)
2016-02-03 11:34:01.436697: step 13140, loss = 0.97 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 11:34:06.057236: step 13150, loss = 0.72 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 11:34:10.757992: step 13160, loss = 0.94 (286.8 examples/sec; 0.446 sec/batch)
2016-02-03 11:34:15.423361: step 13170, loss = 0.92 (292.3 examples/sec; 0.438 sec/batch)
2016-02-03 11:34:20.027466: step 13180, loss = 0.92 (285.0 examples/sec; 0.449 sec/batch)
2016-02-03 11:34:24.650163: step 13190, loss = 0.93 (291.2 examples/sec; 0.439 sec/batch)
2016-02-03 11:34:29.374605: step 13200, loss = 0.93 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 11:34:34.570871: step 13210, loss = 0.86 (291.6 examples/sec; 0.439 sec/batch)
2016-02-03 11:34:39.251570: step 13220, loss = 0.89 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 11:34:43.935015: step 13230, loss = 0.70 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 11:34:48.613273: step 13240, loss = 0.96 (285.8 examples/sec; 0.448 sec/batch)
2016-02-03 11:34:53.324838: step 13250, loss = 0.75 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 11:34:58.053247: step 13260, loss = 0.74 (286.0 examples/sec; 0.448 sec/batch)
2016-02-03 11:35:02.808866: step 13270, loss = 0.75 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 11:35:07.509553: step 13280, loss = 0.88 (282.8 examples/sec; 0.453 sec/batch)
2016-02-03 11:35:12.206089: step 13290, loss = 0.70 (270.3 examples/sec; 0.473 sec/batch)
2016-02-03 11:35:16.892783: step 13300, loss = 0.88 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 11:35:22.171613: step 13310, loss = 0.97 (272.7 examples/sec; 0.469 sec/batch)
2016-02-03 11:35:26.779107: step 13320, loss = 1.04 (281.7 examples/sec; 0.454 sec/batch)
2016-02-03 11:35:31.488519: step 13330, loss = 0.83 (250.0 examples/sec; 0.512 sec/batch)
2016-02-03 11:35:36.183561: step 13340, loss = 0.90 (289.2 examples/sec; 0.443 sec/batch)
2016-02-03 11:35:40.921460: step 13350, loss = 1.01 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 11:35:45.659270: step 13360, loss = 0.86 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 11:35:50.316885: step 13370, loss = 0.72 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 11:35:55.033188: step 13380, loss = 0.80 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 11:35:59.685486: step 13390, loss = 1.00 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 11:36:04.404449: step 13400, loss = 0.92 (284.6 examples/sec; 0.450 sec/batch)
2016-02-03 11:36:09.628978: step 13410, loss = 0.80 (279.4 examples/sec; 0.458 sec/batch)
2016-02-03 11:36:14.331935: step 13420, loss = 0.82 (254.9 examples/sec; 0.502 sec/batch)
2016-02-03 11:36:19.032070: step 13430, loss = 0.79 (239.1 examples/sec; 0.535 sec/batch)
2016-02-03 11:36:23.710592: step 13440, loss = 0.85 (258.5 examples/sec; 0.495 sec/batch)
2016-02-03 11:36:28.370791: step 13450, loss = 1.00 (247.8 examples/sec; 0.517 sec/batch)
2016-02-03 11:36:33.089502: step 13460, loss = 0.89 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 11:36:37.824788: step 13470, loss = 0.79 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 11:36:42.553918: step 13480, loss = 0.81 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 11:36:47.259938: step 13490, loss = 0.80 (230.6 examples/sec; 0.555 sec/batch)
2016-02-03 11:36:51.974527: step 13500, loss = 0.99 (292.6 examples/sec; 0.437 sec/batch)
2016-02-03 11:36:57.154780: step 13510, loss = 0.88 (290.8 examples/sec; 0.440 sec/batch)
2016-02-03 11:37:01.826408: step 13520, loss = 0.92 (308.4 examples/sec; 0.415 sec/batch)
2016-02-03 11:37:06.501317: step 13530, loss = 0.89 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 11:37:11.231614: step 13540, loss = 0.81 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 11:37:15.916548: step 13550, loss = 1.10 (255.3 examples/sec; 0.501 sec/batch)
2016-02-03 11:37:20.599989: step 13560, loss = 0.85 (258.4 examples/sec; 0.495 sec/batch)
2016-02-03 11:37:25.320004: step 13570, loss = 0.71 (254.7 examples/sec; 0.502 sec/batch)
2016-02-03 11:37:30.015065: step 13580, loss = 1.07 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 11:37:34.676375: step 13590, loss = 0.86 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 11:37:39.290398: step 13600, loss = 0.80 (252.8 examples/sec; 0.506 sec/batch)
2016-02-03 11:37:44.489277: step 13610, loss = 0.85 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 11:37:49.123495: step 13620, loss = 1.02 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 11:37:53.768425: step 13630, loss = 0.92 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 11:37:58.438338: step 13640, loss = 0.73 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 11:38:03.169799: step 13650, loss = 0.93 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 11:38:07.862426: step 13660, loss = 0.85 (257.6 examples/sec; 0.497 sec/batch)
2016-02-03 11:38:12.516968: step 13670, loss = 0.85 (256.8 examples/sec; 0.499 sec/batch)
2016-02-03 11:38:17.265332: step 13680, loss = 0.91 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 11:38:22.025126: step 13690, loss = 0.91 (269.3 examples/sec; 0.475 sec/batch)
2016-02-03 11:38:26.725050: step 13700, loss = 0.95 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 11:38:31.948097: step 13710, loss = 0.85 (289.9 examples/sec; 0.441 sec/batch)
2016-02-03 11:38:36.695089: step 13720, loss = 0.91 (292.0 examples/sec; 0.438 sec/batch)
2016-02-03 11:38:41.353945: step 13730, loss = 0.74 (259.1 examples/sec; 0.494 sec/batch)
2016-02-03 11:38:46.064354: step 13740, loss = 0.87 (270.5 examples/sec; 0.473 sec/batch)
2016-02-03 11:38:50.783013: step 13750, loss = 0.81 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 11:38:55.447673: step 13760, loss = 0.79 (289.7 examples/sec; 0.442 sec/batch)
2016-02-03 11:39:00.235130: step 13770, loss = 0.74 (255.4 examples/sec; 0.501 sec/batch)
2016-02-03 11:39:04.947031: step 13780, loss = 0.85 (254.2 examples/sec; 0.503 sec/batch)
2016-02-03 11:39:09.584902: step 13790, loss = 0.93 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 11:39:14.225723: step 13800, loss = 0.96 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 11:39:19.419821: step 13810, loss = 0.72 (261.0 examples/sec; 0.490 sec/batch)
2016-02-03 11:39:24.129517: step 13820, loss = 0.85 (250.5 examples/sec; 0.511 sec/batch)
2016-02-03 11:39:28.754439: step 13830, loss = 0.82 (259.3 examples/sec; 0.494 sec/batch)
2016-02-03 11:39:33.494457: step 13840, loss = 0.84 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 11:39:38.276686: step 13850, loss = 0.85 (264.2 examples/sec; 0.484 sec/batch)
2016-02-03 11:39:42.999667: step 13860, loss = 0.89 (268.0 examples/sec; 0.478 sec/batch)
2016-02-03 11:39:47.705270: step 13870, loss = 0.91 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 11:39:52.392878: step 13880, loss = 0.87 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 11:39:57.129254: step 13890, loss = 0.69 (284.1 examples/sec; 0.451 sec/batch)
2016-02-03 11:40:01.845344: step 13900, loss = 0.85 (279.6 examples/sec; 0.458 sec/batch)
2016-02-03 11:40:06.944061: step 13910, loss = 0.74 (317.7 examples/sec; 0.403 sec/batch)
2016-02-03 11:40:11.637396: step 13920, loss = 0.95 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 11:40:16.337965: step 13930, loss = 0.81 (246.7 examples/sec; 0.519 sec/batch)
2016-02-03 11:40:21.051069: step 13940, loss = 0.87 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 11:40:25.726063: step 13950, loss = 0.86 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 11:40:30.400332: step 13960, loss = 0.82 (253.7 examples/sec; 0.504 sec/batch)
2016-02-03 11:40:35.073985: step 13970, loss = 0.91 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 11:40:39.776998: step 13980, loss = 0.70 (259.4 examples/sec; 0.494 sec/batch)
2016-02-03 11:40:44.436662: step 13990, loss = 0.74 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 11:40:49.031563: step 14000, loss = 0.66 (282.5 examples/sec; 0.453 sec/batch)
2016-02-03 11:40:54.301033: step 14010, loss = 1.01 (283.3 examples/sec; 0.452 sec/batch)
2016-02-03 11:40:58.986710: step 14020, loss = 0.79 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 11:41:03.646308: step 14030, loss = 0.78 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 11:41:08.308581: step 14040, loss = 0.82 (276.4 examples/sec; 0.463 sec/batch)
2016-02-03 11:41:12.974564: step 14050, loss = 0.84 (286.0 examples/sec; 0.448 sec/batch)
2016-02-03 11:41:17.637099: step 14060, loss = 0.68 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 11:41:22.288276: step 14070, loss = 0.78 (316.5 examples/sec; 0.404 sec/batch)
2016-02-03 11:41:26.918745: step 14080, loss = 0.84 (276.9 examples/sec; 0.462 sec/batch)
2016-02-03 11:41:31.558656: step 14090, loss = 0.71 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 11:41:36.223646: step 14100, loss = 0.88 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 11:41:41.470326: step 14110, loss = 0.77 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 11:41:46.131274: step 14120, loss = 0.80 (290.8 examples/sec; 0.440 sec/batch)
2016-02-03 11:41:50.839468: step 14130, loss = 0.79 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 11:41:55.525298: step 14140, loss = 0.88 (260.3 examples/sec; 0.492 sec/batch)
2016-02-03 11:42:00.255288: step 14150, loss = 0.85 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 11:42:04.952772: step 14160, loss = 0.90 (270.9 examples/sec; 0.472 sec/batch)
2016-02-03 11:42:09.720225: step 14170, loss = 1.12 (246.9 examples/sec; 0.518 sec/batch)
2016-02-03 11:42:14.379311: step 14180, loss = 0.83 (293.2 examples/sec; 0.436 sec/batch)
2016-02-03 11:42:19.062106: step 14190, loss = 0.85 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 11:42:23.780689: step 14200, loss = 0.81 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 11:42:28.963575: step 14210, loss = 0.83 (265.8 examples/sec; 0.481 sec/batch)
2016-02-03 11:42:33.545640: step 14220, loss = 0.98 (306.5 examples/sec; 0.418 sec/batch)
2016-02-03 11:42:38.143492: step 14230, loss = 0.71 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 11:42:42.804648: step 14240, loss = 0.82 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 11:42:47.525017: step 14250, loss = 0.91 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 11:42:52.193953: step 14260, loss = 1.01 (276.6 examples/sec; 0.463 sec/batch)
2016-02-03 11:42:56.879638: step 14270, loss = 0.83 (301.2 examples/sec; 0.425 sec/batch)
2016-02-03 11:43:01.580358: step 14280, loss = 0.94 (261.0 examples/sec; 0.490 sec/batch)
2016-02-03 11:43:06.349854: step 14290, loss = 0.89 (287.5 examples/sec; 0.445 sec/batch)
2016-02-03 11:43:11.019113: step 14300, loss = 0.89 (281.7 examples/sec; 0.454 sec/batch)
2016-02-03 11:43:16.255750: step 14310, loss = 0.94 (260.9 examples/sec; 0.491 sec/batch)
2016-02-03 11:43:20.972801: step 14320, loss = 0.78 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 11:43:25.695461: step 14330, loss = 0.88 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 11:43:30.340805: step 14340, loss = 0.92 (299.0 examples/sec; 0.428 sec/batch)
2016-02-03 11:43:35.088316: step 14350, loss = 0.88 (279.2 examples/sec; 0.458 sec/batch)
2016-02-03 11:43:39.874103: step 14360, loss = 0.95 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 11:43:44.609093: step 14370, loss = 0.91 (249.8 examples/sec; 0.512 sec/batch)
2016-02-03 11:43:49.438967: step 14380, loss = 1.02 (236.1 examples/sec; 0.542 sec/batch)
2016-02-03 11:43:54.048877: step 14390, loss = 0.99 (280.4 examples/sec; 0.456 sec/batch)
2016-02-03 11:43:58.778335: step 14400, loss = 0.78 (293.2 examples/sec; 0.437 sec/batch)
2016-02-03 11:44:04.019154: step 14410, loss = 0.79 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 11:44:08.756895: step 14420, loss = 0.97 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 11:44:13.421636: step 14430, loss = 0.99 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 11:44:18.184381: step 14440, loss = 0.86 (244.9 examples/sec; 0.523 sec/batch)
2016-02-03 11:44:22.869694: step 14450, loss = 0.88 (288.1 examples/sec; 0.444 sec/batch)
2016-02-03 11:44:27.590703: step 14460, loss = 0.80 (250.1 examples/sec; 0.512 sec/batch)
2016-02-03 11:44:32.294419: step 14470, loss = 0.95 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 11:44:37.060912: step 14480, loss = 1.03 (257.2 examples/sec; 0.498 sec/batch)
2016-02-03 11:44:41.727197: step 14490, loss = 0.90 (282.6 examples/sec; 0.453 sec/batch)
2016-02-03 11:44:46.473789: step 14500, loss = 0.88 (288.3 examples/sec; 0.444 sec/batch)
2016-02-03 11:44:51.646758: step 14510, loss = 0.92 (288.6 examples/sec; 0.443 sec/batch)
2016-02-03 11:44:56.350181: step 14520, loss = 0.67 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 11:45:01.032531: step 14530, loss = 0.83 (280.3 examples/sec; 0.457 sec/batch)
2016-02-03 11:45:05.744663: step 14540, loss = 0.84 (292.7 examples/sec; 0.437 sec/batch)
2016-02-03 11:45:10.501169: step 14550, loss = 0.81 (246.0 examples/sec; 0.520 sec/batch)
2016-02-03 11:45:15.180413: step 14560, loss = 0.86 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 11:45:19.886147: step 14570, loss = 0.80 (279.2 examples/sec; 0.458 sec/batch)
2016-02-03 11:45:24.601409: step 14580, loss = 1.01 (267.0 examples/sec; 0.479 sec/batch)
2016-02-03 11:45:29.280664: step 14590, loss = 0.78 (258.3 examples/sec; 0.495 sec/batch)
2016-02-03 11:45:33.977306: step 14600, loss = 0.87 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 11:45:39.072350: step 14610, loss = 0.79 (292.6 examples/sec; 0.437 sec/batch)
2016-02-03 11:45:43.762869: step 14620, loss = 0.91 (285.9 examples/sec; 0.448 sec/batch)
2016-02-03 11:45:48.503414: step 14630, loss = 1.06 (255.1 examples/sec; 0.502 sec/batch)
2016-02-03 11:45:53.151327: step 14640, loss = 0.73 (280.6 examples/sec; 0.456 sec/batch)
2016-02-03 11:45:57.789952: step 14650, loss = 0.85 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 11:46:02.572745: step 14660, loss = 0.72 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 11:46:07.226305: step 14670, loss = 0.83 (257.8 examples/sec; 0.497 sec/batch)
2016-02-03 11:46:11.925483: step 14680, loss = 0.86 (286.7 examples/sec; 0.446 sec/batch)
2016-02-03 11:46:16.475647: step 14690, loss = 0.80 (284.6 examples/sec; 0.450 sec/batch)
2016-02-03 11:46:21.087432: step 14700, loss = 0.85 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 11:46:26.199380: step 14710, loss = 0.96 (277.9 examples/sec; 0.461 sec/batch)
2016-02-03 11:46:30.719106: step 14720, loss = 0.93 (287.4 examples/sec; 0.445 sec/batch)
2016-02-03 11:46:35.370418: step 14730, loss = 0.86 (285.4 examples/sec; 0.449 sec/batch)
2016-02-03 11:46:40.034389: step 14740, loss = 0.81 (261.1 examples/sec; 0.490 sec/batch)
2016-02-03 11:46:44.741558: step 14750, loss = 0.82 (289.7 examples/sec; 0.442 sec/batch)
2016-02-03 11:46:49.476438: step 14760, loss = 0.83 (257.7 examples/sec; 0.497 sec/batch)
2016-02-03 11:46:54.190070: step 14770, loss = 0.91 (266.4 examples/sec; 0.480 sec/batch)
2016-02-03 11:46:58.916306: step 14780, loss = 0.86 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 11:47:03.640954: step 14790, loss = 0.94 (282.2 examples/sec; 0.454 sec/batch)
2016-02-03 11:47:08.347992: step 14800, loss = 0.81 (281.4 examples/sec; 0.455 sec/batch)
2016-02-03 11:47:13.524096: step 14810, loss = 0.88 (263.1 examples/sec; 0.486 sec/batch)
2016-02-03 11:47:18.235128: step 14820, loss = 0.87 (307.2 examples/sec; 0.417 sec/batch)
2016-02-03 11:47:22.985804: step 14830, loss = 0.76 (291.5 examples/sec; 0.439 sec/batch)
2016-02-03 11:47:27.684381: step 14840, loss = 0.84 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 11:47:32.374991: step 14850, loss = 0.87 (245.7 examples/sec; 0.521 sec/batch)
2016-02-03 11:47:37.051243: step 14860, loss = 0.84 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 11:47:41.792393: step 14870, loss = 0.94 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 11:47:46.473366: step 14880, loss = 0.88 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 11:47:51.186501: step 14890, loss = 0.99 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 11:47:55.866807: step 14900, loss = 0.76 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 11:48:01.169886: step 14910, loss = 0.77 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 11:48:05.901742: step 14920, loss = 0.91 (246.8 examples/sec; 0.519 sec/batch)
2016-02-03 11:48:10.534991: step 14930, loss = 0.98 (308.4 examples/sec; 0.415 sec/batch)
2016-02-03 11:48:15.273733: step 14940, loss = 0.83 (242.9 examples/sec; 0.527 sec/batch)
2016-02-03 11:48:19.966142: step 14950, loss = 0.88 (248.3 examples/sec; 0.515 sec/batch)
2016-02-03 11:48:24.649960: step 14960, loss = 0.82 (281.4 examples/sec; 0.455 sec/batch)
2016-02-03 11:48:29.390787: step 14970, loss = 0.87 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 11:48:34.137230: step 14980, loss = 0.79 (259.7 examples/sec; 0.493 sec/batch)
2016-02-03 11:48:38.795729: step 14990, loss = 0.96 (311.6 examples/sec; 0.411 sec/batch)
2016-02-03 11:48:43.531755: step 15000, loss = 0.79 (284.6 examples/sec; 0.450 sec/batch)
2016-02-03 11:48:48.769524: step 15010, loss = 0.81 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 11:48:53.413328: step 15020, loss = 0.93 (287.8 examples/sec; 0.445 sec/batch)
2016-02-03 11:48:58.097253: step 15030, loss = 0.84 (282.4 examples/sec; 0.453 sec/batch)
2016-02-03 11:49:02.738748: step 15040, loss = 0.95 (282.8 examples/sec; 0.453 sec/batch)
2016-02-03 11:49:07.364000: step 15050, loss = 0.73 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 11:49:11.988543: step 15060, loss = 0.77 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 11:49:16.566206: step 15070, loss = 0.98 (288.5 examples/sec; 0.444 sec/batch)
2016-02-03 11:49:21.255151: step 15080, loss = 0.98 (288.0 examples/sec; 0.444 sec/batch)
2016-02-03 11:49:25.966525: step 15090, loss = 0.95 (265.3 examples/sec; 0.483 sec/batch)
2016-02-03 11:49:30.632227: step 15100, loss = 1.04 (277.8 examples/sec; 0.461 sec/batch)
2016-02-03 11:49:35.808147: step 15110, loss = 1.05 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 11:49:40.479515: step 15120, loss = 0.91 (286.1 examples/sec; 0.447 sec/batch)
2016-02-03 11:49:45.118055: step 15130, loss = 0.71 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 11:49:49.860629: step 15140, loss = 0.93 (285.9 examples/sec; 0.448 sec/batch)
2016-02-03 11:49:54.516498: step 15150, loss = 0.95 (301.6 examples/sec; 0.424 sec/batch)
2016-02-03 11:49:59.185183: step 15160, loss = 0.84 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 11:50:03.900561: step 15170, loss = 0.80 (256.1 examples/sec; 0.500 sec/batch)
2016-02-03 11:50:08.566733: step 15180, loss = 0.86 (258.7 examples/sec; 0.495 sec/batch)
2016-02-03 11:50:13.228968: step 15190, loss = 0.85 (266.4 examples/sec; 0.481 sec/batch)
2016-02-03 11:50:17.999415: step 15200, loss = 1.03 (250.1 examples/sec; 0.512 sec/batch)
2016-02-03 11:50:23.196026: step 15210, loss = 0.83 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 11:50:27.843476: step 15220, loss = 0.87 (326.6 examples/sec; 0.392 sec/batch)
2016-02-03 11:50:32.546792: step 15230, loss = 0.77 (255.0 examples/sec; 0.502 sec/batch)
2016-02-03 11:50:37.182374: step 15240, loss = 0.93 (280.3 examples/sec; 0.457 sec/batch)
2016-02-03 11:50:41.886647: step 15250, loss = 0.93 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 11:50:46.634791: step 15260, loss = 0.76 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 11:50:51.401371: step 15270, loss = 0.86 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 11:50:56.129382: step 15280, loss = 0.97 (281.5 examples/sec; 0.455 sec/batch)
2016-02-03 11:51:00.865263: step 15290, loss = 0.84 (281.0 examples/sec; 0.455 sec/batch)
2016-02-03 11:51:05.614399: step 15300, loss = 0.83 (276.6 examples/sec; 0.463 sec/batch)
2016-02-03 11:51:10.757267: step 15310, loss = 0.87 (290.0 examples/sec; 0.441 sec/batch)
2016-02-03 11:51:15.569489: step 15320, loss = 1.14 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 11:51:20.322718: step 15330, loss = 0.73 (259.4 examples/sec; 0.494 sec/batch)
2016-02-03 11:51:24.927870: step 15340, loss = 0.80 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 11:51:29.673629: step 15350, loss = 0.85 (257.4 examples/sec; 0.497 sec/batch)
2016-02-03 11:51:34.417728: step 15360, loss = 0.80 (259.5 examples/sec; 0.493 sec/batch)
2016-02-03 11:51:39.071583: step 15370, loss = 0.93 (299.9 examples/sec; 0.427 sec/batch)
2016-02-03 11:51:43.781135: step 15380, loss = 0.86 (273.2 examples/sec; 0.468 sec/batch)
2016-02-03 11:51:48.465825: step 15390, loss = 0.96 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 11:51:53.230469: step 15400, loss = 0.91 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 11:51:58.448488: step 15410, loss = 0.83 (284.4 examples/sec; 0.450 sec/batch)
2016-02-03 11:52:03.124777: step 15420, loss = 0.80 (279.4 examples/sec; 0.458 sec/batch)
2016-02-03 11:52:07.852594: step 15430, loss = 0.78 (299.6 examples/sec; 0.427 sec/batch)
2016-02-03 11:52:12.538021: step 15440, loss = 0.86 (289.4 examples/sec; 0.442 sec/batch)
2016-02-03 11:52:17.326265: step 15450, loss = 0.79 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 11:52:21.978526: step 15460, loss = 0.83 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 11:52:26.671770: step 15470, loss = 0.95 (265.0 examples/sec; 0.483 sec/batch)
2016-02-03 11:52:31.349367: step 15480, loss = 0.86 (283.3 examples/sec; 0.452 sec/batch)
2016-02-03 11:52:36.105822: step 15490, loss = 0.86 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 11:52:40.812998: step 15500, loss = 0.84 (288.8 examples/sec; 0.443 sec/batch)
2016-02-03 11:52:46.050874: step 15510, loss = 0.90 (249.9 examples/sec; 0.512 sec/batch)
2016-02-03 11:52:50.754097: step 15520, loss = 0.75 (274.4 examples/sec; 0.467 sec/batch)
2016-02-03 11:52:55.487497: step 15530, loss = 0.89 (320.3 examples/sec; 0.400 sec/batch)
2016-02-03 11:53:00.173121: step 15540, loss = 0.79 (285.8 examples/sec; 0.448 sec/batch)
2016-02-03 11:53:04.916027: step 15550, loss = 0.89 (264.2 examples/sec; 0.484 sec/batch)
2016-02-03 11:53:09.601984: step 15560, loss = 0.80 (277.6 examples/sec; 0.461 sec/batch)
2016-02-03 11:53:14.370438: step 15570, loss = 0.89 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 11:53:19.050207: step 15580, loss = 0.78 (269.2 examples/sec; 0.475 sec/batch)
2016-02-03 11:53:23.846616: step 15590, loss = 0.79 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 11:53:28.453416: step 15600, loss = 0.93 (292.6 examples/sec; 0.437 sec/batch)
2016-02-03 11:53:33.697946: step 15610, loss = 0.81 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 11:53:38.434691: step 15620, loss = 0.84 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 11:53:43.189682: step 15630, loss = 0.82 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 11:53:47.950852: step 15640, loss = 0.92 (251.5 examples/sec; 0.509 sec/batch)
2016-02-03 11:53:52.578502: step 15650, loss = 0.83 (276.3 examples/sec; 0.463 sec/batch)
2016-02-03 11:53:57.295441: step 15660, loss = 0.97 (291.5 examples/sec; 0.439 sec/batch)
2016-02-03 11:54:02.087056: step 15670, loss = 0.76 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 11:54:06.787685: step 15680, loss = 0.98 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 11:54:11.469300: step 15690, loss = 0.91 (246.5 examples/sec; 0.519 sec/batch)
2016-02-03 11:54:16.078303: step 15700, loss = 0.86 (277.2 examples/sec; 0.462 sec/batch)
2016-02-03 11:54:21.310993: step 15710, loss = 0.80 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 11:54:26.011011: step 15720, loss = 0.77 (260.1 examples/sec; 0.492 sec/batch)
2016-02-03 11:54:30.748933: step 15730, loss = 0.87 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 11:54:35.449823: step 15740, loss = 0.80 (253.6 examples/sec; 0.505 sec/batch)
2016-02-03 11:54:40.188909: step 15750, loss = 0.84 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 11:54:44.941041: step 15760, loss = 0.89 (255.1 examples/sec; 0.502 sec/batch)
2016-02-03 11:54:49.674236: step 15770, loss = 0.81 (245.3 examples/sec; 0.522 sec/batch)
2016-02-03 11:54:54.334541: step 15780, loss = 0.80 (261.1 examples/sec; 0.490 sec/batch)
2016-02-03 11:54:59.068838: step 15790, loss = 0.91 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 11:55:03.816078: step 15800, loss = 0.69 (257.8 examples/sec; 0.496 sec/batch)
2016-02-03 11:55:09.039909: step 15810, loss = 0.68 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 11:55:13.765073: step 15820, loss = 0.81 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 11:55:18.488499: step 15830, loss = 0.92 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 11:55:23.186599: step 15840, loss = 0.93 (254.5 examples/sec; 0.503 sec/batch)
2016-02-03 11:55:27.795027: step 15850, loss = 0.85 (286.8 examples/sec; 0.446 sec/batch)
2016-02-03 11:55:32.591521: step 15860, loss = 0.79 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 11:55:37.383395: step 15870, loss = 0.80 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 11:55:42.097660: step 15880, loss = 0.86 (281.6 examples/sec; 0.454 sec/batch)
2016-02-03 11:55:46.924549: step 15890, loss = 0.78 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 11:55:51.615503: step 15900, loss = 0.88 (288.0 examples/sec; 0.444 sec/batch)
2016-02-03 11:55:56.835660: step 15910, loss = 0.82 (299.5 examples/sec; 0.427 sec/batch)
2016-02-03 11:56:01.574311: step 15920, loss = 0.78 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 11:56:06.296360: step 15930, loss = 0.80 (260.3 examples/sec; 0.492 sec/batch)
2016-02-03 11:56:11.029492: step 15940, loss = 0.76 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 11:56:15.697556: step 15950, loss = 1.01 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 11:56:20.420828: step 15960, loss = 0.98 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 11:56:25.113620: step 15970, loss = 0.81 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 11:56:29.814257: step 15980, loss = 0.79 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 11:56:34.472874: step 15990, loss = 0.96 (272.7 examples/sec; 0.469 sec/batch)
2016-02-03 11:56:39.144598: step 16000, loss = 0.88 (285.9 examples/sec; 0.448 sec/batch)
2016-02-03 11:56:44.444974: step 16010, loss = 0.90 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 11:56:49.175510: step 16020, loss = 0.75 (247.6 examples/sec; 0.517 sec/batch)
2016-02-03 11:56:53.861011: step 16030, loss = 0.83 (283.9 examples/sec; 0.451 sec/batch)
2016-02-03 11:56:58.578954: step 16040, loss = 1.02 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 11:57:03.293082: step 16050, loss = 0.90 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 11:57:07.976676: step 16060, loss = 0.84 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 11:57:12.687690: step 16070, loss = 0.91 (250.0 examples/sec; 0.512 sec/batch)
2016-02-03 11:57:17.342164: step 16080, loss = 1.02 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 11:57:22.060055: step 16090, loss = 0.71 (249.2 examples/sec; 0.514 sec/batch)
2016-02-03 11:57:26.794098: step 16100, loss = 0.95 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 11:57:31.968371: step 16110, loss = 0.75 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 11:57:36.652718: step 16120, loss = 0.93 (252.7 examples/sec; 0.506 sec/batch)
2016-02-03 11:57:41.339398: step 16130, loss = 0.94 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 11:57:46.001614: step 16140, loss = 0.85 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 11:57:50.660154: step 16150, loss = 0.82 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 11:57:55.395723: step 16160, loss = 0.78 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 11:58:00.060516: step 16170, loss = 0.78 (296.1 examples/sec; 0.432 sec/batch)
2016-02-03 11:58:04.751532: step 16180, loss = 0.86 (265.8 examples/sec; 0.481 sec/batch)
2016-02-03 11:58:09.518449: step 16190, loss = 0.84 (259.9 examples/sec; 0.493 sec/batch)
2016-02-03 11:58:14.227476: step 16200, loss = 0.84 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 11:58:19.471841: step 16210, loss = 0.82 (253.0 examples/sec; 0.506 sec/batch)
2016-02-03 11:58:24.120285: step 16220, loss = 0.76 (247.0 examples/sec; 0.518 sec/batch)
2016-02-03 11:58:28.808691: step 16230, loss = 0.81 (260.7 examples/sec; 0.491 sec/batch)
2016-02-03 11:58:33.577477: step 16240, loss = 0.75 (252.7 examples/sec; 0.507 sec/batch)
2016-02-03 11:58:38.261923: step 16250, loss = 0.95 (279.4 examples/sec; 0.458 sec/batch)
2016-02-03 11:58:42.891729: step 16260, loss = 0.79 (306.3 examples/sec; 0.418 sec/batch)
2016-02-03 11:58:47.560989: step 16270, loss = 0.85 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 11:58:52.280223: step 16280, loss = 0.87 (253.7 examples/sec; 0.505 sec/batch)
2016-02-03 11:58:56.940575: step 16290, loss = 0.85 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 11:59:01.688571: step 16300, loss = 0.82 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 11:59:06.884105: step 16310, loss = 0.63 (266.9 examples/sec; 0.480 sec/batch)
2016-02-03 11:59:11.552302: step 16320, loss = 1.06 (286.2 examples/sec; 0.447 sec/batch)
2016-02-03 11:59:16.262906: step 16330, loss = 0.74 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 11:59:20.985704: step 16340, loss = 0.69 (302.5 examples/sec; 0.423 sec/batch)
2016-02-03 11:59:25.755483: step 16350, loss = 0.81 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 11:59:30.475589: step 16360, loss = 0.82 (279.7 examples/sec; 0.458 sec/batch)
2016-02-03 11:59:35.169809: step 16370, loss = 0.89 (259.1 examples/sec; 0.494 sec/batch)
2016-02-03 11:59:39.845673: step 16380, loss = 0.92 (280.4 examples/sec; 0.456 sec/batch)
2016-02-03 11:59:44.512956: step 16390, loss = 0.80 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 11:59:49.134274: step 16400, loss = 0.91 (277.2 examples/sec; 0.462 sec/batch)
2016-02-03 11:59:54.389196: step 16410, loss = 0.72 (283.5 examples/sec; 0.452 sec/batch)
2016-02-03 11:59:59.143077: step 16420, loss = 0.81 (289.4 examples/sec; 0.442 sec/batch)
2016-02-03 12:00:03.813261: step 16430, loss = 1.01 (279.7 examples/sec; 0.458 sec/batch)
2016-02-03 12:00:08.474778: step 16440, loss = 0.93 (255.8 examples/sec; 0.500 sec/batch)
2016-02-03 12:00:13.113958: step 16450, loss = 0.94 (312.2 examples/sec; 0.410 sec/batch)
2016-02-03 12:00:17.797394: step 16460, loss = 0.68 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 12:00:22.417946: step 16470, loss = 0.84 (293.0 examples/sec; 0.437 sec/batch)
2016-02-03 12:00:27.077079: step 16480, loss = 0.79 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 12:00:31.714271: step 16490, loss = 0.81 (257.3 examples/sec; 0.498 sec/batch)
2016-02-03 12:00:36.367344: step 16500, loss = 0.90 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 12:00:41.620436: step 16510, loss = 0.80 (251.9 examples/sec; 0.508 sec/batch)
2016-02-03 12:00:46.217427: step 16520, loss = 0.85 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 12:00:50.827574: step 16530, loss = 0.84 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 12:00:55.460448: step 16540, loss = 0.75 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 12:01:00.142757: step 16550, loss = 0.85 (250.2 examples/sec; 0.512 sec/batch)
2016-02-03 12:01:04.870738: step 16560, loss = 0.95 (258.0 examples/sec; 0.496 sec/batch)
2016-02-03 12:01:09.540097: step 16570, loss = 0.77 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 12:01:14.252531: step 16580, loss = 0.85 (258.2 examples/sec; 0.496 sec/batch)
2016-02-03 12:01:18.875273: step 16590, loss = 0.76 (286.6 examples/sec; 0.447 sec/batch)
2016-02-03 12:01:23.603015: step 16600, loss = 0.72 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 12:01:28.786782: step 16610, loss = 0.94 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 12:01:33.343052: step 16620, loss = 0.96 (267.2 examples/sec; 0.479 sec/batch)
2016-02-03 12:01:38.120421: step 16630, loss = 0.89 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 12:01:42.865684: step 16640, loss = 0.87 (233.4 examples/sec; 0.548 sec/batch)
2016-02-03 12:01:47.586157: step 16650, loss = 0.93 (256.9 examples/sec; 0.498 sec/batch)
2016-02-03 12:01:52.234928: step 16660, loss = 1.01 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 12:01:56.922662: step 16670, loss = 0.68 (255.2 examples/sec; 0.502 sec/batch)
2016-02-03 12:02:01.583208: step 16680, loss = 0.92 (282.5 examples/sec; 0.453 sec/batch)
2016-02-03 12:02:06.297283: step 16690, loss = 1.00 (298.8 examples/sec; 0.428 sec/batch)
2016-02-03 12:02:11.034011: step 16700, loss = 0.77 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 12:02:16.272028: step 16710, loss = 0.83 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 12:02:20.930060: step 16720, loss = 0.86 (293.1 examples/sec; 0.437 sec/batch)
2016-02-03 12:02:25.677937: step 16730, loss = 0.87 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 12:02:30.364596: step 16740, loss = 1.03 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 12:02:35.107859: step 16750, loss = 0.86 (250.9 examples/sec; 0.510 sec/batch)
2016-02-03 12:02:39.888177: step 16760, loss = 0.95 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 12:02:44.637315: step 16770, loss = 0.72 (261.1 examples/sec; 0.490 sec/batch)
2016-02-03 12:02:49.352686: step 16780, loss = 0.87 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 12:02:53.994392: step 16790, loss = 0.81 (311.5 examples/sec; 0.411 sec/batch)
2016-02-03 12:02:58.731510: step 16800, loss = 0.96 (276.4 examples/sec; 0.463 sec/batch)
2016-02-03 12:03:03.926891: step 16810, loss = 0.82 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 12:03:08.651392: step 16820, loss = 0.76 (258.6 examples/sec; 0.495 sec/batch)
2016-02-03 12:03:13.393376: step 16830, loss = 0.84 (294.5 examples/sec; 0.435 sec/batch)
2016-02-03 12:03:18.108294: step 16840, loss = 0.83 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 12:03:22.811140: step 16850, loss = 0.93 (280.9 examples/sec; 0.456 sec/batch)
2016-02-03 12:03:27.446941: step 16860, loss = 0.85 (298.4 examples/sec; 0.429 sec/batch)
2016-02-03 12:03:32.255713: step 16870, loss = 0.97 (251.2 examples/sec; 0.510 sec/batch)
2016-02-03 12:03:37.002678: step 16880, loss = 0.80 (261.4 examples/sec; 0.490 sec/batch)
2016-02-03 12:03:41.711881: step 16890, loss = 0.80 (286.0 examples/sec; 0.448 sec/batch)
2016-02-03 12:03:46.355950: step 16900, loss = 0.90 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 12:03:51.603655: step 16910, loss = 0.97 (248.9 examples/sec; 0.514 sec/batch)
2016-02-03 12:03:56.401588: step 16920, loss = 0.81 (256.5 examples/sec; 0.499 sec/batch)
2016-02-03 12:04:01.166163: step 16930, loss = 1.05 (269.3 examples/sec; 0.475 sec/batch)
2016-02-03 12:04:05.890126: step 16940, loss = 0.87 (260.7 examples/sec; 0.491 sec/batch)
2016-02-03 12:04:10.528111: step 16950, loss = 0.82 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 12:04:15.226661: step 16960, loss = 1.00 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 12:04:19.934571: step 16970, loss = 0.97 (289.5 examples/sec; 0.442 sec/batch)
2016-02-03 12:04:24.523045: step 16980, loss = 1.00 (286.6 examples/sec; 0.447 sec/batch)
2016-02-03 12:04:29.198384: step 16990, loss = 1.05 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 12:04:33.880077: step 17000, loss = 0.89 (267.2 examples/sec; 0.479 sec/batch)
2016-02-03 12:04:39.024545: step 17010, loss = 0.76 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 12:04:43.793962: step 17020, loss = 0.75 (251.1 examples/sec; 0.510 sec/batch)
2016-02-03 12:04:48.532223: step 17030, loss = 0.91 (308.0 examples/sec; 0.416 sec/batch)
2016-02-03 12:04:53.193985: step 17040, loss = 0.85 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 12:04:57.869719: step 17050, loss = 0.83 (257.5 examples/sec; 0.497 sec/batch)
2016-02-03 12:05:02.559161: step 17060, loss = 1.00 (260.0 examples/sec; 0.492 sec/batch)
2016-02-03 12:05:07.170736: step 17070, loss = 0.91 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 12:05:11.914791: step 17080, loss = 0.81 (279.6 examples/sec; 0.458 sec/batch)
2016-02-03 12:05:16.627371: step 17090, loss = 0.89 (261.0 examples/sec; 0.490 sec/batch)
2016-02-03 12:05:21.276015: step 17100, loss = 0.76 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 12:05:26.453845: step 17110, loss = 0.89 (303.8 examples/sec; 0.421 sec/batch)
2016-02-03 12:05:31.140812: step 17120, loss = 0.76 (264.3 examples/sec; 0.484 sec/batch)
2016-02-03 12:05:35.940304: step 17130, loss = 0.96 (288.9 examples/sec; 0.443 sec/batch)
2016-02-03 12:05:40.542449: step 17140, loss = 0.81 (287.7 examples/sec; 0.445 sec/batch)
2016-02-03 12:05:45.233226: step 17150, loss = 0.77 (293.4 examples/sec; 0.436 sec/batch)
2016-02-03 12:05:49.898628: step 17160, loss = 0.76 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 12:05:54.603356: step 17170, loss = 0.92 (309.0 examples/sec; 0.414 sec/batch)
2016-02-03 12:05:59.215860: step 17180, loss = 0.84 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 12:06:03.890797: step 17190, loss = 0.88 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 12:06:08.583408: step 17200, loss = 0.92 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 12:06:13.845710: step 17210, loss = 0.68 (310.3 examples/sec; 0.413 sec/batch)
2016-02-03 12:06:18.488866: step 17220, loss = 0.98 (308.3 examples/sec; 0.415 sec/batch)
2016-02-03 12:06:23.198400: step 17230, loss = 0.80 (257.5 examples/sec; 0.497 sec/batch)
2016-02-03 12:06:27.830108: step 17240, loss = 0.74 (306.5 examples/sec; 0.418 sec/batch)
2016-02-03 12:06:32.524414: step 17250, loss = 0.81 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 12:06:37.187156: step 17260, loss = 0.84 (256.7 examples/sec; 0.499 sec/batch)
2016-02-03 12:06:41.772891: step 17270, loss = 0.73 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 12:06:46.475527: step 17280, loss = 0.94 (255.1 examples/sec; 0.502 sec/batch)
2016-02-03 12:06:51.120562: step 17290, loss = 0.88 (254.8 examples/sec; 0.502 sec/batch)
2016-02-03 12:06:55.716275: step 17300, loss = 0.93 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 12:07:00.956044: step 17310, loss = 0.88 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 12:07:05.573017: step 17320, loss = 0.74 (299.1 examples/sec; 0.428 sec/batch)
2016-02-03 12:07:10.383055: step 17330, loss = 0.87 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 12:07:15.051495: step 17340, loss = 0.91 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 12:07:19.587518: step 17350, loss = 0.73 (309.4 examples/sec; 0.414 sec/batch)
2016-02-03 12:07:24.298653: step 17360, loss = 0.91 (241.0 examples/sec; 0.531 sec/batch)
2016-02-03 12:07:28.952224: step 17370, loss = 1.01 (300.9 examples/sec; 0.425 sec/batch)
2016-02-03 12:07:33.646197: step 17380, loss = 0.89 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 12:07:38.325321: step 17390, loss = 0.89 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 12:07:43.047507: step 17400, loss = 0.89 (258.9 examples/sec; 0.494 sec/batch)
2016-02-03 12:07:48.312368: step 17410, loss = 0.84 (263.9 examples/sec; 0.485 sec/batch)
2016-02-03 12:07:53.027871: step 17420, loss = 0.79 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 12:07:57.703072: step 17430, loss = 0.83 (252.1 examples/sec; 0.508 sec/batch)
2016-02-03 12:08:02.440418: step 17440, loss = 0.88 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 12:08:07.136962: step 17450, loss = 0.81 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 12:08:11.817177: step 17460, loss = 0.96 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 12:08:16.457407: step 17470, loss = 0.88 (301.1 examples/sec; 0.425 sec/batch)
2016-02-03 12:08:21.155571: step 17480, loss = 0.76 (296.0 examples/sec; 0.432 sec/batch)
2016-02-03 12:08:25.916438: step 17490, loss = 0.82 (252.9 examples/sec; 0.506 sec/batch)
2016-02-03 12:08:30.524140: step 17500, loss = 0.78 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 12:08:35.726627: step 17510, loss = 0.78 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 12:08:40.431228: step 17520, loss = 0.98 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 12:08:45.112087: step 17530, loss = 0.71 (294.4 examples/sec; 0.435 sec/batch)
2016-02-03 12:08:49.800530: step 17540, loss = 0.74 (290.9 examples/sec; 0.440 sec/batch)
2016-02-03 12:08:54.469048: step 17550, loss = 0.86 (298.2 examples/sec; 0.429 sec/batch)
2016-02-03 12:08:59.269244: step 17560, loss = 0.92 (256.8 examples/sec; 0.498 sec/batch)
2016-02-03 12:09:04.065768: step 17570, loss = 0.89 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 12:09:08.704549: step 17580, loss = 0.93 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 12:09:13.384671: step 17590, loss = 0.94 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 12:09:18.102429: step 17600, loss = 0.92 (253.9 examples/sec; 0.504 sec/batch)
2016-02-03 12:09:23.214381: step 17610, loss = 0.92 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 12:09:27.900708: step 17620, loss = 0.91 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 12:09:32.601762: step 17630, loss = 0.74 (251.8 examples/sec; 0.508 sec/batch)
2016-02-03 12:09:37.242545: step 17640, loss = 0.76 (287.9 examples/sec; 0.445 sec/batch)
2016-02-03 12:09:41.854444: step 17650, loss = 0.75 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 12:09:46.583957: step 17660, loss = 0.83 (285.7 examples/sec; 0.448 sec/batch)
2016-02-03 12:09:51.303948: step 17670, loss = 0.80 (248.9 examples/sec; 0.514 sec/batch)
2016-02-03 12:09:55.960714: step 17680, loss = 0.85 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 12:10:00.656066: step 17690, loss = 0.93 (252.7 examples/sec; 0.507 sec/batch)
2016-02-03 12:10:05.306650: step 17700, loss = 0.98 (297.0 examples/sec; 0.431 sec/batch)
2016-02-03 12:10:10.466644: step 17710, loss = 0.80 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 12:10:15.094243: step 17720, loss = 0.81 (292.4 examples/sec; 0.438 sec/batch)
2016-02-03 12:10:19.794896: step 17730, loss = 0.72 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 12:10:24.581916: step 17740, loss = 0.92 (250.8 examples/sec; 0.510 sec/batch)
2016-02-03 12:10:29.166645: step 17750, loss = 0.74 (277.9 examples/sec; 0.461 sec/batch)
2016-02-03 12:10:33.845260: step 17760, loss = 0.76 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 12:10:38.561047: step 17770, loss = 1.02 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 12:10:43.335375: step 17780, loss = 0.85 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 12:10:48.039992: step 17790, loss = 0.82 (279.8 examples/sec; 0.457 sec/batch)
2016-02-03 12:10:52.686989: step 17800, loss = 0.81 (297.8 examples/sec; 0.430 sec/batch)
2016-02-03 12:10:57.809223: step 17810, loss = 0.90 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 12:11:02.377124: step 17820, loss = 1.08 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 12:11:06.928351: step 17830, loss = 0.86 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 12:11:11.562019: step 17840, loss = 0.86 (280.4 examples/sec; 0.456 sec/batch)
2016-02-03 12:11:16.166504: step 17850, loss = 0.76 (266.4 examples/sec; 0.480 sec/batch)
2016-02-03 12:11:20.867139: step 17860, loss = 0.82 (269.3 examples/sec; 0.475 sec/batch)
2016-02-03 12:11:25.550866: step 17870, loss = 0.87 (267.2 examples/sec; 0.479 sec/batch)
2016-02-03 12:11:30.204366: step 17880, loss = 0.71 (291.4 examples/sec; 0.439 sec/batch)
2016-02-03 12:11:34.899848: step 17890, loss = 0.86 (306.1 examples/sec; 0.418 sec/batch)
2016-02-03 12:11:39.534339: step 17900, loss = 0.79 (287.5 examples/sec; 0.445 sec/batch)
2016-02-03 12:11:44.779889: step 17910, loss = 0.88 (260.9 examples/sec; 0.491 sec/batch)
2016-02-03 12:11:49.519625: step 17920, loss = 0.75 (265.0 examples/sec; 0.483 sec/batch)
2016-02-03 12:11:54.199265: step 17930, loss = 0.90 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 12:11:58.927753: step 17940, loss = 0.65 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 12:12:03.599765: step 17950, loss = 0.72 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 12:12:08.329444: step 17960, loss = 0.75 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 12:12:13.007581: step 17970, loss = 0.98 (267.5 examples/sec; 0.479 sec/batch)
2016-02-03 12:12:17.665591: step 17980, loss = 0.84 (283.4 examples/sec; 0.452 sec/batch)
2016-02-03 12:12:22.284504: step 17990, loss = 0.94 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 12:12:26.981616: step 18000, loss = 0.69 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 12:12:32.273055: step 18010, loss = 0.98 (241.8 examples/sec; 0.529 sec/batch)
2016-02-03 12:12:36.969486: step 18020, loss = 0.81 (254.2 examples/sec; 0.503 sec/batch)
2016-02-03 12:12:41.532691: step 18030, loss = 0.69 (290.5 examples/sec; 0.441 sec/batch)
2016-02-03 12:12:46.249056: step 18040, loss = 0.89 (272.7 examples/sec; 0.469 sec/batch)
2016-02-03 12:12:50.960081: step 18050, loss = 0.78 (249.5 examples/sec; 0.513 sec/batch)
2016-02-03 12:12:55.570718: step 18060, loss = 1.01 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 12:13:00.195694: step 18070, loss = 0.73 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 12:13:04.960686: step 18080, loss = 0.80 (259.7 examples/sec; 0.493 sec/batch)
2016-02-03 12:13:09.599451: step 18090, loss = 0.85 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 12:13:14.302777: step 18100, loss = 0.79 (281.1 examples/sec; 0.455 sec/batch)
2016-02-03 12:13:19.443918: step 18110, loss = 0.75 (292.6 examples/sec; 0.437 sec/batch)
2016-02-03 12:13:24.148134: step 18120, loss = 1.00 (256.3 examples/sec; 0.499 sec/batch)
2016-02-03 12:13:28.852624: step 18130, loss = 0.92 (284.1 examples/sec; 0.451 sec/batch)
2016-02-03 12:13:33.600446: step 18140, loss = 0.75 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 12:13:38.272725: step 18150, loss = 0.80 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 12:13:42.907966: step 18160, loss = 0.89 (279.8 examples/sec; 0.457 sec/batch)
2016-02-03 12:13:47.727573: step 18170, loss = 0.97 (241.8 examples/sec; 0.529 sec/batch)
2016-02-03 12:13:52.391457: step 18180, loss = 0.72 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 12:13:57.260836: step 18190, loss = 0.80 (252.2 examples/sec; 0.508 sec/batch)
2016-02-03 12:14:01.916691: step 18200, loss = 0.70 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 12:14:07.134587: step 18210, loss = 0.74 (316.0 examples/sec; 0.405 sec/batch)
2016-02-03 12:14:11.912892: step 18220, loss = 0.70 (260.3 examples/sec; 0.492 sec/batch)
2016-02-03 12:14:16.644260: step 18230, loss = 0.92 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 12:14:21.379103: step 18240, loss = 0.83 (260.0 examples/sec; 0.492 sec/batch)
2016-02-03 12:14:26.098869: step 18250, loss = 1.00 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 12:14:30.801387: step 18260, loss = 0.78 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 12:14:35.513441: step 18270, loss = 0.91 (284.4 examples/sec; 0.450 sec/batch)
2016-02-03 12:14:40.193227: step 18280, loss = 0.88 (288.0 examples/sec; 0.444 sec/batch)
2016-02-03 12:14:44.748711: step 18290, loss = 0.81 (285.4 examples/sec; 0.448 sec/batch)
2016-02-03 12:14:49.436044: step 18300, loss = 0.89 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 12:14:54.536078: step 18310, loss = 0.90 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 12:14:59.218510: step 18320, loss = 0.93 (263.9 examples/sec; 0.485 sec/batch)
2016-02-03 12:15:03.863892: step 18330, loss = 0.89 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 12:15:08.467002: step 18340, loss = 0.89 (289.6 examples/sec; 0.442 sec/batch)
2016-02-03 12:15:13.153071: step 18350, loss = 0.69 (283.9 examples/sec; 0.451 sec/batch)
2016-02-03 12:15:17.931218: step 18360, loss = 0.80 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 12:15:22.700038: step 18370, loss = 0.86 (280.5 examples/sec; 0.456 sec/batch)
2016-02-03 12:15:27.416342: step 18380, loss = 1.02 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 12:15:32.077353: step 18390, loss = 0.74 (259.3 examples/sec; 0.494 sec/batch)
2016-02-03 12:15:36.758191: step 18400, loss = 0.72 (244.6 examples/sec; 0.523 sec/batch)
2016-02-03 12:15:41.824587: step 18410, loss = 0.83 (303.7 examples/sec; 0.421 sec/batch)
2016-02-03 12:15:46.479189: step 18420, loss = 1.06 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 12:15:51.246673: step 18430, loss = 0.96 (251.6 examples/sec; 0.509 sec/batch)
2016-02-03 12:15:55.833450: step 18440, loss = 1.08 (277.7 examples/sec; 0.461 sec/batch)
2016-02-03 12:16:00.503063: step 18450, loss = 0.90 (279.8 examples/sec; 0.458 sec/batch)
2016-02-03 12:16:05.152370: step 18460, loss = 1.07 (254.0 examples/sec; 0.504 sec/batch)
2016-02-03 12:16:09.853637: step 18470, loss = 0.78 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 12:16:14.470765: step 18480, loss = 0.76 (276.9 examples/sec; 0.462 sec/batch)
2016-02-03 12:16:19.095175: step 18490, loss = 0.72 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 12:16:23.775311: step 18500, loss = 0.84 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 12:16:28.812201: step 18510, loss = 0.84 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 12:16:33.496871: step 18520, loss = 0.69 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 12:16:38.170788: step 18530, loss = 0.89 (285.1 examples/sec; 0.449 sec/batch)
2016-02-03 12:16:42.839599: step 18540, loss = 0.68 (286.1 examples/sec; 0.447 sec/batch)
2016-02-03 12:16:47.575731: step 18550, loss = 0.81 (263.9 examples/sec; 0.485 sec/batch)
2016-02-03 12:16:52.162581: step 18560, loss = 1.04 (285.8 examples/sec; 0.448 sec/batch)
2016-02-03 12:16:56.813008: step 18570, loss = 0.68 (284.9 examples/sec; 0.449 sec/batch)
2016-02-03 12:17:01.431883: step 18580, loss = 0.79 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 12:17:06.141532: step 18590, loss = 0.88 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 12:17:10.834160: step 18600, loss = 0.95 (283.1 examples/sec; 0.452 sec/batch)
2016-02-03 12:17:16.064405: step 18610, loss = 0.95 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 12:17:20.706930: step 18620, loss = 0.77 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 12:17:25.368420: step 18630, loss = 0.77 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 12:17:30.083242: step 18640, loss = 0.85 (267.0 examples/sec; 0.479 sec/batch)
2016-02-03 12:17:34.776609: step 18650, loss = 0.68 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 12:17:39.427892: step 18660, loss = 0.86 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 12:17:44.003201: step 18670, loss = 0.88 (298.5 examples/sec; 0.429 sec/batch)
2016-02-03 12:17:48.730782: step 18680, loss = 0.78 (253.9 examples/sec; 0.504 sec/batch)
2016-02-03 12:17:53.368645: step 18690, loss = 0.66 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 12:17:58.010589: step 18700, loss = 0.89 (260.4 examples/sec; 0.491 sec/batch)
2016-02-03 12:18:03.183472: step 18710, loss = 0.85 (264.5 examples/sec; 0.484 sec/batch)
2016-02-03 12:18:07.970124: step 18720, loss = 0.84 (234.4 examples/sec; 0.546 sec/batch)
2016-02-03 12:18:12.727124: step 18730, loss = 0.78 (246.3 examples/sec; 0.520 sec/batch)
2016-02-03 12:18:17.428729: step 18740, loss = 0.83 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 12:18:22.193032: step 18750, loss = 0.83 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 12:18:26.893468: step 18760, loss = 0.93 (258.7 examples/sec; 0.495 sec/batch)
2016-02-03 12:18:31.567635: step 18770, loss = 0.72 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 12:18:36.296551: step 18780, loss = 0.93 (233.6 examples/sec; 0.548 sec/batch)
2016-02-03 12:18:40.892113: step 18790, loss = 0.75 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 12:18:45.560701: step 18800, loss = 0.85 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 12:18:50.732356: step 18810, loss = 0.82 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 12:18:55.451719: step 18820, loss = 0.79 (277.6 examples/sec; 0.461 sec/batch)
2016-02-03 12:19:00.123118: step 18830, loss = 0.78 (290.6 examples/sec; 0.441 sec/batch)
2016-02-03 12:19:04.879765: step 18840, loss = 0.82 (247.7 examples/sec; 0.517 sec/batch)
2016-02-03 12:19:09.597709: step 18850, loss = 0.78 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 12:19:14.331384: step 18860, loss = 0.90 (289.6 examples/sec; 0.442 sec/batch)
2016-02-03 12:19:19.036483: step 18870, loss = 0.83 (251.1 examples/sec; 0.510 sec/batch)
2016-02-03 12:19:23.776596: step 18880, loss = 0.72 (275.6 examples/sec; 0.464 sec/batch)
2016-02-03 12:19:28.513167: step 18890, loss = 0.76 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 12:19:33.200848: step 18900, loss = 0.72 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 12:19:38.379504: step 18910, loss = 0.69 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 12:19:43.083239: step 18920, loss = 0.72 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 12:19:47.788843: step 18930, loss = 0.95 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 12:19:52.481283: step 18940, loss = 0.67 (276.3 examples/sec; 0.463 sec/batch)
2016-02-03 12:19:57.112897: step 18950, loss = 0.83 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 12:20:01.765533: step 18960, loss = 0.72 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 12:20:06.364098: step 18970, loss = 0.78 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 12:20:11.076720: step 18980, loss = 0.64 (305.3 examples/sec; 0.419 sec/batch)
2016-02-03 12:20:15.761761: step 18990, loss = 0.76 (260.9 examples/sec; 0.491 sec/batch)
2016-02-03 12:20:20.423941: step 19000, loss = 0.86 (260.5 examples/sec; 0.491 sec/batch)
2016-02-03 12:20:25.659011: step 19010, loss = 0.76 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 12:20:30.314605: step 19020, loss = 0.99 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 12:20:34.962153: step 19030, loss = 0.87 (287.7 examples/sec; 0.445 sec/batch)
2016-02-03 12:20:39.699684: step 19040, loss = 0.82 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 12:20:44.417463: step 19050, loss = 0.75 (257.9 examples/sec; 0.496 sec/batch)
2016-02-03 12:20:49.132948: step 19060, loss = 0.86 (260.1 examples/sec; 0.492 sec/batch)
2016-02-03 12:20:53.841947: step 19070, loss = 0.98 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 12:20:58.516179: step 19080, loss = 0.77 (283.1 examples/sec; 0.452 sec/batch)
2016-02-03 12:21:03.165543: step 19090, loss = 0.82 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 12:21:07.847867: step 19100, loss = 0.70 (290.2 examples/sec; 0.441 sec/batch)
2016-02-03 12:21:13.019939: step 19110, loss = 0.70 (260.7 examples/sec; 0.491 sec/batch)
2016-02-03 12:21:17.803398: step 19120, loss = 0.84 (260.7 examples/sec; 0.491 sec/batch)
2016-02-03 12:21:22.583733: step 19130, loss = 0.81 (254.9 examples/sec; 0.502 sec/batch)
2016-02-03 12:21:27.238135: step 19140, loss = 0.72 (255.1 examples/sec; 0.502 sec/batch)
2016-02-03 12:21:31.919743: step 19150, loss = 0.93 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 12:21:36.634812: step 19160, loss = 0.71 (250.3 examples/sec; 0.511 sec/batch)
2016-02-03 12:21:41.327482: step 19170, loss = 0.87 (282.4 examples/sec; 0.453 sec/batch)
2016-02-03 12:21:46.023693: step 19180, loss = 0.66 (250.0 examples/sec; 0.512 sec/batch)
2016-02-03 12:21:50.676914: step 19190, loss = 1.02 (284.0 examples/sec; 0.451 sec/batch)
2016-02-03 12:21:55.358201: step 19200, loss = 0.91 (288.2 examples/sec; 0.444 sec/batch)
2016-02-03 12:22:00.569804: step 19210, loss = 0.86 (277.7 examples/sec; 0.461 sec/batch)
2016-02-03 12:22:05.279843: step 19220, loss = 0.89 (261.4 examples/sec; 0.490 sec/batch)
2016-02-03 12:22:09.932066: step 19230, loss = 0.89 (285.5 examples/sec; 0.448 sec/batch)
2016-02-03 12:22:14.566913: step 19240, loss = 0.92 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 12:22:19.174514: step 19250, loss = 0.95 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 12:22:23.853782: step 19260, loss = 0.85 (293.1 examples/sec; 0.437 sec/batch)
2016-02-03 12:22:28.532523: step 19270, loss = 0.84 (267.3 examples/sec; 0.479 sec/batch)
2016-02-03 12:22:33.286098: step 19280, loss = 0.78 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 12:22:37.999382: step 19290, loss = 0.79 (281.0 examples/sec; 0.456 sec/batch)
2016-02-03 12:22:42.598874: step 19300, loss = 0.85 (268.6 examples/sec; 0.477 sec/batch)
2016-02-03 12:22:47.860283: step 19310, loss = 0.73 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 12:22:52.571652: step 19320, loss = 0.83 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 12:22:57.276354: step 19330, loss = 0.82 (260.5 examples/sec; 0.491 sec/batch)
2016-02-03 12:23:01.943209: step 19340, loss = 0.79 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 12:23:06.630077: step 19350, loss = 0.82 (260.4 examples/sec; 0.492 sec/batch)
2016-02-03 12:23:11.188226: step 19360, loss = 0.85 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 12:23:15.931305: step 19370, loss = 1.01 (295.2 examples/sec; 0.434 sec/batch)
2016-02-03 12:23:20.677097: step 19380, loss = 0.86 (256.7 examples/sec; 0.499 sec/batch)
2016-02-03 12:23:25.375424: step 19390, loss = 0.77 (272.6 examples/sec; 0.470 sec/batch)
2016-02-03 12:23:30.064551: step 19400, loss = 0.85 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 12:23:35.279961: step 19410, loss = 0.96 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 12:23:39.938933: step 19420, loss = 0.71 (252.2 examples/sec; 0.507 sec/batch)
2016-02-03 12:23:44.641559: step 19430, loss = 1.02 (262.0 examples/sec; 0.489 sec/batch)
2016-02-03 12:23:49.330791: step 19440, loss = 0.81 (292.7 examples/sec; 0.437 sec/batch)
2016-02-03 12:23:54.001357: step 19450, loss = 0.97 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 12:23:58.674421: step 19460, loss = 0.66 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 12:24:03.452893: step 19470, loss = 0.76 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 12:24:08.128381: step 19480, loss = 0.96 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 12:24:12.844400: step 19490, loss = 0.90 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 12:24:17.428008: step 19500, loss = 0.83 (294.2 examples/sec; 0.435 sec/batch)
2016-02-03 12:24:22.577163: step 19510, loss = 0.79 (308.6 examples/sec; 0.415 sec/batch)
2016-02-03 12:24:27.191559: step 19520, loss = 0.75 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 12:24:31.848131: step 19530, loss = 0.90 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 12:24:36.499412: step 19540, loss = 0.90 (272.6 examples/sec; 0.470 sec/batch)
2016-02-03 12:24:41.092961: step 19550, loss = 0.78 (264.5 examples/sec; 0.484 sec/batch)
2016-02-03 12:24:45.665565: step 19560, loss = 1.05 (265.0 examples/sec; 0.483 sec/batch)
2016-02-03 12:24:50.284457: step 19570, loss = 0.84 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 12:24:54.977283: step 19580, loss = 0.89 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 12:24:59.630604: step 19590, loss = 0.80 (294.1 examples/sec; 0.435 sec/batch)
2016-02-03 12:25:04.348615: step 19600, loss = 1.06 (303.5 examples/sec; 0.422 sec/batch)
2016-02-03 12:25:09.634282: step 19610, loss = 0.94 (251.6 examples/sec; 0.509 sec/batch)
2016-02-03 12:25:14.253660: step 19620, loss = 0.84 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 12:25:19.045636: step 19630, loss = 0.90 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 12:25:23.680760: step 19640, loss = 0.86 (283.4 examples/sec; 0.452 sec/batch)
2016-02-03 12:25:28.421394: step 19650, loss = 0.74 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 12:25:33.120253: step 19660, loss = 0.72 (280.4 examples/sec; 0.457 sec/batch)
2016-02-03 12:25:37.864268: step 19670, loss = 0.83 (265.3 examples/sec; 0.483 sec/batch)
2016-02-03 12:25:42.571410: step 19680, loss = 0.90 (254.1 examples/sec; 0.504 sec/batch)
2016-02-03 12:25:47.313167: step 19690, loss = 0.84 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 12:25:51.999713: step 19700, loss = 0.88 (273.8 examples/sec; 0.468 sec/batch)
2016-02-03 12:25:57.244109: step 19710, loss = 0.76 (252.9 examples/sec; 0.506 sec/batch)
2016-02-03 12:26:01.932444: step 19720, loss = 0.94 (276.9 examples/sec; 0.462 sec/batch)
2016-02-03 12:26:06.625371: step 19730, loss = 0.83 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 12:26:11.321922: step 19740, loss = 0.81 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 12:26:16.033359: step 19750, loss = 0.72 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 12:26:20.771106: step 19760, loss = 0.73 (272.7 examples/sec; 0.469 sec/batch)
2016-02-03 12:26:25.481996: step 19770, loss = 0.83 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 12:26:30.202445: step 19780, loss = 0.83 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 12:26:34.816217: step 19790, loss = 0.82 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 12:26:39.402392: step 19800, loss = 0.79 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 12:26:44.537979: step 19810, loss = 0.98 (282.2 examples/sec; 0.454 sec/batch)
2016-02-03 12:26:49.241519: step 19820, loss = 0.80 (257.8 examples/sec; 0.497 sec/batch)
2016-02-03 12:26:53.884358: step 19830, loss = 0.69 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 12:26:58.539279: step 19840, loss = 0.95 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 12:27:03.181699: step 19850, loss = 0.89 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 12:27:07.811807: step 19860, loss = 0.74 (287.0 examples/sec; 0.446 sec/batch)
2016-02-03 12:27:12.508653: step 19870, loss = 0.76 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 12:27:17.118903: step 19880, loss = 0.74 (288.1 examples/sec; 0.444 sec/batch)
2016-02-03 12:27:21.815948: step 19890, loss = 0.96 (258.3 examples/sec; 0.496 sec/batch)
2016-02-03 12:27:26.432039: step 19900, loss = 0.89 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 12:27:31.637651: step 19910, loss = 0.93 (263.1 examples/sec; 0.486 sec/batch)
2016-02-03 12:27:36.283003: step 19920, loss = 0.94 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 12:27:40.994059: step 19930, loss = 0.82 (254.3 examples/sec; 0.503 sec/batch)
2016-02-03 12:27:45.653017: step 19940, loss = 0.88 (294.3 examples/sec; 0.435 sec/batch)
2016-02-03 12:27:50.371899: step 19950, loss = 0.72 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 12:27:55.075511: step 19960, loss = 0.89 (255.3 examples/sec; 0.501 sec/batch)
2016-02-03 12:27:59.691228: step 19970, loss = 0.89 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 12:28:04.333083: step 19980, loss = 0.73 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 12:28:08.971877: step 19990, loss = 0.85 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 12:28:13.609844: step 20000, loss = 0.90 (257.3 examples/sec; 0.497 sec/batch)
2016-02-03 12:28:18.715273: step 20010, loss = 0.83 (280.3 examples/sec; 0.457 sec/batch)
2016-02-03 12:28:23.416407: step 20020, loss = 0.88 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 12:28:28.041319: step 20030, loss = 0.84 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 12:28:32.771508: step 20040, loss = 0.85 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 12:28:37.418582: step 20050, loss = 0.89 (259.4 examples/sec; 0.493 sec/batch)
2016-02-03 12:28:42.071872: step 20060, loss = 0.81 (283.7 examples/sec; 0.451 sec/batch)
2016-02-03 12:28:46.683319: step 20070, loss = 0.87 (311.8 examples/sec; 0.411 sec/batch)
2016-02-03 12:28:51.329048: step 20080, loss = 0.72 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 12:28:56.038689: step 20090, loss = 0.94 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 12:29:00.746112: step 20100, loss = 0.75 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 12:29:05.929977: step 20110, loss = 0.72 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 12:29:10.633451: step 20120, loss = 0.83 (291.2 examples/sec; 0.440 sec/batch)
2016-02-03 12:29:15.353143: step 20130, loss = 0.76 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 12:29:20.055393: step 20140, loss = 0.76 (285.2 examples/sec; 0.449 sec/batch)
2016-02-03 12:29:24.703851: step 20150, loss = 0.75 (285.5 examples/sec; 0.448 sec/batch)
2016-02-03 12:29:29.431894: step 20160, loss = 0.88 (241.5 examples/sec; 0.530 sec/batch)
2016-02-03 12:29:34.110941: step 20170, loss = 0.85 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 12:29:38.760878: step 20180, loss = 1.09 (280.4 examples/sec; 0.456 sec/batch)
2016-02-03 12:29:43.452754: step 20190, loss = 0.76 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 12:29:48.153499: step 20200, loss = 0.76 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 12:29:53.326232: step 20210, loss = 0.84 (288.6 examples/sec; 0.443 sec/batch)
2016-02-03 12:29:58.042297: step 20220, loss = 0.83 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 12:30:02.648696: step 20230, loss = 0.93 (293.9 examples/sec; 0.435 sec/batch)
2016-02-03 12:30:07.258419: step 20240, loss = 1.05 (272.7 examples/sec; 0.469 sec/batch)
2016-02-03 12:30:11.864828: step 20250, loss = 0.82 (291.3 examples/sec; 0.439 sec/batch)
2016-02-03 12:30:16.580612: step 20260, loss = 0.91 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 12:30:21.260135: step 20270, loss = 0.66 (280.9 examples/sec; 0.456 sec/batch)
2016-02-03 12:30:25.972348: step 20280, loss = 0.94 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 12:30:30.603055: step 20290, loss = 0.87 (284.7 examples/sec; 0.450 sec/batch)
2016-02-03 12:30:35.269825: step 20300, loss = 0.88 (275.7 examples/sec; 0.464 sec/batch)
2016-02-03 12:30:40.454178: step 20310, loss = 0.84 (288.1 examples/sec; 0.444 sec/batch)
2016-02-03 12:30:45.180547: step 20320, loss = 0.93 (261.1 examples/sec; 0.490 sec/batch)
2016-02-03 12:30:49.807969: step 20330, loss = 0.90 (285.4 examples/sec; 0.448 sec/batch)
2016-02-03 12:30:54.411708: step 20340, loss = 0.87 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 12:30:59.140483: step 20350, loss = 0.96 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 12:31:03.820116: step 20360, loss = 0.86 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 12:31:08.551521: step 20370, loss = 0.78 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 12:31:13.222379: step 20380, loss = 0.79 (256.2 examples/sec; 0.500 sec/batch)
2016-02-03 12:31:17.880641: step 20390, loss = 0.76 (290.6 examples/sec; 0.440 sec/batch)
2016-02-03 12:31:22.570361: step 20400, loss = 0.82 (298.8 examples/sec; 0.428 sec/batch)
2016-02-03 12:31:27.826206: step 20410, loss = 0.85 (268.2 examples/sec; 0.477 sec/batch)
2016-02-03 12:31:32.602383: step 20420, loss = 0.90 (264.5 examples/sec; 0.484 sec/batch)
2016-02-03 12:31:37.284272: step 20430, loss = 0.81 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 12:31:41.989713: step 20440, loss = 0.96 (255.6 examples/sec; 0.501 sec/batch)
2016-02-03 12:31:46.689807: step 20450, loss = 0.83 (274.4 examples/sec; 0.466 sec/batch)
2016-02-03 12:31:51.320852: step 20460, loss = 0.71 (268.2 examples/sec; 0.477 sec/batch)
2016-02-03 12:31:55.965152: step 20470, loss = 0.85 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 12:32:00.604741: step 20480, loss = 0.62 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 12:32:05.289792: step 20490, loss = 0.67 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 12:32:10.008633: step 20500, loss = 0.88 (287.4 examples/sec; 0.445 sec/batch)
2016-02-03 12:32:15.127996: step 20510, loss = 0.85 (270.9 examples/sec; 0.472 sec/batch)
2016-02-03 12:32:19.921517: step 20520, loss = 0.90 (251.2 examples/sec; 0.510 sec/batch)
2016-02-03 12:32:24.575315: step 20530, loss = 0.93 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 12:32:29.204060: step 20540, loss = 0.79 (288.4 examples/sec; 0.444 sec/batch)
2016-02-03 12:32:33.851055: step 20550, loss = 0.69 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 12:32:38.422912: step 20560, loss = 0.94 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 12:32:43.054921: step 20570, loss = 0.71 (286.0 examples/sec; 0.448 sec/batch)
2016-02-03 12:32:47.785605: step 20580, loss = 0.99 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 12:32:52.455021: step 20590, loss = 0.91 (267.0 examples/sec; 0.479 sec/batch)
2016-02-03 12:32:57.119531: step 20600, loss = 0.85 (295.7 examples/sec; 0.433 sec/batch)
2016-02-03 12:33:02.255091: step 20610, loss = 0.75 (314.5 examples/sec; 0.407 sec/batch)
2016-02-03 12:33:06.850891: step 20620, loss = 0.89 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 12:33:11.486313: step 20630, loss = 0.88 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 12:33:16.157129: step 20640, loss = 0.77 (277.8 examples/sec; 0.461 sec/batch)
2016-02-03 12:33:20.819973: step 20650, loss = 0.97 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 12:33:25.496111: step 20660, loss = 0.91 (279.8 examples/sec; 0.458 sec/batch)
2016-02-03 12:33:30.090853: step 20670, loss = 0.82 (260.9 examples/sec; 0.491 sec/batch)
2016-02-03 12:33:34.650940: step 20680, loss = 0.90 (297.4 examples/sec; 0.430 sec/batch)
2016-02-03 12:33:39.205745: step 20690, loss = 0.83 (294.3 examples/sec; 0.435 sec/batch)
2016-02-03 12:33:43.910034: step 20700, loss = 0.83 (298.1 examples/sec; 0.429 sec/batch)
2016-02-03 12:33:48.971839: step 20710, loss = 0.79 (295.7 examples/sec; 0.433 sec/batch)
2016-02-03 12:33:53.711279: step 20720, loss = 0.83 (267.0 examples/sec; 0.479 sec/batch)
2016-02-03 12:33:58.367347: step 20730, loss = 0.76 (285.6 examples/sec; 0.448 sec/batch)
2016-02-03 12:34:03.086485: step 20740, loss = 0.83 (280.1 examples/sec; 0.457 sec/batch)
2016-02-03 12:34:07.913845: step 20750, loss = 0.74 (258.8 examples/sec; 0.495 sec/batch)
2016-02-03 12:34:12.572996: step 20760, loss = 0.80 (263.2 examples/sec; 0.486 sec/batch)
2016-02-03 12:34:17.292867: step 20770, loss = 0.91 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 12:34:21.937143: step 20780, loss = 0.71 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 12:34:26.616449: step 20790, loss = 0.89 (284.4 examples/sec; 0.450 sec/batch)
2016-02-03 12:34:31.383942: step 20800, loss = 0.67 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 12:34:36.592881: step 20810, loss = 0.95 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 12:34:41.347041: step 20820, loss = 0.88 (248.2 examples/sec; 0.516 sec/batch)
2016-02-03 12:34:46.086610: step 20830, loss = 0.78 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 12:34:50.836663: step 20840, loss = 0.89 (256.2 examples/sec; 0.500 sec/batch)
2016-02-03 12:34:55.493738: step 20850, loss = 0.75 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 12:35:00.260359: step 20860, loss = 0.82 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 12:35:04.932096: step 20870, loss = 0.97 (277.6 examples/sec; 0.461 sec/batch)
2016-02-03 12:35:09.642994: step 20880, loss = 0.79 (259.4 examples/sec; 0.493 sec/batch)
2016-02-03 12:35:14.283923: step 20890, loss = 0.74 (284.1 examples/sec; 0.451 sec/batch)
2016-02-03 12:35:18.986925: step 20900, loss = 0.75 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 12:35:24.221146: step 20910, loss = 0.75 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 12:35:28.901425: step 20920, loss = 0.87 (259.5 examples/sec; 0.493 sec/batch)
2016-02-03 12:35:33.656248: step 20930, loss = 0.91 (251.6 examples/sec; 0.509 sec/batch)
2016-02-03 12:35:38.346129: step 20940, loss = 0.97 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 12:35:43.083828: step 20950, loss = 0.94 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 12:35:47.840195: step 20960, loss = 0.75 (279.8 examples/sec; 0.457 sec/batch)
2016-02-03 12:35:52.590942: step 20970, loss = 1.18 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 12:35:57.363258: step 20980, loss = 0.83 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 12:36:02.045718: step 20990, loss = 0.63 (274.4 examples/sec; 0.466 sec/batch)
2016-02-03 12:36:06.723557: step 21000, loss = 0.73 (255.2 examples/sec; 0.502 sec/batch)
2016-02-03 12:36:12.046660: step 21010, loss = 0.96 (249.3 examples/sec; 0.513 sec/batch)
2016-02-03 12:36:16.623021: step 21020, loss = 0.77 (264.3 examples/sec; 0.484 sec/batch)
2016-02-03 12:36:21.319662: step 21030, loss = 0.67 (292.4 examples/sec; 0.438 sec/batch)
2016-02-03 12:36:26.040866: step 21040, loss = 0.88 (303.3 examples/sec; 0.422 sec/batch)
2016-02-03 12:36:30.794008: step 21050, loss = 0.90 (280.6 examples/sec; 0.456 sec/batch)
2016-02-03 12:36:35.516827: step 21060, loss = 1.03 (292.5 examples/sec; 0.438 sec/batch)
2016-02-03 12:36:40.201878: step 21070, loss = 0.98 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 12:36:44.900988: step 21080, loss = 0.91 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 12:36:49.653422: step 21090, loss = 0.85 (285.7 examples/sec; 0.448 sec/batch)
2016-02-03 12:36:54.279945: step 21100, loss = 0.82 (292.3 examples/sec; 0.438 sec/batch)
2016-02-03 12:36:59.503704: step 21110, loss = 0.90 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 12:37:04.239028: step 21120, loss = 0.83 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 12:37:08.929796: step 21130, loss = 0.92 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 12:37:13.682136: step 21140, loss = 0.83 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 12:37:18.363980: step 21150, loss = 0.78 (288.3 examples/sec; 0.444 sec/batch)
2016-02-03 12:37:23.027179: step 21160, loss = 0.92 (306.7 examples/sec; 0.417 sec/batch)
2016-02-03 12:37:27.785455: step 21170, loss = 0.85 (256.6 examples/sec; 0.499 sec/batch)
2016-02-03 12:37:32.492702: step 21180, loss = 0.74 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 12:37:37.187480: step 21190, loss = 0.80 (279.8 examples/sec; 0.457 sec/batch)
2016-02-03 12:37:41.884576: step 21200, loss = 0.58 (259.4 examples/sec; 0.493 sec/batch)
2016-02-03 12:37:47.080283: step 21210, loss = 0.79 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 12:37:51.915865: step 21220, loss = 0.89 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 12:37:56.577600: step 21230, loss = 0.74 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 12:38:01.254192: step 21240, loss = 0.68 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 12:38:05.989957: step 21250, loss = 0.97 (273.8 examples/sec; 0.468 sec/batch)
2016-02-03 12:38:10.705331: step 21260, loss = 0.81 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 12:38:15.436438: step 21270, loss = 0.71 (266.4 examples/sec; 0.480 sec/batch)
2016-02-03 12:38:20.155108: step 21280, loss = 0.83 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 12:38:24.881614: step 21290, loss = 0.86 (279.8 examples/sec; 0.457 sec/batch)
2016-02-03 12:38:29.583913: step 21300, loss = 0.94 (273.8 examples/sec; 0.468 sec/batch)
2016-02-03 12:38:34.847369: step 21310, loss = 0.73 (275.7 examples/sec; 0.464 sec/batch)
2016-02-03 12:38:39.607443: step 21320, loss = 0.72 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 12:38:44.264925: step 21330, loss = 0.85 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 12:38:48.978483: step 21340, loss = 0.66 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 12:38:53.679123: step 21350, loss = 0.75 (254.5 examples/sec; 0.503 sec/batch)
2016-02-03 12:38:58.408623: step 21360, loss = 0.79 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 12:39:03.085618: step 21370, loss = 0.81 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 12:39:07.869320: step 21380, loss = 0.85 (259.5 examples/sec; 0.493 sec/batch)
2016-02-03 12:39:12.520352: step 21390, loss = 0.85 (277.7 examples/sec; 0.461 sec/batch)
2016-02-03 12:39:17.237166: step 21400, loss = 0.89 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 12:39:22.516765: step 21410, loss = 0.68 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 12:39:27.170499: step 21420, loss = 0.75 (256.9 examples/sec; 0.498 sec/batch)
2016-02-03 12:39:31.842524: step 21430, loss = 0.64 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 12:39:36.670572: step 21440, loss = 0.83 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 12:39:41.375345: step 21450, loss = 0.98 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 12:39:46.036055: step 21460, loss = 0.86 (284.9 examples/sec; 0.449 sec/batch)
2016-02-03 12:39:50.712454: step 21470, loss = 0.87 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 12:39:55.362341: step 21480, loss = 0.74 (285.8 examples/sec; 0.448 sec/batch)
2016-02-03 12:39:59.994041: step 21490, loss = 0.73 (258.4 examples/sec; 0.495 sec/batch)
2016-02-03 12:40:04.615663: step 21500, loss = 0.90 (308.6 examples/sec; 0.415 sec/batch)
2016-02-03 12:40:09.842667: step 21510, loss = 0.85 (255.8 examples/sec; 0.500 sec/batch)
2016-02-03 12:40:14.553926: step 21520, loss = 0.96 (257.8 examples/sec; 0.497 sec/batch)
2016-02-03 12:40:19.253294: step 21530, loss = 0.83 (282.8 examples/sec; 0.453 sec/batch)
2016-02-03 12:40:23.944698: step 21540, loss = 0.87 (263.0 examples/sec; 0.487 sec/batch)
2016-02-03 12:40:28.620190: step 21550, loss = 0.83 (295.1 examples/sec; 0.434 sec/batch)
2016-02-03 12:40:33.378857: step 21560, loss = 0.78 (256.7 examples/sec; 0.499 sec/batch)
2016-02-03 12:40:38.081121: step 21570, loss = 0.90 (284.0 examples/sec; 0.451 sec/batch)
2016-02-03 12:40:42.745148: step 21580, loss = 0.74 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 12:40:47.405998: step 21590, loss = 0.69 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 12:40:52.041285: step 21600, loss = 0.73 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 12:40:57.305028: step 21610, loss = 0.92 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 12:41:02.009964: step 21620, loss = 0.81 (265.8 examples/sec; 0.482 sec/batch)
2016-02-03 12:41:06.708309: step 21630, loss = 0.98 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 12:41:11.395932: step 21640, loss = 0.82 (249.5 examples/sec; 0.513 sec/batch)
2016-02-03 12:41:16.105169: step 21650, loss = 0.83 (282.8 examples/sec; 0.453 sec/batch)
2016-02-03 12:41:20.707299: step 21660, loss = 0.75 (294.9 examples/sec; 0.434 sec/batch)
2016-02-03 12:41:25.492605: step 21670, loss = 0.70 (267.5 examples/sec; 0.479 sec/batch)
2016-02-03 12:41:30.192389: step 21680, loss = 0.69 (275.6 examples/sec; 0.464 sec/batch)
2016-02-03 12:41:34.840871: step 21690, loss = 0.73 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 12:41:39.494483: step 21700, loss = 0.83 (277.6 examples/sec; 0.461 sec/batch)
2016-02-03 12:41:44.650758: step 21710, loss = 0.85 (303.3 examples/sec; 0.422 sec/batch)
2016-02-03 12:41:49.393918: step 21720, loss = 0.81 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 12:41:54.140786: step 21730, loss = 0.67 (275.0 examples/sec; 0.465 sec/batch)
2016-02-03 12:41:58.896263: step 21740, loss = 0.77 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 12:42:03.421657: step 21750, loss = 0.84 (306.1 examples/sec; 0.418 sec/batch)
2016-02-03 12:42:08.149964: step 21760, loss = 0.78 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 12:42:12.851942: step 21770, loss = 0.83 (280.3 examples/sec; 0.457 sec/batch)
2016-02-03 12:42:17.469902: step 21780, loss = 0.72 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 12:42:22.096498: step 21790, loss = 0.70 (280.3 examples/sec; 0.457 sec/batch)
2016-02-03 12:42:26.801861: step 21800, loss = 1.07 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 12:42:31.959591: step 21810, loss = 0.73 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 12:42:36.665870: step 21820, loss = 0.90 (282.6 examples/sec; 0.453 sec/batch)
2016-02-03 12:42:41.415692: step 21830, loss = 0.97 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 12:42:46.088498: step 21840, loss = 1.07 (293.3 examples/sec; 0.436 sec/batch)
2016-02-03 12:42:50.736670: step 21850, loss = 0.82 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 12:42:55.374168: step 21860, loss = 0.76 (296.8 examples/sec; 0.431 sec/batch)
2016-02-03 12:43:00.140809: step 21870, loss = 0.71 (270.9 examples/sec; 0.473 sec/batch)
2016-02-03 12:43:04.863469: step 21880, loss = 0.84 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 12:43:09.509781: step 21890, loss = 0.83 (261.5 examples/sec; 0.489 sec/batch)
2016-02-03 12:43:14.157034: step 21900, loss = 0.74 (280.1 examples/sec; 0.457 sec/batch)
2016-02-03 12:43:19.396750: step 21910, loss = 0.75 (300.8 examples/sec; 0.425 sec/batch)
2016-02-03 12:43:24.071182: step 21920, loss = 0.67 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 12:43:28.733269: step 21930, loss = 0.72 (286.7 examples/sec; 0.446 sec/batch)
2016-02-03 12:43:33.471271: step 21940, loss = 0.73 (260.9 examples/sec; 0.491 sec/batch)
2016-02-03 12:43:38.157368: step 21950, loss = 0.87 (286.0 examples/sec; 0.447 sec/batch)
2016-02-03 12:43:42.917642: step 21960, loss = 0.77 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 12:43:47.732931: step 21970, loss = 0.90 (263.0 examples/sec; 0.487 sec/batch)
2016-02-03 12:43:52.353857: step 21980, loss = 0.85 (289.5 examples/sec; 0.442 sec/batch)
2016-02-03 12:43:56.953668: step 21990, loss = 0.79 (313.7 examples/sec; 0.408 sec/batch)
2016-02-03 12:44:01.610733: step 22000, loss = 0.83 (265.3 examples/sec; 0.483 sec/batch)
2016-02-03 12:44:06.712120: step 22010, loss = 0.91 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 12:44:11.240815: step 22020, loss = 0.80 (299.5 examples/sec; 0.427 sec/batch)
2016-02-03 12:44:16.039729: step 22030, loss = 0.75 (247.4 examples/sec; 0.517 sec/batch)
2016-02-03 12:44:20.743139: step 22040, loss = 0.84 (305.8 examples/sec; 0.419 sec/batch)
2016-02-03 12:44:25.401256: step 22050, loss = 0.79 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 12:44:30.118451: step 22060, loss = 0.79 (261.4 examples/sec; 0.490 sec/batch)
2016-02-03 12:44:34.622720: step 22070, loss = 0.71 (292.8 examples/sec; 0.437 sec/batch)
2016-02-03 12:44:39.348598: step 22080, loss = 0.99 (237.7 examples/sec; 0.538 sec/batch)
2016-02-03 12:44:44.041295: step 22090, loss = 0.73 (258.7 examples/sec; 0.495 sec/batch)
2016-02-03 12:44:48.703665: step 22100, loss = 0.77 (251.7 examples/sec; 0.508 sec/batch)
2016-02-03 12:44:53.866645: step 22110, loss = 0.88 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 12:44:58.490341: step 22120, loss = 0.83 (257.4 examples/sec; 0.497 sec/batch)
2016-02-03 12:45:03.193658: step 22130, loss = 0.74 (277.6 examples/sec; 0.461 sec/batch)
2016-02-03 12:45:07.904237: step 22140, loss = 0.88 (256.2 examples/sec; 0.500 sec/batch)
2016-02-03 12:45:12.650660: step 22150, loss = 0.82 (283.3 examples/sec; 0.452 sec/batch)
2016-02-03 12:45:17.317214: step 22160, loss = 0.69 (288.2 examples/sec; 0.444 sec/batch)
2016-02-03 12:45:21.977828: step 22170, loss = 0.63 (265.8 examples/sec; 0.482 sec/batch)
2016-02-03 12:45:26.608137: step 22180, loss = 0.83 (291.1 examples/sec; 0.440 sec/batch)
2016-02-03 12:45:31.336512: step 22190, loss = 0.69 (247.4 examples/sec; 0.517 sec/batch)
2016-02-03 12:45:36.093518: step 22200, loss = 1.01 (251.6 examples/sec; 0.509 sec/batch)
2016-02-03 12:45:41.271674: step 22210, loss = 0.96 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 12:45:45.907917: step 22220, loss = 0.87 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 12:45:50.622381: step 22230, loss = 0.86 (261.9 examples/sec; 0.489 sec/batch)
2016-02-03 12:45:55.308796: step 22240, loss = 0.88 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 12:46:00.006657: step 22250, loss = 0.87 (262.6 examples/sec; 0.487 sec/batch)
2016-02-03 12:46:04.623585: step 22260, loss = 0.78 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 12:46:09.284059: step 22270, loss = 0.86 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 12:46:13.991697: step 22280, loss = 0.73 (277.2 examples/sec; 0.462 sec/batch)
2016-02-03 12:46:18.606937: step 22290, loss = 0.76 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 12:46:23.350545: step 22300, loss = 0.71 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 12:46:28.574565: step 22310, loss = 0.78 (285.5 examples/sec; 0.448 sec/batch)
2016-02-03 12:46:33.349017: step 22320, loss = 0.72 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 12:46:37.996898: step 22330, loss = 0.68 (279.7 examples/sec; 0.458 sec/batch)
2016-02-03 12:46:42.477608: step 22340, loss = 0.84 (301.2 examples/sec; 0.425 sec/batch)
2016-02-03 12:46:47.120955: step 22350, loss = 0.81 (287.6 examples/sec; 0.445 sec/batch)
2016-02-03 12:46:51.856341: step 22360, loss = 0.73 (305.6 examples/sec; 0.419 sec/batch)
2016-02-03 12:46:56.572836: step 22370, loss = 0.94 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 12:47:01.296600: step 22380, loss = 1.04 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 12:47:05.994873: step 22390, loss = 0.86 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 12:47:10.727438: step 22400, loss = 0.74 (302.3 examples/sec; 0.423 sec/batch)
2016-02-03 12:47:15.912669: step 22410, loss = 0.99 (291.5 examples/sec; 0.439 sec/batch)
2016-02-03 12:47:20.650696: step 22420, loss = 0.88 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 12:47:25.300341: step 22430, loss = 0.94 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 12:47:30.042383: step 22440, loss = 0.80 (295.2 examples/sec; 0.434 sec/batch)
2016-02-03 12:47:34.789786: step 22450, loss = 0.87 (276.3 examples/sec; 0.463 sec/batch)
2016-02-03 12:47:39.535568: step 22460, loss = 0.78 (301.3 examples/sec; 0.425 sec/batch)
2016-02-03 12:47:44.171990: step 22470, loss = 0.87 (296.4 examples/sec; 0.432 sec/batch)
2016-02-03 12:47:48.960379: step 22480, loss = 0.70 (242.7 examples/sec; 0.527 sec/batch)
2016-02-03 12:47:53.701272: step 22490, loss = 0.78 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 12:47:58.364999: step 22500, loss = 0.84 (261.3 examples/sec; 0.490 sec/batch)
2016-02-03 12:48:03.515072: step 22510, loss = 0.74 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 12:48:08.270788: step 22520, loss = 0.94 (279.6 examples/sec; 0.458 sec/batch)
2016-02-03 12:48:13.007118: step 22530, loss = 0.84 (276.6 examples/sec; 0.463 sec/batch)
2016-02-03 12:48:17.827089: step 22540, loss = 1.01 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 12:48:22.541811: step 22550, loss = 0.89 (258.0 examples/sec; 0.496 sec/batch)
2016-02-03 12:48:27.189344: step 22560, loss = 0.77 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 12:48:31.820146: step 22570, loss = 0.89 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 12:48:36.527875: step 22580, loss = 0.85 (262.6 examples/sec; 0.487 sec/batch)
2016-02-03 12:48:41.158031: step 22590, loss = 0.77 (287.6 examples/sec; 0.445 sec/batch)
2016-02-03 12:48:45.927903: step 22600, loss = 0.90 (248.7 examples/sec; 0.515 sec/batch)
2016-02-03 12:48:51.188652: step 22610, loss = 0.91 (272.6 examples/sec; 0.470 sec/batch)
2016-02-03 12:48:55.938051: step 22620, loss = 0.95 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 12:49:00.731959: step 22630, loss = 0.71 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 12:49:05.510649: step 22640, loss = 0.92 (259.1 examples/sec; 0.494 sec/batch)
2016-02-03 12:49:10.287912: step 22650, loss = 0.73 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 12:49:15.008752: step 22660, loss = 0.85 (260.7 examples/sec; 0.491 sec/batch)
2016-02-03 12:49:19.685865: step 22670, loss = 0.95 (291.8 examples/sec; 0.439 sec/batch)
2016-02-03 12:49:24.303357: step 22680, loss = 0.60 (264.5 examples/sec; 0.484 sec/batch)
2016-02-03 12:49:28.973823: step 22690, loss = 0.90 (287.2 examples/sec; 0.446 sec/batch)
2016-02-03 12:49:33.692216: step 22700, loss = 0.68 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 12:49:38.794706: step 22710, loss = 0.84 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 12:49:43.576259: step 22720, loss = 0.78 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 12:49:48.298733: step 22730, loss = 0.82 (248.3 examples/sec; 0.516 sec/batch)
2016-02-03 12:49:53.025225: step 22740, loss = 0.74 (294.8 examples/sec; 0.434 sec/batch)
2016-02-03 12:49:57.744952: step 22750, loss = 0.79 (249.5 examples/sec; 0.513 sec/batch)
2016-02-03 12:50:02.471358: step 22760, loss = 0.91 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 12:50:07.161429: step 22770, loss = 0.79 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 12:50:11.843505: step 22780, loss = 0.78 (257.4 examples/sec; 0.497 sec/batch)
2016-02-03 12:50:16.522892: step 22790, loss = 0.83 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 12:50:21.209909: step 22800, loss = 0.89 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 12:50:26.422520: step 22810, loss = 0.80 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 12:50:31.141533: step 22820, loss = 0.93 (260.7 examples/sec; 0.491 sec/batch)
2016-02-03 12:50:35.929887: step 22830, loss = 0.93 (275.7 examples/sec; 0.464 sec/batch)
2016-02-03 12:50:40.563912: step 22840, loss = 0.80 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 12:50:45.207148: step 22850, loss = 0.94 (246.5 examples/sec; 0.519 sec/batch)
2016-02-03 12:50:49.958401: step 22860, loss = 0.92 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 12:50:54.695788: step 22870, loss = 0.89 (266.9 examples/sec; 0.480 sec/batch)
2016-02-03 12:50:59.377673: step 22880, loss = 0.93 (254.4 examples/sec; 0.503 sec/batch)
2016-02-03 12:51:04.168467: step 22890, loss = 0.60 (248.3 examples/sec; 0.515 sec/batch)
2016-02-03 12:51:08.919389: step 22900, loss = 0.83 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 12:51:14.200633: step 22910, loss = 1.01 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 12:51:19.005209: step 22920, loss = 0.76 (228.6 examples/sec; 0.560 sec/batch)
2016-02-03 12:51:23.600498: step 22930, loss = 0.89 (303.2 examples/sec; 0.422 sec/batch)
2016-02-03 12:51:28.290716: step 22940, loss = 0.92 (268.6 examples/sec; 0.477 sec/batch)
2016-02-03 12:51:32.977417: step 22950, loss = 0.92 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 12:51:37.632587: step 22960, loss = 0.71 (288.0 examples/sec; 0.444 sec/batch)
2016-02-03 12:51:42.318932: step 22970, loss = 0.82 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 12:51:46.922623: step 22980, loss = 0.94 (264.5 examples/sec; 0.484 sec/batch)
2016-02-03 12:51:51.608608: step 22990, loss = 0.71 (296.3 examples/sec; 0.432 sec/batch)
2016-02-03 12:51:56.263664: step 23000, loss = 0.80 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 12:52:01.473213: step 23010, loss = 0.89 (277.7 examples/sec; 0.461 sec/batch)
2016-02-03 12:52:06.154204: step 23020, loss = 0.96 (284.9 examples/sec; 0.449 sec/batch)
2016-02-03 12:52:10.799113: step 23030, loss = 0.99 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 12:52:15.387946: step 23040, loss = 0.69 (301.5 examples/sec; 0.424 sec/batch)
2016-02-03 12:52:19.999145: step 23050, loss = 0.73 (284.8 examples/sec; 0.449 sec/batch)
2016-02-03 12:52:24.571605: step 23060, loss = 0.99 (289.6 examples/sec; 0.442 sec/batch)
2016-02-03 12:52:29.242166: step 23070, loss = 0.83 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 12:52:33.846197: step 23080, loss = 0.90 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 12:52:38.537168: step 23090, loss = 0.67 (290.7 examples/sec; 0.440 sec/batch)
2016-02-03 12:52:43.160922: step 23100, loss = 0.81 (287.8 examples/sec; 0.445 sec/batch)
2016-02-03 12:52:48.370038: step 23110, loss = 0.80 (284.8 examples/sec; 0.449 sec/batch)
2016-02-03 12:52:53.035254: step 23120, loss = 0.90 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 12:52:57.624162: step 23130, loss = 0.64 (291.0 examples/sec; 0.440 sec/batch)
2016-02-03 12:53:02.319228: step 23140, loss = 0.70 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 12:53:06.994827: step 23150, loss = 0.81 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 12:53:11.622067: step 23160, loss = 0.83 (291.1 examples/sec; 0.440 sec/batch)
2016-02-03 12:53:16.224726: step 23170, loss = 0.66 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 12:53:20.973335: step 23180, loss = 0.88 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 12:53:25.734407: step 23190, loss = 0.75 (260.6 examples/sec; 0.491 sec/batch)
2016-02-03 12:53:30.444224: step 23200, loss = 0.71 (291.6 examples/sec; 0.439 sec/batch)
2016-02-03 12:53:35.681180: step 23210, loss = 0.90 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 12:53:40.404406: step 23220, loss = 0.90 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 12:53:45.034100: step 23230, loss = 0.72 (279.4 examples/sec; 0.458 sec/batch)
2016-02-03 12:53:49.774599: step 23240, loss = 0.90 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 12:53:54.409059: step 23250, loss = 0.74 (304.8 examples/sec; 0.420 sec/batch)
2016-02-03 12:53:59.089456: step 23260, loss = 1.03 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 12:54:03.868913: step 23270, loss = 0.88 (264.1 examples/sec; 0.485 sec/batch)
2016-02-03 12:54:08.557975: step 23280, loss = 0.82 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 12:54:13.226244: step 23290, loss = 0.91 (280.1 examples/sec; 0.457 sec/batch)
2016-02-03 12:54:17.946568: step 23300, loss = 1.02 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 12:54:23.134731: step 23310, loss = 1.02 (285.8 examples/sec; 0.448 sec/batch)
2016-02-03 12:54:27.840469: step 23320, loss = 0.76 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 12:54:32.524156: step 23330, loss = 0.92 (287.8 examples/sec; 0.445 sec/batch)
2016-02-03 12:54:37.116141: step 23340, loss = 0.84 (270.5 examples/sec; 0.473 sec/batch)
2016-02-03 12:54:41.834199: step 23350, loss = 0.74 (255.2 examples/sec; 0.501 sec/batch)
2016-02-03 12:54:46.500696: step 23360, loss = 0.77 (281.1 examples/sec; 0.455 sec/batch)
2016-02-03 12:54:51.225865: step 23370, loss = 0.75 (264.2 examples/sec; 0.484 sec/batch)
2016-02-03 12:54:56.045548: step 23380, loss = 0.92 (247.9 examples/sec; 0.516 sec/batch)
2016-02-03 12:55:00.766994: step 23390, loss = 0.81 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 12:55:05.464187: step 23400, loss = 0.66 (257.3 examples/sec; 0.497 sec/batch)
2016-02-03 12:55:10.676352: step 23410, loss = 0.73 (257.1 examples/sec; 0.498 sec/batch)
2016-02-03 12:55:15.418724: step 23420, loss = 0.80 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 12:55:20.121468: step 23430, loss = 0.81 (285.0 examples/sec; 0.449 sec/batch)
2016-02-03 12:55:24.823937: step 23440, loss = 0.74 (307.9 examples/sec; 0.416 sec/batch)
2016-02-03 12:55:29.571464: step 23450, loss = 0.96 (262.0 examples/sec; 0.489 sec/batch)
2016-02-03 12:55:34.274355: step 23460, loss = 0.88 (253.8 examples/sec; 0.504 sec/batch)
2016-02-03 12:55:38.946119: step 23470, loss = 0.91 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 12:55:43.587532: step 23480, loss = 0.86 (333.9 examples/sec; 0.383 sec/batch)
2016-02-03 12:55:48.190617: step 23490, loss = 0.87 (286.2 examples/sec; 0.447 sec/batch)
2016-02-03 12:55:52.841148: step 23500, loss = 0.85 (265.8 examples/sec; 0.482 sec/batch)
2016-02-03 12:55:58.019781: step 23510, loss = 0.81 (268.0 examples/sec; 0.478 sec/batch)
2016-02-03 12:56:02.699984: step 23520, loss = 0.82 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 12:56:07.359308: step 23530, loss = 0.65 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 12:56:12.036811: step 23540, loss = 0.79 (312.9 examples/sec; 0.409 sec/batch)
2016-02-03 12:56:16.760088: step 23550, loss = 0.81 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 12:56:21.412863: step 23560, loss = 0.85 (293.3 examples/sec; 0.436 sec/batch)
2016-02-03 12:56:26.220293: step 23570, loss = 0.92 (255.7 examples/sec; 0.501 sec/batch)
2016-02-03 12:56:30.856731: step 23580, loss = 0.87 (285.3 examples/sec; 0.449 sec/batch)
2016-02-03 12:56:35.568428: step 23590, loss = 0.99 (267.2 examples/sec; 0.479 sec/batch)
2016-02-03 12:56:40.227957: step 23600, loss = 0.90 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 12:56:45.433873: step 23610, loss = 0.87 (267.3 examples/sec; 0.479 sec/batch)
2016-02-03 12:56:50.126345: step 23620, loss = 0.84 (268.6 examples/sec; 0.477 sec/batch)
2016-02-03 12:56:54.854238: step 23630, loss = 0.74 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 12:56:59.566943: step 23640, loss = 0.82 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 12:57:04.249758: step 23650, loss = 0.81 (254.1 examples/sec; 0.504 sec/batch)
2016-02-03 12:57:08.919948: step 23660, loss = 0.93 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 12:57:13.595638: step 23670, loss = 0.65 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 12:57:18.326041: step 23680, loss = 0.81 (296.9 examples/sec; 0.431 sec/batch)
2016-02-03 12:57:23.039587: step 23690, loss = 0.76 (288.5 examples/sec; 0.444 sec/batch)
2016-02-03 12:57:27.779197: step 23700, loss = 0.68 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 12:57:33.005778: step 23710, loss = 0.88 (285.6 examples/sec; 0.448 sec/batch)
2016-02-03 12:57:37.707319: step 23720, loss = 0.83 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 12:57:42.444962: step 23730, loss = 0.79 (317.7 examples/sec; 0.403 sec/batch)
2016-02-03 12:57:47.095752: step 23740, loss = 0.76 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 12:57:51.769275: step 23750, loss = 0.97 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 12:57:56.473557: step 23760, loss = 0.83 (291.8 examples/sec; 0.439 sec/batch)
2016-02-03 12:58:01.186605: step 23770, loss = 0.83 (287.5 examples/sec; 0.445 sec/batch)
2016-02-03 12:58:05.968086: step 23780, loss = 0.77 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 12:58:10.578004: step 23790, loss = 0.99 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 12:58:15.294427: step 23800, loss = 0.88 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 12:58:20.518783: step 23810, loss = 0.71 (297.8 examples/sec; 0.430 sec/batch)
2016-02-03 12:58:25.209878: step 23820, loss = 0.78 (253.3 examples/sec; 0.505 sec/batch)
2016-02-03 12:58:29.947309: step 23830, loss = 0.74 (262.9 examples/sec; 0.487 sec/batch)
2016-02-03 12:58:34.675174: step 23840, loss = 0.84 (265.8 examples/sec; 0.482 sec/batch)
2016-02-03 12:58:39.291537: step 23850, loss = 0.92 (261.4 examples/sec; 0.490 sec/batch)
2016-02-03 12:58:43.914622: step 23860, loss = 0.67 (253.1 examples/sec; 0.506 sec/batch)
2016-02-03 12:58:48.660053: step 23870, loss = 0.74 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 12:58:53.320746: step 23880, loss = 0.93 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 12:58:57.934125: step 23890, loss = 0.97 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 12:59:02.648461: step 23900, loss = 0.81 (289.5 examples/sec; 0.442 sec/batch)
2016-02-03 12:59:07.896428: step 23910, loss = 0.75 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 12:59:12.635949: step 23920, loss = 0.79 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 12:59:17.296856: step 23930, loss = 0.89 (291.1 examples/sec; 0.440 sec/batch)
2016-02-03 12:59:21.919960: step 23940, loss = 0.81 (262.0 examples/sec; 0.489 sec/batch)
2016-02-03 12:59:26.447422: step 23950, loss = 0.95 (291.8 examples/sec; 0.439 sec/batch)
2016-02-03 12:59:31.134235: step 23960, loss = 0.90 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 12:59:35.782789: step 23970, loss = 0.75 (280.6 examples/sec; 0.456 sec/batch)
2016-02-03 12:59:40.337556: step 23980, loss = 0.81 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 12:59:45.008174: step 23990, loss = 0.87 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 12:59:49.706166: step 24000, loss = 0.83 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 12:59:54.978409: step 24010, loss = 0.72 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 12:59:59.607870: step 24020, loss = 0.82 (279.7 examples/sec; 0.458 sec/batch)
2016-02-03 13:00:04.286482: step 24030, loss = 0.95 (281.7 examples/sec; 0.454 sec/batch)
2016-02-03 13:00:08.980284: step 24040, loss = 0.74 (255.3 examples/sec; 0.501 sec/batch)
2016-02-03 13:00:13.668394: step 24050, loss = 0.76 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 13:00:18.249811: step 24060, loss = 0.75 (288.3 examples/sec; 0.444 sec/batch)
2016-02-03 13:00:22.933541: step 24070, loss = 0.80 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 13:00:27.656531: step 24080, loss = 0.67 (287.6 examples/sec; 0.445 sec/batch)
2016-02-03 13:00:32.333645: step 24090, loss = 0.82 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 13:00:36.952859: step 24100, loss = 0.84 (293.4 examples/sec; 0.436 sec/batch)
2016-02-03 13:00:42.090164: step 24110, loss = 0.76 (279.7 examples/sec; 0.458 sec/batch)
2016-02-03 13:00:46.740651: step 24120, loss = 0.66 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 13:00:51.406585: step 24130, loss = 0.79 (291.9 examples/sec; 0.438 sec/batch)
2016-02-03 13:00:56.050733: step 24140, loss = 0.61 (253.4 examples/sec; 0.505 sec/batch)
2016-02-03 13:01:00.614351: step 24150, loss = 0.87 (289.6 examples/sec; 0.442 sec/batch)
2016-02-03 13:01:05.259060: step 24160, loss = 0.83 (262.1 examples/sec; 0.488 sec/batch)
2016-02-03 13:01:09.974921: step 24170, loss = 0.66 (262.1 examples/sec; 0.488 sec/batch)
2016-02-03 13:01:14.594802: step 24180, loss = 0.85 (299.6 examples/sec; 0.427 sec/batch)
2016-02-03 13:01:19.287353: step 24190, loss = 0.78 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 13:01:23.817866: step 24200, loss = 0.83 (288.4 examples/sec; 0.444 sec/batch)
2016-02-03 13:01:29.003594: step 24210, loss = 0.94 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 13:01:33.537032: step 24220, loss = 0.73 (287.6 examples/sec; 0.445 sec/batch)
2016-02-03 13:01:38.150631: step 24230, loss = 0.74 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 13:01:42.709774: step 24240, loss = 0.86 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 13:01:47.340656: step 24250, loss = 0.75 (291.8 examples/sec; 0.439 sec/batch)
2016-02-03 13:01:52.079018: step 24260, loss = 0.68 (289.4 examples/sec; 0.442 sec/batch)
2016-02-03 13:01:56.814374: step 24270, loss = 0.80 (259.8 examples/sec; 0.493 sec/batch)
2016-02-03 13:02:01.469985: step 24280, loss = 0.76 (259.2 examples/sec; 0.494 sec/batch)
2016-02-03 13:02:06.123625: step 24290, loss = 1.02 (283.3 examples/sec; 0.452 sec/batch)
2016-02-03 13:02:10.831173: step 24300, loss = 0.88 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 13:02:16.063063: step 24310, loss = 0.85 (292.8 examples/sec; 0.437 sec/batch)
2016-02-03 13:02:20.808810: step 24320, loss = 0.99 (281.0 examples/sec; 0.456 sec/batch)
2016-02-03 13:02:25.519920: step 24330, loss = 0.89 (257.9 examples/sec; 0.496 sec/batch)
2016-02-03 13:02:30.147749: step 24340, loss = 0.84 (296.6 examples/sec; 0.432 sec/batch)
2016-02-03 13:02:34.903886: step 24350, loss = 0.88 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 13:02:39.538143: step 24360, loss = 0.83 (298.8 examples/sec; 0.428 sec/batch)
2016-02-03 13:02:44.389941: step 24370, loss = 0.78 (242.3 examples/sec; 0.528 sec/batch)
2016-02-03 13:02:49.014347: step 24380, loss = 0.93 (280.6 examples/sec; 0.456 sec/batch)
2016-02-03 13:02:53.781329: step 24390, loss = 0.88 (257.4 examples/sec; 0.497 sec/batch)
2016-02-03 13:02:58.437345: step 24400, loss = 0.76 (275.7 examples/sec; 0.464 sec/batch)
2016-02-03 13:03:03.655289: step 24410, loss = 0.81 (284.2 examples/sec; 0.450 sec/batch)
2016-02-03 13:03:08.314150: step 24420, loss = 0.71 (291.9 examples/sec; 0.439 sec/batch)
2016-02-03 13:03:13.017074: step 24430, loss = 0.83 (286.4 examples/sec; 0.447 sec/batch)
2016-02-03 13:03:17.717711: step 24440, loss = 0.87 (259.7 examples/sec; 0.493 sec/batch)
2016-02-03 13:03:22.463505: step 24450, loss = 0.80 (254.8 examples/sec; 0.502 sec/batch)
2016-02-03 13:03:27.128909: step 24460, loss = 0.84 (280.5 examples/sec; 0.456 sec/batch)
2016-02-03 13:03:31.839574: step 24470, loss = 0.89 (258.6 examples/sec; 0.495 sec/batch)
2016-02-03 13:03:36.533159: step 24480, loss = 0.79 (254.7 examples/sec; 0.503 sec/batch)
2016-02-03 13:03:41.209586: step 24490, loss = 0.87 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 13:03:45.815238: step 24500, loss = 0.77 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 13:03:51.062072: step 24510, loss = 0.85 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 13:03:55.717652: step 24520, loss = 0.90 (291.0 examples/sec; 0.440 sec/batch)
2016-02-03 13:04:00.439560: step 24530, loss = 0.66 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 13:04:05.101724: step 24540, loss = 0.89 (277.8 examples/sec; 0.461 sec/batch)
2016-02-03 13:04:09.724154: step 24550, loss = 0.85 (270.5 examples/sec; 0.473 sec/batch)
2016-02-03 13:04:14.300849: step 24560, loss = 0.91 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 13:04:19.002565: step 24570, loss = 0.72 (252.4 examples/sec; 0.507 sec/batch)
2016-02-03 13:04:23.713618: step 24580, loss = 0.76 (251.4 examples/sec; 0.509 sec/batch)
2016-02-03 13:04:28.433444: step 24590, loss = 0.85 (256.2 examples/sec; 0.500 sec/batch)
2016-02-03 13:04:33.099685: step 24600, loss = 0.67 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 13:04:38.357287: step 24610, loss = 0.79 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 13:04:43.039514: step 24620, loss = 0.81 (270.9 examples/sec; 0.473 sec/batch)
2016-02-03 13:04:47.794869: step 24630, loss = 0.79 (258.6 examples/sec; 0.495 sec/batch)
2016-02-03 13:04:52.505092: step 24640, loss = 0.78 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 13:04:57.203662: step 24650, loss = 0.77 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 13:05:01.885387: step 24660, loss = 0.66 (261.0 examples/sec; 0.490 sec/batch)
2016-02-03 13:05:06.554002: step 24670, loss = 0.78 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 13:05:11.240571: step 24680, loss = 0.72 (263.0 examples/sec; 0.487 sec/batch)
2016-02-03 13:05:15.986095: step 24690, loss = 0.80 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 13:05:20.684598: step 24700, loss = 0.84 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 13:05:25.880373: step 24710, loss = 0.78 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 13:05:30.552415: step 24720, loss = 0.85 (281.4 examples/sec; 0.455 sec/batch)
2016-02-03 13:05:35.277208: step 24730, loss = 0.85 (265.3 examples/sec; 0.482 sec/batch)
2016-02-03 13:05:39.958513: step 24740, loss = 0.84 (290.3 examples/sec; 0.441 sec/batch)
2016-02-03 13:05:44.616310: step 24750, loss = 0.78 (286.9 examples/sec; 0.446 sec/batch)
2016-02-03 13:05:49.302635: step 24760, loss = 0.81 (279.6 examples/sec; 0.458 sec/batch)
2016-02-03 13:05:53.927988: step 24770, loss = 0.85 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 13:05:58.682588: step 24780, loss = 0.73 (286.6 examples/sec; 0.447 sec/batch)
2016-02-03 13:06:03.439227: step 24790, loss = 0.68 (257.5 examples/sec; 0.497 sec/batch)
2016-02-03 13:06:08.162671: step 24800, loss = 0.99 (238.8 examples/sec; 0.536 sec/batch)
2016-02-03 13:06:13.283838: step 24810, loss = 0.83 (293.8 examples/sec; 0.436 sec/batch)
2016-02-03 13:06:17.927418: step 24820, loss = 0.95 (293.5 examples/sec; 0.436 sec/batch)
2016-02-03 13:06:22.676027: step 24830, loss = 0.77 (297.7 examples/sec; 0.430 sec/batch)
2016-02-03 13:06:27.395776: step 24840, loss = 0.78 (265.8 examples/sec; 0.481 sec/batch)
2016-02-03 13:06:31.985803: step 24850, loss = 0.89 (301.9 examples/sec; 0.424 sec/batch)
2016-02-03 13:06:36.636044: step 24860, loss = 0.75 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 13:06:41.325314: step 24870, loss = 0.83 (299.9 examples/sec; 0.427 sec/batch)
2016-02-03 13:06:46.005101: step 24880, loss = 0.83 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 13:06:50.711747: step 24890, loss = 0.74 (286.4 examples/sec; 0.447 sec/batch)
2016-02-03 13:06:55.295011: step 24900, loss = 0.77 (294.8 examples/sec; 0.434 sec/batch)
2016-02-03 13:07:00.434592: step 24910, loss = 0.77 (288.3 examples/sec; 0.444 sec/batch)
2016-02-03 13:07:05.207442: step 24920, loss = 0.84 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 13:07:09.849503: step 24930, loss = 0.78 (270.5 examples/sec; 0.473 sec/batch)
2016-02-03 13:07:14.600513: step 24940, loss = 0.79 (283.8 examples/sec; 0.451 sec/batch)
2016-02-03 13:07:19.290670: step 24950, loss = 0.77 (287.2 examples/sec; 0.446 sec/batch)
2016-02-03 13:07:23.964340: step 24960, loss = 0.80 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 13:07:28.631823: step 24970, loss = 0.71 (275.7 examples/sec; 0.464 sec/batch)
2016-02-03 13:07:33.340813: step 24980, loss = 0.91 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 13:07:38.044282: step 24990, loss = 0.98 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 13:07:42.730249: step 25000, loss = 0.98 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 13:07:47.916630: step 25010, loss = 0.71 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 13:07:52.629269: step 25020, loss = 0.75 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 13:07:57.292484: step 25030, loss = 0.77 (279.7 examples/sec; 0.458 sec/batch)
2016-02-03 13:08:01.977775: step 25040, loss = 0.70 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 13:08:06.625112: step 25050, loss = 0.92 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 13:08:11.302858: step 25060, loss = 0.82 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 13:08:15.908012: step 25070, loss = 0.68 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 13:08:20.690857: step 25080, loss = 0.89 (238.8 examples/sec; 0.536 sec/batch)
2016-02-03 13:08:25.335799: step 25090, loss = 1.11 (286.9 examples/sec; 0.446 sec/batch)
2016-02-03 13:08:29.982648: step 25100, loss = 0.82 (256.5 examples/sec; 0.499 sec/batch)
2016-02-03 13:08:35.133537: step 25110, loss = 0.81 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 13:08:39.827816: step 25120, loss = 0.75 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 13:08:44.488032: step 25130, loss = 0.87 (292.7 examples/sec; 0.437 sec/batch)
2016-02-03 13:08:49.210345: step 25140, loss = 0.87 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 13:08:53.795804: step 25150, loss = 0.85 (315.2 examples/sec; 0.406 sec/batch)
2016-02-03 13:08:58.501667: step 25160, loss = 0.78 (279.2 examples/sec; 0.458 sec/batch)
2016-02-03 13:09:03.103978: step 25170, loss = 0.81 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 13:09:07.805611: step 25180, loss = 0.90 (293.5 examples/sec; 0.436 sec/batch)
2016-02-03 13:09:12.484150: step 25190, loss = 0.79 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 13:09:17.155843: step 25200, loss = 0.66 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 13:09:22.448646: step 25210, loss = 0.77 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 13:09:26.983617: step 25220, loss = 0.73 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 13:09:31.715646: step 25230, loss = 0.74 (259.5 examples/sec; 0.493 sec/batch)
2016-02-03 13:09:36.386847: step 25240, loss = 0.93 (264.3 examples/sec; 0.484 sec/batch)
2016-02-03 13:09:41.039651: step 25250, loss = 0.78 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 13:09:45.694600: step 25260, loss = 0.94 (257.8 examples/sec; 0.496 sec/batch)
2016-02-03 13:09:50.413129: step 25270, loss = 0.85 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 13:09:55.146814: step 25280, loss = 0.90 (252.6 examples/sec; 0.507 sec/batch)
2016-02-03 13:09:59.882764: step 25290, loss = 0.90 (265.0 examples/sec; 0.483 sec/batch)
2016-02-03 13:10:04.553580: step 25300, loss = 0.76 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 13:10:09.845720: step 25310, loss = 0.80 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 13:10:14.548083: step 25320, loss = 0.64 (256.6 examples/sec; 0.499 sec/batch)
2016-02-03 13:10:19.207211: step 25330, loss = 0.72 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 13:10:23.935754: step 25340, loss = 0.82 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 13:10:28.642522: step 25350, loss = 0.84 (265.3 examples/sec; 0.483 sec/batch)
2016-02-03 13:10:33.331656: step 25360, loss = 0.88 (280.9 examples/sec; 0.456 sec/batch)
2016-02-03 13:10:38.069479: step 25370, loss = 0.86 (248.8 examples/sec; 0.514 sec/batch)
2016-02-03 13:10:42.629759: step 25380, loss = 0.82 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 13:10:47.340288: step 25390, loss = 0.85 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 13:10:51.929015: step 25400, loss = 0.87 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 13:10:57.066869: step 25410, loss = 0.96 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 13:11:01.686354: step 25420, loss = 0.95 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 13:11:06.353476: step 25430, loss = 0.83 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 13:11:11.066162: step 25440, loss = 0.79 (256.9 examples/sec; 0.498 sec/batch)
2016-02-03 13:11:15.743031: step 25450, loss = 0.69 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 13:11:20.420224: step 25460, loss = 0.75 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 13:11:25.135702: step 25470, loss = 0.79 (259.0 examples/sec; 0.494 sec/batch)
2016-02-03 13:11:29.859419: step 25480, loss = 0.70 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 13:11:34.565428: step 25490, loss = 0.77 (300.4 examples/sec; 0.426 sec/batch)
2016-02-03 13:11:39.286256: step 25500, loss = 0.86 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 13:11:44.434582: step 25510, loss = 0.70 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 13:11:49.192238: step 25520, loss = 0.85 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 13:11:53.880117: step 25530, loss = 0.83 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 13:11:58.513616: step 25540, loss = 0.71 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 13:12:03.273288: step 25550, loss = 0.79 (253.7 examples/sec; 0.505 sec/batch)
2016-02-03 13:12:08.048966: step 25560, loss = 0.77 (263.9 examples/sec; 0.485 sec/batch)
2016-02-03 13:12:12.716920: step 25570, loss = 0.92 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 13:12:17.446448: step 25580, loss = 0.91 (267.5 examples/sec; 0.479 sec/batch)
2016-02-03 13:12:22.133176: step 25590, loss = 0.82 (279.8 examples/sec; 0.457 sec/batch)
2016-02-03 13:12:26.846498: step 25600, loss = 0.79 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 13:12:32.079418: step 25610, loss = 0.83 (263.1 examples/sec; 0.486 sec/batch)
2016-02-03 13:12:36.730517: step 25620, loss = 1.01 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 13:12:41.464730: step 25630, loss = 0.60 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 13:12:46.215702: step 25640, loss = 0.94 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 13:12:50.964904: step 25650, loss = 0.79 (249.3 examples/sec; 0.513 sec/batch)
2016-02-03 13:12:55.673241: step 25660, loss = 0.62 (255.7 examples/sec; 0.501 sec/batch)
2016-02-03 13:13:00.371449: step 25670, loss = 0.77 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 13:13:05.094256: step 25680, loss = 0.84 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 13:13:09.717330: step 25690, loss = 0.88 (300.9 examples/sec; 0.425 sec/batch)
2016-02-03 13:13:14.439667: step 25700, loss = 0.83 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 13:13:19.706453: step 25710, loss = 0.71 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 13:13:24.415180: step 25720, loss = 0.94 (276.9 examples/sec; 0.462 sec/batch)
2016-02-03 13:13:29.121289: step 25730, loss = 0.86 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 13:13:33.726896: step 25740, loss = 0.65 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 13:13:38.411193: step 25750, loss = 0.97 (295.2 examples/sec; 0.434 sec/batch)
2016-02-03 13:13:43.087729: step 25760, loss = 0.79 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 13:13:47.826066: step 25770, loss = 0.79 (240.1 examples/sec; 0.533 sec/batch)
2016-02-03 13:13:52.511804: step 25780, loss = 0.82 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 13:13:57.286403: step 25790, loss = 0.82 (249.3 examples/sec; 0.513 sec/batch)
2016-02-03 13:14:02.018213: step 25800, loss = 0.79 (280.1 examples/sec; 0.457 sec/batch)
2016-02-03 13:14:07.236461: step 25810, loss = 0.82 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 13:14:11.991044: step 25820, loss = 0.87 (270.3 examples/sec; 0.473 sec/batch)
2016-02-03 13:14:16.634486: step 25830, loss = 0.90 (286.8 examples/sec; 0.446 sec/batch)
2016-02-03 13:14:21.357258: step 25840, loss = 0.85 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 13:14:26.095246: step 25850, loss = 0.78 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 13:14:30.772187: step 25860, loss = 0.75 (275.0 examples/sec; 0.465 sec/batch)
2016-02-03 13:14:35.461424: step 25870, loss = 0.85 (259.9 examples/sec; 0.492 sec/batch)
2016-02-03 13:14:40.122929: step 25880, loss = 0.85 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 13:14:44.855496: step 25890, loss = 1.07 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 13:14:49.574615: step 25900, loss = 0.89 (246.3 examples/sec; 0.520 sec/batch)
2016-02-03 13:14:54.783117: step 25910, loss = 0.75 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 13:14:59.556269: step 25920, loss = 1.01 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 13:15:04.290662: step 25930, loss = 0.92 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 13:15:08.970224: step 25940, loss = 0.76 (246.7 examples/sec; 0.519 sec/batch)
2016-02-03 13:15:13.580728: step 25950, loss = 0.74 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 13:15:18.165361: step 25960, loss = 0.86 (305.0 examples/sec; 0.420 sec/batch)
2016-02-03 13:15:22.780127: step 25970, loss = 0.79 (283.3 examples/sec; 0.452 sec/batch)
2016-02-03 13:15:27.524996: step 25980, loss = 0.76 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 13:15:32.181923: step 25990, loss = 0.80 (285.7 examples/sec; 0.448 sec/batch)
2016-02-03 13:15:36.855298: step 26000, loss = 0.97 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 13:15:42.120982: step 26010, loss = 0.96 (275.6 examples/sec; 0.464 sec/batch)
2016-02-03 13:15:46.841448: step 26020, loss = 0.76 (253.8 examples/sec; 0.504 sec/batch)
2016-02-03 13:15:51.597803: step 26030, loss = 0.83 (261.0 examples/sec; 0.490 sec/batch)
2016-02-03 13:15:56.332106: step 26040, loss = 0.81 (254.7 examples/sec; 0.503 sec/batch)
2016-02-03 13:16:01.055233: step 26050, loss = 0.80 (258.1 examples/sec; 0.496 sec/batch)
2016-02-03 13:16:05.608953: step 26060, loss = 0.82 (289.8 examples/sec; 0.442 sec/batch)
2016-02-03 13:16:10.359012: step 26070, loss = 0.76 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 13:16:15.077723: step 26080, loss = 0.87 (282.6 examples/sec; 0.453 sec/batch)
2016-02-03 13:16:19.767956: step 26090, loss = 0.79 (263.2 examples/sec; 0.486 sec/batch)
2016-02-03 13:16:24.432353: step 26100, loss = 0.81 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 13:16:29.724146: step 26110, loss = 0.66 (288.7 examples/sec; 0.443 sec/batch)
2016-02-03 13:16:34.535007: step 26120, loss = 0.72 (269.2 examples/sec; 0.475 sec/batch)
2016-02-03 13:16:39.374807: step 26130, loss = 0.81 (241.9 examples/sec; 0.529 sec/batch)
2016-02-03 13:16:44.117177: step 26140, loss = 0.87 (283.4 examples/sec; 0.452 sec/batch)
2016-02-03 13:16:48.924294: step 26150, loss = 0.62 (254.1 examples/sec; 0.504 sec/batch)
2016-02-03 13:16:53.666228: step 26160, loss = 0.83 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 13:16:58.319723: step 26170, loss = 0.84 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 13:17:03.053755: step 26180, loss = 0.81 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 13:17:07.774655: step 26190, loss = 0.93 (283.4 examples/sec; 0.452 sec/batch)
2016-02-03 13:17:12.537825: step 26200, loss = 0.83 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 13:17:17.846064: step 26210, loss = 0.78 (259.7 examples/sec; 0.493 sec/batch)
2016-02-03 13:17:22.565316: step 26220, loss = 0.87 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 13:17:27.326202: step 26230, loss = 0.79 (256.8 examples/sec; 0.498 sec/batch)
2016-02-03 13:17:32.007767: step 26240, loss = 0.97 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 13:17:36.717432: step 26250, loss = 0.66 (301.5 examples/sec; 0.425 sec/batch)
2016-02-03 13:17:41.417173: step 26260, loss = 0.83 (309.0 examples/sec; 0.414 sec/batch)
2016-02-03 13:17:46.101942: step 26270, loss = 0.79 (301.2 examples/sec; 0.425 sec/batch)
2016-02-03 13:17:50.809987: step 26280, loss = 0.94 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 13:17:55.468304: step 26290, loss = 0.80 (312.6 examples/sec; 0.409 sec/batch)
2016-02-03 13:18:00.184181: step 26300, loss = 0.79 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 13:18:05.406233: step 26310, loss = 0.79 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 13:18:10.076957: step 26320, loss = 0.94 (286.9 examples/sec; 0.446 sec/batch)
2016-02-03 13:18:14.812599: step 26330, loss = 0.74 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 13:18:19.455636: step 26340, loss = 0.74 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 13:18:24.147416: step 26350, loss = 0.95 (276.4 examples/sec; 0.463 sec/batch)
2016-02-03 13:18:28.789375: step 26360, loss = 0.69 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 13:18:33.432330: step 26370, loss = 0.80 (262.6 examples/sec; 0.487 sec/batch)
2016-02-03 13:18:38.081087: step 26380, loss = 0.85 (302.2 examples/sec; 0.424 sec/batch)
2016-02-03 13:18:42.834394: step 26390, loss = 0.83 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 13:18:47.520973: step 26400, loss = 0.76 (295.4 examples/sec; 0.433 sec/batch)
2016-02-03 13:18:52.790117: step 26410, loss = 0.83 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 13:18:57.480800: step 26420, loss = 0.90 (254.0 examples/sec; 0.504 sec/batch)
2016-02-03 13:19:02.154563: step 26430, loss = 0.73 (274.4 examples/sec; 0.466 sec/batch)
2016-02-03 13:19:06.775869: step 26440, loss = 0.86 (295.8 examples/sec; 0.433 sec/batch)
2016-02-03 13:19:11.556977: step 26450, loss = 0.90 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 13:19:16.360910: step 26460, loss = 0.82 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 13:19:21.037201: step 26470, loss = 0.76 (275.7 examples/sec; 0.464 sec/batch)
2016-02-03 13:19:25.748743: step 26480, loss = 0.76 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 13:19:30.455922: step 26490, loss = 0.69 (287.3 examples/sec; 0.445 sec/batch)
2016-02-03 13:19:35.249615: step 26500, loss = 0.83 (251.1 examples/sec; 0.510 sec/batch)
2016-02-03 13:19:40.482423: step 26510, loss = 0.99 (253.8 examples/sec; 0.504 sec/batch)
2016-02-03 13:19:45.214505: step 26520, loss = 0.76 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 13:19:49.942113: step 26530, loss = 0.94 (255.6 examples/sec; 0.501 sec/batch)
2016-02-03 13:19:54.599601: step 26540, loss = 0.60 (262.1 examples/sec; 0.488 sec/batch)
2016-02-03 13:19:59.280415: step 26550, loss = 0.87 (263.9 examples/sec; 0.485 sec/batch)
2016-02-03 13:20:03.989568: step 26560, loss = 0.85 (269.2 examples/sec; 0.475 sec/batch)
2016-02-03 13:20:08.691833: step 26570, loss = 0.80 (259.2 examples/sec; 0.494 sec/batch)
2016-02-03 13:20:13.404278: step 26580, loss = 0.82 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 13:20:18.016435: step 26590, loss = 0.76 (281.2 examples/sec; 0.455 sec/batch)
2016-02-03 13:20:22.740183: step 26600, loss = 0.76 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 13:20:27.948561: step 26610, loss = 0.82 (258.8 examples/sec; 0.495 sec/batch)
2016-02-03 13:20:32.707491: step 26620, loss = 0.80 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 13:20:37.400104: step 26630, loss = 0.83 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 13:20:42.194422: step 26640, loss = 0.88 (259.2 examples/sec; 0.494 sec/batch)
2016-02-03 13:20:46.913102: step 26650, loss = 0.99 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 13:20:51.646148: step 26660, loss = 0.77 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 13:20:56.334197: step 26670, loss = 0.80 (277.2 examples/sec; 0.462 sec/batch)
2016-02-03 13:21:01.016886: step 26680, loss = 1.09 (297.5 examples/sec; 0.430 sec/batch)
2016-02-03 13:21:05.809299: step 26690, loss = 0.82 (278.6 examples/sec; 0.460 sec/batch)
2016-02-03 13:21:10.488030: step 26700, loss = 0.73 (260.1 examples/sec; 0.492 sec/batch)
2016-02-03 13:21:15.743315: step 26710, loss = 0.95 (262.6 examples/sec; 0.487 sec/batch)
2016-02-03 13:21:20.495033: step 26720, loss = 0.78 (250.3 examples/sec; 0.511 sec/batch)
2016-02-03 13:21:25.207506: step 26730, loss = 0.83 (291.5 examples/sec; 0.439 sec/batch)
2016-02-03 13:21:29.987611: step 26740, loss = 0.71 (248.5 examples/sec; 0.515 sec/batch)
2016-02-03 13:21:34.712231: step 26750, loss = 0.86 (288.8 examples/sec; 0.443 sec/batch)
2016-02-03 13:21:39.517893: step 26760, loss = 0.97 (251.1 examples/sec; 0.510 sec/batch)
2016-02-03 13:21:44.215175: step 26770, loss = 0.92 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 13:21:48.876059: step 26780, loss = 1.02 (286.7 examples/sec; 0.447 sec/batch)
2016-02-03 13:21:53.547541: step 26790, loss = 0.93 (272.6 examples/sec; 0.470 sec/batch)
2016-02-03 13:21:58.296488: step 26800, loss = 0.77 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 13:22:03.575283: step 26810, loss = 0.77 (246.1 examples/sec; 0.520 sec/batch)
2016-02-03 13:22:08.209538: step 26820, loss = 0.83 (292.3 examples/sec; 0.438 sec/batch)
2016-02-03 13:22:12.979095: step 26830, loss = 0.94 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 13:22:17.728006: step 26840, loss = 0.82 (258.2 examples/sec; 0.496 sec/batch)
2016-02-03 13:22:22.467375: step 26850, loss = 0.78 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 13:22:27.119390: step 26860, loss = 0.76 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 13:22:31.805048: step 26870, loss = 0.89 (290.8 examples/sec; 0.440 sec/batch)
2016-02-03 13:22:36.549893: step 26880, loss = 0.98 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 13:22:41.267105: step 26890, loss = 0.80 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 13:22:45.884731: step 26900, loss = 0.77 (300.3 examples/sec; 0.426 sec/batch)
2016-02-03 13:22:50.896978: step 26910, loss = 0.86 (292.9 examples/sec; 0.437 sec/batch)
2016-02-03 13:22:55.501975: step 26920, loss = 0.74 (287.3 examples/sec; 0.446 sec/batch)
2016-02-03 13:23:00.163894: step 26930, loss = 0.72 (262.0 examples/sec; 0.488 sec/batch)
2016-02-03 13:23:04.914547: step 26940, loss = 0.81 (251.8 examples/sec; 0.508 sec/batch)
2016-02-03 13:23:09.502195: step 26950, loss = 0.81 (250.0 examples/sec; 0.512 sec/batch)
2016-02-03 13:23:14.131319: step 26960, loss = 0.82 (270.5 examples/sec; 0.473 sec/batch)
2016-02-03 13:23:18.784536: step 26970, loss = 0.73 (310.5 examples/sec; 0.412 sec/batch)
2016-02-03 13:23:23.407798: step 26980, loss = 0.83 (289.6 examples/sec; 0.442 sec/batch)
2016-02-03 13:23:28.074741: step 26990, loss = 0.81 (297.0 examples/sec; 0.431 sec/batch)
2016-02-03 13:23:32.775009: step 27000, loss = 0.76 (248.0 examples/sec; 0.516 sec/batch)
2016-02-03 13:23:37.937016: step 27010, loss = 0.94 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 13:23:42.619594: step 27020, loss = 0.74 (259.8 examples/sec; 0.493 sec/batch)
2016-02-03 13:23:47.347475: step 27030, loss = 0.73 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 13:23:52.059822: step 27040, loss = 0.97 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 13:23:56.664303: step 27050, loss = 1.03 (296.2 examples/sec; 0.432 sec/batch)
2016-02-03 13:24:01.398002: step 27060, loss = 0.70 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 13:24:06.157635: step 27070, loss = 0.85 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 13:24:10.822962: step 27080, loss = 0.78 (283.0 examples/sec; 0.452 sec/batch)
2016-02-03 13:24:15.582920: step 27090, loss = 0.78 (268.2 examples/sec; 0.477 sec/batch)
2016-02-03 13:24:20.351453: step 27100, loss = 0.75 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 13:24:25.637135: step 27110, loss = 0.87 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 13:24:30.331148: step 27120, loss = 0.79 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 13:24:35.034607: step 27130, loss = 0.77 (243.5 examples/sec; 0.526 sec/batch)
2016-02-03 13:24:39.769857: step 27140, loss = 0.82 (259.8 examples/sec; 0.493 sec/batch)
2016-02-03 13:24:44.455194: step 27150, loss = 0.93 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 13:24:49.141235: step 27160, loss = 0.84 (248.9 examples/sec; 0.514 sec/batch)
2016-02-03 13:24:53.737350: step 27170, loss = 0.77 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 13:24:58.444672: step 27180, loss = 0.74 (301.3 examples/sec; 0.425 sec/batch)
2016-02-03 13:25:03.137459: step 27190, loss = 0.81 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 13:25:07.862008: step 27200, loss = 0.73 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 13:25:13.088044: step 27210, loss = 0.70 (244.5 examples/sec; 0.524 sec/batch)
2016-02-03 13:25:17.728927: step 27220, loss = 0.81 (289.2 examples/sec; 0.443 sec/batch)
2016-02-03 13:25:22.461767: step 27230, loss = 0.92 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 13:25:27.024911: step 27240, loss = 0.73 (286.9 examples/sec; 0.446 sec/batch)
2016-02-03 13:25:31.655283: step 27250, loss = 0.66 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 13:25:36.471417: step 27260, loss = 0.69 (254.8 examples/sec; 0.502 sec/batch)
2016-02-03 13:25:41.206628: step 27270, loss = 0.95 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 13:25:45.924383: step 27280, loss = 0.78 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 13:25:50.633828: step 27290, loss = 0.75 (277.2 examples/sec; 0.462 sec/batch)
2016-02-03 13:25:55.404651: step 27300, loss = 0.76 (280.9 examples/sec; 0.456 sec/batch)
2016-02-03 13:26:00.674174: step 27310, loss = 0.92 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 13:26:05.489998: step 27320, loss = 1.02 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 13:26:10.202423: step 27330, loss = 0.80 (289.6 examples/sec; 0.442 sec/batch)
2016-02-03 13:26:14.851034: step 27340, loss = 0.79 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 13:26:19.541010: step 27350, loss = 0.78 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 13:26:24.145640: step 27360, loss = 0.71 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 13:26:28.883526: step 27370, loss = 0.83 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 13:26:33.637521: step 27380, loss = 0.82 (264.1 examples/sec; 0.485 sec/batch)
2016-02-03 13:26:38.334357: step 27390, loss = 0.74 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 13:26:43.035055: step 27400, loss = 0.76 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 13:26:48.310998: step 27410, loss = 0.80 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 13:26:52.997997: step 27420, loss = 0.72 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 13:26:57.699513: step 27430, loss = 0.84 (300.2 examples/sec; 0.426 sec/batch)
2016-02-03 13:27:02.490060: step 27440, loss = 0.85 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 13:27:07.135394: step 27450, loss = 0.69 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 13:27:11.830390: step 27460, loss = 0.81 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 13:27:16.556015: step 27470, loss = 0.81 (284.6 examples/sec; 0.450 sec/batch)
2016-02-03 13:27:21.259766: step 27480, loss = 0.74 (250.4 examples/sec; 0.511 sec/batch)
2016-02-03 13:27:25.923781: step 27490, loss = 0.70 (297.9 examples/sec; 0.430 sec/batch)
2016-02-03 13:27:30.713069: step 27500, loss = 0.63 (244.9 examples/sec; 0.523 sec/batch)
2016-02-03 13:27:35.939156: step 27510, loss = 0.78 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 13:27:40.664926: step 27520, loss = 0.85 (257.3 examples/sec; 0.497 sec/batch)
2016-02-03 13:27:45.413920: step 27530, loss = 0.78 (245.4 examples/sec; 0.522 sec/batch)
2016-02-03 13:27:50.055848: step 27540, loss = 0.77 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 13:27:54.731213: step 27550, loss = 0.87 (302.5 examples/sec; 0.423 sec/batch)
2016-02-03 13:27:59.378996: step 27560, loss = 0.68 (278.6 examples/sec; 0.460 sec/batch)
2016-02-03 13:28:04.035407: step 27570, loss = 0.74 (284.6 examples/sec; 0.450 sec/batch)
2016-02-03 13:28:08.705424: step 27580, loss = 0.69 (276.9 examples/sec; 0.462 sec/batch)
2016-02-03 13:28:13.361227: step 27590, loss = 1.04 (254.5 examples/sec; 0.503 sec/batch)
2016-02-03 13:28:17.946663: step 27600, loss = 0.87 (287.3 examples/sec; 0.446 sec/batch)
2016-02-03 13:28:23.084558: step 27610, loss = 0.85 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 13:28:27.767753: step 27620, loss = 0.82 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 13:28:32.464538: step 27630, loss = 0.91 (292.1 examples/sec; 0.438 sec/batch)
2016-02-03 13:28:37.136908: step 27640, loss = 0.87 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 13:28:41.802746: step 27650, loss = 0.73 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 13:28:46.429181: step 27660, loss = 0.82 (304.3 examples/sec; 0.421 sec/batch)
2016-02-03 13:28:51.118643: step 27670, loss = 0.81 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 13:28:55.876663: step 27680, loss = 0.70 (256.3 examples/sec; 0.499 sec/batch)
2016-02-03 13:29:00.572734: step 27690, loss = 1.09 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 13:29:05.259920: step 27700, loss = 0.74 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 13:29:10.365135: step 27710, loss = 0.76 (279.2 examples/sec; 0.458 sec/batch)
2016-02-03 13:29:15.031719: step 27720, loss = 0.71 (293.2 examples/sec; 0.437 sec/batch)
2016-02-03 13:29:19.813631: step 27730, loss = 0.69 (263.0 examples/sec; 0.487 sec/batch)
2016-02-03 13:29:24.465225: step 27740, loss = 0.74 (280.9 examples/sec; 0.456 sec/batch)
2016-02-03 13:29:29.169052: step 27750, loss = 0.77 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 13:29:33.903993: step 27760, loss = 0.70 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 13:29:38.597761: step 27770, loss = 0.75 (256.6 examples/sec; 0.499 sec/batch)
2016-02-03 13:29:43.340879: step 27780, loss = 0.94 (264.5 examples/sec; 0.484 sec/batch)
2016-02-03 13:29:48.044970: step 27790, loss = 0.77 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 13:29:52.742179: step 27800, loss = 0.82 (263.1 examples/sec; 0.487 sec/batch)
2016-02-03 13:29:57.998552: step 27810, loss = 0.65 (303.9 examples/sec; 0.421 sec/batch)
2016-02-03 13:30:02.635644: step 27820, loss = 0.69 (297.5 examples/sec; 0.430 sec/batch)
2016-02-03 13:30:07.401137: step 27830, loss = 0.88 (257.0 examples/sec; 0.498 sec/batch)
2016-02-03 13:30:12.123406: step 27840, loss = 0.74 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 13:30:16.833853: step 27850, loss = 0.82 (247.6 examples/sec; 0.517 sec/batch)
2016-02-03 13:30:21.544905: step 27860, loss = 0.83 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 13:30:26.260700: step 27870, loss = 0.75 (274.4 examples/sec; 0.466 sec/batch)
2016-02-03 13:30:30.957089: step 27880, loss = 0.84 (261.5 examples/sec; 0.489 sec/batch)
2016-02-03 13:30:35.580895: step 27890, loss = 0.89 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 13:30:40.279616: step 27900, loss = 0.66 (244.5 examples/sec; 0.524 sec/batch)
2016-02-03 13:30:45.587462: step 27910, loss = 0.68 (245.2 examples/sec; 0.522 sec/batch)
2016-02-03 13:30:50.286495: step 27920, loss = 0.78 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 13:30:55.054558: step 27930, loss = 0.75 (234.8 examples/sec; 0.545 sec/batch)
2016-02-03 13:30:59.812256: step 27940, loss = 0.72 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 13:31:04.603451: step 27950, loss = 0.72 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 13:31:09.364753: step 27960, loss = 0.76 (256.9 examples/sec; 0.498 sec/batch)
2016-02-03 13:31:14.022188: step 27970, loss = 0.91 (289.4 examples/sec; 0.442 sec/batch)
2016-02-03 13:31:18.643367: step 27980, loss = 0.75 (282.8 examples/sec; 0.453 sec/batch)
2016-02-03 13:31:23.283641: step 27990, loss = 0.65 (251.3 examples/sec; 0.509 sec/batch)
2016-02-03 13:31:27.912851: step 28000, loss = 0.76 (297.4 examples/sec; 0.430 sec/batch)
2016-02-03 13:31:33.172869: step 28010, loss = 0.87 (280.1 examples/sec; 0.457 sec/batch)
2016-02-03 13:31:37.805826: step 28020, loss = 0.80 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 13:31:42.402616: step 28030, loss = 0.80 (283.7 examples/sec; 0.451 sec/batch)
2016-02-03 13:31:46.987515: step 28040, loss = 0.82 (286.0 examples/sec; 0.448 sec/batch)
2016-02-03 13:31:51.581424: step 28050, loss = 0.77 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 13:31:56.244268: step 28060, loss = 0.91 (263.2 examples/sec; 0.486 sec/batch)
2016-02-03 13:32:00.930719: step 28070, loss = 0.76 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 13:32:05.613009: step 28080, loss = 0.86 (282.9 examples/sec; 0.453 sec/batch)
2016-02-03 13:32:10.245823: step 28090, loss = 0.76 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 13:32:14.858647: step 28100, loss = 0.79 (258.9 examples/sec; 0.494 sec/batch)
2016-02-03 13:32:20.037595: step 28110, loss = 0.73 (264.5 examples/sec; 0.484 sec/batch)
2016-02-03 13:32:24.664467: step 28120, loss = 0.91 (303.9 examples/sec; 0.421 sec/batch)
2016-02-03 13:32:29.357732: step 28130, loss = 0.65 (290.1 examples/sec; 0.441 sec/batch)
2016-02-03 13:32:34.067039: step 28140, loss = 0.90 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 13:32:38.800412: step 28150, loss = 0.90 (318.1 examples/sec; 0.402 sec/batch)
2016-02-03 13:32:43.435247: step 28160, loss = 0.85 (287.9 examples/sec; 0.445 sec/batch)
2016-02-03 13:32:48.139671: step 28170, loss = 0.66 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 13:32:52.864296: step 28180, loss = 0.72 (282.5 examples/sec; 0.453 sec/batch)
2016-02-03 13:32:57.582964: step 28190, loss = 0.82 (284.6 examples/sec; 0.450 sec/batch)
2016-02-03 13:33:02.302103: step 28200, loss = 0.89 (310.5 examples/sec; 0.412 sec/batch)
2016-02-03 13:33:07.486749: step 28210, loss = 0.85 (263.0 examples/sec; 0.487 sec/batch)
2016-02-03 13:33:12.234077: step 28220, loss = 0.92 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 13:33:16.861827: step 28230, loss = 0.76 (284.4 examples/sec; 0.450 sec/batch)
2016-02-03 13:33:21.507817: step 28240, loss = 0.90 (321.2 examples/sec; 0.398 sec/batch)
2016-02-03 13:33:26.230944: step 28250, loss = 0.81 (278.6 examples/sec; 0.460 sec/batch)
2016-02-03 13:33:30.920852: step 28260, loss = 0.83 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 13:33:35.626925: step 28270, loss = 1.04 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 13:33:40.380842: step 28280, loss = 0.79 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 13:33:45.091717: step 28290, loss = 0.61 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 13:33:49.697727: step 28300, loss = 0.82 (289.9 examples/sec; 0.441 sec/batch)
2016-02-03 13:33:54.911237: step 28310, loss = 0.68 (288.1 examples/sec; 0.444 sec/batch)
2016-02-03 13:33:59.614861: step 28320, loss = 0.73 (272.7 examples/sec; 0.469 sec/batch)
2016-02-03 13:34:04.391330: step 28330, loss = 0.80 (277.6 examples/sec; 0.461 sec/batch)
2016-02-03 13:34:09.012356: step 28340, loss = 0.82 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 13:34:13.758003: step 28350, loss = 0.76 (290.8 examples/sec; 0.440 sec/batch)
2016-02-03 13:34:18.533208: step 28360, loss = 0.77 (238.6 examples/sec; 0.536 sec/batch)
2016-02-03 13:34:23.222495: step 28370, loss = 0.82 (277.6 examples/sec; 0.461 sec/batch)
2016-02-03 13:34:27.875242: step 28380, loss = 0.69 (274.4 examples/sec; 0.466 sec/batch)
2016-02-03 13:34:32.517101: step 28390, loss = 0.90 (264.5 examples/sec; 0.484 sec/batch)
2016-02-03 13:34:37.236409: step 28400, loss = 0.76 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 13:34:42.493000: step 28410, loss = 0.66 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 13:34:47.314718: step 28420, loss = 0.83 (249.5 examples/sec; 0.513 sec/batch)
2016-02-03 13:34:52.068633: step 28430, loss = 0.76 (255.0 examples/sec; 0.502 sec/batch)
2016-02-03 13:34:56.867350: step 28440, loss = 0.89 (272.6 examples/sec; 0.470 sec/batch)
2016-02-03 13:35:01.557927: step 28450, loss = 0.80 (295.4 examples/sec; 0.433 sec/batch)
2016-02-03 13:35:06.315559: step 28460, loss = 0.93 (261.4 examples/sec; 0.490 sec/batch)
2016-02-03 13:35:10.912897: step 28470, loss = 0.88 (286.6 examples/sec; 0.447 sec/batch)
2016-02-03 13:35:15.681385: step 28480, loss = 0.76 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 13:35:20.434989: step 28490, loss = 0.92 (270.5 examples/sec; 0.473 sec/batch)
2016-02-03 13:35:25.122938: step 28500, loss = 0.68 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 13:35:30.406344: step 28510, loss = 0.85 (283.4 examples/sec; 0.452 sec/batch)
2016-02-03 13:35:35.083098: step 28520, loss = 0.91 (256.8 examples/sec; 0.498 sec/batch)
2016-02-03 13:35:39.756303: step 28530, loss = 0.79 (246.5 examples/sec; 0.519 sec/batch)
2016-02-03 13:35:44.418636: step 28540, loss = 0.84 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 13:35:49.129745: step 28550, loss = 0.86 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 13:35:53.767887: step 28560, loss = 0.79 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 13:35:58.377677: step 28570, loss = 0.78 (279.6 examples/sec; 0.458 sec/batch)
2016-02-03 13:36:03.036759: step 28580, loss = 0.85 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 13:36:07.719079: step 28590, loss = 0.79 (276.9 examples/sec; 0.462 sec/batch)
2016-02-03 13:36:12.344904: step 28600, loss = 0.82 (288.1 examples/sec; 0.444 sec/batch)
2016-02-03 13:36:17.601299: step 28610, loss = 0.89 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 13:36:22.250085: step 28620, loss = 0.85 (267.0 examples/sec; 0.479 sec/batch)
2016-02-03 13:36:26.949718: step 28630, loss = 0.77 (285.2 examples/sec; 0.449 sec/batch)
2016-02-03 13:36:31.722498: step 28640, loss = 1.00 (259.2 examples/sec; 0.494 sec/batch)
2016-02-03 13:36:36.460416: step 28650, loss = 0.88 (257.2 examples/sec; 0.498 sec/batch)
2016-02-03 13:36:41.140839: step 28660, loss = 0.81 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 13:36:45.777781: step 28670, loss = 0.92 (277.7 examples/sec; 0.461 sec/batch)
2016-02-03 13:36:50.353214: step 28680, loss = 0.75 (272.6 examples/sec; 0.470 sec/batch)
2016-02-03 13:36:55.112522: step 28690, loss = 0.80 (258.6 examples/sec; 0.495 sec/batch)
2016-02-03 13:36:59.684612: step 28700, loss = 0.70 (309.0 examples/sec; 0.414 sec/batch)
2016-02-03 13:37:04.931721: step 28710, loss = 0.87 (264.3 examples/sec; 0.484 sec/batch)
2016-02-03 13:37:09.670348: step 28720, loss = 0.76 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 13:37:14.386046: step 28730, loss = 0.74 (272.7 examples/sec; 0.469 sec/batch)
2016-02-03 13:37:19.133717: step 28740, loss = 0.84 (240.2 examples/sec; 0.533 sec/batch)
2016-02-03 13:37:23.844033: step 28750, loss = 0.71 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 13:37:28.547087: step 28760, loss = 0.86 (259.4 examples/sec; 0.493 sec/batch)
2016-02-03 13:37:33.193126: step 28770, loss = 0.80 (247.7 examples/sec; 0.517 sec/batch)
2016-02-03 13:37:37.869354: step 28780, loss = 0.81 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 13:37:42.620088: step 28790, loss = 0.99 (275.0 examples/sec; 0.465 sec/batch)
2016-02-03 13:37:47.385836: step 28800, loss = 0.80 (276.6 examples/sec; 0.463 sec/batch)
2016-02-03 13:37:52.671829: step 28810, loss = 0.74 (292.2 examples/sec; 0.438 sec/batch)
2016-02-03 13:37:57.302772: step 28820, loss = 0.74 (277.7 examples/sec; 0.461 sec/batch)
2016-02-03 13:38:01.978562: step 28830, loss = 0.75 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 13:38:06.648646: step 28840, loss = 0.68 (310.4 examples/sec; 0.412 sec/batch)
2016-02-03 13:38:11.359805: step 28850, loss = 0.81 (296.1 examples/sec; 0.432 sec/batch)
2016-02-03 13:38:16.153004: step 28860, loss = 0.92 (265.0 examples/sec; 0.483 sec/batch)
2016-02-03 13:38:20.884036: step 28870, loss = 0.69 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 13:38:25.625849: step 28880, loss = 0.83 (258.1 examples/sec; 0.496 sec/batch)
2016-02-03 13:38:30.323677: step 28890, loss = 0.79 (268.6 examples/sec; 0.477 sec/batch)
2016-02-03 13:38:35.083234: step 28900, loss = 0.72 (260.7 examples/sec; 0.491 sec/batch)
2016-02-03 13:38:40.339653: step 28910, loss = 0.84 (275.6 examples/sec; 0.465 sec/batch)
2016-02-03 13:38:44.995738: step 28920, loss = 0.88 (285.7 examples/sec; 0.448 sec/batch)
2016-02-03 13:38:49.730552: step 28930, loss = 0.72 (290.7 examples/sec; 0.440 sec/batch)
2016-02-03 13:38:54.465460: step 28940, loss = 0.86 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 13:38:59.217809: step 28950, loss = 0.71 (257.2 examples/sec; 0.498 sec/batch)
2016-02-03 13:39:04.025172: step 28960, loss = 0.75 (283.4 examples/sec; 0.452 sec/batch)
2016-02-03 13:39:08.760152: step 28970, loss = 0.79 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 13:39:13.483226: step 28980, loss = 0.89 (288.9 examples/sec; 0.443 sec/batch)
2016-02-03 13:39:18.205704: step 28990, loss = 0.78 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 13:39:22.903838: step 29000, loss = 0.78 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 13:39:28.207571: step 29010, loss = 0.86 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 13:39:32.991338: step 29020, loss = 0.64 (266.4 examples/sec; 0.480 sec/batch)
2016-02-03 13:39:37.730387: step 29030, loss = 0.72 (254.7 examples/sec; 0.502 sec/batch)
2016-02-03 13:39:42.437043: step 29040, loss = 0.77 (276.6 examples/sec; 0.463 sec/batch)
2016-02-03 13:39:47.199261: step 29050, loss = 0.79 (253.8 examples/sec; 0.504 sec/batch)
2016-02-03 13:39:51.928086: step 29060, loss = 0.74 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 13:39:56.631501: step 29070, loss = 0.84 (279.2 examples/sec; 0.459 sec/batch)
2016-02-03 13:40:01.431872: step 29080, loss = 0.73 (260.6 examples/sec; 0.491 sec/batch)
2016-02-03 13:40:06.178591: step 29090, loss = 0.77 (277.7 examples/sec; 0.461 sec/batch)
2016-02-03 13:40:10.920350: step 29100, loss = 0.78 (287.3 examples/sec; 0.446 sec/batch)
2016-02-03 13:40:16.219297: step 29110, loss = 0.74 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 13:40:20.920719: step 29120, loss = 0.80 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 13:40:25.681833: step 29130, loss = 0.89 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 13:40:30.421913: step 29140, loss = 0.72 (259.4 examples/sec; 0.494 sec/batch)
2016-02-03 13:40:35.149749: step 29150, loss = 0.79 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 13:40:39.946137: step 29160, loss = 0.75 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 13:40:44.694589: step 29170, loss = 0.68 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 13:40:49.397451: step 29180, loss = 0.70 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 13:40:54.096523: step 29190, loss = 0.69 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 13:40:58.886806: step 29200, loss = 0.80 (255.9 examples/sec; 0.500 sec/batch)
2016-02-03 13:41:04.055970: step 29210, loss = 0.74 (253.3 examples/sec; 0.505 sec/batch)
2016-02-03 13:41:08.792442: step 29220, loss = 0.77 (269.2 examples/sec; 0.475 sec/batch)
2016-02-03 13:41:13.543476: step 29230, loss = 0.85 (249.2 examples/sec; 0.514 sec/batch)
2016-02-03 13:41:18.224646: step 29240, loss = 0.67 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 13:41:22.927131: step 29250, loss = 0.82 (255.3 examples/sec; 0.501 sec/batch)
2016-02-03 13:41:27.696147: step 29260, loss = 0.83 (251.6 examples/sec; 0.509 sec/batch)
2016-02-03 13:41:32.496337: step 29270, loss = 0.81 (252.5 examples/sec; 0.507 sec/batch)
2016-02-03 13:41:37.218058: step 29280, loss = 0.88 (291.4 examples/sec; 0.439 sec/batch)
2016-02-03 13:41:42.020910: step 29290, loss = 0.71 (242.4 examples/sec; 0.528 sec/batch)
2016-02-03 13:41:46.756841: step 29300, loss = 0.73 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 13:41:52.012344: step 29310, loss = 0.91 (279.7 examples/sec; 0.458 sec/batch)
2016-02-03 13:41:56.683559: step 29320, loss = 0.85 (305.0 examples/sec; 0.420 sec/batch)
2016-02-03 13:42:01.432634: step 29330, loss = 0.83 (255.8 examples/sec; 0.500 sec/batch)
2016-02-03 13:42:06.139406: step 29340, loss = 0.76 (267.0 examples/sec; 0.479 sec/batch)
2016-02-03 13:42:10.845859: step 29350, loss = 0.85 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 13:42:15.648452: step 29360, loss = 0.67 (261.9 examples/sec; 0.489 sec/batch)
2016-02-03 13:42:20.374772: step 29370, loss = 0.80 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 13:42:25.061821: step 29380, loss = 0.70 (293.3 examples/sec; 0.436 sec/batch)
2016-02-03 13:42:29.770396: step 29390, loss = 0.74 (289.3 examples/sec; 0.442 sec/batch)
2016-02-03 13:42:34.413250: step 29400, loss = 0.72 (289.3 examples/sec; 0.442 sec/batch)
2016-02-03 13:42:39.648509: step 29410, loss = 0.72 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 13:42:44.306562: step 29420, loss = 0.87 (308.2 examples/sec; 0.415 sec/batch)
2016-02-03 13:42:49.079734: step 29430, loss = 0.91 (248.0 examples/sec; 0.516 sec/batch)
2016-02-03 13:42:53.811594: step 29440, loss = 0.73 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 13:42:58.470736: step 29450, loss = 0.64 (281.0 examples/sec; 0.455 sec/batch)
2016-02-03 13:43:03.078437: step 29460, loss = 0.64 (266.4 examples/sec; 0.481 sec/batch)
2016-02-03 13:43:07.788891: step 29470, loss = 0.81 (281.7 examples/sec; 0.454 sec/batch)
2016-02-03 13:43:12.574771: step 29480, loss = 0.85 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 13:43:17.305288: step 29490, loss = 0.75 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 13:43:22.099480: step 29500, loss = 0.73 (257.6 examples/sec; 0.497 sec/batch)
2016-02-03 13:43:27.368573: step 29510, loss = 0.78 (256.7 examples/sec; 0.499 sec/batch)
2016-02-03 13:43:32.036426: step 29520, loss = 0.73 (285.9 examples/sec; 0.448 sec/batch)
2016-02-03 13:43:36.873412: step 29530, loss = 1.01 (259.8 examples/sec; 0.493 sec/batch)
2016-02-03 13:43:41.645041: step 29540, loss = 0.81 (242.9 examples/sec; 0.527 sec/batch)
2016-02-03 13:43:46.375127: step 29550, loss = 0.92 (281.2 examples/sec; 0.455 sec/batch)
2016-02-03 13:43:51.103601: step 29560, loss = 0.85 (266.9 examples/sec; 0.480 sec/batch)
2016-02-03 13:43:55.943030: step 29570, loss = 0.80 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 13:44:00.670755: step 29580, loss = 0.93 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 13:44:05.356320: step 29590, loss = 0.85 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 13:44:10.080910: step 29600, loss = 0.77 (259.7 examples/sec; 0.493 sec/batch)
2016-02-03 13:44:15.162055: step 29610, loss = 0.75 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 13:44:19.859484: step 29620, loss = 0.84 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 13:44:24.653389: step 29630, loss = 0.80 (258.1 examples/sec; 0.496 sec/batch)
2016-02-03 13:44:29.417554: step 29640, loss = 0.80 (257.6 examples/sec; 0.497 sec/batch)
2016-02-03 13:44:34.116169: step 29650, loss = 0.79 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 13:44:38.756554: step 29660, loss = 0.72 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 13:44:43.484229: step 29670, loss = 0.75 (286.3 examples/sec; 0.447 sec/batch)
2016-02-03 13:44:48.157085: step 29680, loss = 0.76 (260.9 examples/sec; 0.491 sec/batch)
2016-02-03 13:44:52.906495: step 29690, loss = 0.81 (234.1 examples/sec; 0.547 sec/batch)
2016-02-03 13:44:57.604415: step 29700, loss = 0.79 (289.7 examples/sec; 0.442 sec/batch)
2016-02-03 13:45:03.029040: step 29710, loss = 0.61 (260.3 examples/sec; 0.492 sec/batch)
2016-02-03 13:45:07.808414: step 29720, loss = 0.85 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 13:45:12.583090: step 29730, loss = 0.69 (264.5 examples/sec; 0.484 sec/batch)
2016-02-03 13:45:17.306246: step 29740, loss = 0.70 (281.4 examples/sec; 0.455 sec/batch)
2016-02-03 13:45:21.993413: step 29750, loss = 0.90 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 13:45:26.683667: step 29760, loss = 0.83 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 13:45:31.419057: step 29770, loss = 0.88 (243.6 examples/sec; 0.525 sec/batch)
2016-02-03 13:45:36.206709: step 29780, loss = 0.82 (248.1 examples/sec; 0.516 sec/batch)
2016-02-03 13:45:40.942994: step 29790, loss = 0.68 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 13:45:45.734301: step 29800, loss = 0.81 (259.9 examples/sec; 0.493 sec/batch)
2016-02-03 13:45:50.997402: step 29810, loss = 0.69 (263.6 examples/sec; 0.486 sec/batch)
2016-02-03 13:45:55.739927: step 29820, loss = 0.71 (270.3 examples/sec; 0.473 sec/batch)
2016-02-03 13:46:00.470950: step 29830, loss = 0.78 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 13:46:05.206914: step 29840, loss = 0.72 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 13:46:10.032192: step 29850, loss = 0.98 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 13:46:14.793390: step 29860, loss = 0.89 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 13:46:19.489531: step 29870, loss = 0.72 (280.1 examples/sec; 0.457 sec/batch)
2016-02-03 13:46:24.184808: step 29880, loss = 0.86 (304.1 examples/sec; 0.421 sec/batch)
2016-02-03 13:46:28.865397: step 29890, loss = 0.66 (284.9 examples/sec; 0.449 sec/batch)
2016-02-03 13:46:33.579584: step 29900, loss = 0.73 (280.6 examples/sec; 0.456 sec/batch)
2016-02-03 13:46:38.957289: step 29910, loss = 0.83 (255.4 examples/sec; 0.501 sec/batch)
2016-02-03 13:46:43.670183: step 29920, loss = 0.67 (258.5 examples/sec; 0.495 sec/batch)
2016-02-03 13:46:48.369362: step 29930, loss = 0.89 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 13:46:53.111950: step 29940, loss = 0.76 (286.6 examples/sec; 0.447 sec/batch)
2016-02-03 13:46:57.824476: step 29950, loss = 0.77 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 13:47:02.566490: step 29960, loss = 0.77 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 13:47:07.256794: step 29970, loss = 1.05 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 13:47:11.971727: step 29980, loss = 0.76 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 13:47:16.656950: step 29990, loss = 0.75 (306.7 examples/sec; 0.417 sec/batch)
2016-02-03 13:47:21.366731: step 30000, loss = 0.75 (293.2 examples/sec; 0.437 sec/batch)
2016-02-03 13:47:26.579203: step 30010, loss = 0.90 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 13:47:31.253472: step 30020, loss = 0.90 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 13:47:36.049587: step 30030, loss = 0.78 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 13:47:40.803673: step 30040, loss = 0.84 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 13:47:45.413072: step 30050, loss = 0.66 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 13:47:50.175468: step 30060, loss = 0.78 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 13:47:54.956098: step 30070, loss = 0.89 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 13:47:59.674392: step 30080, loss = 0.84 (259.5 examples/sec; 0.493 sec/batch)
2016-02-03 13:48:04.364510: step 30090, loss = 0.82 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 13:48:09.031213: step 30100, loss = 0.85 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 13:48:14.248205: step 30110, loss = 0.75 (287.8 examples/sec; 0.445 sec/batch)
2016-02-03 13:48:18.941772: step 30120, loss = 0.81 (290.9 examples/sec; 0.440 sec/batch)
2016-02-03 13:48:23.585195: step 30130, loss = 0.89 (286.1 examples/sec; 0.447 sec/batch)
2016-02-03 13:48:28.298087: step 30140, loss = 0.82 (298.3 examples/sec; 0.429 sec/batch)
2016-02-03 13:48:33.048722: step 30150, loss = 0.83 (279.6 examples/sec; 0.458 sec/batch)
2016-02-03 13:48:37.779875: step 30160, loss = 0.75 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 13:48:42.474113: step 30170, loss = 0.75 (259.9 examples/sec; 0.493 sec/batch)
2016-02-03 13:48:47.109171: step 30180, loss = 0.92 (299.8 examples/sec; 0.427 sec/batch)
2016-02-03 13:48:51.836899: step 30190, loss = 0.82 (304.5 examples/sec; 0.420 sec/batch)
2016-02-03 13:48:56.626877: step 30200, loss = 0.77 (255.4 examples/sec; 0.501 sec/batch)
2016-02-03 13:49:01.832699: step 30210, loss = 1.02 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 13:49:06.534650: step 30220, loss = 0.82 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 13:49:11.281827: step 30230, loss = 0.80 (258.3 examples/sec; 0.496 sec/batch)
2016-02-03 13:49:15.935570: step 30240, loss = 0.67 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 13:49:20.706775: step 30250, loss = 0.85 (256.1 examples/sec; 0.500 sec/batch)
2016-02-03 13:49:25.467055: step 30260, loss = 0.76 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 13:49:30.212868: step 30270, loss = 0.76 (267.3 examples/sec; 0.479 sec/batch)
2016-02-03 13:49:34.888110: step 30280, loss = 0.82 (263.6 examples/sec; 0.486 sec/batch)
2016-02-03 13:49:39.532982: step 30290, loss = 0.79 (287.0 examples/sec; 0.446 sec/batch)
2016-02-03 13:49:44.383162: step 30300, loss = 0.71 (262.2 examples/sec; 0.488 sec/batch)
2016-02-03 13:49:49.610245: step 30310, loss = 0.77 (280.5 examples/sec; 0.456 sec/batch)
2016-02-03 13:49:54.447100: step 30320, loss = 0.86 (246.3 examples/sec; 0.520 sec/batch)
2016-02-03 13:49:59.179254: step 30330, loss = 0.78 (270.3 examples/sec; 0.473 sec/batch)
2016-02-03 13:50:03.916855: step 30340, loss = 0.85 (248.2 examples/sec; 0.516 sec/batch)
2016-02-03 13:50:08.549349: step 30350, loss = 0.85 (276.3 examples/sec; 0.463 sec/batch)
2016-02-03 13:50:13.259946: step 30360, loss = 0.70 (292.0 examples/sec; 0.438 sec/batch)
2016-02-03 13:50:18.023998: step 30370, loss = 0.89 (229.7 examples/sec; 0.557 sec/batch)
2016-02-03 13:50:22.770579: step 30380, loss = 0.78 (264.5 examples/sec; 0.484 sec/batch)
2016-02-03 13:50:27.484349: step 30390, loss = 0.85 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 13:50:32.202350: step 30400, loss = 0.72 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 13:50:37.503353: step 30410, loss = 0.76 (306.6 examples/sec; 0.418 sec/batch)
2016-02-03 13:50:42.195523: step 30420, loss = 0.93 (258.2 examples/sec; 0.496 sec/batch)
2016-02-03 13:50:46.883066: step 30430, loss = 0.75 (315.8 examples/sec; 0.405 sec/batch)
2016-02-03 13:50:51.602278: step 30440, loss = 0.76 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 13:50:56.311533: step 30450, loss = 0.89 (307.6 examples/sec; 0.416 sec/batch)
2016-02-03 13:51:01.065585: step 30460, loss = 0.74 (283.8 examples/sec; 0.451 sec/batch)
2016-02-03 13:51:05.817211: step 30470, loss = 0.87 (250.6 examples/sec; 0.511 sec/batch)
2016-02-03 13:51:10.458954: step 30480, loss = 0.64 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 13:51:15.113243: step 30490, loss = 0.72 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 13:51:19.839143: step 30500, loss = 0.80 (292.5 examples/sec; 0.438 sec/batch)
2016-02-03 13:51:25.063030: step 30510, loss = 0.70 (249.8 examples/sec; 0.512 sec/batch)
2016-02-03 13:51:29.799483: step 30520, loss = 0.89 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 13:51:34.504384: step 30530, loss = 0.89 (286.8 examples/sec; 0.446 sec/batch)
2016-02-03 13:51:39.256667: step 30540, loss = 0.82 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 13:51:44.018996: step 30550, loss = 0.90 (239.8 examples/sec; 0.534 sec/batch)
2016-02-03 13:51:48.759861: step 30560, loss = 0.77 (260.6 examples/sec; 0.491 sec/batch)
2016-02-03 13:51:53.488421: step 30570, loss = 0.76 (290.4 examples/sec; 0.441 sec/batch)
2016-02-03 13:51:58.213591: step 30580, loss = 0.80 (277.8 examples/sec; 0.461 sec/batch)
2016-02-03 13:52:02.848465: step 30590, loss = 0.81 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 13:52:07.476627: step 30600, loss = 0.86 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 13:52:12.627956: step 30610, loss = 0.72 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 13:52:17.367906: step 30620, loss = 0.81 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 13:52:22.076152: step 30630, loss = 0.91 (240.9 examples/sec; 0.531 sec/batch)
2016-02-03 13:52:26.752699: step 30640, loss = 0.84 (288.9 examples/sec; 0.443 sec/batch)
2016-02-03 13:52:31.412000: step 30650, loss = 0.85 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 13:52:36.140879: step 30660, loss = 0.68 (240.2 examples/sec; 0.533 sec/batch)
2016-02-03 13:52:40.882316: step 30670, loss = 0.78 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 13:52:45.522929: step 30680, loss = 0.80 (296.8 examples/sec; 0.431 sec/batch)
2016-02-03 13:52:50.259057: step 30690, loss = 0.82 (279.8 examples/sec; 0.457 sec/batch)
2016-02-03 13:52:54.977700: step 30700, loss = 0.76 (273.2 examples/sec; 0.469 sec/batch)
2016-02-03 13:53:00.158907: step 30710, loss = 0.80 (275.6 examples/sec; 0.464 sec/batch)
2016-02-03 13:53:04.764282: step 30720, loss = 0.85 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 13:53:09.480662: step 30730, loss = 0.73 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 13:53:14.135040: step 30740, loss = 0.91 (275.6 examples/sec; 0.464 sec/batch)
2016-02-03 13:53:18.811920: step 30750, loss = 0.89 (263.6 examples/sec; 0.486 sec/batch)
2016-02-03 13:53:23.587528: step 30760, loss = 0.71 (264.7 examples/sec; 0.483 sec/batch)
2016-02-03 13:53:28.286646: step 30770, loss = 0.80 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 13:53:33.040135: step 30780, loss = 0.86 (264.7 examples/sec; 0.484 sec/batch)
2016-02-03 13:53:37.769018: step 30790, loss = 0.79 (258.8 examples/sec; 0.495 sec/batch)
2016-02-03 13:53:42.507770: step 30800, loss = 0.66 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 13:53:47.769484: step 30810, loss = 0.78 (310.4 examples/sec; 0.412 sec/batch)
2016-02-03 13:53:52.545916: step 30820, loss = 0.78 (255.1 examples/sec; 0.502 sec/batch)
2016-02-03 13:53:57.269175: step 30830, loss = 0.75 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 13:54:01.961331: step 30840, loss = 0.87 (292.6 examples/sec; 0.437 sec/batch)
2016-02-03 13:54:06.664349: step 30850, loss = 0.91 (296.1 examples/sec; 0.432 sec/batch)
2016-02-03 13:54:11.341021: step 30860, loss = 1.00 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 13:54:15.998081: step 30870, loss = 0.75 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 13:54:20.666939: step 30880, loss = 0.79 (303.8 examples/sec; 0.421 sec/batch)
2016-02-03 13:54:25.356024: step 30890, loss = 0.75 (285.6 examples/sec; 0.448 sec/batch)
2016-02-03 13:54:30.071328: step 30900, loss = 0.84 (296.2 examples/sec; 0.432 sec/batch)
2016-02-03 13:54:35.375933: step 30910, loss = 0.80 (277.7 examples/sec; 0.461 sec/batch)
2016-02-03 13:54:40.054937: step 30920, loss = 0.79 (264.2 examples/sec; 0.484 sec/batch)
2016-02-03 13:54:44.793329: step 30930, loss = 0.89 (254.3 examples/sec; 0.503 sec/batch)
2016-02-03 13:54:49.512096: step 30940, loss = 0.79 (252.1 examples/sec; 0.508 sec/batch)
2016-02-03 13:54:54.245315: step 30950, loss = 0.71 (297.9 examples/sec; 0.430 sec/batch)
2016-02-03 13:54:58.889029: step 30960, loss = 0.81 (284.4 examples/sec; 0.450 sec/batch)
2016-02-03 13:55:03.638324: step 30970, loss = 0.76 (266.4 examples/sec; 0.480 sec/batch)
2016-02-03 13:55:08.356536: step 30980, loss = 0.88 (261.9 examples/sec; 0.489 sec/batch)
2016-02-03 13:55:13.085543: step 30990, loss = 0.95 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 13:55:17.932777: step 31000, loss = 0.80 (288.1 examples/sec; 0.444 sec/batch)
2016-02-03 13:55:23.251824: step 31010, loss = 0.90 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 13:55:27.981882: step 31020, loss = 0.65 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 13:55:32.658397: step 31030, loss = 0.85 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 13:55:37.385219: step 31040, loss = 0.71 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 13:55:42.131604: step 31050, loss = 0.73 (258.6 examples/sec; 0.495 sec/batch)
2016-02-03 13:55:46.764787: step 31060, loss = 0.79 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 13:55:51.504550: step 31070, loss = 0.80 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 13:55:56.120680: step 31080, loss = 0.82 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 13:56:00.796958: step 31090, loss = 0.82 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 13:56:05.505902: step 31100, loss = 0.92 (265.2 examples/sec; 0.483 sec/batch)
2016-02-03 13:56:10.668781: step 31110, loss = 0.71 (256.2 examples/sec; 0.500 sec/batch)
2016-02-03 13:56:15.405218: step 31120, loss = 0.86 (253.6 examples/sec; 0.505 sec/batch)
2016-02-03 13:56:20.122349: step 31130, loss = 0.88 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 13:56:24.763871: step 31140, loss = 0.75 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 13:56:29.391405: step 31150, loss = 0.75 (262.2 examples/sec; 0.488 sec/batch)
2016-02-03 13:56:34.062970: step 31160, loss = 0.75 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 13:56:38.754616: step 31170, loss = 0.78 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 13:56:43.402810: step 31180, loss = 0.77 (264.7 examples/sec; 0.484 sec/batch)
2016-02-03 13:56:48.042083: step 31190, loss = 0.64 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 13:56:52.771271: step 31200, loss = 0.85 (298.9 examples/sec; 0.428 sec/batch)
2016-02-03 13:56:58.002758: step 31210, loss = 0.75 (283.3 examples/sec; 0.452 sec/batch)
2016-02-03 13:57:02.709764: step 31220, loss = 0.80 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 13:57:07.449891: step 31230, loss = 0.73 (255.2 examples/sec; 0.502 sec/batch)
2016-02-03 13:57:12.142439: step 31240, loss = 0.82 (309.3 examples/sec; 0.414 sec/batch)
2016-02-03 13:57:16.805051: step 31250, loss = 0.70 (296.4 examples/sec; 0.432 sec/batch)
2016-02-03 13:57:21.513800: step 31260, loss = 0.76 (260.7 examples/sec; 0.491 sec/batch)
2016-02-03 13:57:26.226376: step 31270, loss = 0.88 (289.3 examples/sec; 0.443 sec/batch)
2016-02-03 13:57:30.883896: step 31280, loss = 0.89 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 13:57:35.649076: step 31290, loss = 0.94 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 13:57:40.407809: step 31300, loss = 0.77 (292.1 examples/sec; 0.438 sec/batch)
2016-02-03 13:57:45.737131: step 31310, loss = 0.90 (264.3 examples/sec; 0.484 sec/batch)
2016-02-03 13:57:50.387810: step 31320, loss = 0.87 (282.6 examples/sec; 0.453 sec/batch)
2016-02-03 13:57:55.078207: step 31330, loss = 0.89 (288.9 examples/sec; 0.443 sec/batch)
2016-02-03 13:57:59.825002: step 31340, loss = 0.77 (276.7 examples/sec; 0.463 sec/batch)
2016-02-03 13:58:04.559874: step 31350, loss = 0.76 (268.6 examples/sec; 0.477 sec/batch)
2016-02-03 13:58:09.306099: step 31360, loss = 0.68 (254.8 examples/sec; 0.502 sec/batch)
2016-02-03 13:58:14.098375: step 31370, loss = 0.72 (263.0 examples/sec; 0.487 sec/batch)
2016-02-03 13:58:18.732529: step 31380, loss = 0.83 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 13:58:23.487326: step 31390, loss = 0.69 (265.3 examples/sec; 0.482 sec/batch)
2016-02-03 13:58:28.270678: step 31400, loss = 0.79 (260.3 examples/sec; 0.492 sec/batch)
2016-02-03 13:58:33.461413: step 31410, loss = 0.78 (295.5 examples/sec; 0.433 sec/batch)
2016-02-03 13:58:38.172609: step 31420, loss = 0.85 (290.1 examples/sec; 0.441 sec/batch)
2016-02-03 13:58:42.868354: step 31430, loss = 0.77 (278.6 examples/sec; 0.460 sec/batch)
2016-02-03 13:58:47.683657: step 31440, loss = 0.81 (258.4 examples/sec; 0.495 sec/batch)
2016-02-03 13:58:52.451886: step 31450, loss = 0.81 (261.0 examples/sec; 0.490 sec/batch)
2016-02-03 13:58:57.192417: step 31460, loss = 0.99 (295.3 examples/sec; 0.433 sec/batch)
2016-02-03 13:59:01.913757: step 31470, loss = 0.60 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 13:59:06.595996: step 31480, loss = 0.73 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 13:59:11.388285: step 31490, loss = 0.83 (244.1 examples/sec; 0.524 sec/batch)
2016-02-03 13:59:16.009908: step 31500, loss = 0.90 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 13:59:21.228706: step 31510, loss = 0.60 (279.7 examples/sec; 0.458 sec/batch)
2016-02-03 13:59:25.968755: step 31520, loss = 0.71 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 13:59:30.647898: step 31530, loss = 0.86 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 13:59:35.381799: step 31540, loss = 0.77 (282.6 examples/sec; 0.453 sec/batch)
2016-02-03 13:59:40.215399: step 31550, loss = 0.89 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 13:59:45.022151: step 31560, loss = 0.72 (253.8 examples/sec; 0.504 sec/batch)
2016-02-03 13:59:49.756529: step 31570, loss = 0.68 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 13:59:54.517221: step 31580, loss = 0.64 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 13:59:59.140657: step 31590, loss = 0.72 (292.3 examples/sec; 0.438 sec/batch)
2016-02-03 14:00:03.879210: step 31600, loss = 0.69 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 14:00:09.092748: step 31610, loss = 0.89 (259.8 examples/sec; 0.493 sec/batch)
2016-02-03 14:00:13.824239: step 31620, loss = 0.75 (256.2 examples/sec; 0.500 sec/batch)
2016-02-03 14:00:18.536490: step 31630, loss = 0.95 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 14:00:23.317504: step 31640, loss = 0.82 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 14:00:28.070845: step 31650, loss = 0.95 (288.8 examples/sec; 0.443 sec/batch)
2016-02-03 14:00:32.841025: step 31660, loss = 0.69 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 14:00:37.593993: step 31670, loss = 0.82 (289.3 examples/sec; 0.443 sec/batch)
2016-02-03 14:00:42.361646: step 31680, loss = 0.80 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 14:00:47.069422: step 31690, loss = 0.80 (283.0 examples/sec; 0.452 sec/batch)
2016-02-03 14:00:51.790309: step 31700, loss = 0.73 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 14:00:56.992128: step 31710, loss = 0.77 (285.7 examples/sec; 0.448 sec/batch)
2016-02-03 14:01:01.725016: step 31720, loss = 0.78 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 14:01:06.493304: step 31730, loss = 0.76 (264.7 examples/sec; 0.484 sec/batch)
2016-02-03 14:01:11.198811: step 31740, loss = 0.84 (255.3 examples/sec; 0.501 sec/batch)
2016-02-03 14:01:15.880077: step 31750, loss = 0.73 (260.8 examples/sec; 0.491 sec/batch)
2016-02-03 14:01:20.611178: step 31760, loss = 0.73 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 14:01:25.327434: step 31770, loss = 0.75 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 14:01:30.070863: step 31780, loss = 0.78 (285.9 examples/sec; 0.448 sec/batch)
2016-02-03 14:01:34.761884: step 31790, loss = 0.83 (285.7 examples/sec; 0.448 sec/batch)
2016-02-03 14:01:39.465980: step 31800, loss = 0.97 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 14:01:44.697658: step 31810, loss = 0.87 (290.4 examples/sec; 0.441 sec/batch)
2016-02-03 14:01:49.471156: step 31820, loss = 0.82 (254.1 examples/sec; 0.504 sec/batch)
2016-02-03 14:01:54.248756: step 31830, loss = 0.86 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 14:01:59.033854: step 31840, loss = 0.84 (253.9 examples/sec; 0.504 sec/batch)
2016-02-03 14:02:03.710817: step 31850, loss = 0.86 (251.6 examples/sec; 0.509 sec/batch)
2016-02-03 14:02:08.403980: step 31860, loss = 0.74 (261.7 examples/sec; 0.489 sec/batch)
2016-02-03 14:02:13.187537: step 31870, loss = 0.82 (273.8 examples/sec; 0.468 sec/batch)
2016-02-03 14:02:17.904547: step 31880, loss = 0.64 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 14:02:22.629336: step 31890, loss = 0.72 (305.4 examples/sec; 0.419 sec/batch)
2016-02-03 14:02:27.331389: step 31900, loss = 0.83 (261.1 examples/sec; 0.490 sec/batch)
2016-02-03 14:02:32.517187: step 31910, loss = 0.75 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 14:02:37.213152: step 31920, loss = 0.90 (289.5 examples/sec; 0.442 sec/batch)
2016-02-03 14:02:41.899174: step 31930, loss = 1.01 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 14:02:46.586278: step 31940, loss = 0.95 (294.4 examples/sec; 0.435 sec/batch)
2016-02-03 14:02:51.292999: step 31950, loss = 0.80 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 14:02:55.960238: step 31960, loss = 0.69 (258.6 examples/sec; 0.495 sec/batch)
2016-02-03 14:03:00.662576: step 31970, loss = 0.61 (260.6 examples/sec; 0.491 sec/batch)
2016-02-03 14:03:05.326666: step 31980, loss = 1.02 (263.9 examples/sec; 0.485 sec/batch)
2016-02-03 14:03:09.977139: step 31990, loss = 0.81 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 14:03:14.693022: step 32000, loss = 0.71 (253.8 examples/sec; 0.504 sec/batch)
2016-02-03 14:03:19.985038: step 32010, loss = 0.71 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 14:03:24.726011: step 32020, loss = 0.70 (251.9 examples/sec; 0.508 sec/batch)
2016-02-03 14:03:29.405818: step 32030, loss = 0.74 (261.3 examples/sec; 0.490 sec/batch)
2016-02-03 14:03:34.077442: step 32040, loss = 0.74 (289.6 examples/sec; 0.442 sec/batch)
2016-02-03 14:03:38.794183: step 32050, loss = 0.75 (273.8 examples/sec; 0.468 sec/batch)
2016-02-03 14:03:43.417651: step 32060, loss = 0.74 (290.3 examples/sec; 0.441 sec/batch)
2016-02-03 14:03:48.088947: step 32070, loss = 0.59 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 14:03:52.736766: step 32080, loss = 0.83 (300.0 examples/sec; 0.427 sec/batch)
2016-02-03 14:03:57.495461: step 32090, loss = 0.67 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 14:04:02.144417: step 32100, loss = 0.72 (314.4 examples/sec; 0.407 sec/batch)
2016-02-03 14:04:07.303249: step 32110, loss = 0.89 (297.3 examples/sec; 0.430 sec/batch)
2016-02-03 14:04:12.084112: step 32120, loss = 0.76 (260.7 examples/sec; 0.491 sec/batch)
2016-02-03 14:04:16.779486: step 32130, loss = 1.02 (262.6 examples/sec; 0.487 sec/batch)
2016-02-03 14:04:21.450676: step 32140, loss = 0.71 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 14:04:26.089071: step 32150, loss = 0.90 (282.4 examples/sec; 0.453 sec/batch)
2016-02-03 14:04:30.740464: step 32160, loss = 0.67 (296.1 examples/sec; 0.432 sec/batch)
2016-02-03 14:04:35.379706: step 32170, loss = 0.74 (255.7 examples/sec; 0.501 sec/batch)
2016-02-03 14:04:40.059539: step 32180, loss = 0.84 (291.8 examples/sec; 0.439 sec/batch)
2016-02-03 14:04:44.716325: step 32190, loss = 0.88 (284.4 examples/sec; 0.450 sec/batch)
2016-02-03 14:04:49.365262: step 32200, loss = 0.68 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 14:04:54.523332: step 32210, loss = 0.94 (286.8 examples/sec; 0.446 sec/batch)
2016-02-03 14:04:59.211628: step 32220, loss = 0.61 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 14:05:03.837473: step 32230, loss = 0.85 (304.6 examples/sec; 0.420 sec/batch)
2016-02-03 14:05:08.397519: step 32240, loss = 0.77 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 14:05:13.055212: step 32250, loss = 0.83 (268.2 examples/sec; 0.477 sec/batch)
2016-02-03 14:05:17.744614: step 32260, loss = 0.84 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 14:05:22.316413: step 32270, loss = 0.75 (282.4 examples/sec; 0.453 sec/batch)
2016-02-03 14:05:27.048852: step 32280, loss = 0.84 (284.4 examples/sec; 0.450 sec/batch)
2016-02-03 14:05:31.656416: step 32290, loss = 0.70 (261.1 examples/sec; 0.490 sec/batch)
2016-02-03 14:05:36.333698: step 32300, loss = 0.83 (275.0 examples/sec; 0.465 sec/batch)
2016-02-03 14:05:41.406436: step 32310, loss = 0.92 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 14:05:45.931088: step 32320, loss = 0.78 (291.5 examples/sec; 0.439 sec/batch)
2016-02-03 14:05:50.539585: step 32330, loss = 1.00 (299.2 examples/sec; 0.428 sec/batch)
2016-02-03 14:05:55.147639: step 32340, loss = 0.83 (276.6 examples/sec; 0.463 sec/batch)
2016-02-03 14:05:59.722022: step 32350, loss = 0.87 (278.6 examples/sec; 0.460 sec/batch)
2016-02-03 14:06:04.328771: step 32360, loss = 0.84 (290.2 examples/sec; 0.441 sec/batch)
2016-02-03 14:06:09.019073: step 32370, loss = 0.82 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 14:06:13.666136: step 32380, loss = 0.68 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 14:06:18.299233: step 32390, loss = 0.73 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 14:06:22.812968: step 32400, loss = 0.68 (292.7 examples/sec; 0.437 sec/batch)
2016-02-03 14:06:28.039263: step 32410, loss = 0.66 (279.2 examples/sec; 0.458 sec/batch)
2016-02-03 14:06:32.676788: step 32420, loss = 0.73 (254.0 examples/sec; 0.504 sec/batch)
2016-02-03 14:06:37.253767: step 32430, loss = 0.66 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 14:06:41.891960: step 32440, loss = 0.74 (261.4 examples/sec; 0.490 sec/batch)
2016-02-03 14:06:46.475284: step 32450, loss = 0.89 (276.7 examples/sec; 0.463 sec/batch)
2016-02-03 14:06:51.052690: step 32460, loss = 0.76 (282.6 examples/sec; 0.453 sec/batch)
2016-02-03 14:06:55.717216: step 32470, loss = 0.85 (285.9 examples/sec; 0.448 sec/batch)
2016-02-03 14:07:00.338030: step 32480, loss = 0.66 (277.9 examples/sec; 0.461 sec/batch)
2016-02-03 14:07:05.021539: step 32490, loss = 0.68 (247.5 examples/sec; 0.517 sec/batch)
2016-02-03 14:07:09.605685: step 32500, loss = 0.77 (253.2 examples/sec; 0.506 sec/batch)
2016-02-03 14:07:14.726767: step 32510, loss = 0.80 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 14:07:19.400609: step 32520, loss = 0.68 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 14:07:24.161889: step 32530, loss = 1.03 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 14:07:28.829225: step 32540, loss = 0.64 (264.7 examples/sec; 0.483 sec/batch)
2016-02-03 14:07:33.507907: step 32550, loss = 0.67 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 14:07:38.242117: step 32560, loss = 0.85 (294.5 examples/sec; 0.435 sec/batch)
2016-02-03 14:07:42.886351: step 32570, loss = 0.74 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 14:07:47.635544: step 32580, loss = 0.86 (254.2 examples/sec; 0.503 sec/batch)
2016-02-03 14:07:52.291249: step 32590, loss = 0.93 (284.1 examples/sec; 0.450 sec/batch)
2016-02-03 14:07:56.978004: step 32600, loss = 0.67 (308.2 examples/sec; 0.415 sec/batch)
2016-02-03 14:08:02.298691: step 32610, loss = 0.67 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 14:08:06.981749: step 32620, loss = 0.67 (264.9 examples/sec; 0.483 sec/batch)
2016-02-03 14:08:11.679157: step 32630, loss = 0.82 (285.5 examples/sec; 0.448 sec/batch)
2016-02-03 14:08:16.440405: step 32640, loss = 0.63 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 14:08:21.145192: step 32650, loss = 0.73 (257.2 examples/sec; 0.498 sec/batch)
2016-02-03 14:08:25.810382: step 32660, loss = 0.78 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 14:08:30.611686: step 32670, loss = 0.73 (256.3 examples/sec; 0.499 sec/batch)
2016-02-03 14:08:35.308680: step 32680, loss = 0.65 (249.0 examples/sec; 0.514 sec/batch)
2016-02-03 14:08:40.018807: step 32690, loss = 0.80 (252.2 examples/sec; 0.507 sec/batch)
2016-02-03 14:08:44.746968: step 32700, loss = 0.96 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 14:08:49.976262: step 32710, loss = 0.81 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 14:08:54.620053: step 32720, loss = 0.79 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 14:08:59.415031: step 32730, loss = 0.84 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 14:09:04.120495: step 32740, loss = 0.78 (245.2 examples/sec; 0.522 sec/batch)
2016-02-03 14:09:08.900030: step 32750, loss = 0.75 (279.4 examples/sec; 0.458 sec/batch)
2016-02-03 14:09:13.624574: step 32760, loss = 0.76 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 14:09:18.272874: step 32770, loss = 0.84 (278.5 examples/sec; 0.460 sec/batch)
2016-02-03 14:09:22.998391: step 32780, loss = 0.75 (239.5 examples/sec; 0.534 sec/batch)
2016-02-03 14:09:27.701716: step 32790, loss = 0.91 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 14:09:32.437730: step 32800, loss = 0.81 (276.3 examples/sec; 0.463 sec/batch)
2016-02-03 14:09:37.749923: step 32810, loss = 0.85 (241.5 examples/sec; 0.530 sec/batch)
2016-02-03 14:09:42.451755: step 32820, loss = 0.76 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 14:09:47.241913: step 32830, loss = 0.69 (253.5 examples/sec; 0.505 sec/batch)
2016-02-03 14:09:51.925100: step 32840, loss = 0.69 (304.9 examples/sec; 0.420 sec/batch)
2016-02-03 14:09:56.637606: step 32850, loss = 0.67 (265.2 examples/sec; 0.483 sec/batch)
2016-02-03 14:10:01.321651: step 32860, loss = 0.71 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 14:10:06.014608: step 32870, loss = 0.73 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 14:10:10.820595: step 32880, loss = 0.65 (264.2 examples/sec; 0.485 sec/batch)
2016-02-03 14:10:15.538749: step 32890, loss = 0.67 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 14:10:20.359469: step 32900, loss = 0.69 (266.4 examples/sec; 0.480 sec/batch)
2016-02-03 14:10:25.661540: step 32910, loss = 0.71 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 14:10:30.377885: step 32920, loss = 0.74 (283.0 examples/sec; 0.452 sec/batch)
2016-02-03 14:10:35.048902: step 32930, loss = 0.81 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 14:10:39.768230: step 32940, loss = 0.73 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 14:10:44.525809: step 32950, loss = 0.78 (246.5 examples/sec; 0.519 sec/batch)
2016-02-03 14:10:49.187665: step 32960, loss = 0.81 (260.3 examples/sec; 0.492 sec/batch)
2016-02-03 14:10:53.938376: step 32970, loss = 0.82 (272.7 examples/sec; 0.469 sec/batch)
2016-02-03 14:10:58.616627: step 32980, loss = 0.92 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 14:11:03.390984: step 32990, loss = 0.82 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 14:11:08.105262: step 33000, loss = 0.78 (282.9 examples/sec; 0.453 sec/batch)
2016-02-03 14:11:13.383737: step 33010, loss = 0.73 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 14:11:18.062094: step 33020, loss = 0.92 (286.7 examples/sec; 0.447 sec/batch)
2016-02-03 14:11:22.822973: step 33030, loss = 0.78 (294.7 examples/sec; 0.434 sec/batch)
2016-02-03 14:11:27.632259: step 33040, loss = 0.79 (257.5 examples/sec; 0.497 sec/batch)
2016-02-03 14:11:32.338717: step 33050, loss = 0.79 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 14:11:37.029896: step 33060, loss = 0.84 (261.5 examples/sec; 0.489 sec/batch)
2016-02-03 14:11:41.688062: step 33070, loss = 0.65 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 14:11:46.449285: step 33080, loss = 0.99 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 14:11:51.152402: step 33090, loss = 0.83 (264.1 examples/sec; 0.485 sec/batch)
2016-02-03 14:11:55.774343: step 33100, loss = 0.72 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 14:12:01.047416: step 33110, loss = 0.84 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 14:12:05.835720: step 33120, loss = 0.90 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 14:12:10.468976: step 33130, loss = 0.79 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 14:12:15.146146: step 33140, loss = 0.85 (286.9 examples/sec; 0.446 sec/batch)
2016-02-03 14:12:19.896080: step 33150, loss = 0.89 (248.3 examples/sec; 0.516 sec/batch)
2016-02-03 14:12:24.614047: step 33160, loss = 0.79 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 14:12:29.321143: step 33170, loss = 0.90 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 14:12:34.042012: step 33180, loss = 0.78 (289.4 examples/sec; 0.442 sec/batch)
2016-02-03 14:12:38.685188: step 33190, loss = 0.98 (289.9 examples/sec; 0.442 sec/batch)
2016-02-03 14:12:43.396305: step 33200, loss = 0.72 (273.2 examples/sec; 0.468 sec/batch)
2016-02-03 14:12:48.603508: step 33210, loss = 0.90 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 14:12:53.305192: step 33220, loss = 0.78 (253.0 examples/sec; 0.506 sec/batch)
2016-02-03 14:12:57.971611: step 33230, loss = 0.75 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 14:13:02.711470: step 33240, loss = 0.87 (234.7 examples/sec; 0.545 sec/batch)
2016-02-03 14:13:07.417815: step 33250, loss = 0.69 (304.6 examples/sec; 0.420 sec/batch)
2016-02-03 14:13:12.164997: step 33260, loss = 0.81 (268.6 examples/sec; 0.477 sec/batch)
2016-02-03 14:13:16.913440: step 33270, loss = 0.66 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 14:13:21.584969: step 33280, loss = 0.69 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 14:13:26.267317: step 33290, loss = 0.84 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 14:13:30.952045: step 33300, loss = 0.74 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 14:13:36.249882: step 33310, loss = 0.80 (301.1 examples/sec; 0.425 sec/batch)
2016-02-03 14:13:40.934811: step 33320, loss = 1.01 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 14:13:45.667085: step 33330, loss = 0.76 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 14:13:50.343169: step 33340, loss = 0.96 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 14:13:55.093977: step 33350, loss = 0.77 (259.5 examples/sec; 0.493 sec/batch)
2016-02-03 14:13:59.738619: step 33360, loss = 0.65 (300.3 examples/sec; 0.426 sec/batch)
2016-02-03 14:14:04.396723: step 33370, loss = 0.77 (304.0 examples/sec; 0.421 sec/batch)
2016-02-03 14:14:09.167530: step 33380, loss = 0.74 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 14:14:13.905149: step 33390, loss = 0.81 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 14:14:18.625319: step 33400, loss = 0.89 (285.6 examples/sec; 0.448 sec/batch)
2016-02-03 14:14:23.784897: step 33410, loss = 0.72 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 14:14:28.520358: step 33420, loss = 0.62 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 14:14:33.283078: step 33430, loss = 0.99 (243.0 examples/sec; 0.527 sec/batch)
2016-02-03 14:14:38.038280: step 33440, loss = 0.75 (287.3 examples/sec; 0.446 sec/batch)
2016-02-03 14:14:42.697389: step 33450, loss = 0.78 (302.8 examples/sec; 0.423 sec/batch)
2016-02-03 14:14:47.547408: step 33460, loss = 0.78 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 14:14:52.319850: step 33470, loss = 0.81 (297.9 examples/sec; 0.430 sec/batch)
2016-02-03 14:14:57.075304: step 33480, loss = 0.78 (264.2 examples/sec; 0.484 sec/batch)
2016-02-03 14:15:01.808490: step 33490, loss = 0.66 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 14:15:06.549114: step 33500, loss = 0.71 (282.5 examples/sec; 0.453 sec/batch)
2016-02-03 14:15:11.764504: step 33510, loss = 0.82 (284.7 examples/sec; 0.450 sec/batch)
2016-02-03 14:15:16.480168: step 33520, loss = 0.80 (285.3 examples/sec; 0.449 sec/batch)
2016-02-03 14:15:21.156531: step 33530, loss = 0.93 (280.1 examples/sec; 0.457 sec/batch)
2016-02-03 14:15:25.843910: step 33540, loss = 0.70 (258.2 examples/sec; 0.496 sec/batch)
2016-02-03 14:15:30.504766: step 33550, loss = 0.81 (285.7 examples/sec; 0.448 sec/batch)
2016-02-03 14:15:35.265843: step 33560, loss = 0.79 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 14:15:39.949896: step 33570, loss = 0.81 (288.3 examples/sec; 0.444 sec/batch)
2016-02-03 14:15:44.586289: step 33580, loss = 0.87 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 14:15:49.303180: step 33590, loss = 0.78 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 14:15:53.980849: step 33600, loss = 0.81 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 14:15:59.097935: step 33610, loss = 0.87 (289.1 examples/sec; 0.443 sec/batch)
2016-02-03 14:16:03.767281: step 33620, loss = 0.80 (289.7 examples/sec; 0.442 sec/batch)
2016-02-03 14:16:08.466627: step 33630, loss = 0.83 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 14:16:13.196447: step 33640, loss = 0.85 (263.6 examples/sec; 0.486 sec/batch)
2016-02-03 14:16:17.895152: step 33650, loss = 0.82 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 14:16:22.562312: step 33660, loss = 0.81 (276.7 examples/sec; 0.463 sec/batch)
2016-02-03 14:16:27.260050: step 33670, loss = 0.89 (276.8 examples/sec; 0.463 sec/batch)
2016-02-03 14:16:31.927464: step 33680, loss = 0.92 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 14:16:36.578159: step 33690, loss = 0.88 (294.3 examples/sec; 0.435 sec/batch)
2016-02-03 14:16:41.354187: step 33700, loss = 0.74 (260.6 examples/sec; 0.491 sec/batch)
2016-02-03 14:16:46.532139: step 33710, loss = 0.77 (262.9 examples/sec; 0.487 sec/batch)
2016-02-03 14:16:51.245526: step 33720, loss = 0.76 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 14:16:55.943574: step 33730, loss = 0.76 (283.0 examples/sec; 0.452 sec/batch)
2016-02-03 14:17:00.689485: step 33740, loss = 0.74 (261.4 examples/sec; 0.490 sec/batch)
2016-02-03 14:17:05.441748: step 33750, loss = 0.86 (253.2 examples/sec; 0.505 sec/batch)
2016-02-03 14:17:10.123786: step 33760, loss = 0.68 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 14:17:14.827544: step 33770, loss = 0.89 (248.1 examples/sec; 0.516 sec/batch)
2016-02-03 14:17:19.459454: step 33780, loss = 0.79 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 14:17:24.130367: step 33790, loss = 0.77 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 14:17:28.839273: step 33800, loss = 0.79 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 14:17:34.071917: step 33810, loss = 0.82 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 14:17:38.733704: step 33820, loss = 0.66 (289.9 examples/sec; 0.442 sec/batch)
2016-02-03 14:17:43.435660: step 33830, loss = 0.80 (277.9 examples/sec; 0.461 sec/batch)
2016-02-03 14:17:48.181776: step 33840, loss = 0.74 (299.0 examples/sec; 0.428 sec/batch)
2016-02-03 14:17:52.870314: step 33850, loss = 0.71 (295.9 examples/sec; 0.433 sec/batch)
2016-02-03 14:17:57.680550: step 33860, loss = 0.85 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 14:18:02.326482: step 33870, loss = 0.77 (305.6 examples/sec; 0.419 sec/batch)
2016-02-03 14:18:07.111640: step 33880, loss = 0.85 (247.6 examples/sec; 0.517 sec/batch)
2016-02-03 14:18:11.808836: step 33890, loss = 0.77 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 14:18:16.406356: step 33900, loss = 0.84 (311.8 examples/sec; 0.411 sec/batch)
2016-02-03 14:18:21.663336: step 33910, loss = 0.75 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 14:18:26.404581: step 33920, loss = 0.73 (282.5 examples/sec; 0.453 sec/batch)
2016-02-03 14:18:31.119353: step 33930, loss = 0.62 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 14:18:35.723260: step 33940, loss = 0.74 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 14:18:40.479717: step 33950, loss = 0.80 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 14:18:45.203515: step 33960, loss = 0.74 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 14:18:49.855624: step 33970, loss = 0.92 (249.5 examples/sec; 0.513 sec/batch)
2016-02-03 14:18:54.520250: step 33980, loss = 0.89 (286.2 examples/sec; 0.447 sec/batch)
2016-02-03 14:18:59.327280: step 33990, loss = 0.89 (246.1 examples/sec; 0.520 sec/batch)
2016-02-03 14:19:04.037897: step 34000, loss = 0.72 (293.1 examples/sec; 0.437 sec/batch)
2016-02-03 14:19:09.273867: step 34010, loss = 0.88 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 14:19:13.977763: step 34020, loss = 0.82 (261.9 examples/sec; 0.489 sec/batch)
2016-02-03 14:19:18.668050: step 34030, loss = 0.72 (258.8 examples/sec; 0.495 sec/batch)
2016-02-03 14:19:23.347922: step 34040, loss = 0.84 (260.3 examples/sec; 0.492 sec/batch)
2016-02-03 14:19:28.059186: step 34050, loss = 0.83 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 14:19:32.737635: step 34060, loss = 0.77 (309.3 examples/sec; 0.414 sec/batch)
2016-02-03 14:19:37.500957: step 34070, loss = 0.76 (270.9 examples/sec; 0.472 sec/batch)
2016-02-03 14:19:42.238611: step 34080, loss = 0.71 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 14:19:46.883184: step 34090, loss = 0.84 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 14:19:51.620893: step 34100, loss = 0.93 (287.4 examples/sec; 0.445 sec/batch)
2016-02-03 14:19:56.881604: step 34110, loss = 0.72 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 14:20:01.615515: step 34120, loss = 0.85 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 14:20:06.317247: step 34130, loss = 0.73 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 14:20:11.047485: step 34140, loss = 0.73 (244.2 examples/sec; 0.524 sec/batch)
2016-02-03 14:20:15.740386: step 34150, loss = 0.82 (262.1 examples/sec; 0.488 sec/batch)
2016-02-03 14:20:20.422570: step 34160, loss = 0.84 (260.4 examples/sec; 0.492 sec/batch)
2016-02-03 14:20:25.152831: step 34170, loss = 0.71 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 14:20:29.857505: step 34180, loss = 0.78 (258.8 examples/sec; 0.495 sec/batch)
2016-02-03 14:20:34.508583: step 34190, loss = 0.76 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 14:20:39.169141: step 34200, loss = 0.74 (251.8 examples/sec; 0.508 sec/batch)
2016-02-03 14:20:44.273948: step 34210, loss = 0.70 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 14:20:48.977164: step 34220, loss = 0.81 (302.0 examples/sec; 0.424 sec/batch)
2016-02-03 14:20:53.741385: step 34230, loss = 0.80 (255.0 examples/sec; 0.502 sec/batch)
2016-02-03 14:20:58.376585: step 34240, loss = 0.94 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 14:21:03.106526: step 34250, loss = 0.80 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 14:21:07.783368: step 34260, loss = 0.80 (276.4 examples/sec; 0.463 sec/batch)
2016-02-03 14:21:12.518969: step 34270, loss = 0.66 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 14:21:17.137198: step 34280, loss = 0.77 (260.3 examples/sec; 0.492 sec/batch)
2016-02-03 14:21:21.846305: step 34290, loss = 0.69 (239.2 examples/sec; 0.535 sec/batch)
2016-02-03 14:21:26.451637: step 34300, loss = 0.83 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 14:21:31.774935: step 34310, loss = 0.75 (235.0 examples/sec; 0.545 sec/batch)
2016-02-03 14:21:36.411382: step 34320, loss = 0.71 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 14:21:41.133199: step 34330, loss = 0.73 (269.2 examples/sec; 0.475 sec/batch)
2016-02-03 14:21:45.792891: step 34340, loss = 0.82 (285.6 examples/sec; 0.448 sec/batch)
2016-02-03 14:21:50.458913: step 34350, loss = 0.71 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 14:21:55.118636: step 34360, loss = 0.72 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 14:21:59.811698: step 34370, loss = 0.72 (291.3 examples/sec; 0.439 sec/batch)
2016-02-03 14:22:04.544324: step 34380, loss = 0.78 (280.4 examples/sec; 0.456 sec/batch)
2016-02-03 14:22:09.247601: step 34390, loss = 0.84 (282.2 examples/sec; 0.454 sec/batch)
2016-02-03 14:22:14.002280: step 34400, loss = 0.87 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 14:22:19.199084: step 34410, loss = 0.66 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 14:22:23.960124: step 34420, loss = 0.80 (242.4 examples/sec; 0.528 sec/batch)
2016-02-03 14:22:28.667258: step 34430, loss = 0.76 (246.1 examples/sec; 0.520 sec/batch)
2016-02-03 14:22:33.352830: step 34440, loss = 0.68 (293.9 examples/sec; 0.436 sec/batch)
2016-02-03 14:22:38.065161: step 34450, loss = 0.80 (265.3 examples/sec; 0.482 sec/batch)
2016-02-03 14:22:42.759493: step 34460, loss = 0.91 (276.3 examples/sec; 0.463 sec/batch)
2016-02-03 14:22:47.429850: step 34470, loss = 0.88 (258.9 examples/sec; 0.494 sec/batch)
2016-02-03 14:22:52.108768: step 34480, loss = 0.79 (270.9 examples/sec; 0.472 sec/batch)
2016-02-03 14:22:56.779608: step 34490, loss = 0.71 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 14:23:01.479826: step 34500, loss = 0.83 (290.3 examples/sec; 0.441 sec/batch)
2016-02-03 14:23:06.686467: step 34510, loss = 0.73 (240.4 examples/sec; 0.532 sec/batch)
2016-02-03 14:23:11.327331: step 34520, loss = 0.85 (302.4 examples/sec; 0.423 sec/batch)
2016-02-03 14:23:16.032681: step 34530, loss = 0.81 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 14:23:20.769411: step 34540, loss = 0.80 (279.7 examples/sec; 0.458 sec/batch)
2016-02-03 14:23:25.521171: step 34550, loss = 0.95 (290.0 examples/sec; 0.441 sec/batch)
2016-02-03 14:23:30.228683: step 34560, loss = 0.75 (286.5 examples/sec; 0.447 sec/batch)
2016-02-03 14:23:34.962392: step 34570, loss = 0.93 (283.4 examples/sec; 0.452 sec/batch)
2016-02-03 14:23:39.705492: step 34580, loss = 0.82 (268.0 examples/sec; 0.478 sec/batch)
2016-02-03 14:23:44.418325: step 34590, loss = 0.69 (266.9 examples/sec; 0.480 sec/batch)
2016-02-03 14:23:49.261276: step 34600, loss = 0.77 (267.0 examples/sec; 0.479 sec/batch)
2016-02-03 14:23:54.417425: step 34610, loss = 0.91 (290.1 examples/sec; 0.441 sec/batch)
2016-02-03 14:23:58.980191: step 34620, loss = 0.75 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 14:24:03.676597: step 34630, loss = 0.81 (298.1 examples/sec; 0.429 sec/batch)
2016-02-03 14:24:08.379205: step 34640, loss = 0.68 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 14:24:13.006332: step 34650, loss = 0.73 (287.2 examples/sec; 0.446 sec/batch)
2016-02-03 14:24:17.685216: step 34660, loss = 0.60 (258.6 examples/sec; 0.495 sec/batch)
2016-02-03 14:24:22.437495: step 34670, loss = 0.90 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 14:24:27.165566: step 34680, loss = 0.68 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 14:24:31.866654: step 34690, loss = 0.70 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 14:24:36.546793: step 34700, loss = 0.84 (257.9 examples/sec; 0.496 sec/batch)
2016-02-03 14:24:41.743377: step 34710, loss = 0.69 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 14:24:46.486852: step 34720, loss = 0.76 (299.7 examples/sec; 0.427 sec/batch)
2016-02-03 14:24:51.241111: step 34730, loss = 0.68 (276.4 examples/sec; 0.463 sec/batch)
2016-02-03 14:24:55.953061: step 34740, loss = 0.95 (249.9 examples/sec; 0.512 sec/batch)
2016-02-03 14:25:00.704644: step 34750, loss = 0.78 (251.5 examples/sec; 0.509 sec/batch)
2016-02-03 14:25:05.400782: step 34760, loss = 0.80 (266.9 examples/sec; 0.479 sec/batch)
2016-02-03 14:25:10.043767: step 34770, loss = 0.78 (289.8 examples/sec; 0.442 sec/batch)
2016-02-03 14:25:14.685885: step 34780, loss = 0.74 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 14:25:19.402384: step 34790, loss = 0.72 (286.9 examples/sec; 0.446 sec/batch)
2016-02-03 14:25:24.238901: step 34800, loss = 0.97 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 14:25:29.485449: step 34810, loss = 0.71 (262.6 examples/sec; 0.487 sec/batch)
2016-02-03 14:25:34.149331: step 34820, loss = 0.97 (282.4 examples/sec; 0.453 sec/batch)
2016-02-03 14:25:38.786647: step 34830, loss = 0.78 (313.5 examples/sec; 0.408 sec/batch)
2016-02-03 14:25:43.566746: step 34840, loss = 0.92 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 14:25:48.242851: step 34850, loss = 0.74 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 14:25:52.910909: step 34860, loss = 0.78 (256.6 examples/sec; 0.499 sec/batch)
2016-02-03 14:25:57.615887: step 34870, loss = 0.67 (285.5 examples/sec; 0.448 sec/batch)
2016-02-03 14:26:02.391264: step 34880, loss = 0.71 (261.1 examples/sec; 0.490 sec/batch)
2016-02-03 14:26:07.082862: step 34890, loss = 0.73 (277.6 examples/sec; 0.461 sec/batch)
2016-02-03 14:26:11.836722: step 34900, loss = 0.88 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 14:26:17.193463: step 34910, loss = 0.69 (262.1 examples/sec; 0.488 sec/batch)
2016-02-03 14:26:21.944781: step 34920, loss = 0.77 (287.3 examples/sec; 0.446 sec/batch)
2016-02-03 14:26:26.700572: step 34930, loss = 0.69 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 14:26:31.387639: step 34940, loss = 0.77 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 14:26:36.097177: step 34950, loss = 0.68 (285.7 examples/sec; 0.448 sec/batch)
2016-02-03 14:26:40.777546: step 34960, loss = 0.85 (262.2 examples/sec; 0.488 sec/batch)
2016-02-03 14:26:45.501428: step 34970, loss = 0.88 (255.6 examples/sec; 0.501 sec/batch)
2016-02-03 14:26:50.224349: step 34980, loss = 0.62 (288.9 examples/sec; 0.443 sec/batch)
2016-02-03 14:26:54.949550: step 34990, loss = 0.89 (259.3 examples/sec; 0.494 sec/batch)
2016-02-03 14:26:59.608460: step 35000, loss = 0.74 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 14:27:04.885210: step 35010, loss = 0.82 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 14:27:09.601108: step 35020, loss = 0.78 (285.6 examples/sec; 0.448 sec/batch)
2016-02-03 14:27:14.300303: step 35030, loss = 0.81 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 14:27:18.922401: step 35040, loss = 0.76 (305.2 examples/sec; 0.419 sec/batch)
2016-02-03 14:27:23.730038: step 35050, loss = 0.68 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 14:27:28.433366: step 35060, loss = 0.87 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 14:27:33.249161: step 35070, loss = 0.85 (282.4 examples/sec; 0.453 sec/batch)
2016-02-03 14:27:38.031240: step 35080, loss = 0.74 (253.5 examples/sec; 0.505 sec/batch)
2016-02-03 14:27:42.705904: step 35090, loss = 0.79 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 14:27:47.438061: step 35100, loss = 0.66 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 14:27:52.714260: step 35110, loss = 0.72 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 14:27:57.539964: step 35120, loss = 0.85 (255.7 examples/sec; 0.501 sec/batch)
2016-02-03 14:28:02.242528: step 35130, loss = 0.87 (281.6 examples/sec; 0.454 sec/batch)
2016-02-03 14:28:06.956091: step 35140, loss = 0.74 (285.8 examples/sec; 0.448 sec/batch)
2016-02-03 14:28:11.638052: step 35150, loss = 0.89 (282.5 examples/sec; 0.453 sec/batch)
2016-02-03 14:28:16.301230: step 35160, loss = 0.80 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 14:28:20.923177: step 35170, loss = 0.87 (279.2 examples/sec; 0.458 sec/batch)
2016-02-03 14:28:25.665606: step 35180, loss = 0.83 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 14:28:30.453345: step 35190, loss = 0.86 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 14:28:35.221540: step 35200, loss = 0.83 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 14:28:40.506746: step 35210, loss = 0.90 (263.0 examples/sec; 0.487 sec/batch)
2016-02-03 14:28:45.245977: step 35220, loss = 0.83 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 14:28:49.941381: step 35230, loss = 0.78 (288.5 examples/sec; 0.444 sec/batch)
2016-02-03 14:28:54.709440: step 35240, loss = 0.84 (286.4 examples/sec; 0.447 sec/batch)
2016-02-03 14:28:59.468963: step 35250, loss = 0.84 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 14:29:04.230787: step 35260, loss = 0.78 (266.4 examples/sec; 0.480 sec/batch)
2016-02-03 14:29:08.843160: step 35270, loss = 0.76 (285.7 examples/sec; 0.448 sec/batch)
2016-02-03 14:29:13.550806: step 35280, loss = 0.87 (286.3 examples/sec; 0.447 sec/batch)
2016-02-03 14:29:18.217042: step 35290, loss = 0.76 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 14:29:22.935667: step 35300, loss = 0.81 (290.2 examples/sec; 0.441 sec/batch)
2016-02-03 14:29:28.115123: step 35310, loss = 0.69 (278.5 examples/sec; 0.460 sec/batch)
2016-02-03 14:29:32.819854: step 35320, loss = 0.77 (253.9 examples/sec; 0.504 sec/batch)
2016-02-03 14:29:37.539114: step 35330, loss = 0.78 (261.4 examples/sec; 0.490 sec/batch)
2016-02-03 14:29:42.222816: step 35340, loss = 0.74 (283.5 examples/sec; 0.451 sec/batch)
2016-02-03 14:29:46.989877: step 35350, loss = 0.85 (257.2 examples/sec; 0.498 sec/batch)
2016-02-03 14:29:51.712943: step 35360, loss = 0.83 (255.1 examples/sec; 0.502 sec/batch)
2016-02-03 14:29:56.470450: step 35370, loss = 0.78 (277.6 examples/sec; 0.461 sec/batch)
2016-02-03 14:30:01.163640: step 35380, loss = 0.66 (284.9 examples/sec; 0.449 sec/batch)
2016-02-03 14:30:05.820059: step 35390, loss = 0.75 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 14:30:10.509015: step 35400, loss = 0.87 (256.9 examples/sec; 0.498 sec/batch)
2016-02-03 14:30:15.674677: step 35410, loss = 0.74 (299.2 examples/sec; 0.428 sec/batch)
2016-02-03 14:30:20.366238: step 35420, loss = 0.62 (289.6 examples/sec; 0.442 sec/batch)
2016-02-03 14:30:25.075648: step 35430, loss = 0.89 (259.7 examples/sec; 0.493 sec/batch)
2016-02-03 14:30:29.760642: step 35440, loss = 0.84 (272.7 examples/sec; 0.469 sec/batch)
2016-02-03 14:30:34.515977: step 35450, loss = 0.82 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 14:30:39.300325: step 35460, loss = 0.72 (289.2 examples/sec; 0.443 sec/batch)
2016-02-03 14:30:44.077008: step 35470, loss = 0.74 (261.7 examples/sec; 0.489 sec/batch)
2016-02-03 14:30:48.792423: step 35480, loss = 0.80 (274.4 examples/sec; 0.467 sec/batch)
2016-02-03 14:30:53.460038: step 35490, loss = 0.70 (310.3 examples/sec; 0.413 sec/batch)
2016-02-03 14:30:58.182045: step 35500, loss = 0.68 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 14:31:03.359685: step 35510, loss = 0.84 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 14:31:08.106703: step 35520, loss = 0.74 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 14:31:12.660972: step 35530, loss = 0.67 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 14:31:17.291624: step 35540, loss = 0.69 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 14:31:22.083188: step 35550, loss = 0.81 (226.4 examples/sec; 0.565 sec/batch)
2016-02-03 14:31:26.793710: step 35560, loss = 0.89 (253.5 examples/sec; 0.505 sec/batch)
2016-02-03 14:31:31.561389: step 35570, loss = 0.75 (272.7 examples/sec; 0.469 sec/batch)
2016-02-03 14:31:36.235300: step 35580, loss = 0.68 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 14:31:40.927900: step 35590, loss = 0.70 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 14:31:45.644691: step 35600, loss = 0.80 (256.1 examples/sec; 0.500 sec/batch)
2016-02-03 14:31:50.841119: step 35610, loss = 0.54 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 14:31:55.497533: step 35620, loss = 0.71 (280.3 examples/sec; 0.457 sec/batch)
2016-02-03 14:32:00.186565: step 35630, loss = 0.57 (274.4 examples/sec; 0.466 sec/batch)
2016-02-03 14:32:04.974279: step 35640, loss = 0.79 (253.2 examples/sec; 0.506 sec/batch)
2016-02-03 14:32:09.651931: step 35650, loss = 0.68 (277.7 examples/sec; 0.461 sec/batch)
2016-02-03 14:32:14.336397: step 35660, loss = 0.89 (291.6 examples/sec; 0.439 sec/batch)
2016-02-03 14:32:19.109064: step 35670, loss = 0.73 (243.2 examples/sec; 0.526 sec/batch)
2016-02-03 14:32:23.771333: step 35680, loss = 0.83 (286.6 examples/sec; 0.447 sec/batch)
2016-02-03 14:32:28.581859: step 35690, loss = 0.88 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 14:32:33.183352: step 35700, loss = 0.90 (309.9 examples/sec; 0.413 sec/batch)
2016-02-03 14:32:38.498337: step 35710, loss = 0.93 (283.6 examples/sec; 0.451 sec/batch)
2016-02-03 14:32:43.315608: step 35720, loss = 0.83 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 14:32:48.080610: step 35730, loss = 0.95 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 14:32:52.791473: step 35740, loss = 0.82 (279.2 examples/sec; 0.458 sec/batch)
2016-02-03 14:32:57.501693: step 35750, loss = 0.87 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 14:33:02.255157: step 35760, loss = 0.66 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 14:33:06.927590: step 35770, loss = 0.80 (285.6 examples/sec; 0.448 sec/batch)
2016-02-03 14:33:11.622608: step 35780, loss = 0.79 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 14:33:16.426522: step 35790, loss = 0.77 (252.2 examples/sec; 0.508 sec/batch)
2016-02-03 14:33:21.149938: step 35800, loss = 0.88 (260.1 examples/sec; 0.492 sec/batch)
2016-02-03 14:33:26.288554: step 35810, loss = 0.77 (313.1 examples/sec; 0.409 sec/batch)
2016-02-03 14:33:30.945874: step 35820, loss = 0.77 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 14:33:35.621063: step 35830, loss = 0.83 (289.5 examples/sec; 0.442 sec/batch)
2016-02-03 14:33:40.406959: step 35840, loss = 0.87 (289.7 examples/sec; 0.442 sec/batch)
2016-02-03 14:33:45.190766: step 35850, loss = 0.81 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 14:33:49.883980: step 35860, loss = 0.77 (291.8 examples/sec; 0.439 sec/batch)
2016-02-03 14:33:54.649861: step 35870, loss = 0.84 (269.2 examples/sec; 0.476 sec/batch)
2016-02-03 14:33:59.351603: step 35880, loss = 0.94 (255.4 examples/sec; 0.501 sec/batch)
2016-02-03 14:34:04.090692: step 35890, loss = 0.78 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 14:34:08.753706: step 35900, loss = 0.67 (297.8 examples/sec; 0.430 sec/batch)
2016-02-03 14:34:14.014472: step 35910, loss = 0.71 (281.0 examples/sec; 0.456 sec/batch)
2016-02-03 14:34:18.698915: step 35920, loss = 0.71 (255.8 examples/sec; 0.500 sec/batch)
2016-02-03 14:34:23.503100: step 35930, loss = 0.78 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 14:34:28.240757: step 35940, loss = 0.72 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 14:34:32.999391: step 35950, loss = 0.64 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 14:34:37.733621: step 35960, loss = 0.81 (297.1 examples/sec; 0.431 sec/batch)
2016-02-03 14:34:42.392899: step 35970, loss = 0.81 (286.0 examples/sec; 0.448 sec/batch)
2016-02-03 14:34:47.129274: step 35980, loss = 0.99 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 14:34:51.825601: step 35990, loss = 0.87 (288.1 examples/sec; 0.444 sec/batch)
2016-02-03 14:34:56.531849: step 36000, loss = 0.78 (257.9 examples/sec; 0.496 sec/batch)
2016-02-03 14:35:01.676929: step 36010, loss = 0.86 (266.9 examples/sec; 0.479 sec/batch)
2016-02-03 14:35:06.342945: step 36020, loss = 0.77 (302.0 examples/sec; 0.424 sec/batch)
2016-02-03 14:35:11.092359: step 36030, loss = 0.85 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 14:35:15.756551: step 36040, loss = 0.86 (308.6 examples/sec; 0.415 sec/batch)
2016-02-03 14:35:20.493544: step 36050, loss = 0.69 (251.8 examples/sec; 0.508 sec/batch)
2016-02-03 14:35:25.147612: step 36060, loss = 0.85 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 14:35:29.799712: step 36070, loss = 0.84 (290.4 examples/sec; 0.441 sec/batch)
2016-02-03 14:35:34.451332: step 36080, loss = 0.68 (286.4 examples/sec; 0.447 sec/batch)
2016-02-03 14:35:39.151880: step 36090, loss = 0.72 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 14:35:43.777300: step 36100, loss = 0.76 (291.0 examples/sec; 0.440 sec/batch)
2016-02-03 14:35:48.986443: step 36110, loss = 0.77 (283.7 examples/sec; 0.451 sec/batch)
2016-02-03 14:35:53.657894: step 36120, loss = 0.75 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 14:35:58.397708: step 36130, loss = 0.71 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 14:36:03.022047: step 36140, loss = 0.72 (277.9 examples/sec; 0.461 sec/batch)
2016-02-03 14:36:07.796823: step 36150, loss = 0.85 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 14:36:12.478107: step 36160, loss = 0.75 (256.6 examples/sec; 0.499 sec/batch)
2016-02-03 14:36:17.194931: step 36170, loss = 0.76 (285.8 examples/sec; 0.448 sec/batch)
2016-02-03 14:36:21.932353: step 36180, loss = 0.75 (258.7 examples/sec; 0.495 sec/batch)
2016-02-03 14:36:26.580387: step 36190, loss = 0.71 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 14:36:31.312369: step 36200, loss = 0.66 (254.7 examples/sec; 0.503 sec/batch)
2016-02-03 14:36:36.506769: step 36210, loss = 0.88 (267.2 examples/sec; 0.479 sec/batch)
2016-02-03 14:36:41.127699: step 36220, loss = 0.78 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 14:36:45.775584: step 36230, loss = 0.73 (278.5 examples/sec; 0.460 sec/batch)
2016-02-03 14:36:50.443371: step 36240, loss = 0.87 (283.6 examples/sec; 0.451 sec/batch)
2016-02-03 14:36:55.188074: step 36250, loss = 0.74 (259.2 examples/sec; 0.494 sec/batch)
2016-02-03 14:36:59.877650: step 36260, loss = 0.63 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 14:37:04.678561: step 36270, loss = 0.66 (252.6 examples/sec; 0.507 sec/batch)
2016-02-03 14:37:09.484247: step 36280, loss = 0.95 (252.6 examples/sec; 0.507 sec/batch)
2016-02-03 14:37:14.175259: step 36290, loss = 0.79 (302.4 examples/sec; 0.423 sec/batch)
2016-02-03 14:37:18.922155: step 36300, loss = 0.75 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 14:37:24.235701: step 36310, loss = 0.76 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 14:37:28.947871: step 36320, loss = 0.86 (287.2 examples/sec; 0.446 sec/batch)
2016-02-03 14:37:33.728635: step 36330, loss = 0.86 (253.3 examples/sec; 0.505 sec/batch)
2016-02-03 14:37:38.431901: step 36340, loss = 0.76 (291.2 examples/sec; 0.439 sec/batch)
2016-02-03 14:37:43.166966: step 36350, loss = 0.74 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 14:37:47.871097: step 36360, loss = 0.82 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 14:37:52.567096: step 36370, loss = 0.76 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 14:37:57.236423: step 36380, loss = 0.95 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 14:38:01.874635: step 36390, loss = 0.75 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 14:38:06.593419: step 36400, loss = 0.87 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 14:38:11.879106: step 36410, loss = 0.84 (246.9 examples/sec; 0.518 sec/batch)
2016-02-03 14:38:16.482031: step 36420, loss = 0.70 (306.0 examples/sec; 0.418 sec/batch)
2016-02-03 14:38:21.159815: step 36430, loss = 0.85 (285.0 examples/sec; 0.449 sec/batch)
2016-02-03 14:38:25.931734: step 36440, loss = 0.62 (258.0 examples/sec; 0.496 sec/batch)
2016-02-03 14:38:30.635715: step 36450, loss = 0.86 (278.6 examples/sec; 0.460 sec/batch)
2016-02-03 14:38:35.336495: step 36460, loss = 0.73 (283.0 examples/sec; 0.452 sec/batch)
2016-02-03 14:38:40.049637: step 36470, loss = 0.78 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 14:38:44.706807: step 36480, loss = 0.68 (283.6 examples/sec; 0.451 sec/batch)
2016-02-03 14:38:49.456416: step 36490, loss = 0.76 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 14:38:54.162816: step 36500, loss = 0.81 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 14:38:59.392721: step 36510, loss = 0.59 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 14:39:04.002891: step 36520, loss = 0.87 (291.5 examples/sec; 0.439 sec/batch)
2016-02-03 14:39:08.764640: step 36530, loss = 0.72 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 14:39:13.514736: step 36540, loss = 0.79 (264.2 examples/sec; 0.485 sec/batch)
2016-02-03 14:39:18.241061: step 36550, loss = 0.70 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 14:39:22.891118: step 36560, loss = 0.72 (256.0 examples/sec; 0.500 sec/batch)
2016-02-03 14:39:27.643890: step 36570, loss = 0.90 (287.7 examples/sec; 0.445 sec/batch)
2016-02-03 14:39:32.357191: step 36580, loss = 0.78 (306.4 examples/sec; 0.418 sec/batch)
2016-02-03 14:39:37.028275: step 36590, loss = 0.77 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 14:39:41.707822: step 36600, loss = 0.60 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 14:39:46.956763: step 36610, loss = 0.58 (258.5 examples/sec; 0.495 sec/batch)
2016-02-03 14:39:51.638489: step 36620, loss = 0.69 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 14:39:56.310212: step 36630, loss = 0.95 (262.6 examples/sec; 0.487 sec/batch)
2016-02-03 14:40:00.954465: step 36640, loss = 0.71 (288.6 examples/sec; 0.443 sec/batch)
2016-02-03 14:40:05.709587: step 36650, loss = 0.71 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 14:40:10.473665: step 36660, loss = 0.88 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 14:40:15.121104: step 36670, loss = 0.73 (259.9 examples/sec; 0.492 sec/batch)
2016-02-03 14:40:19.801137: step 36680, loss = 0.82 (262.1 examples/sec; 0.488 sec/batch)
2016-02-03 14:40:24.517070: step 36690, loss = 0.84 (275.0 examples/sec; 0.465 sec/batch)
2016-02-03 14:40:29.181044: step 36700, loss = 0.73 (252.0 examples/sec; 0.508 sec/batch)
2016-02-03 14:40:34.352630: step 36710, loss = 0.77 (286.3 examples/sec; 0.447 sec/batch)
2016-02-03 14:40:38.971610: step 36720, loss = 0.75 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 14:40:43.691745: step 36730, loss = 0.83 (293.1 examples/sec; 0.437 sec/batch)
2016-02-03 14:40:48.450530: step 36740, loss = 0.74 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 14:40:53.158663: step 36750, loss = 0.72 (261.5 examples/sec; 0.489 sec/batch)
2016-02-03 14:40:57.857256: step 36760, loss = 0.90 (256.6 examples/sec; 0.499 sec/batch)
2016-02-03 14:41:02.534045: step 36770, loss = 0.84 (256.4 examples/sec; 0.499 sec/batch)
2016-02-03 14:41:07.241309: step 36780, loss = 0.75 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 14:41:11.947304: step 36790, loss = 0.75 (251.8 examples/sec; 0.508 sec/batch)
2016-02-03 14:41:16.690464: step 36800, loss = 0.82 (252.5 examples/sec; 0.507 sec/batch)
2016-02-03 14:41:21.852879: step 36810, loss = 0.91 (270.9 examples/sec; 0.473 sec/batch)
2016-02-03 14:41:26.556691: step 36820, loss = 0.66 (261.4 examples/sec; 0.490 sec/batch)
2016-02-03 14:41:31.344350: step 36830, loss = 0.87 (267.5 examples/sec; 0.479 sec/batch)
2016-02-03 14:41:36.020464: step 36840, loss = 0.73 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 14:41:40.649833: step 36850, loss = 0.75 (273.2 examples/sec; 0.468 sec/batch)
2016-02-03 14:41:45.337783: step 36860, loss = 0.69 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 14:41:50.043148: step 36870, loss = 0.75 (276.9 examples/sec; 0.462 sec/batch)
2016-02-03 14:41:54.628303: step 36880, loss = 0.83 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 14:41:59.390655: step 36890, loss = 0.77 (247.4 examples/sec; 0.517 sec/batch)
2016-02-03 14:42:04.046094: step 36900, loss = 0.65 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 14:42:09.302365: step 36910, loss = 0.62 (252.0 examples/sec; 0.508 sec/batch)
2016-02-03 14:42:13.959774: step 36920, loss = 0.92 (259.1 examples/sec; 0.494 sec/batch)
2016-02-03 14:42:18.688620: step 36930, loss = 0.74 (269.3 examples/sec; 0.475 sec/batch)
2016-02-03 14:42:23.362131: step 36940, loss = 0.82 (284.9 examples/sec; 0.449 sec/batch)
2016-02-03 14:42:28.078749: step 36950, loss = 0.75 (253.9 examples/sec; 0.504 sec/batch)
2016-02-03 14:42:32.773536: step 36960, loss = 0.69 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 14:42:37.404989: step 36970, loss = 0.72 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 14:42:42.161032: step 36980, loss = 0.79 (258.7 examples/sec; 0.495 sec/batch)
2016-02-03 14:42:46.932943: step 36990, loss = 0.73 (294.1 examples/sec; 0.435 sec/batch)
2016-02-03 14:42:51.632488: step 37000, loss = 0.71 (265.8 examples/sec; 0.482 sec/batch)
2016-02-03 14:42:56.873286: step 37010, loss = 0.80 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 14:43:01.614129: step 37020, loss = 0.73 (295.3 examples/sec; 0.433 sec/batch)
2016-02-03 14:43:06.363124: step 37030, loss = 0.88 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 14:43:11.105845: step 37040, loss = 0.75 (277.8 examples/sec; 0.461 sec/batch)
2016-02-03 14:43:15.867892: step 37050, loss = 0.77 (280.4 examples/sec; 0.457 sec/batch)
2016-02-03 14:43:20.546674: step 37060, loss = 0.76 (263.6 examples/sec; 0.485 sec/batch)
2016-02-03 14:43:25.348115: step 37070, loss = 0.73 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 14:43:30.056351: step 37080, loss = 0.73 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 14:43:34.768768: step 37090, loss = 0.66 (290.9 examples/sec; 0.440 sec/batch)
2016-02-03 14:43:39.579597: step 37100, loss = 0.83 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 14:43:44.832049: step 37110, loss = 0.83 (262.9 examples/sec; 0.487 sec/batch)
2016-02-03 14:43:49.574934: step 37120, loss = 0.76 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 14:43:54.262346: step 37130, loss = 0.74 (254.4 examples/sec; 0.503 sec/batch)
2016-02-03 14:43:59.003699: step 37140, loss = 0.78 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 14:44:03.780268: step 37150, loss = 1.02 (263.2 examples/sec; 0.486 sec/batch)
2016-02-03 14:44:08.503985: step 37160, loss = 0.84 (251.2 examples/sec; 0.509 sec/batch)
2016-02-03 14:44:13.250205: step 37170, loss = 0.72 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 14:44:17.964452: step 37180, loss = 0.73 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 14:44:22.623449: step 37190, loss = 0.79 (283.3 examples/sec; 0.452 sec/batch)
2016-02-03 14:44:27.454368: step 37200, loss = 0.76 (254.7 examples/sec; 0.503 sec/batch)
2016-02-03 14:44:32.624317: step 37210, loss = 0.61 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 14:44:37.417010: step 37220, loss = 0.72 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 14:44:42.133499: step 37230, loss = 0.69 (278.0 examples/sec; 0.461 sec/batch)
2016-02-03 14:44:46.873877: step 37240, loss = 0.73 (265.0 examples/sec; 0.483 sec/batch)
2016-02-03 14:44:51.563446: step 37250, loss = 0.76 (280.6 examples/sec; 0.456 sec/batch)
2016-02-03 14:44:56.312647: step 37260, loss = 0.65 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 14:45:01.008083: step 37270, loss = 0.75 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 14:45:05.685165: step 37280, loss = 0.88 (255.0 examples/sec; 0.502 sec/batch)
2016-02-03 14:45:10.383629: step 37290, loss = 0.80 (270.3 examples/sec; 0.473 sec/batch)
2016-02-03 14:45:15.059920: step 37300, loss = 0.80 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 14:45:20.239352: step 37310, loss = 0.75 (264.7 examples/sec; 0.484 sec/batch)
2016-02-03 14:45:24.975057: step 37320, loss = 0.86 (249.9 examples/sec; 0.512 sec/batch)
2016-02-03 14:45:29.541036: step 37330, loss = 0.66 (263.6 examples/sec; 0.485 sec/batch)
2016-02-03 14:45:34.243153: step 37340, loss = 0.70 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 14:45:38.933529: step 37350, loss = 0.81 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 14:45:43.612058: step 37360, loss = 0.82 (289.1 examples/sec; 0.443 sec/batch)
2016-02-03 14:45:48.330523: step 37370, loss = 0.66 (288.8 examples/sec; 0.443 sec/batch)
2016-02-03 14:45:53.056930: step 37380, loss = 0.89 (251.7 examples/sec; 0.509 sec/batch)
2016-02-03 14:45:57.779928: step 37390, loss = 0.66 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 14:46:02.410342: step 37400, loss = 0.83 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 14:46:07.691882: step 37410, loss = 0.77 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 14:46:12.395294: step 37420, loss = 0.81 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 14:46:17.125250: step 37430, loss = 0.80 (291.8 examples/sec; 0.439 sec/batch)
2016-02-03 14:46:21.852752: step 37440, loss = 0.79 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 14:46:26.628891: step 37450, loss = 0.87 (294.3 examples/sec; 0.435 sec/batch)
2016-02-03 14:46:31.353979: step 37460, loss = 0.91 (258.8 examples/sec; 0.495 sec/batch)
2016-02-03 14:46:36.101736: step 37470, loss = 0.69 (287.8 examples/sec; 0.445 sec/batch)
2016-02-03 14:46:40.891706: step 37480, loss = 0.76 (273.2 examples/sec; 0.469 sec/batch)
2016-02-03 14:46:45.582669: step 37490, loss = 0.63 (265.0 examples/sec; 0.483 sec/batch)
2016-02-03 14:46:50.306422: step 37500, loss = 0.78 (277.7 examples/sec; 0.461 sec/batch)
2016-02-03 14:46:55.514913: step 37510, loss = 0.83 (253.0 examples/sec; 0.506 sec/batch)
2016-02-03 14:47:00.250905: step 37520, loss = 0.78 (254.0 examples/sec; 0.504 sec/batch)
2016-02-03 14:47:05.011606: step 37530, loss = 0.76 (253.2 examples/sec; 0.505 sec/batch)
2016-02-03 14:47:09.680857: step 37540, loss = 0.82 (287.7 examples/sec; 0.445 sec/batch)
2016-02-03 14:47:14.418167: step 37550, loss = 0.75 (259.2 examples/sec; 0.494 sec/batch)
2016-02-03 14:47:19.076758: step 37560, loss = 0.93 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 14:47:23.824968: step 37570, loss = 0.84 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 14:47:28.549695: step 37580, loss = 0.78 (264.7 examples/sec; 0.484 sec/batch)
2016-02-03 14:47:33.349375: step 37590, loss = 0.83 (251.4 examples/sec; 0.509 sec/batch)
2016-02-03 14:47:38.161712: step 37600, loss = 0.76 (258.4 examples/sec; 0.495 sec/batch)
2016-02-03 14:47:43.385229: step 37610, loss = 0.74 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 14:47:48.070108: step 37620, loss = 0.78 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 14:47:52.865812: step 37630, loss = 0.85 (258.5 examples/sec; 0.495 sec/batch)
2016-02-03 14:47:57.576014: step 37640, loss = 0.86 (254.0 examples/sec; 0.504 sec/batch)
2016-02-03 14:48:02.245423: step 37650, loss = 0.63 (286.9 examples/sec; 0.446 sec/batch)
2016-02-03 14:48:06.944578: step 37660, loss = 0.78 (306.9 examples/sec; 0.417 sec/batch)
2016-02-03 14:48:11.710648: step 37670, loss = 0.93 (241.2 examples/sec; 0.531 sec/batch)
2016-02-03 14:48:16.448261: step 37680, loss = 0.72 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 14:48:21.132372: step 37690, loss = 0.81 (288.0 examples/sec; 0.445 sec/batch)
2016-02-03 14:48:25.888313: step 37700, loss = 0.85 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 14:48:31.081431: step 37710, loss = 0.82 (285.7 examples/sec; 0.448 sec/batch)
2016-02-03 14:48:35.805933: step 37720, loss = 0.69 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 14:48:40.457545: step 37730, loss = 0.75 (289.9 examples/sec; 0.441 sec/batch)
2016-02-03 14:48:45.180248: step 37740, loss = 0.92 (287.6 examples/sec; 0.445 sec/batch)
2016-02-03 14:48:49.866821: step 37750, loss = 0.87 (252.4 examples/sec; 0.507 sec/batch)
2016-02-03 14:48:54.585366: step 37760, loss = 0.78 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 14:48:59.356232: step 37770, loss = 0.79 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 14:49:04.091780: step 37780, loss = 0.74 (250.6 examples/sec; 0.511 sec/batch)
2016-02-03 14:49:08.672874: step 37790, loss = 0.76 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 14:49:13.410339: step 37800, loss = 0.76 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 14:49:18.582468: step 37810, loss = 0.74 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 14:49:23.228891: step 37820, loss = 0.76 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 14:49:27.964598: step 37830, loss = 0.82 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 14:49:32.661283: step 37840, loss = 0.77 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 14:49:37.365427: step 37850, loss = 0.93 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 14:49:42.083368: step 37860, loss = 0.68 (261.1 examples/sec; 0.490 sec/batch)
2016-02-03 14:49:46.786430: step 37870, loss = 0.73 (285.3 examples/sec; 0.449 sec/batch)
2016-02-03 14:49:51.515885: step 37880, loss = 0.82 (240.7 examples/sec; 0.532 sec/batch)
2016-02-03 14:49:56.171402: step 37890, loss = 0.82 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 14:50:00.915237: step 37900, loss = 0.78 (250.1 examples/sec; 0.512 sec/batch)
2016-02-03 14:50:06.115110: step 37910, loss = 0.84 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 14:50:10.763822: step 37920, loss = 0.76 (259.5 examples/sec; 0.493 sec/batch)
2016-02-03 14:50:15.486823: step 37930, loss = 0.77 (307.1 examples/sec; 0.417 sec/batch)
2016-02-03 14:50:20.277297: step 37940, loss = 0.69 (244.7 examples/sec; 0.523 sec/batch)
2016-02-03 14:50:24.988358: step 37950, loss = 0.77 (256.4 examples/sec; 0.499 sec/batch)
2016-02-03 14:50:29.682637: step 37960, loss = 0.80 (297.9 examples/sec; 0.430 sec/batch)
2016-02-03 14:50:34.351752: step 37970, loss = 0.81 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 14:50:39.069583: step 37980, loss = 0.69 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 14:50:43.778410: step 37990, loss = 0.79 (266.3 examples/sec; 0.481 sec/batch)
2016-02-03 14:50:48.433514: step 38000, loss = 0.69 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 14:50:53.677805: step 38010, loss = 0.83 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 14:50:58.342992: step 38020, loss = 0.70 (273.2 examples/sec; 0.468 sec/batch)
2016-02-03 14:51:03.143138: step 38030, loss = 0.78 (260.0 examples/sec; 0.492 sec/batch)
2016-02-03 14:51:07.943910: step 38040, loss = 0.90 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 14:51:12.651835: step 38050, loss = 0.75 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 14:51:17.429622: step 38060, loss = 0.76 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 14:51:22.091761: step 38070, loss = 0.76 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 14:51:26.721612: step 38080, loss = 0.86 (284.7 examples/sec; 0.450 sec/batch)
2016-02-03 14:51:31.390149: step 38090, loss = 0.75 (254.0 examples/sec; 0.504 sec/batch)
2016-02-03 14:51:36.004953: step 38100, loss = 0.75 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 14:51:41.184564: step 38110, loss = 0.77 (286.4 examples/sec; 0.447 sec/batch)
2016-02-03 14:51:45.842555: step 38120, loss = 0.81 (259.4 examples/sec; 0.493 sec/batch)
2016-02-03 14:51:50.519324: step 38130, loss = 0.80 (299.5 examples/sec; 0.427 sec/batch)
2016-02-03 14:51:55.204431: step 38140, loss = 0.70 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 14:51:59.898402: step 38150, loss = 0.75 (254.2 examples/sec; 0.504 sec/batch)
2016-02-03 14:52:04.693255: step 38160, loss = 0.78 (245.5 examples/sec; 0.521 sec/batch)
2016-02-03 14:52:09.385687: step 38170, loss = 0.81 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 14:52:14.054732: step 38180, loss = 0.71 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 14:52:18.777400: step 38190, loss = 0.73 (293.2 examples/sec; 0.437 sec/batch)
2016-02-03 14:52:23.516952: step 38200, loss = 0.74 (257.4 examples/sec; 0.497 sec/batch)
2016-02-03 14:52:28.681226: step 38210, loss = 0.84 (284.8 examples/sec; 0.449 sec/batch)
2016-02-03 14:52:33.384045: step 38220, loss = 0.85 (240.3 examples/sec; 0.533 sec/batch)
2016-02-03 14:52:38.024148: step 38230, loss = 0.81 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 14:52:42.761745: step 38240, loss = 0.81 (278.6 examples/sec; 0.460 sec/batch)
2016-02-03 14:52:47.364960: step 38250, loss = 0.72 (259.9 examples/sec; 0.493 sec/batch)
2016-02-03 14:52:52.027657: step 38260, loss = 0.93 (287.6 examples/sec; 0.445 sec/batch)
2016-02-03 14:52:56.776246: step 38270, loss = 0.85 (287.5 examples/sec; 0.445 sec/batch)
2016-02-03 14:53:01.533143: step 38280, loss = 0.62 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 14:53:06.117004: step 38290, loss = 0.75 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 14:53:10.822864: step 38300, loss = 0.72 (269.2 examples/sec; 0.475 sec/batch)
2016-02-03 14:53:16.019204: step 38310, loss = 0.58 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 14:53:20.745087: step 38320, loss = 0.83 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 14:53:25.533539: step 38330, loss = 0.71 (250.3 examples/sec; 0.511 sec/batch)
2016-02-03 14:53:30.151221: step 38340, loss = 0.84 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 14:53:34.851609: step 38350, loss = 0.95 (272.7 examples/sec; 0.469 sec/batch)
2016-02-03 14:53:39.603345: step 38360, loss = 0.70 (257.5 examples/sec; 0.497 sec/batch)
2016-02-03 14:53:44.321806: step 38370, loss = 0.82 (275.7 examples/sec; 0.464 sec/batch)
2016-02-03 14:53:48.979051: step 38380, loss = 0.86 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 14:53:53.716692: step 38390, loss = 0.78 (256.0 examples/sec; 0.500 sec/batch)
2016-02-03 14:53:58.491051: step 38400, loss = 0.65 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 14:54:03.738004: step 38410, loss = 0.65 (266.4 examples/sec; 0.481 sec/batch)
2016-02-03 14:54:08.528842: step 38420, loss = 0.89 (287.5 examples/sec; 0.445 sec/batch)
2016-02-03 14:54:13.266912: step 38430, loss = 0.72 (262.6 examples/sec; 0.487 sec/batch)
2016-02-03 14:54:17.980360: step 38440, loss = 0.71 (255.0 examples/sec; 0.502 sec/batch)
2016-02-03 14:54:22.712256: step 38450, loss = 0.80 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 14:54:27.520900: step 38460, loss = 0.85 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 14:54:32.176851: step 38470, loss = 0.63 (299.2 examples/sec; 0.428 sec/batch)
2016-02-03 14:54:36.943256: step 38480, loss = 0.72 (257.6 examples/sec; 0.497 sec/batch)
2016-02-03 14:54:41.701482: step 38490, loss = 0.85 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 14:54:46.427403: step 38500, loss = 0.88 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 14:54:51.701153: step 38510, loss = 0.74 (279.2 examples/sec; 0.459 sec/batch)
2016-02-03 14:54:56.472954: step 38520, loss = 0.87 (248.5 examples/sec; 0.515 sec/batch)
2016-02-03 14:55:01.139632: step 38530, loss = 0.77 (292.3 examples/sec; 0.438 sec/batch)
2016-02-03 14:55:05.830403: step 38540, loss = 0.73 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 14:55:10.597177: step 38550, loss = 0.58 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 14:55:15.332614: step 38560, loss = 0.74 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 14:55:19.939965: step 38570, loss = 0.65 (303.1 examples/sec; 0.422 sec/batch)
2016-02-03 14:55:24.692525: step 38580, loss = 0.69 (268.6 examples/sec; 0.477 sec/batch)
2016-02-03 14:55:29.378304: step 38590, loss = 0.68 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 14:55:34.139518: step 38600, loss = 0.71 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 14:55:39.307737: step 38610, loss = 0.75 (261.1 examples/sec; 0.490 sec/batch)
2016-02-03 14:55:44.033136: step 38620, loss = 0.77 (262.2 examples/sec; 0.488 sec/batch)
2016-02-03 14:55:48.737367: step 38630, loss = 0.77 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 14:55:53.412184: step 38640, loss = 0.88 (280.4 examples/sec; 0.456 sec/batch)
2016-02-03 14:55:58.122702: step 38650, loss = 0.81 (241.0 examples/sec; 0.531 sec/batch)
2016-02-03 14:56:02.814334: step 38660, loss = 0.73 (288.2 examples/sec; 0.444 sec/batch)
2016-02-03 14:56:07.467235: step 38670, loss = 0.83 (294.7 examples/sec; 0.434 sec/batch)
2016-02-03 14:56:12.110376: step 38680, loss = 0.83 (298.9 examples/sec; 0.428 sec/batch)
2016-02-03 14:56:16.772561: step 38690, loss = 0.87 (295.0 examples/sec; 0.434 sec/batch)
2016-02-03 14:56:21.441404: step 38700, loss = 0.69 (250.9 examples/sec; 0.510 sec/batch)
2016-02-03 14:56:26.653344: step 38710, loss = 0.95 (255.1 examples/sec; 0.502 sec/batch)
2016-02-03 14:56:31.161031: step 38720, loss = 0.68 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 14:56:35.866908: step 38730, loss = 0.90 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 14:56:40.631065: step 38740, loss = 0.85 (264.9 examples/sec; 0.483 sec/batch)
2016-02-03 14:56:45.410498: step 38750, loss = 0.76 (260.5 examples/sec; 0.491 sec/batch)
2016-02-03 14:56:50.212588: step 38760, loss = 0.84 (270.5 examples/sec; 0.473 sec/batch)
2016-02-03 14:56:54.879567: step 38770, loss = 0.84 (283.3 examples/sec; 0.452 sec/batch)
2016-02-03 14:56:59.491420: step 38780, loss = 0.76 (259.5 examples/sec; 0.493 sec/batch)
2016-02-03 14:57:04.193599: step 38790, loss = 0.72 (255.2 examples/sec; 0.502 sec/batch)
2016-02-03 14:57:08.956528: step 38800, loss = 0.80 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 14:57:14.151026: step 38810, loss = 0.78 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 14:57:18.785971: step 38820, loss = 0.70 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 14:57:23.475500: step 38830, loss = 0.64 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 14:57:28.228476: step 38840, loss = 0.78 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 14:57:32.918021: step 38850, loss = 0.83 (256.0 examples/sec; 0.500 sec/batch)
2016-02-03 14:57:37.545322: step 38860, loss = 0.79 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 14:57:42.178961: step 38870, loss = 0.71 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 14:57:46.827379: step 38880, loss = 0.76 (297.2 examples/sec; 0.431 sec/batch)
2016-02-03 14:57:51.527315: step 38890, loss = 0.76 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 14:57:56.148689: step 38900, loss = 0.79 (288.6 examples/sec; 0.444 sec/batch)
2016-02-03 14:58:01.363379: step 38910, loss = 0.93 (259.7 examples/sec; 0.493 sec/batch)
2016-02-03 14:58:06.061785: step 38920, loss = 0.75 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 14:58:10.683709: step 38930, loss = 0.94 (263.2 examples/sec; 0.486 sec/batch)
2016-02-03 14:58:15.494529: step 38940, loss = 0.88 (253.4 examples/sec; 0.505 sec/batch)
2016-02-03 14:58:20.171783: step 38950, loss = 0.66 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 14:58:24.811667: step 38960, loss = 0.72 (278.0 examples/sec; 0.460 sec/batch)
2016-02-03 14:58:29.424385: step 38970, loss = 0.73 (304.3 examples/sec; 0.421 sec/batch)
2016-02-03 14:58:34.060819: step 38980, loss = 0.83 (303.8 examples/sec; 0.421 sec/batch)
2016-02-03 14:58:38.792576: step 38990, loss = 0.81 (252.9 examples/sec; 0.506 sec/batch)
2016-02-03 14:58:43.429582: step 39000, loss = 0.74 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 14:58:48.553962: step 39010, loss = 0.71 (288.7 examples/sec; 0.443 sec/batch)
2016-02-03 14:58:53.250689: step 39020, loss = 0.83 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 14:58:57.867805: step 39030, loss = 0.77 (276.7 examples/sec; 0.463 sec/batch)
2016-02-03 14:59:02.557262: step 39040, loss = 0.72 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 14:59:07.130003: step 39050, loss = 0.89 (280.4 examples/sec; 0.456 sec/batch)
2016-02-03 14:59:11.889685: step 39060, loss = 0.87 (281.2 examples/sec; 0.455 sec/batch)
2016-02-03 14:59:16.584089: step 39070, loss = 0.74 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 14:59:21.291489: step 39080, loss = 0.69 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 14:59:25.949797: step 39090, loss = 0.93 (259.3 examples/sec; 0.494 sec/batch)
2016-02-03 14:59:30.562060: step 39100, loss = 0.76 (290.3 examples/sec; 0.441 sec/batch)
2016-02-03 14:59:35.794473: step 39110, loss = 0.76 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 14:59:40.517651: step 39120, loss = 0.83 (292.8 examples/sec; 0.437 sec/batch)
2016-02-03 14:59:45.217855: step 39130, loss = 0.85 (260.3 examples/sec; 0.492 sec/batch)
2016-02-03 14:59:49.906512: step 39140, loss = 0.79 (263.2 examples/sec; 0.486 sec/batch)
2016-02-03 14:59:54.538213: step 39150, loss = 0.77 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 14:59:59.223022: step 39160, loss = 0.80 (254.6 examples/sec; 0.503 sec/batch)
2016-02-03 15:00:03.973709: step 39170, loss = 0.69 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 15:00:08.688960: step 39180, loss = 0.97 (262.9 examples/sec; 0.487 sec/batch)
2016-02-03 15:00:13.416393: step 39190, loss = 0.73 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 15:00:18.102954: step 39200, loss = 0.86 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 15:00:23.400112: step 39210, loss = 0.82 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 15:00:28.141643: step 39220, loss = 0.72 (243.8 examples/sec; 0.525 sec/batch)
2016-02-03 15:00:32.882421: step 39230, loss = 0.62 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 15:00:37.551587: step 39240, loss = 0.71 (285.0 examples/sec; 0.449 sec/batch)
2016-02-03 15:00:42.243321: step 39250, loss = 0.90 (301.8 examples/sec; 0.424 sec/batch)
2016-02-03 15:00:46.936807: step 39260, loss = 0.91 (293.4 examples/sec; 0.436 sec/batch)
2016-02-03 15:00:51.670536: step 39270, loss = 0.69 (276.9 examples/sec; 0.462 sec/batch)
2016-02-03 15:00:56.408759: step 39280, loss = 0.89 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 15:01:01.154578: step 39290, loss = 0.65 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 15:01:05.902656: step 39300, loss = 0.73 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 15:01:11.114525: step 39310, loss = 0.69 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 15:01:15.816050: step 39320, loss = 0.74 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 15:01:20.629572: step 39330, loss = 0.90 (255.7 examples/sec; 0.501 sec/batch)
2016-02-03 15:01:25.341617: step 39340, loss = 0.74 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 15:01:30.053908: step 39350, loss = 0.76 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 15:01:34.795849: step 39360, loss = 0.73 (300.7 examples/sec; 0.426 sec/batch)
2016-02-03 15:01:39.541277: step 39370, loss = 0.92 (285.5 examples/sec; 0.448 sec/batch)
2016-02-03 15:01:44.336940: step 39380, loss = 0.89 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 15:01:49.091170: step 39390, loss = 0.91 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 15:01:53.756807: step 39400, loss = 0.73 (288.2 examples/sec; 0.444 sec/batch)
2016-02-03 15:01:58.979566: step 39410, loss = 0.83 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 15:02:03.727122: step 39420, loss = 0.81 (253.1 examples/sec; 0.506 sec/batch)
2016-02-03 15:02:08.486576: step 39430, loss = 0.80 (284.8 examples/sec; 0.449 sec/batch)
2016-02-03 15:02:13.161623: step 39440, loss = 0.69 (284.0 examples/sec; 0.451 sec/batch)
2016-02-03 15:02:17.862165: step 39450, loss = 0.75 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 15:02:22.552528: step 39460, loss = 0.72 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 15:02:27.230751: step 39470, loss = 0.71 (305.3 examples/sec; 0.419 sec/batch)
2016-02-03 15:02:31.986669: step 39480, loss = 0.88 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 15:02:36.725273: step 39490, loss = 0.87 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 15:02:41.479086: step 39500, loss = 0.97 (253.9 examples/sec; 0.504 sec/batch)
2016-02-03 15:02:46.680368: step 39510, loss = 0.63 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 15:02:51.386278: step 39520, loss = 0.63 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 15:02:56.159276: step 39530, loss = 0.87 (267.2 examples/sec; 0.479 sec/batch)
2016-02-03 15:03:00.872234: step 39540, loss = 0.66 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 15:03:05.560580: step 39550, loss = 0.73 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 15:03:10.266741: step 39560, loss = 0.79 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 15:03:14.907687: step 39570, loss = 0.86 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 15:03:19.651699: step 39580, loss = 0.71 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 15:03:24.284162: step 39590, loss = 0.72 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 15:03:29.042503: step 39600, loss = 0.76 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 15:03:34.286419: step 39610, loss = 0.82 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 15:03:38.959288: step 39620, loss = 0.97 (284.8 examples/sec; 0.449 sec/batch)
2016-02-03 15:03:43.683440: step 39630, loss = 0.80 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 15:03:48.342267: step 39640, loss = 0.72 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 15:03:52.982100: step 39650, loss = 0.81 (295.3 examples/sec; 0.433 sec/batch)
2016-02-03 15:03:57.679841: step 39660, loss = 0.77 (282.5 examples/sec; 0.453 sec/batch)
2016-02-03 15:04:02.342868: step 39670, loss = 0.70 (286.1 examples/sec; 0.447 sec/batch)
2016-02-03 15:04:07.101842: step 39680, loss = 0.94 (261.1 examples/sec; 0.490 sec/batch)
2016-02-03 15:04:11.820581: step 39690, loss = 0.79 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 15:04:16.538503: step 39700, loss = 0.63 (279.2 examples/sec; 0.459 sec/batch)
2016-02-03 15:04:21.805956: step 39710, loss = 0.84 (286.1 examples/sec; 0.447 sec/batch)
2016-02-03 15:04:26.509196: step 39720, loss = 0.87 (282.9 examples/sec; 0.453 sec/batch)
2016-02-03 15:04:31.253219: step 39730, loss = 0.78 (275.0 examples/sec; 0.465 sec/batch)
2016-02-03 15:04:36.015292: step 39740, loss = 0.74 (257.5 examples/sec; 0.497 sec/batch)
2016-02-03 15:04:40.727406: step 39750, loss = 0.87 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 15:04:45.423421: step 39760, loss = 0.93 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 15:04:50.135946: step 39770, loss = 0.75 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 15:04:54.869853: step 39780, loss = 0.60 (252.9 examples/sec; 0.506 sec/batch)
2016-02-03 15:04:59.576093: step 39790, loss = 0.75 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 15:05:04.262093: step 39800, loss = 0.73 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 15:05:09.551577: step 39810, loss = 0.97 (262.1 examples/sec; 0.488 sec/batch)
2016-02-03 15:05:14.254540: step 39820, loss = 0.83 (291.7 examples/sec; 0.439 sec/batch)
2016-02-03 15:05:18.944374: step 39830, loss = 0.82 (287.9 examples/sec; 0.445 sec/batch)
2016-02-03 15:05:23.599839: step 39840, loss = 0.81 (304.0 examples/sec; 0.421 sec/batch)
2016-02-03 15:05:28.341009: step 39850, loss = 0.78 (265.3 examples/sec; 0.482 sec/batch)
2016-02-03 15:05:32.995127: step 39860, loss = 0.64 (257.6 examples/sec; 0.497 sec/batch)
2016-02-03 15:05:37.608652: step 39870, loss = 0.85 (268.1 examples/sec; 0.478 sec/batch)
2016-02-03 15:05:42.330626: step 39880, loss = 0.96 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 15:05:47.017436: step 39890, loss = 0.83 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 15:05:51.715586: step 39900, loss = 0.83 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 15:05:56.941280: step 39910, loss = 0.62 (257.9 examples/sec; 0.496 sec/batch)
2016-02-03 15:06:01.706885: step 39920, loss = 0.69 (257.9 examples/sec; 0.496 sec/batch)
2016-02-03 15:06:06.473616: step 39930, loss = 0.69 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 15:06:11.195578: step 39940, loss = 0.86 (276.3 examples/sec; 0.463 sec/batch)
2016-02-03 15:06:15.869302: step 39950, loss = 0.74 (292.3 examples/sec; 0.438 sec/batch)
2016-02-03 15:06:20.641738: step 39960, loss = 0.75 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 15:06:25.320369: step 39970, loss = 0.64 (258.2 examples/sec; 0.496 sec/batch)
2016-02-03 15:06:30.059562: step 39980, loss = 0.79 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 15:06:34.807832: step 39990, loss = 0.67 (286.6 examples/sec; 0.447 sec/batch)
2016-02-03 15:06:39.611587: step 40000, loss = 0.75 (279.7 examples/sec; 0.458 sec/batch)
2016-02-03 15:06:44.930531: step 40010, loss = 0.84 (252.7 examples/sec; 0.506 sec/batch)
2016-02-03 15:06:49.729749: step 40020, loss = 0.78 (267.3 examples/sec; 0.479 sec/batch)
2016-02-03 15:06:54.511423: step 40030, loss = 0.72 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 15:06:59.229784: step 40040, loss = 0.74 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 15:07:03.920591: step 40050, loss = 0.65 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 15:07:08.658438: step 40060, loss = 0.72 (258.3 examples/sec; 0.495 sec/batch)
2016-02-03 15:07:13.406556: step 40070, loss = 0.81 (264.7 examples/sec; 0.484 sec/batch)
2016-02-03 15:07:18.127139: step 40080, loss = 0.60 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 15:07:22.827128: step 40090, loss = 0.79 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 15:07:27.535323: step 40100, loss = 0.68 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 15:07:32.795393: step 40110, loss = 0.76 (252.1 examples/sec; 0.508 sec/batch)
2016-02-03 15:07:37.454087: step 40120, loss = 0.70 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 15:07:42.149270: step 40130, loss = 0.90 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 15:07:46.796996: step 40140, loss = 0.68 (263.6 examples/sec; 0.486 sec/batch)
2016-02-03 15:07:51.523601: step 40150, loss = 0.70 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 15:07:56.337327: step 40160, loss = 0.97 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 15:08:01.109394: step 40170, loss = 0.77 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 15:08:05.755116: step 40180, loss = 0.72 (258.0 examples/sec; 0.496 sec/batch)
2016-02-03 15:08:10.391584: step 40190, loss = 0.63 (295.3 examples/sec; 0.433 sec/batch)
2016-02-03 15:08:15.087904: step 40200, loss = 0.55 (279.2 examples/sec; 0.458 sec/batch)
2016-02-03 15:08:20.332534: step 40210, loss = 0.73 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 15:08:25.098936: step 40220, loss = 0.72 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 15:08:29.733886: step 40230, loss = 0.94 (290.5 examples/sec; 0.441 sec/batch)
2016-02-03 15:08:34.497701: step 40240, loss = 0.80 (263.0 examples/sec; 0.487 sec/batch)
2016-02-03 15:08:39.198232: step 40250, loss = 0.81 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 15:08:43.909525: step 40260, loss = 0.89 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 15:08:48.578055: step 40270, loss = 0.82 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 15:08:53.314358: step 40280, loss = 0.88 (280.1 examples/sec; 0.457 sec/batch)
2016-02-03 15:08:58.071191: step 40290, loss = 0.77 (250.3 examples/sec; 0.511 sec/batch)
2016-02-03 15:09:02.781154: step 40300, loss = 0.82 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 15:09:07.941942: step 40310, loss = 0.94 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 15:09:12.584812: step 40320, loss = 0.65 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 15:09:17.350083: step 40330, loss = 0.79 (291.1 examples/sec; 0.440 sec/batch)
2016-02-03 15:09:22.030492: step 40340, loss = 0.83 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 15:09:26.669668: step 40350, loss = 0.80 (293.9 examples/sec; 0.436 sec/batch)
2016-02-03 15:09:31.448351: step 40360, loss = 0.77 (253.5 examples/sec; 0.505 sec/batch)
2016-02-03 15:09:36.200551: step 40370, loss = 0.74 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 15:09:40.847804: step 40380, loss = 0.79 (259.1 examples/sec; 0.494 sec/batch)
2016-02-03 15:09:45.565107: step 40390, loss = 0.93 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 15:09:50.278439: step 40400, loss = 0.81 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 15:09:55.537442: step 40410, loss = 0.85 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 15:10:00.256695: step 40420, loss = 0.79 (287.8 examples/sec; 0.445 sec/batch)
2016-02-03 15:10:04.966817: step 40430, loss = 0.73 (304.8 examples/sec; 0.420 sec/batch)
2016-02-03 15:10:09.701337: step 40440, loss = 0.62 (277.2 examples/sec; 0.462 sec/batch)
2016-02-03 15:10:14.343821: step 40450, loss = 0.86 (298.8 examples/sec; 0.428 sec/batch)
2016-02-03 15:10:18.978176: step 40460, loss = 0.75 (290.9 examples/sec; 0.440 sec/batch)
2016-02-03 15:10:23.707962: step 40470, loss = 0.76 (265.0 examples/sec; 0.483 sec/batch)
2016-02-03 15:10:28.462766: step 40480, loss = 0.75 (270.5 examples/sec; 0.473 sec/batch)
2016-02-03 15:10:33.160561: step 40490, loss = 0.63 (290.9 examples/sec; 0.440 sec/batch)
2016-02-03 15:10:37.885932: step 40500, loss = 0.54 (284.1 examples/sec; 0.451 sec/batch)
2016-02-03 15:10:43.107916: step 40510, loss = 0.64 (265.8 examples/sec; 0.482 sec/batch)
2016-02-03 15:10:47.878046: step 40520, loss = 0.91 (250.8 examples/sec; 0.510 sec/batch)
2016-02-03 15:10:52.565557: step 40530, loss = 0.80 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 15:10:57.276141: step 40540, loss = 0.68 (287.8 examples/sec; 0.445 sec/batch)
2016-02-03 15:11:02.074611: step 40550, loss = 0.79 (258.1 examples/sec; 0.496 sec/batch)
2016-02-03 15:11:06.879910: step 40560, loss = 0.74 (241.1 examples/sec; 0.531 sec/batch)
2016-02-03 15:11:11.609349: step 40570, loss = 0.70 (256.2 examples/sec; 0.500 sec/batch)
2016-02-03 15:11:16.301529: step 40580, loss = 0.92 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 15:11:21.033729: step 40590, loss = 0.78 (282.4 examples/sec; 0.453 sec/batch)
2016-02-03 15:11:25.844875: step 40600, loss = 0.76 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 15:11:31.029916: step 40610, loss = 0.67 (289.3 examples/sec; 0.442 sec/batch)
2016-02-03 15:11:35.743474: step 40620, loss = 0.76 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 15:11:40.444730: step 40630, loss = 0.79 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 15:11:45.161881: step 40640, loss = 0.77 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 15:11:49.953493: step 40650, loss = 0.68 (256.8 examples/sec; 0.499 sec/batch)
2016-02-03 15:11:54.641166: step 40660, loss = 0.96 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 15:11:59.367876: step 40670, loss = 0.73 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 15:12:04.081857: step 40680, loss = 0.69 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 15:12:08.774741: step 40690, loss = 0.65 (287.2 examples/sec; 0.446 sec/batch)
2016-02-03 15:12:13.553416: step 40700, loss = 0.71 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 15:12:18.796313: step 40710, loss = 0.96 (285.7 examples/sec; 0.448 sec/batch)
2016-02-03 15:12:23.556588: step 40720, loss = 0.67 (255.5 examples/sec; 0.501 sec/batch)
2016-02-03 15:12:28.259883: step 40730, loss = 0.74 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 15:12:32.970406: step 40740, loss = 0.70 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 15:12:37.681788: step 40750, loss = 0.66 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 15:12:42.392622: step 40760, loss = 0.89 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 15:12:47.183125: step 40770, loss = 0.79 (249.4 examples/sec; 0.513 sec/batch)
2016-02-03 15:12:51.859045: step 40780, loss = 0.78 (255.0 examples/sec; 0.502 sec/batch)
2016-02-03 15:12:56.591929: step 40790, loss = 0.89 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 15:13:01.259978: step 40800, loss = 0.76 (283.7 examples/sec; 0.451 sec/batch)
2016-02-03 15:13:06.547187: step 40810, loss = 0.73 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 15:13:11.240542: step 40820, loss = 0.84 (251.2 examples/sec; 0.509 sec/batch)
2016-02-03 15:13:15.981848: step 40830, loss = 0.56 (245.2 examples/sec; 0.522 sec/batch)
2016-02-03 15:13:20.782070: step 40840, loss = 0.75 (238.2 examples/sec; 0.537 sec/batch)
2016-02-03 15:13:25.543074: step 40850, loss = 0.65 (247.7 examples/sec; 0.517 sec/batch)
2016-02-03 15:13:30.291100: step 40860, loss = 0.83 (285.8 examples/sec; 0.448 sec/batch)
2016-02-03 15:13:35.011327: step 40870, loss = 0.60 (291.8 examples/sec; 0.439 sec/batch)
2016-02-03 15:13:39.778403: step 40880, loss = 0.62 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 15:13:44.469201: step 40890, loss = 0.87 (288.3 examples/sec; 0.444 sec/batch)
2016-02-03 15:13:49.150897: step 40900, loss = 0.79 (263.6 examples/sec; 0.486 sec/batch)
2016-02-03 15:13:54.394390: step 40910, loss = 0.79 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 15:13:59.069255: step 40920, loss = 0.75 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 15:14:03.790786: step 40930, loss = 0.66 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 15:14:08.552638: step 40940, loss = 0.65 (255.4 examples/sec; 0.501 sec/batch)
2016-02-03 15:14:13.219388: step 40950, loss = 0.73 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 15:14:17.948724: step 40960, loss = 0.77 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 15:14:22.694921: step 40970, loss = 0.82 (258.0 examples/sec; 0.496 sec/batch)
2016-02-03 15:14:27.391094: step 40980, loss = 0.64 (256.7 examples/sec; 0.499 sec/batch)
2016-02-03 15:14:32.133987: step 40990, loss = 0.90 (253.7 examples/sec; 0.504 sec/batch)
2016-02-03 15:14:36.787133: step 41000, loss = 0.88 (286.1 examples/sec; 0.447 sec/batch)
2016-02-03 15:14:42.052219: step 41010, loss = 0.76 (256.4 examples/sec; 0.499 sec/batch)
2016-02-03 15:14:46.706807: step 41020, loss = 0.74 (268.0 examples/sec; 0.478 sec/batch)
2016-02-03 15:14:51.455441: step 41030, loss = 0.78 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 15:14:56.290073: step 41040, loss = 0.75 (253.3 examples/sec; 0.505 sec/batch)
2016-02-03 15:15:00.946492: step 41050, loss = 0.80 (259.0 examples/sec; 0.494 sec/batch)
2016-02-03 15:15:05.693243: step 41060, loss = 0.77 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 15:15:10.411539: step 41070, loss = 0.78 (253.7 examples/sec; 0.505 sec/batch)
2016-02-03 15:15:15.159519: step 41080, loss = 0.77 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 15:15:19.896817: step 41090, loss = 0.84 (280.6 examples/sec; 0.456 sec/batch)
2016-02-03 15:15:24.637037: step 41100, loss = 0.83 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 15:15:29.972450: step 41110, loss = 0.72 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 15:15:34.775154: step 41120, loss = 0.69 (248.5 examples/sec; 0.515 sec/batch)
2016-02-03 15:15:39.443598: step 41130, loss = 0.68 (267.0 examples/sec; 0.479 sec/batch)
2016-02-03 15:15:44.193370: step 41140, loss = 0.83 (258.0 examples/sec; 0.496 sec/batch)
2016-02-03 15:15:48.923423: step 41150, loss = 0.83 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 15:15:53.628307: step 41160, loss = 0.68 (261.7 examples/sec; 0.489 sec/batch)
2016-02-03 15:15:58.353837: step 41170, loss = 0.73 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 15:16:03.194193: step 41180, loss = 0.88 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 15:16:07.963370: step 41190, loss = 0.83 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 15:16:12.695441: step 41200, loss = 0.73 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 15:16:17.843940: step 41210, loss = 0.69 (289.7 examples/sec; 0.442 sec/batch)
2016-02-03 15:16:22.592173: step 41220, loss = 0.92 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 15:16:27.272908: step 41230, loss = 0.89 (287.0 examples/sec; 0.446 sec/batch)
2016-02-03 15:16:32.020928: step 41240, loss = 0.92 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 15:16:36.732017: step 41250, loss = 0.78 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 15:16:41.435233: step 41260, loss = 0.70 (260.8 examples/sec; 0.491 sec/batch)
2016-02-03 15:16:46.102154: step 41270, loss = 0.75 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 15:16:50.745075: step 41280, loss = 0.75 (315.0 examples/sec; 0.406 sec/batch)
2016-02-03 15:16:55.442676: step 41290, loss = 0.59 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 15:17:00.164305: step 41300, loss = 0.77 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 15:17:05.381465: step 41310, loss = 0.78 (290.7 examples/sec; 0.440 sec/batch)
2016-02-03 15:17:10.106003: step 41320, loss = 0.58 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 15:17:14.846749: step 41330, loss = 0.78 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 15:17:19.485862: step 41340, loss = 0.84 (299.5 examples/sec; 0.427 sec/batch)
2016-02-03 15:17:24.289650: step 41350, loss = 0.85 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 15:17:28.908936: step 41360, loss = 0.89 (255.2 examples/sec; 0.502 sec/batch)
2016-02-03 15:17:33.554558: step 41370, loss = 0.78 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 15:17:38.219535: step 41380, loss = 0.76 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 15:17:42.927267: step 41390, loss = 0.73 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 15:17:47.590771: step 41400, loss = 0.90 (312.9 examples/sec; 0.409 sec/batch)
2016-02-03 15:17:52.801310: step 41410, loss = 0.83 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 15:17:57.425144: step 41420, loss = 0.76 (282.6 examples/sec; 0.453 sec/batch)
2016-02-03 15:18:02.187942: step 41430, loss = 0.74 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 15:18:06.854074: step 41440, loss = 0.67 (253.8 examples/sec; 0.504 sec/batch)
2016-02-03 15:18:11.498160: step 41450, loss = 0.66 (261.7 examples/sec; 0.489 sec/batch)
2016-02-03 15:18:16.194996: step 41460, loss = 0.67 (250.5 examples/sec; 0.511 sec/batch)
2016-02-03 15:18:20.968182: step 41470, loss = 0.69 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 15:18:25.593739: step 41480, loss = 0.88 (301.0 examples/sec; 0.425 sec/batch)
2016-02-03 15:18:30.217693: step 41490, loss = 0.73 (294.4 examples/sec; 0.435 sec/batch)
2016-02-03 15:18:34.909663: step 41500, loss = 0.76 (293.6 examples/sec; 0.436 sec/batch)
2016-02-03 15:18:40.054315: step 41510, loss = 0.87 (260.4 examples/sec; 0.492 sec/batch)
2016-02-03 15:18:44.768779: step 41520, loss = 0.78 (264.2 examples/sec; 0.485 sec/batch)
2016-02-03 15:18:49.501635: step 41530, loss = 0.99 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 15:18:54.295576: step 41540, loss = 0.71 (261.7 examples/sec; 0.489 sec/batch)
2016-02-03 15:18:59.080759: step 41550, loss = 0.72 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 15:19:03.814598: step 41560, loss = 0.90 (274.4 examples/sec; 0.466 sec/batch)
2016-02-03 15:19:08.462407: step 41570, loss = 0.62 (261.3 examples/sec; 0.490 sec/batch)
2016-02-03 15:19:13.108702: step 41580, loss = 0.71 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 15:19:17.790949: step 41590, loss = 0.83 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 15:19:22.445501: step 41600, loss = 0.65 (308.2 examples/sec; 0.415 sec/batch)
2016-02-03 15:19:27.643845: step 41610, loss = 0.67 (247.8 examples/sec; 0.517 sec/batch)
2016-02-03 15:19:32.271174: step 41620, loss = 0.78 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 15:19:36.988176: step 41630, loss = 0.81 (290.5 examples/sec; 0.441 sec/batch)
2016-02-03 15:19:41.779846: step 41640, loss = 0.84 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 15:19:46.475478: step 41650, loss = 0.68 (286.1 examples/sec; 0.447 sec/batch)
2016-02-03 15:19:51.195432: step 41660, loss = 0.69 (256.2 examples/sec; 0.500 sec/batch)
2016-02-03 15:19:55.847671: step 41670, loss = 0.85 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 15:20:00.574470: step 41680, loss = 0.69 (299.5 examples/sec; 0.427 sec/batch)
2016-02-03 15:20:05.442652: step 41690, loss = 0.75 (260.0 examples/sec; 0.492 sec/batch)
2016-02-03 15:20:10.169537: step 41700, loss = 0.85 (258.1 examples/sec; 0.496 sec/batch)
2016-02-03 15:20:15.451795: step 41710, loss = 0.71 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 15:20:20.202693: step 41720, loss = 0.80 (254.8 examples/sec; 0.502 sec/batch)
2016-02-03 15:20:24.962046: step 41730, loss = 0.78 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 15:20:29.719264: step 41740, loss = 0.88 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 15:20:34.516440: step 41750, loss = 0.85 (259.7 examples/sec; 0.493 sec/batch)
2016-02-03 15:20:39.249672: step 41760, loss = 0.79 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 15:20:44.009706: step 41770, loss = 0.73 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 15:20:48.741632: step 41780, loss = 0.65 (259.9 examples/sec; 0.492 sec/batch)
2016-02-03 15:20:53.454618: step 41790, loss = 0.76 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 15:20:58.188042: step 41800, loss = 0.69 (286.2 examples/sec; 0.447 sec/batch)
2016-02-03 15:21:03.484885: step 41810, loss = 0.75 (304.4 examples/sec; 0.420 sec/batch)
2016-02-03 15:21:08.226856: step 41820, loss = 0.76 (278.5 examples/sec; 0.460 sec/batch)
2016-02-03 15:21:12.877202: step 41830, loss = 0.78 (297.6 examples/sec; 0.430 sec/batch)
2016-02-03 15:21:17.657755: step 41840, loss = 0.76 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 15:21:22.333220: step 41850, loss = 0.67 (296.1 examples/sec; 0.432 sec/batch)
2016-02-03 15:21:27.051040: step 41860, loss = 0.78 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 15:21:31.754257: step 41870, loss = 0.76 (252.1 examples/sec; 0.508 sec/batch)
2016-02-03 15:21:36.448543: step 41880, loss = 0.82 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 15:21:41.160166: step 41890, loss = 0.77 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 15:21:45.853948: step 41900, loss = 0.62 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 15:21:51.068873: step 41910, loss = 0.76 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 15:21:55.700463: step 41920, loss = 0.70 (290.5 examples/sec; 0.441 sec/batch)
2016-02-03 15:22:00.396994: step 41930, loss = 0.86 (255.4 examples/sec; 0.501 sec/batch)
2016-02-03 15:22:05.075828: step 41940, loss = 0.71 (286.1 examples/sec; 0.447 sec/batch)
2016-02-03 15:22:09.863588: step 41950, loss = 0.61 (247.2 examples/sec; 0.518 sec/batch)
2016-02-03 15:22:14.468565: step 41960, loss = 0.76 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 15:22:19.208577: step 41970, loss = 0.73 (257.3 examples/sec; 0.498 sec/batch)
2016-02-03 15:22:23.826910: step 41980, loss = 0.85 (269.2 examples/sec; 0.475 sec/batch)
2016-02-03 15:22:28.419667: step 41990, loss = 0.75 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 15:22:33.041411: step 42000, loss = 0.66 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 15:22:38.182456: step 42010, loss = 0.79 (293.8 examples/sec; 0.436 sec/batch)
2016-02-03 15:22:42.930485: step 42020, loss = 0.80 (264.9 examples/sec; 0.483 sec/batch)
2016-02-03 15:22:47.629020: step 42030, loss = 0.75 (299.1 examples/sec; 0.428 sec/batch)
2016-02-03 15:22:52.383214: step 42040, loss = 0.69 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 15:22:57.073088: step 42050, loss = 0.94 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 15:23:01.782311: step 42060, loss = 0.94 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 15:23:06.476168: step 42070, loss = 0.70 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 15:23:11.105986: step 42080, loss = 0.71 (258.0 examples/sec; 0.496 sec/batch)
2016-02-03 15:23:15.867501: step 42090, loss = 0.72 (253.4 examples/sec; 0.505 sec/batch)
2016-02-03 15:23:20.547681: step 42100, loss = 0.76 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 15:23:25.706916: step 42110, loss = 0.60 (294.0 examples/sec; 0.435 sec/batch)
2016-02-03 15:23:30.307495: step 42120, loss = 0.78 (284.1 examples/sec; 0.451 sec/batch)
2016-02-03 15:23:34.966577: step 42130, loss = 0.83 (289.3 examples/sec; 0.443 sec/batch)
2016-02-03 15:23:39.647631: step 42140, loss = 0.82 (276.6 examples/sec; 0.463 sec/batch)
2016-02-03 15:23:44.388668: step 42150, loss = 0.72 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 15:23:49.104985: step 42160, loss = 0.72 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 15:23:53.822423: step 42170, loss = 0.74 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 15:23:58.530332: step 42180, loss = 0.95 (289.0 examples/sec; 0.443 sec/batch)
2016-02-03 15:24:03.261245: step 42190, loss = 0.69 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 15:24:07.982584: step 42200, loss = 0.86 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 15:24:13.214575: step 42210, loss = 0.81 (284.6 examples/sec; 0.450 sec/batch)
2016-02-03 15:24:17.949921: step 42220, loss = 0.81 (259.2 examples/sec; 0.494 sec/batch)
2016-02-03 15:24:22.626095: step 42230, loss = 0.80 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 15:24:27.416857: step 42240, loss = 0.82 (267.0 examples/sec; 0.479 sec/batch)
2016-02-03 15:24:32.134410: step 42250, loss = 0.85 (281.0 examples/sec; 0.455 sec/batch)
2016-02-03 15:24:36.917603: step 42260, loss = 0.61 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 15:24:41.630632: step 42270, loss = 0.71 (307.7 examples/sec; 0.416 sec/batch)
2016-02-03 15:24:46.329219: step 42280, loss = 0.80 (266.3 examples/sec; 0.481 sec/batch)
2016-02-03 15:24:51.004309: step 42290, loss = 0.87 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 15:24:55.662589: step 42300, loss = 0.72 (290.1 examples/sec; 0.441 sec/batch)
2016-02-03 15:25:00.848711: step 42310, loss = 0.78 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 15:25:05.452506: step 42320, loss = 0.74 (258.3 examples/sec; 0.496 sec/batch)
2016-02-03 15:25:10.079332: step 42330, loss = 0.78 (282.5 examples/sec; 0.453 sec/batch)
2016-02-03 15:25:14.738664: step 42340, loss = 0.81 (291.4 examples/sec; 0.439 sec/batch)
2016-02-03 15:25:19.388296: step 42350, loss = 0.81 (301.1 examples/sec; 0.425 sec/batch)
2016-02-03 15:25:24.138467: step 42360, loss = 0.68 (285.8 examples/sec; 0.448 sec/batch)
2016-02-03 15:25:28.836228: step 42370, loss = 0.75 (291.0 examples/sec; 0.440 sec/batch)
2016-02-03 15:25:33.529588: step 42380, loss = 0.72 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 15:25:38.231616: step 42390, loss = 0.85 (294.6 examples/sec; 0.434 sec/batch)
2016-02-03 15:25:42.917907: step 42400, loss = 0.84 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 15:25:48.158794: step 42410, loss = 0.73 (257.7 examples/sec; 0.497 sec/batch)
2016-02-03 15:25:52.919347: step 42420, loss = 0.75 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 15:25:57.513389: step 42430, loss = 0.80 (279.4 examples/sec; 0.458 sec/batch)
2016-02-03 15:26:02.257943: step 42440, loss = 0.78 (280.9 examples/sec; 0.456 sec/batch)
2016-02-03 15:26:06.994425: step 42450, loss = 0.75 (277.8 examples/sec; 0.461 sec/batch)
2016-02-03 15:26:11.714660: step 42460, loss = 0.93 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 15:26:16.444394: step 42470, loss = 0.83 (265.8 examples/sec; 0.482 sec/batch)
2016-02-03 15:26:21.219717: step 42480, loss = 0.67 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 15:26:25.898143: step 42490, loss = 0.78 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 15:26:30.556963: step 42500, loss = 0.77 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 15:26:35.786425: step 42510, loss = 0.97 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 15:26:40.498247: step 42520, loss = 0.86 (249.4 examples/sec; 0.513 sec/batch)
2016-02-03 15:26:45.268739: step 42530, loss = 0.75 (310.3 examples/sec; 0.413 sec/batch)
2016-02-03 15:26:50.015986: step 42540, loss = 0.74 (260.1 examples/sec; 0.492 sec/batch)
2016-02-03 15:26:54.629870: step 42550, loss = 0.76 (309.6 examples/sec; 0.413 sec/batch)
2016-02-03 15:26:59.374508: step 42560, loss = 0.72 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 15:27:04.045963: step 42570, loss = 0.76 (264.1 examples/sec; 0.485 sec/batch)
2016-02-03 15:27:08.772820: step 42580, loss = 0.86 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 15:27:13.446176: step 42590, loss = 0.71 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 15:27:18.188286: step 42600, loss = 0.67 (252.8 examples/sec; 0.506 sec/batch)
2016-02-03 15:27:23.403529: step 42610, loss = 0.82 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 15:27:28.116247: step 42620, loss = 0.85 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 15:27:32.771998: step 42630, loss = 0.59 (252.3 examples/sec; 0.507 sec/batch)
2016-02-03 15:27:37.411062: step 42640, loss = 0.78 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 15:27:42.064048: step 42650, loss = 0.72 (273.2 examples/sec; 0.469 sec/batch)
2016-02-03 15:27:46.772618: step 42660, loss = 0.85 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 15:27:51.441260: step 42670, loss = 0.89 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 15:27:56.104243: step 42680, loss = 0.84 (295.1 examples/sec; 0.434 sec/batch)
2016-02-03 15:28:00.754810: step 42690, loss = 0.94 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 15:28:05.409976: step 42700, loss = 0.73 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 15:28:10.630985: step 42710, loss = 1.06 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 15:28:15.285465: step 42720, loss = 0.76 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 15:28:20.023282: step 42730, loss = 0.61 (296.4 examples/sec; 0.432 sec/batch)
2016-02-03 15:28:24.755973: step 42740, loss = 0.58 (257.7 examples/sec; 0.497 sec/batch)
2016-02-03 15:28:29.503181: step 42750, loss = 0.69 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 15:28:34.203027: step 42760, loss = 0.75 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 15:28:38.785099: step 42770, loss = 0.84 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 15:28:43.449343: step 42780, loss = 0.74 (283.1 examples/sec; 0.452 sec/batch)
2016-02-03 15:28:48.119333: step 42790, loss = 0.74 (267.5 examples/sec; 0.478 sec/batch)
2016-02-03 15:28:52.832187: step 42800, loss = 0.89 (293.0 examples/sec; 0.437 sec/batch)
2016-02-03 15:28:58.049443: step 42810, loss = 0.66 (279.2 examples/sec; 0.459 sec/batch)
2016-02-03 15:29:02.723873: step 42820, loss = 0.75 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 15:29:07.418694: step 42830, loss = 0.90 (261.7 examples/sec; 0.489 sec/batch)
2016-02-03 15:29:12.105287: step 42840, loss = 0.71 (238.4 examples/sec; 0.537 sec/batch)
2016-02-03 15:29:16.742484: step 42850, loss = 0.72 (259.1 examples/sec; 0.494 sec/batch)
2016-02-03 15:29:21.440397: step 42860, loss = 0.70 (263.1 examples/sec; 0.487 sec/batch)
2016-02-03 15:29:26.155587: step 42870, loss = 0.68 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 15:29:30.858409: step 42880, loss = 0.72 (264.2 examples/sec; 0.485 sec/batch)
2016-02-03 15:29:35.598745: step 42890, loss = 0.79 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 15:29:40.313621: step 42900, loss = 0.86 (297.0 examples/sec; 0.431 sec/batch)
2016-02-03 15:29:45.512360: step 42910, loss = 0.71 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 15:29:50.219098: step 42920, loss = 0.75 (277.6 examples/sec; 0.461 sec/batch)
2016-02-03 15:29:54.825913: step 42930, loss = 0.74 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 15:29:59.511862: step 42940, loss = 0.81 (316.3 examples/sec; 0.405 sec/batch)
2016-02-03 15:30:04.161882: step 42950, loss = 0.77 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 15:30:08.818714: step 42960, loss = 0.83 (284.6 examples/sec; 0.450 sec/batch)
2016-02-03 15:30:13.565117: step 42970, loss = 0.79 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 15:30:18.266136: step 42980, loss = 0.71 (286.6 examples/sec; 0.447 sec/batch)
2016-02-03 15:30:22.909493: step 42990, loss = 0.82 (299.3 examples/sec; 0.428 sec/batch)
2016-02-03 15:30:27.657565: step 43000, loss = 0.87 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 15:30:32.806847: step 43010, loss = 0.67 (296.6 examples/sec; 0.431 sec/batch)
2016-02-03 15:30:37.606886: step 43020, loss = 0.78 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 15:30:42.310324: step 43030, loss = 0.73 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 15:30:47.058374: step 43040, loss = 0.74 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 15:30:51.812641: step 43050, loss = 0.81 (285.7 examples/sec; 0.448 sec/batch)
2016-02-03 15:30:56.548834: step 43060, loss = 0.61 (244.0 examples/sec; 0.525 sec/batch)
2016-02-03 15:31:01.175911: step 43070, loss = 0.85 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 15:31:05.863260: step 43080, loss = 0.89 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 15:31:10.580305: step 43090, loss = 0.70 (283.0 examples/sec; 0.452 sec/batch)
2016-02-03 15:31:15.266634: step 43100, loss = 0.68 (308.0 examples/sec; 0.416 sec/batch)
2016-02-03 15:31:20.462265: step 43110, loss = 0.81 (276.7 examples/sec; 0.463 sec/batch)
2016-02-03 15:31:25.131837: step 43120, loss = 0.73 (253.0 examples/sec; 0.506 sec/batch)
2016-02-03 15:31:29.887795: step 43130, loss = 0.89 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 15:31:34.501663: step 43140, loss = 0.73 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 15:31:39.226922: step 43150, loss = 0.76 (262.9 examples/sec; 0.487 sec/batch)
2016-02-03 15:31:44.017065: step 43160, loss = 0.68 (248.6 examples/sec; 0.515 sec/batch)
2016-02-03 15:31:48.730509: step 43170, loss = 0.81 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 15:31:53.423415: step 43180, loss = 0.84 (287.9 examples/sec; 0.445 sec/batch)
2016-02-03 15:31:58.096156: step 43190, loss = 0.71 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 15:32:02.761873: step 43200, loss = 0.90 (287.5 examples/sec; 0.445 sec/batch)
2016-02-03 15:32:08.061781: step 43210, loss = 0.78 (299.2 examples/sec; 0.428 sec/batch)
2016-02-03 15:32:12.802826: step 43220, loss = 0.72 (257.0 examples/sec; 0.498 sec/batch)
2016-02-03 15:32:17.535797: step 43230, loss = 0.74 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 15:32:22.277158: step 43240, loss = 0.71 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 15:32:26.964736: step 43250, loss = 0.80 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 15:32:31.706070: step 43260, loss = 0.74 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 15:32:36.397374: step 43270, loss = 0.79 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 15:32:41.111451: step 43280, loss = 0.75 (272.7 examples/sec; 0.469 sec/batch)
2016-02-03 15:32:45.870605: step 43290, loss = 0.71 (280.1 examples/sec; 0.457 sec/batch)
2016-02-03 15:32:50.558958: step 43300, loss = 0.74 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 15:32:55.759401: step 43310, loss = 0.64 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 15:33:00.530412: step 43320, loss = 0.77 (252.9 examples/sec; 0.506 sec/batch)
2016-02-03 15:33:05.140808: step 43330, loss = 0.72 (284.8 examples/sec; 0.449 sec/batch)
2016-02-03 15:33:09.791448: step 43340, loss = 0.71 (283.8 examples/sec; 0.451 sec/batch)
2016-02-03 15:33:14.462665: step 43350, loss = 0.76 (282.9 examples/sec; 0.453 sec/batch)
2016-02-03 15:33:19.144682: step 43360, loss = 0.71 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 15:33:23.816056: step 43370, loss = 0.81 (287.8 examples/sec; 0.445 sec/batch)
2016-02-03 15:33:28.445224: step 43380, loss = 0.78 (288.0 examples/sec; 0.445 sec/batch)
2016-02-03 15:33:33.108963: step 43390, loss = 0.84 (289.3 examples/sec; 0.442 sec/batch)
2016-02-03 15:33:37.810329: step 43400, loss = 0.78 (264.5 examples/sec; 0.484 sec/batch)
2016-02-03 15:33:43.075573: step 43410, loss = 0.74 (256.9 examples/sec; 0.498 sec/batch)
2016-02-03 15:33:47.785572: step 43420, loss = 0.80 (265.0 examples/sec; 0.483 sec/batch)
2016-02-03 15:33:52.515244: step 43430, loss = 0.71 (247.0 examples/sec; 0.518 sec/batch)
2016-02-03 15:33:57.192934: step 43440, loss = 0.68 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 15:34:01.992770: step 43450, loss = 0.68 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 15:34:06.732236: step 43460, loss = 0.64 (265.0 examples/sec; 0.483 sec/batch)
2016-02-03 15:34:11.402131: step 43470, loss = 0.70 (285.2 examples/sec; 0.449 sec/batch)
2016-02-03 15:34:16.225712: step 43480, loss = 0.92 (243.4 examples/sec; 0.526 sec/batch)
2016-02-03 15:34:20.912321: step 43490, loss = 0.67 (284.9 examples/sec; 0.449 sec/batch)
2016-02-03 15:34:25.686563: step 43500, loss = 0.77 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 15:34:30.974729: step 43510, loss = 0.65 (262.9 examples/sec; 0.487 sec/batch)
2016-02-03 15:34:35.716128: step 43520, loss = 0.83 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 15:34:40.476202: step 43530, loss = 0.82 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 15:34:45.149237: step 43540, loss = 0.64 (279.8 examples/sec; 0.457 sec/batch)
2016-02-03 15:34:49.928579: step 43550, loss = 0.82 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 15:34:54.631230: step 43560, loss = 0.92 (295.8 examples/sec; 0.433 sec/batch)
2016-02-03 15:34:59.373434: step 43570, loss = 0.70 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 15:35:04.123764: step 43580, loss = 0.73 (266.4 examples/sec; 0.480 sec/batch)
2016-02-03 15:35:08.920045: step 43590, loss = 0.66 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 15:35:13.531022: step 43600, loss = 0.66 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 15:35:18.679883: step 43610, loss = 0.70 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 15:35:23.471094: step 43620, loss = 0.57 (253.9 examples/sec; 0.504 sec/batch)
2016-02-03 15:35:28.158866: step 43630, loss = 0.72 (282.6 examples/sec; 0.453 sec/batch)
2016-02-03 15:35:32.904539: step 43640, loss = 0.72 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 15:35:37.652851: step 43650, loss = 0.72 (254.9 examples/sec; 0.502 sec/batch)
2016-02-03 15:35:42.415837: step 43660, loss = 0.76 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 15:35:47.045856: step 43670, loss = 0.79 (276.3 examples/sec; 0.463 sec/batch)
2016-02-03 15:35:51.731652: step 43680, loss = 0.71 (281.7 examples/sec; 0.454 sec/batch)
2016-02-03 15:35:56.518947: step 43690, loss = 0.65 (288.7 examples/sec; 0.443 sec/batch)
2016-02-03 15:36:01.216625: step 43700, loss = 0.89 (261.3 examples/sec; 0.490 sec/batch)
2016-02-03 15:36:06.359313: step 43710, loss = 0.71 (281.1 examples/sec; 0.455 sec/batch)
2016-02-03 15:36:11.060533: step 43720, loss = 0.77 (260.1 examples/sec; 0.492 sec/batch)
2016-02-03 15:36:15.812184: step 43730, loss = 0.94 (261.1 examples/sec; 0.490 sec/batch)
2016-02-03 15:36:20.522613: step 43740, loss = 0.73 (254.0 examples/sec; 0.504 sec/batch)
2016-02-03 15:36:25.176488: step 43750, loss = 0.71 (288.4 examples/sec; 0.444 sec/batch)
2016-02-03 15:36:29.925471: step 43760, loss = 0.75 (270.5 examples/sec; 0.473 sec/batch)
2016-02-03 15:36:34.542780: step 43770, loss = 0.65 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 15:36:39.264811: step 43780, loss = 0.93 (264.3 examples/sec; 0.484 sec/batch)
2016-02-03 15:36:43.886730: step 43790, loss = 0.64 (283.3 examples/sec; 0.452 sec/batch)
2016-02-03 15:36:48.547018: step 43800, loss = 0.60 (286.8 examples/sec; 0.446 sec/batch)
2016-02-03 15:36:53.829698: step 43810, loss = 0.84 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 15:36:58.601558: step 43820, loss = 0.68 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 15:37:03.247977: step 43830, loss = 0.77 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 15:37:07.923017: step 43840, loss = 0.76 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 15:37:12.605313: step 43850, loss = 0.71 (253.8 examples/sec; 0.504 sec/batch)
2016-02-03 15:37:17.304942: step 43860, loss = 0.70 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 15:37:22.060429: step 43870, loss = 0.64 (260.3 examples/sec; 0.492 sec/batch)
2016-02-03 15:37:26.751362: step 43880, loss = 0.83 (302.3 examples/sec; 0.423 sec/batch)
2016-02-03 15:37:31.417947: step 43890, loss = 0.73 (276.6 examples/sec; 0.463 sec/batch)
2016-02-03 15:37:36.084601: step 43900, loss = 0.81 (286.6 examples/sec; 0.447 sec/batch)
2016-02-03 15:37:41.320848: step 43910, loss = 0.79 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 15:37:46.062550: step 43920, loss = 0.75 (292.0 examples/sec; 0.438 sec/batch)
2016-02-03 15:37:50.854773: step 43930, loss = 0.60 (261.7 examples/sec; 0.489 sec/batch)
2016-02-03 15:37:55.526893: step 43940, loss = 0.91 (316.2 examples/sec; 0.405 sec/batch)
2016-02-03 15:38:00.224377: step 43950, loss = 0.86 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 15:38:04.991240: step 43960, loss = 0.58 (264.9 examples/sec; 0.483 sec/batch)
2016-02-03 15:38:09.727980: step 43970, loss = 0.82 (285.5 examples/sec; 0.448 sec/batch)
2016-02-03 15:38:14.440955: step 43980, loss = 0.70 (280.3 examples/sec; 0.457 sec/batch)
2016-02-03 15:38:19.176281: step 43990, loss = 0.71 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 15:38:23.897223: step 44000, loss = 0.62 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 15:38:29.155882: step 44010, loss = 0.76 (275.7 examples/sec; 0.464 sec/batch)
2016-02-03 15:38:33.869274: step 44020, loss = 0.87 (251.4 examples/sec; 0.509 sec/batch)
2016-02-03 15:38:38.620066: step 44030, loss = 0.78 (267.0 examples/sec; 0.479 sec/batch)
2016-02-03 15:38:43.360496: step 44040, loss = 0.71 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 15:38:48.080002: step 44050, loss = 0.82 (230.3 examples/sec; 0.556 sec/batch)
2016-02-03 15:38:52.822600: step 44060, loss = 0.81 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 15:38:57.520856: step 44070, loss = 0.74 (296.0 examples/sec; 0.432 sec/batch)
2016-02-03 15:39:02.251533: step 44080, loss = 0.65 (279.8 examples/sec; 0.458 sec/batch)
2016-02-03 15:39:07.015374: step 44090, loss = 0.85 (279.8 examples/sec; 0.457 sec/batch)
2016-02-03 15:39:11.756099: step 44100, loss = 0.67 (254.5 examples/sec; 0.503 sec/batch)
2016-02-03 15:39:17.020429: step 44110, loss = 0.80 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 15:39:21.789073: step 44120, loss = 0.70 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 15:39:26.444957: step 44130, loss = 0.58 (306.8 examples/sec; 0.417 sec/batch)
2016-02-03 15:39:31.155940: step 44140, loss = 0.89 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 15:39:35.883638: step 44150, loss = 0.71 (264.1 examples/sec; 0.485 sec/batch)
2016-02-03 15:39:40.646120: step 44160, loss = 0.79 (260.8 examples/sec; 0.491 sec/batch)
2016-02-03 15:39:45.494294: step 44170, loss = 0.70 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 15:39:50.202637: step 44180, loss = 0.75 (277.7 examples/sec; 0.461 sec/batch)
2016-02-03 15:39:54.901190: step 44190, loss = 0.61 (286.0 examples/sec; 0.448 sec/batch)
2016-02-03 15:39:59.690652: step 44200, loss = 0.74 (258.8 examples/sec; 0.495 sec/batch)
2016-02-03 15:40:04.923636: step 44210, loss = 0.76 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 15:40:09.578485: step 44220, loss = 0.72 (299.1 examples/sec; 0.428 sec/batch)
2016-02-03 15:40:14.366355: step 44230, loss = 0.59 (285.4 examples/sec; 0.448 sec/batch)
2016-02-03 15:40:19.091454: step 44240, loss = 0.80 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 15:40:23.789546: step 44250, loss = 0.75 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 15:40:28.492445: step 44260, loss = 0.85 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 15:40:33.208736: step 44270, loss = 0.67 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 15:40:37.951613: step 44280, loss = 0.74 (258.5 examples/sec; 0.495 sec/batch)
2016-02-03 15:40:42.645274: step 44290, loss = 0.93 (281.5 examples/sec; 0.455 sec/batch)
2016-02-03 15:40:47.416817: step 44300, loss = 0.75 (260.1 examples/sec; 0.492 sec/batch)
2016-02-03 15:40:52.598249: step 44310, loss = 0.79 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 15:40:57.278336: step 44320, loss = 0.76 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 15:41:02.040979: step 44330, loss = 0.79 (257.0 examples/sec; 0.498 sec/batch)
2016-02-03 15:41:06.789531: step 44340, loss = 0.79 (260.5 examples/sec; 0.491 sec/batch)
2016-02-03 15:41:11.476537: step 44350, loss = 0.69 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 15:41:16.183141: step 44360, loss = 0.74 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 15:41:20.902234: step 44370, loss = 0.90 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 15:41:25.597042: step 44380, loss = 0.78 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 15:41:30.301474: step 44390, loss = 0.74 (254.9 examples/sec; 0.502 sec/batch)
2016-02-03 15:41:34.999128: step 44400, loss = 0.66 (252.5 examples/sec; 0.507 sec/batch)
2016-02-03 15:41:40.205054: step 44410, loss = 1.03 (297.9 examples/sec; 0.430 sec/batch)
2016-02-03 15:41:44.946200: step 44420, loss = 0.87 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 15:41:49.696586: step 44430, loss = 0.73 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 15:41:54.371629: step 44440, loss = 0.77 (297.5 examples/sec; 0.430 sec/batch)
2016-02-03 15:41:59.071161: step 44450, loss = 0.83 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 15:42:03.763102: step 44460, loss = 0.71 (261.3 examples/sec; 0.490 sec/batch)
2016-02-03 15:42:08.478640: step 44470, loss = 0.84 (254.2 examples/sec; 0.504 sec/batch)
2016-02-03 15:42:13.164857: step 44480, loss = 0.82 (255.5 examples/sec; 0.501 sec/batch)
2016-02-03 15:42:17.838485: step 44490, loss = 0.66 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 15:42:22.511933: step 44500, loss = 0.81 (295.2 examples/sec; 0.434 sec/batch)
2016-02-03 15:42:27.755409: step 44510, loss = 0.77 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 15:42:32.523228: step 44520, loss = 0.92 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 15:42:37.150917: step 44530, loss = 0.82 (311.1 examples/sec; 0.411 sec/batch)
2016-02-03 15:42:41.835323: step 44540, loss = 0.77 (257.7 examples/sec; 0.497 sec/batch)
2016-02-03 15:42:46.475407: step 44550, loss = 0.69 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 15:42:51.174647: step 44560, loss = 0.82 (269.2 examples/sec; 0.475 sec/batch)
2016-02-03 15:42:55.802947: step 44570, loss = 0.86 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 15:43:00.529899: step 44580, loss = 0.64 (284.8 examples/sec; 0.449 sec/batch)
2016-02-03 15:43:05.331369: step 44590, loss = 0.69 (256.9 examples/sec; 0.498 sec/batch)
2016-02-03 15:43:10.071941: step 44600, loss = 0.75 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 15:43:15.279725: step 44610, loss = 0.67 (280.5 examples/sec; 0.456 sec/batch)
2016-02-03 15:43:20.029609: step 44620, loss = 0.91 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 15:43:24.780273: step 44630, loss = 0.86 (245.3 examples/sec; 0.522 sec/batch)
2016-02-03 15:43:29.515853: step 44640, loss = 0.85 (256.9 examples/sec; 0.498 sec/batch)
2016-02-03 15:43:34.152441: step 44650, loss = 0.83 (291.2 examples/sec; 0.440 sec/batch)
2016-02-03 15:43:38.921885: step 44660, loss = 0.84 (286.5 examples/sec; 0.447 sec/batch)
2016-02-03 15:43:43.617648: step 44670, loss = 0.69 (292.2 examples/sec; 0.438 sec/batch)
2016-02-03 15:43:48.317932: step 44680, loss = 0.89 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 15:43:52.954307: step 44690, loss = 0.82 (289.7 examples/sec; 0.442 sec/batch)
2016-02-03 15:43:57.576471: step 44700, loss = 0.85 (283.3 examples/sec; 0.452 sec/batch)
2016-02-03 15:44:02.868707: step 44710, loss = 0.63 (263.1 examples/sec; 0.487 sec/batch)
2016-02-03 15:44:07.518474: step 44720, loss = 0.64 (279.2 examples/sec; 0.458 sec/batch)
2016-02-03 15:44:12.210198: step 44730, loss = 0.77 (298.3 examples/sec; 0.429 sec/batch)
2016-02-03 15:44:16.978436: step 44740, loss = 0.79 (289.8 examples/sec; 0.442 sec/batch)
2016-02-03 15:44:21.658098: step 44750, loss = 0.80 (258.4 examples/sec; 0.495 sec/batch)
2016-02-03 15:44:26.306105: step 44760, loss = 0.75 (281.5 examples/sec; 0.455 sec/batch)
2016-02-03 15:44:30.916869: step 44770, loss = 0.80 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 15:44:35.616603: step 44780, loss = 0.65 (281.1 examples/sec; 0.455 sec/batch)
2016-02-03 15:44:40.307853: step 44790, loss = 0.79 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 15:44:45.142717: step 44800, loss = 0.81 (250.2 examples/sec; 0.512 sec/batch)
2016-02-03 15:44:50.230260: step 44810, loss = 0.89 (290.1 examples/sec; 0.441 sec/batch)
2016-02-03 15:44:54.877740: step 44820, loss = 0.83 (295.6 examples/sec; 0.433 sec/batch)
2016-02-03 15:44:59.627112: step 44830, loss = 0.85 (269.3 examples/sec; 0.475 sec/batch)
2016-02-03 15:45:04.419386: step 44840, loss = 0.58 (269.3 examples/sec; 0.475 sec/batch)
2016-02-03 15:45:09.101000: step 44850, loss = 0.78 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 15:45:13.849585: step 44860, loss = 0.74 (264.3 examples/sec; 0.484 sec/batch)
2016-02-03 15:45:18.539765: step 44870, loss = 0.84 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 15:45:23.138238: step 44880, loss = 0.81 (298.6 examples/sec; 0.429 sec/batch)
2016-02-03 15:45:27.902244: step 44890, loss = 0.78 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 15:45:32.637039: step 44900, loss = 0.75 (251.7 examples/sec; 0.508 sec/batch)
2016-02-03 15:45:37.856127: step 44910, loss = 0.70 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 15:45:42.618605: step 44920, loss = 0.84 (275.0 examples/sec; 0.465 sec/batch)
2016-02-03 15:45:47.409487: step 44930, loss = 0.70 (291.1 examples/sec; 0.440 sec/batch)
2016-02-03 15:45:52.262207: step 44940, loss = 0.72 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 15:45:56.972302: step 44950, loss = 0.75 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 15:46:01.711050: step 44960, loss = 0.68 (257.3 examples/sec; 0.498 sec/batch)
2016-02-03 15:46:06.425964: step 44970, loss = 0.80 (252.6 examples/sec; 0.507 sec/batch)
2016-02-03 15:46:11.108472: step 44980, loss = 0.86 (261.9 examples/sec; 0.489 sec/batch)
2016-02-03 15:46:15.769500: step 44990, loss = 0.72 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 15:46:20.479584: step 45000, loss = 0.75 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 15:46:25.632734: step 45010, loss = 0.75 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 15:46:30.407040: step 45020, loss = 0.77 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 15:46:35.064921: step 45030, loss = 0.84 (291.1 examples/sec; 0.440 sec/batch)
2016-02-03 15:46:39.795362: step 45040, loss = 0.63 (263.6 examples/sec; 0.486 sec/batch)
2016-02-03 15:46:44.427926: step 45050, loss = 0.89 (262.1 examples/sec; 0.488 sec/batch)
2016-02-03 15:46:49.126131: step 45060, loss = 0.67 (263.6 examples/sec; 0.486 sec/batch)
2016-02-03 15:46:53.828769: step 45070, loss = 0.74 (282.6 examples/sec; 0.453 sec/batch)
2016-02-03 15:46:58.567084: step 45080, loss = 0.85 (315.5 examples/sec; 0.406 sec/batch)
2016-02-03 15:47:03.354803: step 45090, loss = 0.90 (254.5 examples/sec; 0.503 sec/batch)
2016-02-03 15:47:08.051353: step 45100, loss = 0.72 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 15:47:13.311250: step 45110, loss = 0.82 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 15:47:18.082000: step 45120, loss = 0.66 (250.2 examples/sec; 0.512 sec/batch)
2016-02-03 15:47:22.851926: step 45130, loss = 0.70 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 15:47:27.675676: step 45140, loss = 0.76 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 15:47:32.451405: step 45150, loss = 0.73 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 15:47:37.159796: step 45160, loss = 0.81 (285.9 examples/sec; 0.448 sec/batch)
2016-02-03 15:47:41.935806: step 45170, loss = 0.69 (259.1 examples/sec; 0.494 sec/batch)
2016-02-03 15:47:46.647775: step 45180, loss = 0.71 (290.4 examples/sec; 0.441 sec/batch)
2016-02-03 15:47:51.396385: step 45190, loss = 0.75 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 15:47:56.103728: step 45200, loss = 0.82 (244.7 examples/sec; 0.523 sec/batch)
2016-02-03 15:48:01.341496: step 45210, loss = 0.80 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 15:48:06.026462: step 45220, loss = 0.73 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 15:48:10.685156: step 45230, loss = 0.84 (292.5 examples/sec; 0.438 sec/batch)
2016-02-03 15:48:15.417940: step 45240, loss = 0.82 (252.9 examples/sec; 0.506 sec/batch)
2016-02-03 15:48:20.099325: step 45250, loss = 0.82 (295.6 examples/sec; 0.433 sec/batch)
2016-02-03 15:48:24.800522: step 45260, loss = 0.73 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 15:48:29.518548: step 45270, loss = 0.80 (281.4 examples/sec; 0.455 sec/batch)
2016-02-03 15:48:34.164619: step 45280, loss = 0.64 (284.4 examples/sec; 0.450 sec/batch)
2016-02-03 15:48:38.954648: step 45290, loss = 0.77 (243.1 examples/sec; 0.527 sec/batch)
2016-02-03 15:48:43.629940: step 45300, loss = 0.81 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 15:48:48.932827: step 45310, loss = 0.87 (267.5 examples/sec; 0.478 sec/batch)
2016-02-03 15:48:53.658123: step 45320, loss = 0.78 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 15:48:58.287487: step 45330, loss = 0.77 (292.8 examples/sec; 0.437 sec/batch)
2016-02-03 15:49:03.050551: step 45340, loss = 0.83 (288.2 examples/sec; 0.444 sec/batch)
2016-02-03 15:49:07.728670: step 45350, loss = 0.85 (280.9 examples/sec; 0.456 sec/batch)
2016-02-03 15:49:12.381685: step 45360, loss = 0.65 (275.0 examples/sec; 0.465 sec/batch)
2016-02-03 15:49:17.059656: step 45370, loss = 0.81 (278.5 examples/sec; 0.460 sec/batch)
2016-02-03 15:49:21.751920: step 45380, loss = 0.85 (293.5 examples/sec; 0.436 sec/batch)
2016-02-03 15:49:26.457107: step 45390, loss = 0.77 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 15:49:31.150801: step 45400, loss = 0.74 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 15:49:36.351159: step 45410, loss = 0.66 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 15:49:41.037809: step 45420, loss = 0.74 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 15:49:45.758878: step 45430, loss = 0.78 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 15:49:50.531301: step 45440, loss = 0.80 (272.7 examples/sec; 0.469 sec/batch)
2016-02-03 15:49:55.245320: step 45450, loss = 0.80 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 15:50:00.002273: step 45460, loss = 0.68 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 15:50:04.656247: step 45470, loss = 0.66 (286.4 examples/sec; 0.447 sec/batch)
2016-02-03 15:50:09.381912: step 45480, loss = 0.84 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 15:50:14.133008: step 45490, loss = 0.85 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 15:50:18.873164: step 45500, loss = 0.80 (256.7 examples/sec; 0.499 sec/batch)
2016-02-03 15:50:24.031663: step 45510, loss = 0.80 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 15:50:28.681391: step 45520, loss = 0.71 (269.2 examples/sec; 0.475 sec/batch)
2016-02-03 15:50:33.358233: step 45530, loss = 0.68 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 15:50:38.075581: step 45540, loss = 0.78 (296.6 examples/sec; 0.432 sec/batch)
2016-02-03 15:50:42.820058: step 45550, loss = 0.77 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 15:50:47.454600: step 45560, loss = 0.87 (293.4 examples/sec; 0.436 sec/batch)
2016-02-03 15:50:52.178678: step 45570, loss = 0.79 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 15:50:56.981572: step 45580, loss = 0.72 (241.3 examples/sec; 0.530 sec/batch)
2016-02-03 15:51:01.678176: step 45590, loss = 0.71 (255.1 examples/sec; 0.502 sec/batch)
2016-02-03 15:51:06.348479: step 45600, loss = 0.63 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 15:51:11.550066: step 45610, loss = 0.83 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 15:51:16.243334: step 45620, loss = 0.69 (266.4 examples/sec; 0.481 sec/batch)
2016-02-03 15:51:21.060242: step 45630, loss = 0.63 (290.9 examples/sec; 0.440 sec/batch)
2016-02-03 15:51:25.728451: step 45640, loss = 0.78 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 15:51:30.401498: step 45650, loss = 0.69 (299.8 examples/sec; 0.427 sec/batch)
2016-02-03 15:51:35.071657: step 45660, loss = 0.73 (276.4 examples/sec; 0.463 sec/batch)
2016-02-03 15:51:39.741429: step 45670, loss = 0.79 (283.7 examples/sec; 0.451 sec/batch)
2016-02-03 15:51:44.447387: step 45680, loss = 0.84 (252.1 examples/sec; 0.508 sec/batch)
2016-02-03 15:51:49.217297: step 45690, loss = 0.77 (259.7 examples/sec; 0.493 sec/batch)
2016-02-03 15:51:53.935294: step 45700, loss = 0.79 (257.7 examples/sec; 0.497 sec/batch)
2016-02-03 15:51:59.187801: step 45710, loss = 0.73 (257.8 examples/sec; 0.496 sec/batch)
2016-02-03 15:52:03.883158: step 45720, loss = 0.60 (294.7 examples/sec; 0.434 sec/batch)
2016-02-03 15:52:08.571583: step 45730, loss = 0.66 (273.2 examples/sec; 0.469 sec/batch)
2016-02-03 15:52:13.244955: step 45740, loss = 0.69 (266.9 examples/sec; 0.480 sec/batch)
2016-02-03 15:52:17.861277: step 45750, loss = 0.75 (295.7 examples/sec; 0.433 sec/batch)
2016-02-03 15:52:22.599853: step 45760, loss = 0.58 (278.0 examples/sec; 0.460 sec/batch)
2016-02-03 15:52:27.286817: step 45770, loss = 0.78 (257.4 examples/sec; 0.497 sec/batch)
2016-02-03 15:52:32.035434: step 45780, loss = 0.75 (268.6 examples/sec; 0.477 sec/batch)
2016-02-03 15:52:36.754050: step 45790, loss = 0.70 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 15:52:41.381870: step 45800, loss = 0.78 (286.1 examples/sec; 0.447 sec/batch)
2016-02-03 15:52:46.540478: step 45810, loss = 0.91 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 15:52:51.162205: step 45820, loss = 0.90 (289.7 examples/sec; 0.442 sec/batch)
2016-02-03 15:52:55.895275: step 45830, loss = 0.85 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 15:53:00.605419: step 45840, loss = 0.79 (289.8 examples/sec; 0.442 sec/batch)
2016-02-03 15:53:05.281827: step 45850, loss = 0.84 (293.2 examples/sec; 0.437 sec/batch)
2016-02-03 15:53:09.995777: step 45860, loss = 0.82 (255.4 examples/sec; 0.501 sec/batch)
2016-02-03 15:53:14.647093: step 45870, loss = 0.87 (294.7 examples/sec; 0.434 sec/batch)
2016-02-03 15:53:19.368790: step 45880, loss = 0.81 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 15:53:24.093415: step 45890, loss = 0.64 (264.2 examples/sec; 0.484 sec/batch)
2016-02-03 15:53:28.871508: step 45900, loss = 0.84 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 15:53:34.129857: step 45910, loss = 0.74 (251.2 examples/sec; 0.510 sec/batch)
2016-02-03 15:53:38.774828: step 45920, loss = 0.73 (307.7 examples/sec; 0.416 sec/batch)
2016-02-03 15:53:43.523839: step 45930, loss = 0.87 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 15:53:48.265818: step 45940, loss = 0.90 (257.5 examples/sec; 0.497 sec/batch)
2016-02-03 15:53:52.909472: step 45950, loss = 0.70 (263.6 examples/sec; 0.486 sec/batch)
2016-02-03 15:53:57.680383: step 45960, loss = 0.74 (252.3 examples/sec; 0.507 sec/batch)
2016-02-03 15:54:02.393314: step 45970, loss = 0.61 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 15:54:07.105991: step 45980, loss = 0.73 (259.3 examples/sec; 0.494 sec/batch)
2016-02-03 15:54:11.796088: step 45990, loss = 0.68 (300.2 examples/sec; 0.426 sec/batch)
2016-02-03 15:54:16.541246: step 46000, loss = 0.73 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 15:54:21.695590: step 46010, loss = 0.71 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 15:54:26.334928: step 46020, loss = 0.63 (295.9 examples/sec; 0.433 sec/batch)
2016-02-03 15:54:31.029826: step 46030, loss = 0.85 (264.5 examples/sec; 0.484 sec/batch)
2016-02-03 15:54:35.727531: step 46040, loss = 0.70 (261.0 examples/sec; 0.490 sec/batch)
2016-02-03 15:54:40.409466: step 46050, loss = 0.83 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 15:54:45.118668: step 46060, loss = 0.97 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 15:54:49.871193: step 46070, loss = 0.89 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 15:54:54.628617: step 46080, loss = 0.69 (271.5 examples/sec; 0.472 sec/batch)
2016-02-03 15:54:59.375399: step 46090, loss = 0.69 (266.3 examples/sec; 0.481 sec/batch)
2016-02-03 15:55:04.057691: step 46100, loss = 0.75 (281.1 examples/sec; 0.455 sec/batch)
2016-02-03 15:55:09.280749: step 46110, loss = 0.99 (256.2 examples/sec; 0.500 sec/batch)
2016-02-03 15:55:13.984974: step 46120, loss = 0.60 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 15:55:18.671602: step 46130, loss = 0.70 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 15:55:23.324789: step 46140, loss = 0.72 (264.9 examples/sec; 0.483 sec/batch)
2016-02-03 15:55:27.982289: step 46150, loss = 0.99 (250.3 examples/sec; 0.511 sec/batch)
2016-02-03 15:55:32.619623: step 46160, loss = 0.77 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 15:55:37.368135: step 46170, loss = 0.63 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 15:55:42.073163: step 46180, loss = 0.78 (279.4 examples/sec; 0.458 sec/batch)
2016-02-03 15:55:46.750474: step 46190, loss = 0.65 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 15:55:51.443620: step 46200, loss = 0.82 (285.5 examples/sec; 0.448 sec/batch)
2016-02-03 15:55:56.601948: step 46210, loss = 0.77 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 15:56:01.230277: step 46220, loss = 0.77 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 15:56:05.938679: step 46230, loss = 0.69 (268.0 examples/sec; 0.478 sec/batch)
2016-02-03 15:56:10.599736: step 46240, loss = 0.94 (290.3 examples/sec; 0.441 sec/batch)
2016-02-03 15:56:15.250299: step 46250, loss = 0.74 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 15:56:19.902131: step 46260, loss = 0.76 (284.7 examples/sec; 0.450 sec/batch)
2016-02-03 15:56:24.630260: step 46270, loss = 0.75 (258.8 examples/sec; 0.495 sec/batch)
2016-02-03 15:56:29.429761: step 46280, loss = 0.83 (254.4 examples/sec; 0.503 sec/batch)
2016-02-03 15:56:34.143588: step 46290, loss = 0.74 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 15:56:38.793714: step 46300, loss = 0.65 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 15:56:43.975288: step 46310, loss = 0.75 (284.4 examples/sec; 0.450 sec/batch)
2016-02-03 15:56:48.634298: step 46320, loss = 0.79 (281.2 examples/sec; 0.455 sec/batch)
2016-02-03 15:56:53.398890: step 46330, loss = 0.68 (267.3 examples/sec; 0.479 sec/batch)
2016-02-03 15:56:58.095813: step 46340, loss = 0.79 (283.6 examples/sec; 0.451 sec/batch)
2016-02-03 15:57:02.845157: step 46350, loss = 0.77 (262.0 examples/sec; 0.489 sec/batch)
2016-02-03 15:57:07.523298: step 46360, loss = 0.80 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 15:57:12.380093: step 46370, loss = 0.65 (244.8 examples/sec; 0.523 sec/batch)
2016-02-03 15:57:17.168995: step 46380, loss = 0.83 (254.5 examples/sec; 0.503 sec/batch)
2016-02-03 15:57:21.825458: step 46390, loss = 0.71 (255.6 examples/sec; 0.501 sec/batch)
2016-02-03 15:57:26.524620: step 46400, loss = 0.84 (298.4 examples/sec; 0.429 sec/batch)
2016-02-03 15:57:31.720839: step 46410, loss = 0.60 (283.5 examples/sec; 0.452 sec/batch)
2016-02-03 15:57:36.437433: step 46420, loss = 0.75 (266.4 examples/sec; 0.481 sec/batch)
2016-02-03 15:57:41.073694: step 46430, loss = 0.75 (267.5 examples/sec; 0.479 sec/batch)
2016-02-03 15:57:45.798235: step 46440, loss = 0.72 (245.2 examples/sec; 0.522 sec/batch)
2016-02-03 15:57:50.530613: step 46450, loss = 0.82 (299.3 examples/sec; 0.428 sec/batch)
2016-02-03 15:57:55.244225: step 46460, loss = 1.09 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 15:57:59.944220: step 46470, loss = 0.67 (285.2 examples/sec; 0.449 sec/batch)
2016-02-03 15:58:04.716566: step 46480, loss = 0.85 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 15:58:09.373916: step 46490, loss = 0.71 (294.6 examples/sec; 0.435 sec/batch)
2016-02-03 15:58:14.072390: step 46500, loss = 0.84 (256.9 examples/sec; 0.498 sec/batch)
2016-02-03 15:58:19.262739: step 46510, loss = 0.77 (253.6 examples/sec; 0.505 sec/batch)
2016-02-03 15:58:23.943922: step 46520, loss = 0.83 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 15:58:28.710535: step 46530, loss = 0.82 (251.5 examples/sec; 0.509 sec/batch)
2016-02-03 15:58:33.441531: step 46540, loss = 0.76 (306.0 examples/sec; 0.418 sec/batch)
2016-02-03 15:58:38.153164: step 46550, loss = 0.77 (277.6 examples/sec; 0.461 sec/batch)
2016-02-03 15:58:42.807785: step 46560, loss = 0.75 (282.2 examples/sec; 0.454 sec/batch)
2016-02-03 15:58:47.537200: step 46570, loss = 0.68 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 15:58:52.274629: step 46580, loss = 0.64 (246.2 examples/sec; 0.520 sec/batch)
2016-02-03 15:58:57.004503: step 46590, loss = 0.76 (279.7 examples/sec; 0.458 sec/batch)
2016-02-03 15:59:01.762519: step 46600, loss = 0.67 (277.7 examples/sec; 0.461 sec/batch)
2016-02-03 15:59:06.915699: step 46610, loss = 0.65 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 15:59:11.556735: step 46620, loss = 0.69 (283.6 examples/sec; 0.451 sec/batch)
2016-02-03 15:59:16.284454: step 46630, loss = 0.74 (286.0 examples/sec; 0.448 sec/batch)
2016-02-03 15:59:21.067242: step 46640, loss = 0.79 (242.6 examples/sec; 0.528 sec/batch)
2016-02-03 15:59:25.720770: step 46650, loss = 0.81 (265.3 examples/sec; 0.483 sec/batch)
2016-02-03 15:59:30.465530: step 46660, loss = 0.72 (256.6 examples/sec; 0.499 sec/batch)
2016-02-03 15:59:35.223895: step 46670, loss = 0.70 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 15:59:39.936072: step 46680, loss = 0.70 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 15:59:44.625170: step 46690, loss = 0.66 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 15:59:49.294402: step 46700, loss = 0.80 (287.5 examples/sec; 0.445 sec/batch)
2016-02-03 15:59:54.564889: step 46710, loss = 0.73 (277.8 examples/sec; 0.461 sec/batch)
2016-02-03 15:59:59.232088: step 46720, loss = 0.69 (259.0 examples/sec; 0.494 sec/batch)
2016-02-03 16:00:03.894668: step 46730, loss = 0.63 (263.1 examples/sec; 0.487 sec/batch)
2016-02-03 16:00:08.587654: step 46740, loss = 0.74 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 16:00:13.372191: step 46750, loss = 0.69 (259.8 examples/sec; 0.493 sec/batch)
2016-02-03 16:00:18.086866: step 46760, loss = 0.63 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 16:00:22.795828: step 46770, loss = 0.78 (257.9 examples/sec; 0.496 sec/batch)
2016-02-03 16:00:27.514475: step 46780, loss = 0.83 (249.6 examples/sec; 0.513 sec/batch)
2016-02-03 16:00:32.161486: step 46790, loss = 0.73 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 16:00:36.860175: step 46800, loss = 0.84 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 16:00:42.131351: step 46810, loss = 0.69 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 16:00:46.822815: step 46820, loss = 0.76 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 16:00:51.525641: step 46830, loss = 0.86 (291.5 examples/sec; 0.439 sec/batch)
2016-02-03 16:00:56.157693: step 46840, loss = 0.86 (290.1 examples/sec; 0.441 sec/batch)
2016-02-03 16:01:00.929451: step 46850, loss = 0.63 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 16:01:05.545578: step 46860, loss = 0.97 (302.7 examples/sec; 0.423 sec/batch)
2016-02-03 16:01:10.166150: step 46870, loss = 0.75 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 16:01:14.891858: step 46880, loss = 0.75 (270.5 examples/sec; 0.473 sec/batch)
2016-02-03 16:01:19.541511: step 46890, loss = 0.61 (296.9 examples/sec; 0.431 sec/batch)
2016-02-03 16:01:24.272280: step 46900, loss = 0.63 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 16:01:29.542494: step 46910, loss = 0.71 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 16:01:34.327500: step 46920, loss = 0.87 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 16:01:39.025718: step 46930, loss = 0.71 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 16:01:43.759843: step 46940, loss = 0.76 (265.0 examples/sec; 0.483 sec/batch)
2016-02-03 16:01:48.502022: step 46950, loss = 0.68 (256.5 examples/sec; 0.499 sec/batch)
2016-02-03 16:01:53.209307: step 46960, loss = 0.80 (258.9 examples/sec; 0.494 sec/batch)
2016-02-03 16:01:57.939637: step 46970, loss = 0.80 (273.2 examples/sec; 0.469 sec/batch)
2016-02-03 16:02:02.685483: step 46980, loss = 0.82 (287.3 examples/sec; 0.446 sec/batch)
2016-02-03 16:02:07.364580: step 46990, loss = 0.72 (293.1 examples/sec; 0.437 sec/batch)
2016-02-03 16:02:12.064128: step 47000, loss = 0.85 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 16:02:17.282198: step 47010, loss = 0.72 (249.4 examples/sec; 0.513 sec/batch)
2016-02-03 16:02:21.961798: step 47020, loss = 0.80 (280.1 examples/sec; 0.457 sec/batch)
2016-02-03 16:02:26.742116: step 47030, loss = 0.93 (250.6 examples/sec; 0.511 sec/batch)
2016-02-03 16:02:31.517014: step 47040, loss = 0.72 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 16:02:36.233281: step 47050, loss = 0.70 (257.8 examples/sec; 0.497 sec/batch)
2016-02-03 16:02:40.807831: step 47060, loss = 0.71 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 16:02:45.527780: step 47070, loss = 0.63 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 16:02:50.266650: step 47080, loss = 0.90 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 16:02:54.931293: step 47090, loss = 0.83 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 16:02:59.687822: step 47100, loss = 0.84 (281.0 examples/sec; 0.455 sec/batch)
2016-02-03 16:03:04.886460: step 47110, loss = 0.82 (284.2 examples/sec; 0.450 sec/batch)
2016-02-03 16:03:09.650272: step 47120, loss = 0.87 (259.4 examples/sec; 0.493 sec/batch)
2016-02-03 16:03:14.357614: step 47130, loss = 0.81 (288.7 examples/sec; 0.443 sec/batch)
2016-02-03 16:03:19.034440: step 47140, loss = 0.88 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 16:03:23.789176: step 47150, loss = 0.74 (250.7 examples/sec; 0.511 sec/batch)
2016-02-03 16:03:28.427441: step 47160, loss = 0.82 (278.5 examples/sec; 0.460 sec/batch)
2016-02-03 16:03:33.088161: step 47170, loss = 0.77 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 16:03:37.777073: step 47180, loss = 0.78 (292.1 examples/sec; 0.438 sec/batch)
2016-02-03 16:03:42.529716: step 47190, loss = 0.70 (290.1 examples/sec; 0.441 sec/batch)
2016-02-03 16:03:47.280953: step 47200, loss = 0.67 (277.9 examples/sec; 0.461 sec/batch)
2016-02-03 16:03:52.458867: step 47210, loss = 0.73 (265.3 examples/sec; 0.482 sec/batch)
2016-02-03 16:03:57.198723: step 47220, loss = 0.66 (253.2 examples/sec; 0.506 sec/batch)
2016-02-03 16:04:01.876735: step 47230, loss = 0.57 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 16:04:06.497056: step 47240, loss = 0.90 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 16:04:11.124858: step 47250, loss = 0.95 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 16:04:15.867616: step 47260, loss = 0.84 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 16:04:20.584222: step 47270, loss = 0.77 (265.8 examples/sec; 0.482 sec/batch)
2016-02-03 16:04:25.296300: step 47280, loss = 0.62 (260.1 examples/sec; 0.492 sec/batch)
2016-02-03 16:04:29.989970: step 47290, loss = 0.81 (262.2 examples/sec; 0.488 sec/batch)
2016-02-03 16:04:34.655867: step 47300, loss = 0.83 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 16:04:39.851768: step 47310, loss = 0.76 (265.3 examples/sec; 0.483 sec/batch)
2016-02-03 16:04:44.556954: step 47320, loss = 0.66 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 16:04:49.292216: step 47330, loss = 0.83 (257.5 examples/sec; 0.497 sec/batch)
2016-02-03 16:04:53.980508: step 47340, loss = 0.57 (277.8 examples/sec; 0.461 sec/batch)
2016-02-03 16:04:58.766290: step 47350, loss = 0.72 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 16:05:03.537578: step 47360, loss = 0.72 (245.8 examples/sec; 0.521 sec/batch)
2016-02-03 16:05:08.334646: step 47370, loss = 0.71 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 16:05:13.079328: step 47380, loss = 0.91 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 16:05:17.780633: step 47390, loss = 0.81 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 16:05:22.428180: step 47400, loss = 0.74 (306.5 examples/sec; 0.418 sec/batch)
2016-02-03 16:05:27.715568: step 47410, loss = 0.77 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 16:05:32.494527: step 47420, loss = 0.69 (256.7 examples/sec; 0.499 sec/batch)
2016-02-03 16:05:37.168233: step 47430, loss = 1.01 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 16:05:41.825850: step 47440, loss = 0.86 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 16:05:46.433689: step 47450, loss = 0.78 (300.8 examples/sec; 0.426 sec/batch)
2016-02-03 16:05:51.134577: step 47460, loss = 0.72 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 16:05:55.879873: step 47470, loss = 0.85 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 16:06:00.540317: step 47480, loss = 0.71 (247.5 examples/sec; 0.517 sec/batch)
2016-02-03 16:06:05.182121: step 47490, loss = 0.90 (282.9 examples/sec; 0.453 sec/batch)
2016-02-03 16:06:09.842054: step 47500, loss = 0.77 (286.4 examples/sec; 0.447 sec/batch)
2016-02-03 16:06:14.977856: step 47510, loss = 0.73 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 16:06:19.564137: step 47520, loss = 0.72 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 16:06:24.215043: step 47530, loss = 0.87 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 16:06:28.904845: step 47540, loss = 0.74 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 16:06:33.539803: step 47550, loss = 0.70 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 16:06:38.191791: step 47560, loss = 0.81 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 16:06:42.848393: step 47570, loss = 0.65 (252.8 examples/sec; 0.506 sec/batch)
2016-02-03 16:06:47.516014: step 47580, loss = 0.66 (256.7 examples/sec; 0.499 sec/batch)
2016-02-03 16:06:52.312083: step 47590, loss = 0.77 (243.3 examples/sec; 0.526 sec/batch)
2016-02-03 16:06:57.053565: step 47600, loss = 0.72 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 16:07:02.238869: step 47610, loss = 0.80 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 16:07:06.947046: step 47620, loss = 0.68 (280.9 examples/sec; 0.456 sec/batch)
2016-02-03 16:07:11.687697: step 47630, loss = 0.76 (262.2 examples/sec; 0.488 sec/batch)
2016-02-03 16:07:16.331681: step 47640, loss = 0.65 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 16:07:21.011909: step 47650, loss = 0.81 (292.9 examples/sec; 0.437 sec/batch)
2016-02-03 16:07:25.721919: step 47660, loss = 0.72 (284.2 examples/sec; 0.450 sec/batch)
2016-02-03 16:07:30.411488: step 47670, loss = 0.77 (290.3 examples/sec; 0.441 sec/batch)
2016-02-03 16:07:35.091632: step 47680, loss = 0.69 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 16:07:39.817446: step 47690, loss = 0.80 (256.0 examples/sec; 0.500 sec/batch)
2016-02-03 16:07:44.519745: step 47700, loss = 0.92 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 16:07:49.773565: step 47710, loss = 0.62 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 16:07:54.475116: step 47720, loss = 0.85 (256.5 examples/sec; 0.499 sec/batch)
2016-02-03 16:07:59.091318: step 47730, loss = 0.71 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 16:08:03.754318: step 47740, loss = 0.78 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 16:08:08.499824: step 47750, loss = 0.69 (243.6 examples/sec; 0.526 sec/batch)
2016-02-03 16:08:13.191511: step 47760, loss = 0.83 (262.2 examples/sec; 0.488 sec/batch)
2016-02-03 16:08:17.841730: step 47770, loss = 0.82 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 16:08:22.514334: step 47780, loss = 0.76 (280.4 examples/sec; 0.457 sec/batch)
2016-02-03 16:08:27.190790: step 47790, loss = 0.58 (305.7 examples/sec; 0.419 sec/batch)
2016-02-03 16:08:31.934040: step 47800, loss = 0.78 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 16:08:37.082181: step 47810, loss = 0.70 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 16:08:41.705980: step 47820, loss = 0.82 (267.5 examples/sec; 0.479 sec/batch)
2016-02-03 16:08:46.424028: step 47830, loss = 0.87 (282.6 examples/sec; 0.453 sec/batch)
2016-02-03 16:08:51.071276: step 47840, loss = 0.73 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 16:08:55.748833: step 47850, loss = 0.81 (289.9 examples/sec; 0.442 sec/batch)
2016-02-03 16:09:00.488235: step 47860, loss = 0.74 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 16:09:05.211821: step 47870, loss = 0.85 (288.4 examples/sec; 0.444 sec/batch)
2016-02-03 16:09:09.886998: step 47880, loss = 0.70 (265.2 examples/sec; 0.483 sec/batch)
2016-02-03 16:09:14.594537: step 47890, loss = 0.73 (298.4 examples/sec; 0.429 sec/batch)
2016-02-03 16:09:19.253894: step 47900, loss = 0.78 (277.9 examples/sec; 0.461 sec/batch)
2016-02-03 16:09:24.468190: step 47910, loss = 0.73 (264.2 examples/sec; 0.485 sec/batch)
2016-02-03 16:09:29.147484: step 47920, loss = 0.89 (282.5 examples/sec; 0.453 sec/batch)
2016-02-03 16:09:33.842026: step 47930, loss = 0.59 (270.9 examples/sec; 0.473 sec/batch)
2016-02-03 16:09:38.445044: step 47940, loss = 0.70 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 16:09:43.113328: step 47950, loss = 0.75 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 16:09:47.838963: step 47960, loss = 0.84 (253.3 examples/sec; 0.505 sec/batch)
2016-02-03 16:09:52.519920: step 47970, loss = 0.71 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 16:09:57.312889: step 47980, loss = 0.92 (251.3 examples/sec; 0.509 sec/batch)
2016-02-03 16:10:01.997461: step 47990, loss = 0.99 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 16:10:06.672626: step 48000, loss = 0.82 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 16:10:11.858968: step 48010, loss = 0.91 (258.4 examples/sec; 0.495 sec/batch)
2016-02-03 16:10:16.566717: step 48020, loss = 0.71 (286.3 examples/sec; 0.447 sec/batch)
2016-02-03 16:10:21.304570: step 48030, loss = 0.69 (288.4 examples/sec; 0.444 sec/batch)
2016-02-03 16:10:25.954938: step 48040, loss = 0.69 (324.5 examples/sec; 0.394 sec/batch)
2016-02-03 16:10:30.729089: step 48050, loss = 0.79 (282.2 examples/sec; 0.454 sec/batch)
2016-02-03 16:10:35.463509: step 48060, loss = 0.66 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 16:10:40.155862: step 48070, loss = 0.73 (247.2 examples/sec; 0.518 sec/batch)
2016-02-03 16:10:44.856702: step 48080, loss = 0.72 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 16:10:49.599655: step 48090, loss = 0.82 (253.0 examples/sec; 0.506 sec/batch)
2016-02-03 16:10:54.228607: step 48100, loss = 0.76 (270.5 examples/sec; 0.473 sec/batch)
2016-02-03 16:10:59.323463: step 48110, loss = 0.70 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 16:11:04.093297: step 48120, loss = 0.84 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 16:11:08.857792: step 48130, loss = 0.74 (306.9 examples/sec; 0.417 sec/batch)
2016-02-03 16:11:13.662619: step 48140, loss = 0.72 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 16:11:18.397319: step 48150, loss = 0.74 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 16:11:23.135745: step 48160, loss = 0.78 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 16:11:27.797120: step 48170, loss = 0.79 (304.2 examples/sec; 0.421 sec/batch)
2016-02-03 16:11:32.590378: step 48180, loss = 0.73 (268.0 examples/sec; 0.478 sec/batch)
2016-02-03 16:11:37.290789: step 48190, loss = 0.63 (279.6 examples/sec; 0.458 sec/batch)
2016-02-03 16:11:41.960785: step 48200, loss = 0.72 (282.2 examples/sec; 0.454 sec/batch)
2016-02-03 16:11:47.211119: step 48210, loss = 0.95 (247.6 examples/sec; 0.517 sec/batch)
2016-02-03 16:11:51.938469: step 48220, loss = 0.86 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 16:11:56.681147: step 48230, loss = 0.91 (253.2 examples/sec; 0.506 sec/batch)
2016-02-03 16:12:01.337253: step 48240, loss = 0.72 (307.3 examples/sec; 0.417 sec/batch)
2016-02-03 16:12:06.010087: step 48250, loss = 0.87 (270.9 examples/sec; 0.473 sec/batch)
2016-02-03 16:12:10.626119: step 48260, loss = 0.73 (288.5 examples/sec; 0.444 sec/batch)
2016-02-03 16:12:15.284175: step 48270, loss = 0.77 (295.6 examples/sec; 0.433 sec/batch)
2016-02-03 16:12:20.011267: step 48280, loss = 0.86 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 16:12:24.728355: step 48290, loss = 0.61 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 16:12:29.440457: step 48300, loss = 0.81 (289.2 examples/sec; 0.443 sec/batch)
2016-02-03 16:12:34.624960: step 48310, loss = 0.70 (294.2 examples/sec; 0.435 sec/batch)
2016-02-03 16:12:39.361158: step 48320, loss = 0.72 (259.0 examples/sec; 0.494 sec/batch)
2016-02-03 16:12:44.048650: step 48330, loss = 0.88 (294.3 examples/sec; 0.435 sec/batch)
2016-02-03 16:12:48.826084: step 48340, loss = 0.68 (256.0 examples/sec; 0.500 sec/batch)
2016-02-03 16:12:53.531484: step 48350, loss = 0.63 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 16:12:58.221336: step 48360, loss = 0.85 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 16:13:02.996913: step 48370, loss = 0.63 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 16:13:07.760582: step 48380, loss = 0.89 (275.7 examples/sec; 0.464 sec/batch)
2016-02-03 16:13:12.521530: step 48390, loss = 0.84 (242.7 examples/sec; 0.527 sec/batch)
2016-02-03 16:13:17.154360: step 48400, loss = 0.74 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 16:13:22.349267: step 48410, loss = 0.74 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 16:13:26.999244: step 48420, loss = 0.83 (283.3 examples/sec; 0.452 sec/batch)
2016-02-03 16:13:31.676190: step 48430, loss = 0.79 (246.3 examples/sec; 0.520 sec/batch)
2016-02-03 16:13:36.345637: step 48440, loss = 0.71 (284.2 examples/sec; 0.450 sec/batch)
2016-02-03 16:13:41.132292: step 48450, loss = 0.81 (254.7 examples/sec; 0.503 sec/batch)
2016-02-03 16:13:45.830111: step 48460, loss = 0.74 (276.3 examples/sec; 0.463 sec/batch)
2016-02-03 16:13:50.524153: step 48470, loss = 0.75 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 16:13:55.262139: step 48480, loss = 0.88 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 16:13:59.924788: step 48490, loss = 0.83 (290.3 examples/sec; 0.441 sec/batch)
2016-02-03 16:14:04.662793: step 48500, loss = 0.76 (261.7 examples/sec; 0.489 sec/batch)
2016-02-03 16:14:09.876595: step 48510, loss = 0.70 (295.7 examples/sec; 0.433 sec/batch)
2016-02-03 16:14:14.512732: step 48520, loss = 0.77 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 16:14:19.273510: step 48530, loss = 0.79 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 16:14:23.996293: step 48540, loss = 0.72 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 16:14:28.649669: step 48550, loss = 0.70 (293.3 examples/sec; 0.436 sec/batch)
2016-02-03 16:14:33.423317: step 48560, loss = 0.69 (258.1 examples/sec; 0.496 sec/batch)
2016-02-03 16:14:38.117872: step 48570, loss = 0.69 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 16:14:42.670321: step 48580, loss = 0.82 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 16:14:47.297337: step 48590, loss = 0.91 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 16:14:51.920299: step 48600, loss = 0.69 (295.4 examples/sec; 0.433 sec/batch)
2016-02-03 16:14:56.990779: step 48610, loss = 0.82 (286.7 examples/sec; 0.446 sec/batch)
2016-02-03 16:15:01.556542: step 48620, loss = 0.75 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 16:15:06.173105: step 48630, loss = 0.79 (287.5 examples/sec; 0.445 sec/batch)
2016-02-03 16:15:10.869311: step 48640, loss = 0.72 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 16:15:15.465018: step 48650, loss = 0.63 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 16:15:20.109842: step 48660, loss = 0.72 (255.8 examples/sec; 0.500 sec/batch)
2016-02-03 16:15:24.619871: step 48670, loss = 0.80 (292.5 examples/sec; 0.438 sec/batch)
2016-02-03 16:15:29.234733: step 48680, loss = 0.90 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 16:15:33.947891: step 48690, loss = 0.63 (267.5 examples/sec; 0.478 sec/batch)
2016-02-03 16:15:38.481110: step 48700, loss = 0.76 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 16:15:43.656258: step 48710, loss = 0.72 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 16:15:48.403561: step 48720, loss = 0.64 (259.2 examples/sec; 0.494 sec/batch)
2016-02-03 16:15:53.012924: step 48730, loss = 0.82 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 16:15:57.676865: step 48740, loss = 0.84 (284.4 examples/sec; 0.450 sec/batch)
2016-02-03 16:16:02.366895: step 48750, loss = 0.71 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 16:16:07.002003: step 48760, loss = 0.78 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 16:16:11.711764: step 48770, loss = 0.85 (287.3 examples/sec; 0.446 sec/batch)
2016-02-03 16:16:16.422068: step 48780, loss = 0.68 (288.7 examples/sec; 0.443 sec/batch)
2016-02-03 16:16:21.089183: step 48790, loss = 0.66 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 16:16:25.735499: step 48800, loss = 0.61 (267.5 examples/sec; 0.479 sec/batch)
2016-02-03 16:16:30.900580: step 48810, loss = 0.76 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 16:16:35.737425: step 48820, loss = 0.91 (258.6 examples/sec; 0.495 sec/batch)
2016-02-03 16:16:40.469869: step 48830, loss = 0.76 (253.0 examples/sec; 0.506 sec/batch)
2016-02-03 16:16:45.246730: step 48840, loss = 0.69 (289.4 examples/sec; 0.442 sec/batch)
2016-02-03 16:16:49.999829: step 48850, loss = 0.70 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 16:16:54.737751: step 48860, loss = 0.69 (281.0 examples/sec; 0.455 sec/batch)
2016-02-03 16:16:59.522661: step 48870, loss = 0.77 (258.0 examples/sec; 0.496 sec/batch)
2016-02-03 16:17:04.191940: step 48880, loss = 0.99 (265.3 examples/sec; 0.482 sec/batch)
2016-02-03 16:17:08.917098: step 48890, loss = 0.71 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 16:17:13.668284: step 48900, loss = 0.69 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 16:17:18.866845: step 48910, loss = 0.64 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 16:17:23.626420: step 48920, loss = 0.68 (264.3 examples/sec; 0.484 sec/batch)
2016-02-03 16:17:28.369891: step 48930, loss = 0.71 (282.4 examples/sec; 0.453 sec/batch)
2016-02-03 16:17:33.102328: step 48940, loss = 0.81 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 16:17:37.777198: step 48950, loss = 0.77 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 16:17:42.541779: step 48960, loss = 0.67 (285.3 examples/sec; 0.449 sec/batch)
2016-02-03 16:17:47.344008: step 48970, loss = 0.86 (268.0 examples/sec; 0.478 sec/batch)
2016-02-03 16:17:52.113095: step 48980, loss = 0.95 (269.8 examples/sec; 0.475 sec/batch)
2016-02-03 16:17:56.887765: step 48990, loss = 0.68 (280.1 examples/sec; 0.457 sec/batch)
2016-02-03 16:18:01.636907: step 49000, loss = 0.80 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 16:18:06.867224: step 49010, loss = 0.86 (258.3 examples/sec; 0.495 sec/batch)
2016-02-03 16:18:11.664614: step 49020, loss = 0.80 (231.6 examples/sec; 0.553 sec/batch)
2016-02-03 16:18:16.364201: step 49030, loss = 0.76 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 16:18:20.993912: step 49040, loss = 0.76 (302.4 examples/sec; 0.423 sec/batch)
2016-02-03 16:18:25.811246: step 49050, loss = 0.82 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 16:18:30.561570: step 49060, loss = 0.66 (286.2 examples/sec; 0.447 sec/batch)
2016-02-03 16:18:35.240785: step 49070, loss = 0.81 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 16:18:39.935494: step 49080, loss = 0.74 (282.6 examples/sec; 0.453 sec/batch)
2016-02-03 16:18:44.596375: step 49090, loss = 0.75 (300.7 examples/sec; 0.426 sec/batch)
2016-02-03 16:18:49.340779: step 49100, loss = 0.72 (278.0 examples/sec; 0.460 sec/batch)
2016-02-03 16:18:54.576877: step 49110, loss = 0.84 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 16:18:59.250253: step 49120, loss = 0.72 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 16:19:03.878886: step 49130, loss = 0.95 (292.6 examples/sec; 0.437 sec/batch)
2016-02-03 16:19:08.470931: step 49140, loss = 0.84 (285.6 examples/sec; 0.448 sec/batch)
2016-02-03 16:19:13.161137: step 49150, loss = 0.68 (293.0 examples/sec; 0.437 sec/batch)
2016-02-03 16:19:17.794029: step 49160, loss = 0.78 (272.6 examples/sec; 0.469 sec/batch)
2016-02-03 16:19:22.598212: step 49170, loss = 0.66 (252.9 examples/sec; 0.506 sec/batch)
2016-02-03 16:19:27.297181: step 49180, loss = 0.77 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 16:19:32.060069: step 49190, loss = 0.69 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 16:19:36.725344: step 49200, loss = 0.83 (291.4 examples/sec; 0.439 sec/batch)
2016-02-03 16:19:41.931517: step 49210, loss = 0.86 (245.4 examples/sec; 0.522 sec/batch)
2016-02-03 16:19:46.567395: step 49220, loss = 0.69 (255.5 examples/sec; 0.501 sec/batch)
2016-02-03 16:19:51.180903: step 49230, loss = 0.82 (279.6 examples/sec; 0.458 sec/batch)
2016-02-03 16:19:55.837566: step 49240, loss = 0.77 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 16:20:00.619436: step 49250, loss = 0.90 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 16:20:05.236768: step 49260, loss = 0.72 (254.5 examples/sec; 0.503 sec/batch)
2016-02-03 16:20:09.843958: step 49270, loss = 0.78 (262.1 examples/sec; 0.488 sec/batch)
2016-02-03 16:20:14.493360: step 49280, loss = 0.76 (283.5 examples/sec; 0.451 sec/batch)
2016-02-03 16:20:19.158331: step 49290, loss = 0.56 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 16:20:23.798651: step 49300, loss = 0.78 (303.5 examples/sec; 0.422 sec/batch)
2016-02-03 16:20:29.019741: step 49310, loss = 0.77 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 16:20:33.758264: step 49320, loss = 0.77 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 16:20:38.446921: step 49330, loss = 0.67 (276.7 examples/sec; 0.463 sec/batch)
2016-02-03 16:20:43.096708: step 49340, loss = 0.73 (286.6 examples/sec; 0.447 sec/batch)
2016-02-03 16:20:47.832630: step 49350, loss = 0.81 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 16:20:52.477892: step 49360, loss = 0.78 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 16:20:57.189464: step 49370, loss = 0.75 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 16:21:01.917412: step 49380, loss = 0.86 (264.7 examples/sec; 0.484 sec/batch)
2016-02-03 16:21:06.611134: step 49390, loss = 0.71 (249.2 examples/sec; 0.514 sec/batch)
2016-02-03 16:21:11.271632: step 49400, loss = 0.63 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 16:21:16.476606: step 49410, loss = 0.66 (288.5 examples/sec; 0.444 sec/batch)
2016-02-03 16:21:21.198222: step 49420, loss = 0.74 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 16:21:25.986261: step 49430, loss = 0.69 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 16:21:30.726001: step 49440, loss = 0.82 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 16:21:35.357360: step 49450, loss = 0.60 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 16:21:40.057952: step 49460, loss = 0.67 (280.1 examples/sec; 0.457 sec/batch)
2016-02-03 16:21:44.729782: step 49470, loss = 0.58 (282.5 examples/sec; 0.453 sec/batch)
2016-02-03 16:21:49.427315: step 49480, loss = 0.62 (288.6 examples/sec; 0.443 sec/batch)
2016-02-03 16:21:54.130688: step 49490, loss = 0.89 (266.3 examples/sec; 0.481 sec/batch)
2016-02-03 16:21:58.743368: step 49500, loss = 0.86 (255.4 examples/sec; 0.501 sec/batch)
2016-02-03 16:22:03.888267: step 49510, loss = 0.77 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 16:22:08.623162: step 49520, loss = 0.78 (282.5 examples/sec; 0.453 sec/batch)
2016-02-03 16:22:13.383237: step 49530, loss = 0.76 (253.6 examples/sec; 0.505 sec/batch)
2016-02-03 16:22:18.072201: step 49540, loss = 0.80 (264.7 examples/sec; 0.484 sec/batch)
2016-02-03 16:22:22.777950: step 49550, loss = 0.85 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 16:22:27.466176: step 49560, loss = 0.79 (255.1 examples/sec; 0.502 sec/batch)
2016-02-03 16:22:32.281266: step 49570, loss = 0.67 (251.9 examples/sec; 0.508 sec/batch)
2016-02-03 16:22:36.931752: step 49580, loss = 0.85 (263.2 examples/sec; 0.486 sec/batch)
2016-02-03 16:22:41.622736: step 49590, loss = 0.76 (251.4 examples/sec; 0.509 sec/batch)
2016-02-03 16:22:46.301841: step 49600, loss = 0.72 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 16:22:51.637144: step 49610, loss = 0.69 (266.4 examples/sec; 0.480 sec/batch)
2016-02-03 16:22:56.329555: step 49620, loss = 0.68 (250.8 examples/sec; 0.510 sec/batch)
2016-02-03 16:23:01.019021: step 49630, loss = 0.76 (279.2 examples/sec; 0.458 sec/batch)
2016-02-03 16:23:05.691573: step 49640, loss = 0.80 (287.9 examples/sec; 0.445 sec/batch)
2016-02-03 16:23:10.423925: step 49650, loss = 0.65 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 16:23:15.097747: step 49660, loss = 0.70 (277.8 examples/sec; 0.461 sec/batch)
2016-02-03 16:23:19.779907: step 49670, loss = 0.70 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 16:23:24.540907: step 49680, loss = 0.64 (282.9 examples/sec; 0.452 sec/batch)
2016-02-03 16:23:29.279318: step 49690, loss = 0.78 (297.1 examples/sec; 0.431 sec/batch)
2016-02-03 16:23:34.119624: step 49700, loss = 0.81 (248.7 examples/sec; 0.515 sec/batch)
2016-02-03 16:23:39.282168: step 49710, loss = 0.73 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 16:23:44.003529: step 49720, loss = 0.64 (285.0 examples/sec; 0.449 sec/batch)
2016-02-03 16:23:48.734110: step 49730, loss = 0.61 (249.5 examples/sec; 0.513 sec/batch)
2016-02-03 16:23:53.381400: step 49740, loss = 0.84 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 16:23:58.074121: step 49750, loss = 0.66 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 16:24:02.686587: step 49760, loss = 0.76 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 16:24:07.382362: step 49770, loss = 0.69 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 16:24:12.071757: step 49780, loss = 0.80 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 16:24:16.736172: step 49790, loss = 0.77 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 16:24:21.401155: step 49800, loss = 0.65 (259.3 examples/sec; 0.494 sec/batch)
2016-02-03 16:24:26.632492: step 49810, loss = 0.71 (270.9 examples/sec; 0.472 sec/batch)
2016-02-03 16:24:31.336277: step 49820, loss = 0.73 (302.0 examples/sec; 0.424 sec/batch)
2016-02-03 16:24:36.095272: step 49830, loss = 0.87 (249.0 examples/sec; 0.514 sec/batch)
2016-02-03 16:24:40.808227: step 49840, loss = 0.62 (278.0 examples/sec; 0.460 sec/batch)
2016-02-03 16:24:45.509637: step 49850, loss = 0.66 (289.7 examples/sec; 0.442 sec/batch)
2016-02-03 16:24:50.225502: step 49860, loss = 0.79 (254.7 examples/sec; 0.503 sec/batch)
2016-02-03 16:24:54.910426: step 49870, loss = 0.68 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 16:24:59.685515: step 49880, loss = 0.69 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 16:25:04.417784: step 49890, loss = 0.57 (282.4 examples/sec; 0.453 sec/batch)
2016-02-03 16:25:09.165127: step 49900, loss = 0.77 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 16:25:14.414622: step 49910, loss = 0.71 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 16:25:19.100454: step 49920, loss = 0.66 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 16:25:23.815198: step 49930, loss = 0.74 (291.7 examples/sec; 0.439 sec/batch)
2016-02-03 16:25:28.550233: step 49940, loss = 0.65 (249.2 examples/sec; 0.514 sec/batch)
2016-02-03 16:25:33.237312: step 49950, loss = 0.77 (269.2 examples/sec; 0.475 sec/batch)
2016-02-03 16:25:37.953447: step 49960, loss = 0.74 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 16:25:42.604029: step 49970, loss = 0.83 (283.0 examples/sec; 0.452 sec/batch)
2016-02-03 16:25:47.417052: step 49980, loss = 0.88 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 16:25:52.041384: step 49990, loss = 0.80 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 16:25:56.771694: step 50000, loss = 0.76 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 16:26:01.987916: step 50010, loss = 0.83 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 16:26:06.642043: step 50020, loss = 0.70 (287.3 examples/sec; 0.445 sec/batch)
2016-02-03 16:26:11.261284: step 50030, loss = 0.63 (255.2 examples/sec; 0.502 sec/batch)
2016-02-03 16:26:15.931439: step 50040, loss = 0.88 (279.4 examples/sec; 0.458 sec/batch)
2016-02-03 16:26:20.604402: step 50050, loss = 0.89 (250.3 examples/sec; 0.511 sec/batch)
2016-02-03 16:26:25.278858: step 50060, loss = 0.61 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 16:26:29.991338: step 50070, loss = 0.65 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 16:26:34.667135: step 50080, loss = 0.67 (261.5 examples/sec; 0.490 sec/batch)
2016-02-03 16:26:39.420369: step 50090, loss = 0.93 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 16:26:44.111470: step 50100, loss = 0.67 (290.9 examples/sec; 0.440 sec/batch)
2016-02-03 16:26:49.321124: step 50110, loss = 0.82 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 16:26:54.025814: step 50120, loss = 0.70 (288.8 examples/sec; 0.443 sec/batch)
2016-02-03 16:26:58.786112: step 50130, loss = 0.60 (275.7 examples/sec; 0.464 sec/batch)
2016-02-03 16:27:03.497493: step 50140, loss = 0.87 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 16:27:08.242482: step 50150, loss = 0.66 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 16:27:12.935395: step 50160, loss = 0.86 (260.1 examples/sec; 0.492 sec/batch)
2016-02-03 16:27:17.666872: step 50170, loss = 0.68 (293.8 examples/sec; 0.436 sec/batch)
2016-02-03 16:27:22.403245: step 50180, loss = 0.68 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 16:27:27.107486: step 50190, loss = 0.72 (291.0 examples/sec; 0.440 sec/batch)
2016-02-03 16:27:31.864922: step 50200, loss = 0.96 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 16:27:37.028230: step 50210, loss = 0.63 (269.2 examples/sec; 0.476 sec/batch)
2016-02-03 16:27:41.685631: step 50220, loss = 0.82 (285.5 examples/sec; 0.448 sec/batch)
2016-02-03 16:27:46.390543: step 50230, loss = 0.77 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 16:27:51.070749: step 50240, loss = 0.82 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 16:27:55.820715: step 50250, loss = 0.72 (289.0 examples/sec; 0.443 sec/batch)
2016-02-03 16:28:00.540258: step 50260, loss = 0.67 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 16:28:05.220267: step 50270, loss = 0.69 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 16:28:09.865662: step 50280, loss = 0.77 (302.6 examples/sec; 0.423 sec/batch)
2016-02-03 16:28:14.623097: step 50290, loss = 0.74 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 16:28:19.336256: step 50300, loss = 0.74 (244.1 examples/sec; 0.524 sec/batch)
2016-02-03 16:28:24.517474: step 50310, loss = 0.60 (283.8 examples/sec; 0.451 sec/batch)
2016-02-03 16:28:29.201546: step 50320, loss = 0.92 (283.3 examples/sec; 0.452 sec/batch)
2016-02-03 16:28:33.834226: step 50330, loss = 0.63 (307.2 examples/sec; 0.417 sec/batch)
2016-02-03 16:28:38.623905: step 50340, loss = 0.76 (256.8 examples/sec; 0.498 sec/batch)
2016-02-03 16:28:43.329266: step 50350, loss = 0.74 (288.2 examples/sec; 0.444 sec/batch)
2016-02-03 16:28:48.050718: step 50360, loss = 0.79 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 16:28:52.810295: step 50370, loss = 0.72 (247.3 examples/sec; 0.518 sec/batch)
2016-02-03 16:28:57.473658: step 50380, loss = 0.79 (256.7 examples/sec; 0.499 sec/batch)
2016-02-03 16:29:02.092277: step 50390, loss = 0.83 (264.1 examples/sec; 0.485 sec/batch)
2016-02-03 16:29:06.728083: step 50400, loss = 0.84 (297.0 examples/sec; 0.431 sec/batch)
2016-02-03 16:29:11.948624: step 50410, loss = 0.74 (267.5 examples/sec; 0.479 sec/batch)
2016-02-03 16:29:16.628006: step 50420, loss = 0.83 (266.4 examples/sec; 0.481 sec/batch)
2016-02-03 16:29:21.334520: step 50430, loss = 0.69 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 16:29:26.073990: step 50440, loss = 0.68 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 16:29:30.719698: step 50450, loss = 0.82 (283.0 examples/sec; 0.452 sec/batch)
2016-02-03 16:29:35.358016: step 50460, loss = 0.61 (254.6 examples/sec; 0.503 sec/batch)
2016-02-03 16:29:40.004374: step 50470, loss = 0.73 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 16:29:44.699029: step 50480, loss = 0.75 (275.7 examples/sec; 0.464 sec/batch)
2016-02-03 16:29:49.387412: step 50490, loss = 0.78 (296.1 examples/sec; 0.432 sec/batch)
2016-02-03 16:29:54.136318: step 50500, loss = 0.85 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 16:29:59.395276: step 50510, loss = 0.72 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 16:30:04.160668: step 50520, loss = 0.74 (301.7 examples/sec; 0.424 sec/batch)
2016-02-03 16:30:08.785163: step 50530, loss = 0.84 (323.8 examples/sec; 0.395 sec/batch)
2016-02-03 16:30:13.525266: step 50540, loss = 0.54 (282.8 examples/sec; 0.453 sec/batch)
2016-02-03 16:30:18.218271: step 50550, loss = 0.74 (288.6 examples/sec; 0.444 sec/batch)
2016-02-03 16:30:22.914334: step 50560, loss = 0.72 (253.3 examples/sec; 0.505 sec/batch)
2016-02-03 16:30:27.494146: step 50570, loss = 0.56 (279.8 examples/sec; 0.457 sec/batch)
2016-02-03 16:30:32.086818: step 50580, loss = 0.82 (252.9 examples/sec; 0.506 sec/batch)
2016-02-03 16:30:36.803059: step 50590, loss = 0.66 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 16:30:41.376788: step 50600, loss = 0.77 (291.2 examples/sec; 0.440 sec/batch)
2016-02-03 16:30:46.646683: step 50610, loss = 0.71 (247.4 examples/sec; 0.517 sec/batch)
2016-02-03 16:30:51.358384: step 50620, loss = 0.68 (258.9 examples/sec; 0.494 sec/batch)
2016-02-03 16:30:56.071335: step 50630, loss = 0.78 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 16:31:00.713412: step 50640, loss = 0.69 (284.9 examples/sec; 0.449 sec/batch)
2016-02-03 16:31:05.409548: step 50650, loss = 0.70 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 16:31:10.051580: step 50660, loss = 0.82 (283.1 examples/sec; 0.452 sec/batch)
2016-02-03 16:31:14.732536: step 50670, loss = 0.72 (266.4 examples/sec; 0.480 sec/batch)
2016-02-03 16:31:19.307290: step 50680, loss = 0.65 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 16:31:24.060991: step 50690, loss = 0.56 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 16:31:28.813453: step 50700, loss = 0.78 (258.6 examples/sec; 0.495 sec/batch)
2016-02-03 16:31:34.083065: step 50710, loss = 0.76 (283.6 examples/sec; 0.451 sec/batch)
2016-02-03 16:31:38.731460: step 50720, loss = 0.87 (286.6 examples/sec; 0.447 sec/batch)
2016-02-03 16:31:43.390202: step 50730, loss = 0.72 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 16:31:48.144140: step 50740, loss = 0.77 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 16:31:52.965078: step 50750, loss = 0.77 (274.4 examples/sec; 0.466 sec/batch)
2016-02-03 16:31:57.642788: step 50760, loss = 0.70 (268.0 examples/sec; 0.478 sec/batch)
2016-02-03 16:32:02.389669: step 50770, loss = 0.78 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 16:32:07.033051: step 50780, loss = 0.63 (279.2 examples/sec; 0.458 sec/batch)
2016-02-03 16:32:11.783966: step 50790, loss = 0.71 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 16:32:16.519832: step 50800, loss = 0.64 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 16:32:21.831141: step 50810, loss = 0.70 (262.6 examples/sec; 0.487 sec/batch)
2016-02-03 16:32:26.479166: step 50820, loss = 0.69 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 16:32:31.075670: step 50830, loss = 0.76 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 16:32:35.739202: step 50840, loss = 0.77 (277.8 examples/sec; 0.461 sec/batch)
2016-02-03 16:32:40.514050: step 50850, loss = 0.73 (259.3 examples/sec; 0.494 sec/batch)
2016-02-03 16:32:45.218007: step 50860, loss = 0.75 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 16:32:49.988820: step 50870, loss = 0.77 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 16:32:54.578072: step 50880, loss = 0.75 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 16:32:59.299068: step 50890, loss = 0.83 (284.1 examples/sec; 0.450 sec/batch)
2016-02-03 16:33:04.094697: step 50900, loss = 0.78 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 16:33:09.352489: step 50910, loss = 0.88 (261.9 examples/sec; 0.489 sec/batch)
2016-02-03 16:33:14.100368: step 50920, loss = 0.70 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 16:33:18.827335: step 50930, loss = 0.94 (291.6 examples/sec; 0.439 sec/batch)
2016-02-03 16:33:23.512946: step 50940, loss = 0.75 (257.4 examples/sec; 0.497 sec/batch)
2016-02-03 16:33:28.264906: step 50950, loss = 0.83 (258.4 examples/sec; 0.495 sec/batch)
2016-02-03 16:33:32.974971: step 50960, loss = 0.72 (253.9 examples/sec; 0.504 sec/batch)
2016-02-03 16:33:37.718199: step 50970, loss = 0.70 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 16:33:42.435063: step 50980, loss = 0.79 (266.3 examples/sec; 0.481 sec/batch)
2016-02-03 16:33:47.110386: step 50990, loss = 0.72 (289.7 examples/sec; 0.442 sec/batch)
2016-02-03 16:33:51.781717: step 51000, loss = 0.87 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 16:33:57.050945: step 51010, loss = 0.81 (291.6 examples/sec; 0.439 sec/batch)
2016-02-03 16:34:01.793525: step 51020, loss = 0.82 (261.1 examples/sec; 0.490 sec/batch)
2016-02-03 16:34:06.493789: step 51030, loss = 0.71 (289.5 examples/sec; 0.442 sec/batch)
2016-02-03 16:34:11.158171: step 51040, loss = 0.77 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 16:34:15.887425: step 51050, loss = 0.79 (256.7 examples/sec; 0.499 sec/batch)
2016-02-03 16:34:20.554360: step 51060, loss = 0.81 (257.1 examples/sec; 0.498 sec/batch)
2016-02-03 16:34:25.276724: step 51070, loss = 0.67 (260.4 examples/sec; 0.492 sec/batch)
2016-02-03 16:34:29.982057: step 51080, loss = 0.81 (252.4 examples/sec; 0.507 sec/batch)
2016-02-03 16:34:34.671315: step 51090, loss = 1.07 (285.3 examples/sec; 0.449 sec/batch)
2016-02-03 16:34:39.341594: step 51100, loss = 0.76 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 16:34:44.515488: step 51110, loss = 0.72 (260.5 examples/sec; 0.491 sec/batch)
2016-02-03 16:34:49.135166: step 51120, loss = 0.78 (293.6 examples/sec; 0.436 sec/batch)
2016-02-03 16:34:53.864244: step 51130, loss = 0.66 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 16:34:58.475119: step 51140, loss = 0.62 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 16:35:03.146549: step 51150, loss = 0.81 (297.5 examples/sec; 0.430 sec/batch)
2016-02-03 16:35:07.842476: step 51160, loss = 0.61 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 16:35:12.476273: step 51170, loss = 0.76 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 16:35:17.156958: step 51180, loss = 0.76 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 16:35:21.801152: step 51190, loss = 0.62 (297.1 examples/sec; 0.431 sec/batch)
2016-02-03 16:35:26.489481: step 51200, loss = 0.85 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 16:35:31.802361: step 51210, loss = 0.90 (268.6 examples/sec; 0.476 sec/batch)
2016-02-03 16:35:36.519626: step 51220, loss = 0.69 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 16:35:41.204131: step 51230, loss = 0.71 (286.1 examples/sec; 0.447 sec/batch)
2016-02-03 16:35:45.926121: step 51240, loss = 0.72 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 16:35:50.628291: step 51250, loss = 0.82 (255.7 examples/sec; 0.501 sec/batch)
2016-02-03 16:35:55.300196: step 51260, loss = 0.78 (294.8 examples/sec; 0.434 sec/batch)
2016-02-03 16:36:00.082129: step 51270, loss = 0.69 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 16:36:04.740529: step 51280, loss = 0.72 (276.6 examples/sec; 0.463 sec/batch)
2016-02-03 16:36:09.391214: step 51290, loss = 0.74 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 16:36:14.096806: step 51300, loss = 0.67 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 16:36:19.333117: step 51310, loss = 0.77 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 16:36:24.093834: step 51320, loss = 0.70 (257.9 examples/sec; 0.496 sec/batch)
2016-02-03 16:36:28.823718: step 51330, loss = 0.79 (285.4 examples/sec; 0.448 sec/batch)
2016-02-03 16:36:33.563086: step 51340, loss = 0.63 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 16:36:38.385911: step 51350, loss = 0.74 (250.5 examples/sec; 0.511 sec/batch)
2016-02-03 16:36:43.084849: step 51360, loss = 0.72 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 16:36:47.860317: step 51370, loss = 0.91 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 16:36:52.522481: step 51380, loss = 0.71 (260.3 examples/sec; 0.492 sec/batch)
2016-02-03 16:36:57.304070: step 51390, loss = 0.72 (294.3 examples/sec; 0.435 sec/batch)
2016-02-03 16:37:02.023490: step 51400, loss = 0.70 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 16:37:07.263964: step 51410, loss = 0.64 (261.5 examples/sec; 0.490 sec/batch)
2016-02-03 16:37:12.036887: step 51420, loss = 0.73 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 16:37:16.768654: step 51430, loss = 0.72 (251.7 examples/sec; 0.509 sec/batch)
2016-02-03 16:37:21.452873: step 51440, loss = 0.74 (289.9 examples/sec; 0.441 sec/batch)
2016-02-03 16:37:26.126135: step 51450, loss = 0.85 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 16:37:30.800038: step 51460, loss = 0.73 (286.3 examples/sec; 0.447 sec/batch)
2016-02-03 16:37:35.479367: step 51470, loss = 0.69 (270.5 examples/sec; 0.473 sec/batch)
2016-02-03 16:37:40.177009: step 51480, loss = 0.76 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 16:37:44.912898: step 51490, loss = 0.87 (252.2 examples/sec; 0.507 sec/batch)
2016-02-03 16:37:49.576335: step 51500, loss = 0.73 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 16:37:54.765809: step 51510, loss = 0.72 (294.0 examples/sec; 0.435 sec/batch)
2016-02-03 16:37:59.534283: step 51520, loss = 0.71 (283.3 examples/sec; 0.452 sec/batch)
2016-02-03 16:38:04.206782: step 51530, loss = 0.75 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 16:38:08.777303: step 51540, loss = 0.64 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 16:38:13.400286: step 51550, loss = 0.62 (292.1 examples/sec; 0.438 sec/batch)
2016-02-03 16:38:18.046889: step 51560, loss = 1.05 (279.2 examples/sec; 0.458 sec/batch)
2016-02-03 16:38:22.864870: step 51570, loss = 0.66 (280.5 examples/sec; 0.456 sec/batch)
2016-02-03 16:38:27.627336: step 51580, loss = 0.69 (259.9 examples/sec; 0.492 sec/batch)
2016-02-03 16:38:32.293662: step 51590, loss = 0.85 (273.8 examples/sec; 0.468 sec/batch)
2016-02-03 16:38:37.016088: step 51600, loss = 0.79 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 16:38:42.183439: step 51610, loss = 0.71 (311.8 examples/sec; 0.410 sec/batch)
2016-02-03 16:38:46.751540: step 51620, loss = 0.99 (292.4 examples/sec; 0.438 sec/batch)
2016-02-03 16:38:51.472677: step 51630, loss = 0.79 (284.4 examples/sec; 0.450 sec/batch)
2016-02-03 16:38:56.200610: step 51640, loss = 0.66 (268.2 examples/sec; 0.477 sec/batch)
2016-02-03 16:39:00.875616: step 51650, loss = 0.67 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 16:39:05.647241: step 51660, loss = 0.67 (252.5 examples/sec; 0.507 sec/batch)
2016-02-03 16:39:10.400159: step 51670, loss = 0.66 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 16:39:15.143496: step 51680, loss = 0.78 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 16:39:19.849421: step 51690, loss = 0.73 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 16:39:24.634861: step 51700, loss = 0.77 (240.1 examples/sec; 0.533 sec/batch)
2016-02-03 16:39:29.829958: step 51710, loss = 0.76 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 16:39:34.552835: step 51720, loss = 0.78 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 16:39:39.227065: step 51730, loss = 0.68 (278.6 examples/sec; 0.460 sec/batch)
2016-02-03 16:39:43.950969: step 51740, loss = 0.70 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 16:39:48.651560: step 51750, loss = 0.76 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 16:39:53.377704: step 51760, loss = 0.82 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 16:39:58.022953: step 51770, loss = 0.92 (294.1 examples/sec; 0.435 sec/batch)
2016-02-03 16:40:02.753241: step 51780, loss = 0.78 (302.2 examples/sec; 0.424 sec/batch)
2016-02-03 16:40:07.507053: step 51790, loss = 0.77 (255.7 examples/sec; 0.501 sec/batch)
2016-02-03 16:40:12.231223: step 51800, loss = 0.60 (257.6 examples/sec; 0.497 sec/batch)
2016-02-03 16:40:17.548661: step 51810, loss = 0.66 (283.8 examples/sec; 0.451 sec/batch)
2016-02-03 16:40:22.234313: step 51820, loss = 0.76 (266.9 examples/sec; 0.480 sec/batch)
2016-02-03 16:40:26.940313: step 51830, loss = 0.80 (303.2 examples/sec; 0.422 sec/batch)
2016-02-03 16:40:31.607489: step 51840, loss = 0.62 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 16:40:36.392087: step 51850, loss = 0.79 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 16:40:41.146422: step 51860, loss = 0.68 (254.0 examples/sec; 0.504 sec/batch)
2016-02-03 16:40:45.972477: step 51870, loss = 0.70 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 16:40:50.669958: step 51880, loss = 0.80 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 16:40:55.439186: step 51890, loss = 0.93 (256.0 examples/sec; 0.500 sec/batch)
2016-02-03 16:41:00.111915: step 51900, loss = 0.76 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 16:41:05.248279: step 51910, loss = 0.88 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 16:41:10.019787: step 51920, loss = 0.80 (283.8 examples/sec; 0.451 sec/batch)
2016-02-03 16:41:14.635494: step 51930, loss = 0.74 (300.5 examples/sec; 0.426 sec/batch)
2016-02-03 16:41:19.413396: step 51940, loss = 0.90 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 16:41:24.118064: step 51950, loss = 0.58 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 16:41:28.953193: step 51960, loss = 0.69 (267.5 examples/sec; 0.479 sec/batch)
2016-02-03 16:41:33.599058: step 51970, loss = 0.85 (272.6 examples/sec; 0.470 sec/batch)
2016-02-03 16:41:38.387389: step 51980, loss = 0.75 (252.4 examples/sec; 0.507 sec/batch)
2016-02-03 16:41:43.063445: step 51990, loss = 0.69 (281.1 examples/sec; 0.455 sec/batch)
2016-02-03 16:41:47.776887: step 52000, loss = 0.64 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 16:41:53.047057: step 52010, loss = 0.80 (262.6 examples/sec; 0.488 sec/batch)
2016-02-03 16:41:57.799611: step 52020, loss = 0.72 (245.3 examples/sec; 0.522 sec/batch)
2016-02-03 16:42:02.464966: step 52030, loss = 0.87 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 16:42:07.200505: step 52040, loss = 0.70 (265.0 examples/sec; 0.483 sec/batch)
2016-02-03 16:42:11.893475: step 52050, loss = 0.86 (252.8 examples/sec; 0.506 sec/batch)
2016-02-03 16:42:16.629705: step 52060, loss = 0.73 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 16:42:21.352193: step 52070, loss = 0.62 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 16:42:25.994058: step 52080, loss = 0.68 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 16:42:30.687488: step 52090, loss = 0.92 (296.7 examples/sec; 0.431 sec/batch)
2016-02-03 16:42:35.375886: step 52100, loss = 0.70 (290.5 examples/sec; 0.441 sec/batch)
2016-02-03 16:42:40.592425: step 52110, loss = 0.81 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 16:42:45.306272: step 52120, loss = 0.69 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 16:42:49.946747: step 52130, loss = 0.86 (307.0 examples/sec; 0.417 sec/batch)
2016-02-03 16:42:54.598357: step 52140, loss = 0.75 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 16:42:59.209491: step 52150, loss = 0.71 (283.1 examples/sec; 0.452 sec/batch)
2016-02-03 16:43:03.990559: step 52160, loss = 0.77 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 16:43:08.723934: step 52170, loss = 0.73 (295.3 examples/sec; 0.433 sec/batch)
2016-02-03 16:43:13.489135: step 52180, loss = 0.86 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 16:43:18.161878: step 52190, loss = 0.88 (253.8 examples/sec; 0.504 sec/batch)
2016-02-03 16:43:22.910805: step 52200, loss = 0.87 (253.0 examples/sec; 0.506 sec/batch)
2016-02-03 16:43:28.213389: step 52210, loss = 0.62 (257.8 examples/sec; 0.497 sec/batch)
2016-02-03 16:43:32.831983: step 52220, loss = 0.82 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 16:43:37.616065: step 52230, loss = 0.66 (249.8 examples/sec; 0.512 sec/batch)
2016-02-03 16:43:42.337435: step 52240, loss = 0.79 (264.2 examples/sec; 0.484 sec/batch)
2016-02-03 16:43:47.032333: step 52250, loss = 0.81 (259.2 examples/sec; 0.494 sec/batch)
2016-02-03 16:43:51.730518: step 52260, loss = 0.80 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 16:43:56.499549: step 52270, loss = 0.78 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 16:44:01.168368: step 52280, loss = 0.91 (262.9 examples/sec; 0.487 sec/batch)
2016-02-03 16:44:05.856911: step 52290, loss = 0.92 (255.2 examples/sec; 0.501 sec/batch)
2016-02-03 16:44:10.635767: step 52300, loss = 0.74 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 16:44:15.791846: step 52310, loss = 0.92 (276.9 examples/sec; 0.462 sec/batch)
2016-02-03 16:44:20.603365: step 52320, loss = 0.64 (249.5 examples/sec; 0.513 sec/batch)
2016-02-03 16:44:25.333089: step 52330, loss = 0.69 (248.4 examples/sec; 0.515 sec/batch)
2016-02-03 16:44:30.083421: step 52340, loss = 0.65 (282.2 examples/sec; 0.454 sec/batch)
2016-02-03 16:44:34.766496: step 52350, loss = 0.71 (276.6 examples/sec; 0.463 sec/batch)
2016-02-03 16:44:39.487268: step 52360, loss = 0.69 (257.6 examples/sec; 0.497 sec/batch)
2016-02-03 16:44:44.242189: step 52370, loss = 0.62 (276.6 examples/sec; 0.463 sec/batch)
2016-02-03 16:44:48.922228: step 52380, loss = 0.76 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 16:44:53.632864: step 52390, loss = 0.70 (256.8 examples/sec; 0.498 sec/batch)
2016-02-03 16:44:58.329406: step 52400, loss = 0.77 (293.3 examples/sec; 0.436 sec/batch)
2016-02-03 16:45:03.519321: step 52410, loss = 0.62 (299.8 examples/sec; 0.427 sec/batch)
2016-02-03 16:45:08.123259: step 52420, loss = 0.77 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 16:45:12.790100: step 52430, loss = 0.69 (273.8 examples/sec; 0.468 sec/batch)
2016-02-03 16:45:17.494585: step 52440, loss = 0.85 (260.1 examples/sec; 0.492 sec/batch)
2016-02-03 16:45:22.151785: step 52450, loss = 0.70 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 16:45:26.850888: step 52460, loss = 0.64 (261.0 examples/sec; 0.490 sec/batch)
2016-02-03 16:45:31.596658: step 52470, loss = 0.76 (286.3 examples/sec; 0.447 sec/batch)
2016-02-03 16:45:36.360228: step 52480, loss = 0.68 (260.4 examples/sec; 0.492 sec/batch)
2016-02-03 16:45:41.048310: step 52490, loss = 0.72 (261.1 examples/sec; 0.490 sec/batch)
2016-02-03 16:45:45.780025: step 52500, loss = 0.68 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 16:45:51.077330: step 52510, loss = 0.70 (252.1 examples/sec; 0.508 sec/batch)
2016-02-03 16:45:55.797343: step 52520, loss = 0.75 (247.2 examples/sec; 0.518 sec/batch)
2016-02-03 16:46:00.612787: step 52530, loss = 0.84 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 16:46:05.334253: step 52540, loss = 0.77 (250.9 examples/sec; 0.510 sec/batch)
2016-02-03 16:46:10.054253: step 52550, loss = 0.80 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 16:46:14.805398: step 52560, loss = 0.72 (287.8 examples/sec; 0.445 sec/batch)
2016-02-03 16:46:19.517093: step 52570, loss = 0.90 (289.9 examples/sec; 0.441 sec/batch)
2016-02-03 16:46:24.259454: step 52580, loss = 0.78 (257.7 examples/sec; 0.497 sec/batch)
2016-02-03 16:46:29.013781: step 52590, loss = 0.65 (250.7 examples/sec; 0.511 sec/batch)
2016-02-03 16:46:33.829794: step 52600, loss = 0.73 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 16:46:39.004573: step 52610, loss = 0.73 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 16:46:43.679046: step 52620, loss = 0.63 (252.2 examples/sec; 0.508 sec/batch)
2016-02-03 16:46:48.341602: step 52630, loss = 0.62 (287.6 examples/sec; 0.445 sec/batch)
2016-02-03 16:46:53.022716: step 52640, loss = 0.82 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 16:46:57.714917: step 52650, loss = 0.77 (285.5 examples/sec; 0.448 sec/batch)
2016-02-03 16:47:02.407256: step 52660, loss = 0.72 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 16:47:07.206518: step 52670, loss = 0.66 (257.2 examples/sec; 0.498 sec/batch)
2016-02-03 16:47:11.896476: step 52680, loss = 0.82 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 16:47:16.616363: step 52690, loss = 0.74 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 16:47:21.293954: step 52700, loss = 0.72 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 16:47:26.460944: step 52710, loss = 0.63 (289.5 examples/sec; 0.442 sec/batch)
2016-02-03 16:47:31.132375: step 52720, loss = 0.71 (294.9 examples/sec; 0.434 sec/batch)
2016-02-03 16:47:35.761332: step 52730, loss = 0.84 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 16:47:40.430517: step 52740, loss = 0.76 (301.1 examples/sec; 0.425 sec/batch)
2016-02-03 16:47:45.112360: step 52750, loss = 0.65 (257.6 examples/sec; 0.497 sec/batch)
2016-02-03 16:47:49.786086: step 52760, loss = 0.76 (291.8 examples/sec; 0.439 sec/batch)
2016-02-03 16:47:54.526945: step 52770, loss = 0.85 (255.6 examples/sec; 0.501 sec/batch)
2016-02-03 16:47:59.199854: step 52780, loss = 0.61 (258.1 examples/sec; 0.496 sec/batch)
2016-02-03 16:48:03.871530: step 52790, loss = 0.94 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 16:48:08.562348: step 52800, loss = 0.71 (280.1 examples/sec; 0.457 sec/batch)
2016-02-03 16:48:13.879383: step 52810, loss = 0.71 (279.2 examples/sec; 0.458 sec/batch)
2016-02-03 16:48:18.615748: step 52820, loss = 0.80 (262.9 examples/sec; 0.487 sec/batch)
2016-02-03 16:48:23.247181: step 52830, loss = 0.94 (267.3 examples/sec; 0.479 sec/batch)
2016-02-03 16:48:27.964646: step 52840, loss = 0.75 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 16:48:32.776510: step 52850, loss = 0.65 (255.6 examples/sec; 0.501 sec/batch)
2016-02-03 16:48:37.468331: step 52860, loss = 0.69 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 16:48:42.191556: step 52870, loss = 0.69 (258.1 examples/sec; 0.496 sec/batch)
2016-02-03 16:48:47.014029: step 52880, loss = 0.79 (248.4 examples/sec; 0.515 sec/batch)
2016-02-03 16:48:51.701358: step 52890, loss = 0.82 (286.4 examples/sec; 0.447 sec/batch)
2016-02-03 16:48:56.432017: step 52900, loss = 0.78 (294.8 examples/sec; 0.434 sec/batch)
2016-02-03 16:49:01.718556: step 52910, loss = 0.70 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 16:49:06.443270: step 52920, loss = 0.76 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 16:49:11.150378: step 52930, loss = 0.74 (292.6 examples/sec; 0.437 sec/batch)
2016-02-03 16:49:15.894078: step 52940, loss = 0.65 (254.6 examples/sec; 0.503 sec/batch)
2016-02-03 16:49:20.592214: step 52950, loss = 0.99 (301.4 examples/sec; 0.425 sec/batch)
2016-02-03 16:49:25.285316: step 52960, loss = 0.69 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 16:49:29.977344: step 52970, loss = 0.72 (258.6 examples/sec; 0.495 sec/batch)
2016-02-03 16:49:34.617775: step 52980, loss = 0.68 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 16:49:39.287753: step 52990, loss = 0.62 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 16:49:44.014106: step 53000, loss = 0.73 (260.7 examples/sec; 0.491 sec/batch)
2016-02-03 16:49:49.274523: step 53010, loss = 0.78 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 16:49:53.992862: step 53020, loss = 0.79 (290.8 examples/sec; 0.440 sec/batch)
2016-02-03 16:49:58.682085: step 53030, loss = 0.80 (293.4 examples/sec; 0.436 sec/batch)
2016-02-03 16:50:03.376304: step 53040, loss = 0.76 (280.3 examples/sec; 0.457 sec/batch)
2016-02-03 16:50:08.076954: step 53050, loss = 0.83 (257.1 examples/sec; 0.498 sec/batch)
2016-02-03 16:50:12.750986: step 53060, loss = 0.82 (259.0 examples/sec; 0.494 sec/batch)
2016-02-03 16:50:17.411972: step 53070, loss = 0.61 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 16:50:22.103311: step 53080, loss = 0.80 (301.1 examples/sec; 0.425 sec/batch)
2016-02-03 16:50:26.886469: step 53090, loss = 0.66 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 16:50:31.569125: step 53100, loss = 0.67 (288.7 examples/sec; 0.443 sec/batch)
2016-02-03 16:50:36.795745: step 53110, loss = 0.93 (299.2 examples/sec; 0.428 sec/batch)
2016-02-03 16:50:41.459286: step 53120, loss = 0.76 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 16:50:46.118084: step 53130, loss = 0.73 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 16:50:50.799096: step 53140, loss = 0.83 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 16:50:55.523622: step 53150, loss = 0.71 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 16:51:00.307349: step 53160, loss = 0.69 (261.1 examples/sec; 0.490 sec/batch)
2016-02-03 16:51:05.035723: step 53170, loss = 0.89 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 16:51:09.890690: step 53180, loss = 0.74 (255.2 examples/sec; 0.502 sec/batch)
2016-02-03 16:51:14.515295: step 53190, loss = 0.71 (279.7 examples/sec; 0.458 sec/batch)
2016-02-03 16:51:19.290708: step 53200, loss = 0.71 (272.6 examples/sec; 0.469 sec/batch)
2016-02-03 16:51:24.598318: step 53210, loss = 0.77 (262.1 examples/sec; 0.488 sec/batch)
2016-02-03 16:51:29.272039: step 53220, loss = 0.71 (295.4 examples/sec; 0.433 sec/batch)
2016-02-03 16:51:34.032774: step 53230, loss = 0.70 (258.0 examples/sec; 0.496 sec/batch)
2016-02-03 16:51:38.748136: step 53240, loss = 0.75 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 16:51:43.443130: step 53250, loss = 0.75 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 16:51:48.201976: step 53260, loss = 0.84 (286.3 examples/sec; 0.447 sec/batch)
2016-02-03 16:51:52.965403: step 53270, loss = 0.74 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 16:51:57.813027: step 53280, loss = 0.76 (250.9 examples/sec; 0.510 sec/batch)
2016-02-03 16:52:02.576745: step 53290, loss = 0.72 (253.9 examples/sec; 0.504 sec/batch)
2016-02-03 16:52:07.234888: step 53300, loss = 0.78 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 16:52:12.492810: step 53310, loss = 0.68 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 16:52:17.189826: step 53320, loss = 0.71 (286.4 examples/sec; 0.447 sec/batch)
2016-02-03 16:52:22.003054: step 53330, loss = 0.77 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 16:52:26.783838: step 53340, loss = 0.75 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 16:52:31.443306: step 53350, loss = 0.96 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 16:52:36.192878: step 53360, loss = 0.85 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 16:52:40.994028: step 53370, loss = 0.83 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 16:52:45.653132: step 53380, loss = 0.77 (282.9 examples/sec; 0.452 sec/batch)
2016-02-03 16:52:50.375358: step 53390, loss = 0.71 (324.8 examples/sec; 0.394 sec/batch)
2016-02-03 16:52:55.113425: step 53400, loss = 0.67 (257.8 examples/sec; 0.497 sec/batch)
2016-02-03 16:53:00.283236: step 53410, loss = 0.71 (297.8 examples/sec; 0.430 sec/batch)
2016-02-03 16:53:05.060911: step 53420, loss = 0.68 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 16:53:09.817831: step 53430, loss = 0.69 (255.7 examples/sec; 0.501 sec/batch)
2016-02-03 16:53:14.545251: step 53440, loss = 0.58 (304.5 examples/sec; 0.420 sec/batch)
2016-02-03 16:53:19.313008: step 53450, loss = 0.93 (250.7 examples/sec; 0.511 sec/batch)
2016-02-03 16:53:24.122782: step 53460, loss = 0.79 (267.3 examples/sec; 0.479 sec/batch)
2016-02-03 16:53:28.893661: step 53470, loss = 0.77 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 16:53:33.652393: step 53480, loss = 0.77 (264.5 examples/sec; 0.484 sec/batch)
2016-02-03 16:53:38.388621: step 53490, loss = 0.81 (287.0 examples/sec; 0.446 sec/batch)
2016-02-03 16:53:43.131120: step 53500, loss = 0.85 (245.8 examples/sec; 0.521 sec/batch)
2016-02-03 16:53:48.406732: step 53510, loss = 0.70 (300.7 examples/sec; 0.426 sec/batch)
2016-02-03 16:53:53.144634: step 53520, loss = 0.74 (285.3 examples/sec; 0.449 sec/batch)
2016-02-03 16:53:57.882117: step 53530, loss = 0.74 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 16:54:02.570077: step 53540, loss = 0.64 (293.3 examples/sec; 0.436 sec/batch)
2016-02-03 16:54:07.313841: step 53550, loss = 0.76 (258.9 examples/sec; 0.494 sec/batch)
2016-02-03 16:54:12.038549: step 53560, loss = 0.79 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 16:54:16.686927: step 53570, loss = 0.67 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 16:54:21.405339: step 53580, loss = 0.83 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 16:54:26.116446: step 53590, loss = 0.64 (264.3 examples/sec; 0.484 sec/batch)
2016-02-03 16:54:30.780137: step 53600, loss = 0.64 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 16:54:36.051473: step 53610, loss = 0.74 (277.2 examples/sec; 0.462 sec/batch)
2016-02-03 16:54:40.733582: step 53620, loss = 0.71 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 16:54:45.480964: step 53630, loss = 0.65 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 16:54:50.219751: step 53640, loss = 0.60 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 16:54:54.908904: step 53650, loss = 0.83 (297.4 examples/sec; 0.430 sec/batch)
2016-02-03 16:54:59.639115: step 53660, loss = 0.76 (272.6 examples/sec; 0.470 sec/batch)
2016-02-03 16:55:04.333499: step 53670, loss = 0.70 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 16:55:09.085214: step 53680, loss = 0.68 (250.7 examples/sec; 0.510 sec/batch)
2016-02-03 16:55:13.785476: step 53690, loss = 0.77 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 16:55:18.536296: step 53700, loss = 0.79 (260.7 examples/sec; 0.491 sec/batch)
2016-02-03 16:55:23.692821: step 53710, loss = 0.62 (306.1 examples/sec; 0.418 sec/batch)
2016-02-03 16:55:28.434793: step 53720, loss = 0.69 (255.0 examples/sec; 0.502 sec/batch)
2016-02-03 16:55:33.192673: step 53730, loss = 0.76 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 16:55:37.948599: step 53740, loss = 0.63 (251.7 examples/sec; 0.509 sec/batch)
2016-02-03 16:55:42.623418: step 53750, loss = 0.69 (306.3 examples/sec; 0.418 sec/batch)
2016-02-03 16:55:47.397040: step 53760, loss = 0.87 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 16:55:52.074906: step 53770, loss = 0.79 (252.8 examples/sec; 0.506 sec/batch)
2016-02-03 16:55:56.754669: step 53780, loss = 0.66 (254.9 examples/sec; 0.502 sec/batch)
2016-02-03 16:56:01.401657: step 53790, loss = 0.74 (267.3 examples/sec; 0.479 sec/batch)
2016-02-03 16:56:06.168078: step 53800, loss = 0.75 (288.3 examples/sec; 0.444 sec/batch)
2016-02-03 16:56:11.436010: step 53810, loss = 0.70 (275.7 examples/sec; 0.464 sec/batch)
2016-02-03 16:56:16.207441: step 53820, loss = 0.66 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 16:56:20.915994: step 53830, loss = 0.68 (259.2 examples/sec; 0.494 sec/batch)
2016-02-03 16:56:25.686035: step 53840, loss = 0.80 (276.9 examples/sec; 0.462 sec/batch)
2016-02-03 16:56:30.374278: step 53850, loss = 0.67 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 16:56:35.117232: step 53860, loss = 0.73 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 16:56:39.790856: step 53870, loss = 0.75 (286.9 examples/sec; 0.446 sec/batch)
2016-02-03 16:56:44.590920: step 53880, loss = 0.71 (286.8 examples/sec; 0.446 sec/batch)
2016-02-03 16:56:49.282221: step 53890, loss = 0.83 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 16:56:54.001748: step 53900, loss = 0.62 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 16:56:59.240197: step 53910, loss = 0.84 (255.9 examples/sec; 0.500 sec/batch)
2016-02-03 16:57:03.975342: step 53920, loss = 0.74 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 16:57:08.725376: step 53930, loss = 0.71 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 16:57:13.466338: step 53940, loss = 0.77 (281.4 examples/sec; 0.455 sec/batch)
2016-02-03 16:57:18.242822: step 53950, loss = 0.74 (283.5 examples/sec; 0.452 sec/batch)
2016-02-03 16:57:22.945988: step 53960, loss = 0.71 (253.9 examples/sec; 0.504 sec/batch)
2016-02-03 16:57:27.690917: step 53970, loss = 0.71 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 16:57:32.403319: step 53980, loss = 0.77 (258.9 examples/sec; 0.494 sec/batch)
2016-02-03 16:57:37.094864: step 53990, loss = 0.67 (249.9 examples/sec; 0.512 sec/batch)
2016-02-03 16:57:41.754152: step 54000, loss = 0.73 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 16:57:47.028184: step 54010, loss = 0.67 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 16:57:51.722256: step 54020, loss = 0.76 (265.2 examples/sec; 0.483 sec/batch)
2016-02-03 16:57:56.487422: step 54030, loss = 0.71 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 16:58:01.276539: step 54040, loss = 0.72 (285.3 examples/sec; 0.449 sec/batch)
2016-02-03 16:58:06.025365: step 54050, loss = 0.73 (279.4 examples/sec; 0.458 sec/batch)
2016-02-03 16:58:10.769343: step 54060, loss = 0.72 (297.1 examples/sec; 0.431 sec/batch)
2016-02-03 16:58:15.500664: step 54070, loss = 0.69 (248.7 examples/sec; 0.515 sec/batch)
2016-02-03 16:58:20.169289: step 54080, loss = 0.76 (281.2 examples/sec; 0.455 sec/batch)
2016-02-03 16:58:24.983079: step 54090, loss = 0.74 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 16:58:29.694185: step 54100, loss = 0.79 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 16:58:34.923583: step 54110, loss = 0.83 (287.0 examples/sec; 0.446 sec/batch)
2016-02-03 16:58:39.602303: step 54120, loss = 0.89 (260.0 examples/sec; 0.492 sec/batch)
2016-02-03 16:58:44.387174: step 54130, loss = 0.79 (283.4 examples/sec; 0.452 sec/batch)
2016-02-03 16:58:49.077282: step 54140, loss = 0.83 (282.2 examples/sec; 0.454 sec/batch)
2016-02-03 16:58:53.817443: step 54150, loss = 0.72 (244.5 examples/sec; 0.524 sec/batch)
2016-02-03 16:58:58.514175: step 54160, loss = 0.69 (308.1 examples/sec; 0.415 sec/batch)
2016-02-03 16:59:03.285406: step 54170, loss = 0.61 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 16:59:08.091166: step 54180, loss = 0.70 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 16:59:12.752573: step 54190, loss = 0.76 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 16:59:17.485664: step 54200, loss = 0.86 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 16:59:22.692777: step 54210, loss = 0.71 (259.2 examples/sec; 0.494 sec/batch)
2016-02-03 16:59:27.385079: step 54220, loss = 0.80 (255.8 examples/sec; 0.500 sec/batch)
2016-02-03 16:59:32.057466: step 54230, loss = 0.76 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 16:59:36.825218: step 54240, loss = 0.66 (257.3 examples/sec; 0.497 sec/batch)
2016-02-03 16:59:41.551396: step 54250, loss = 0.88 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 16:59:46.324950: step 54260, loss = 0.79 (251.8 examples/sec; 0.508 sec/batch)
2016-02-03 16:59:51.043809: step 54270, loss = 0.70 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 16:59:55.736460: step 54280, loss = 0.72 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 17:00:00.458201: step 54290, loss = 0.78 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 17:00:05.188262: step 54300, loss = 0.70 (252.9 examples/sec; 0.506 sec/batch)
2016-02-03 17:00:10.520312: step 54310, loss = 0.74 (252.3 examples/sec; 0.507 sec/batch)
2016-02-03 17:00:15.185412: step 54320, loss = 0.74 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 17:00:19.902795: step 54330, loss = 0.67 (302.2 examples/sec; 0.424 sec/batch)
2016-02-03 17:00:24.554395: step 54340, loss = 0.60 (256.3 examples/sec; 0.499 sec/batch)
2016-02-03 17:00:29.189697: step 54350, loss = 0.60 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 17:00:33.878434: step 54360, loss = 0.81 (288.9 examples/sec; 0.443 sec/batch)
2016-02-03 17:00:38.665898: step 54370, loss = 0.87 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 17:00:43.345122: step 54380, loss = 0.74 (257.4 examples/sec; 0.497 sec/batch)
2016-02-03 17:00:48.032086: step 54390, loss = 0.57 (251.9 examples/sec; 0.508 sec/batch)
2016-02-03 17:00:52.717663: step 54400, loss = 0.65 (267.3 examples/sec; 0.479 sec/batch)
2016-02-03 17:00:57.974976: step 54410, loss = 0.80 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 17:01:02.665966: step 54420, loss = 0.77 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 17:01:07.359449: step 54430, loss = 0.67 (262.2 examples/sec; 0.488 sec/batch)
2016-02-03 17:01:11.986209: step 54440, loss = 0.70 (290.1 examples/sec; 0.441 sec/batch)
2016-02-03 17:01:16.747575: step 54450, loss = 0.90 (251.7 examples/sec; 0.509 sec/batch)
2016-02-03 17:01:21.343171: step 54460, loss = 0.71 (282.2 examples/sec; 0.454 sec/batch)
2016-02-03 17:01:25.947260: step 54470, loss = 0.73 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 17:01:30.601686: step 54480, loss = 0.80 (290.0 examples/sec; 0.441 sec/batch)
2016-02-03 17:01:35.367055: step 54490, loss = 0.66 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 17:01:40.064115: step 54500, loss = 0.68 (251.5 examples/sec; 0.509 sec/batch)
2016-02-03 17:01:45.316539: step 54510, loss = 0.66 (285.1 examples/sec; 0.449 sec/batch)
2016-02-03 17:01:50.027656: step 54520, loss = 0.76 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 17:01:54.724989: step 54530, loss = 0.74 (266.3 examples/sec; 0.481 sec/batch)
2016-02-03 17:01:59.376728: step 54540, loss = 0.76 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 17:02:04.062680: step 54550, loss = 0.75 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 17:02:08.707011: step 54560, loss = 0.75 (296.3 examples/sec; 0.432 sec/batch)
2016-02-03 17:02:13.425976: step 54570, loss = 0.70 (260.3 examples/sec; 0.492 sec/batch)
2016-02-03 17:02:18.198024: step 54580, loss = 0.82 (255.1 examples/sec; 0.502 sec/batch)
2016-02-03 17:02:22.810334: step 54590, loss = 0.77 (311.7 examples/sec; 0.411 sec/batch)
2016-02-03 17:02:27.471753: step 54600, loss = 0.65 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 17:02:32.688168: step 54610, loss = 0.72 (263.9 examples/sec; 0.485 sec/batch)
2016-02-03 17:02:37.398289: step 54620, loss = 0.70 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 17:02:42.116206: step 54630, loss = 0.81 (270.5 examples/sec; 0.473 sec/batch)
2016-02-03 17:02:46.839253: step 54640, loss = 0.68 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 17:02:51.569865: step 54650, loss = 0.75 (258.2 examples/sec; 0.496 sec/batch)
2016-02-03 17:02:56.271743: step 54660, loss = 0.71 (269.2 examples/sec; 0.476 sec/batch)
2016-02-03 17:03:00.985172: step 54670, loss = 0.93 (283.8 examples/sec; 0.451 sec/batch)
2016-02-03 17:03:05.718491: step 54680, loss = 0.77 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 17:03:10.425005: step 54690, loss = 0.72 (264.2 examples/sec; 0.485 sec/batch)
2016-02-03 17:03:15.090394: step 54700, loss = 0.66 (276.8 examples/sec; 0.463 sec/batch)
2016-02-03 17:03:20.306090: step 54710, loss = 0.64 (279.4 examples/sec; 0.458 sec/batch)
2016-02-03 17:03:24.999225: step 54720, loss = 0.62 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 17:03:29.743382: step 54730, loss = 0.79 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 17:03:34.528132: step 54740, loss = 0.85 (255.3 examples/sec; 0.501 sec/batch)
2016-02-03 17:03:39.130439: step 54750, loss = 0.77 (269.2 examples/sec; 0.475 sec/batch)
2016-02-03 17:03:43.860770: step 54760, loss = 0.78 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 17:03:48.593692: step 54770, loss = 0.70 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 17:03:53.344465: step 54780, loss = 0.84 (254.1 examples/sec; 0.504 sec/batch)
2016-02-03 17:03:58.051410: step 54790, loss = 0.74 (282.5 examples/sec; 0.453 sec/batch)
2016-02-03 17:04:02.763087: step 54800, loss = 0.75 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 17:04:07.956859: step 54810, loss = 0.74 (253.0 examples/sec; 0.506 sec/batch)
2016-02-03 17:04:12.713159: step 54820, loss = 0.73 (266.4 examples/sec; 0.480 sec/batch)
2016-02-03 17:04:17.456305: step 54830, loss = 0.76 (307.4 examples/sec; 0.416 sec/batch)
2016-02-03 17:04:22.174753: step 54840, loss = 0.80 (257.5 examples/sec; 0.497 sec/batch)
2016-02-03 17:04:26.914418: step 54850, loss = 0.87 (294.0 examples/sec; 0.435 sec/batch)
2016-02-03 17:04:31.600946: step 54860, loss = 0.74 (248.6 examples/sec; 0.515 sec/batch)
2016-02-03 17:04:36.267865: step 54870, loss = 0.72 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 17:04:41.074984: step 54880, loss = 0.63 (257.8 examples/sec; 0.496 sec/batch)
2016-02-03 17:04:45.783076: step 54890, loss = 0.80 (253.5 examples/sec; 0.505 sec/batch)
2016-02-03 17:04:50.464471: step 54900, loss = 0.87 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 17:04:55.721781: step 54910, loss = 0.77 (267.3 examples/sec; 0.479 sec/batch)
2016-02-03 17:05:00.351604: step 54920, loss = 0.66 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 17:05:05.049104: step 54930, loss = 0.86 (265.2 examples/sec; 0.483 sec/batch)
2016-02-03 17:05:09.743206: step 54940, loss = 0.78 (263.1 examples/sec; 0.486 sec/batch)
2016-02-03 17:05:14.475640: step 54950, loss = 0.74 (279.6 examples/sec; 0.458 sec/batch)
2016-02-03 17:05:19.144920: step 54960, loss = 0.71 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 17:05:23.790678: step 54970, loss = 0.75 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 17:05:28.558329: step 54980, loss = 0.75 (254.8 examples/sec; 0.502 sec/batch)
2016-02-03 17:05:33.257921: step 54990, loss = 0.87 (290.5 examples/sec; 0.441 sec/batch)
2016-02-03 17:05:37.962891: step 55000, loss = 0.88 (254.4 examples/sec; 0.503 sec/batch)
2016-02-03 17:05:43.095580: step 55010, loss = 0.69 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 17:05:47.738376: step 55020, loss = 0.73 (256.8 examples/sec; 0.498 sec/batch)
2016-02-03 17:05:52.469367: step 55030, loss = 0.75 (250.5 examples/sec; 0.511 sec/batch)
2016-02-03 17:05:57.233171: step 55040, loss = 0.63 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 17:06:02.008020: step 55050, loss = 0.72 (268.6 examples/sec; 0.476 sec/batch)
2016-02-03 17:06:06.704320: step 55060, loss = 0.80 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 17:06:11.476375: step 55070, loss = 0.67 (283.9 examples/sec; 0.451 sec/batch)
2016-02-03 17:06:16.168582: step 55080, loss = 0.68 (287.3 examples/sec; 0.445 sec/batch)
2016-02-03 17:06:20.873001: step 55090, loss = 0.81 (255.8 examples/sec; 0.500 sec/batch)
2016-02-03 17:06:25.586386: step 55100, loss = 0.70 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 17:06:30.752001: step 55110, loss = 0.65 (285.0 examples/sec; 0.449 sec/batch)
2016-02-03 17:06:35.492497: step 55120, loss = 0.73 (258.3 examples/sec; 0.496 sec/batch)
2016-02-03 17:06:40.204699: step 55130, loss = 0.74 (291.5 examples/sec; 0.439 sec/batch)
2016-02-03 17:06:44.941654: step 55140, loss = 0.84 (296.5 examples/sec; 0.432 sec/batch)
2016-02-03 17:06:49.622220: step 55150, loss = 0.72 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 17:06:54.308919: step 55160, loss = 0.70 (256.3 examples/sec; 0.499 sec/batch)
2016-02-03 17:06:59.045191: step 55170, loss = 0.77 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 17:07:03.768750: step 55180, loss = 0.67 (249.6 examples/sec; 0.513 sec/batch)
2016-02-03 17:07:08.491960: step 55190, loss = 0.65 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 17:07:13.260008: step 55200, loss = 0.75 (257.6 examples/sec; 0.497 sec/batch)
2016-02-03 17:07:18.484661: step 55210, loss = 0.88 (283.3 examples/sec; 0.452 sec/batch)
2016-02-03 17:07:23.316369: step 55220, loss = 0.72 (254.7 examples/sec; 0.503 sec/batch)
2016-02-03 17:07:28.019791: step 55230, loss = 0.88 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 17:07:32.792945: step 55240, loss = 0.83 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 17:07:37.520990: step 55250, loss = 0.65 (299.2 examples/sec; 0.428 sec/batch)
2016-02-03 17:07:42.156182: step 55260, loss = 0.83 (288.2 examples/sec; 0.444 sec/batch)
2016-02-03 17:07:46.880051: step 55270, loss = 0.72 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 17:07:51.618392: step 55280, loss = 0.62 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 17:07:56.345118: step 55290, loss = 0.55 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 17:08:01.115454: step 55300, loss = 0.62 (266.3 examples/sec; 0.481 sec/batch)
2016-02-03 17:08:06.326138: step 55310, loss = 0.61 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 17:08:11.103613: step 55320, loss = 0.89 (253.3 examples/sec; 0.505 sec/batch)
2016-02-03 17:08:15.733189: step 55330, loss = 0.94 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 17:08:20.550707: step 55340, loss = 0.74 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 17:08:25.231629: step 55350, loss = 0.89 (297.2 examples/sec; 0.431 sec/batch)
2016-02-03 17:08:29.995776: step 55360, loss = 0.81 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 17:08:34.722430: step 55370, loss = 0.76 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 17:08:39.477221: step 55380, loss = 0.86 (283.9 examples/sec; 0.451 sec/batch)
2016-02-03 17:08:44.224851: step 55390, loss = 0.64 (260.6 examples/sec; 0.491 sec/batch)
2016-02-03 17:08:48.892767: step 55400, loss = 0.68 (290.0 examples/sec; 0.441 sec/batch)
2016-02-03 17:08:54.231446: step 55410, loss = 0.65 (267.0 examples/sec; 0.479 sec/batch)
2016-02-03 17:08:58.914286: step 55420, loss = 0.87 (289.6 examples/sec; 0.442 sec/batch)
2016-02-03 17:09:03.529114: step 55430, loss = 0.73 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 17:09:08.252003: step 55440, loss = 0.69 (252.1 examples/sec; 0.508 sec/batch)
2016-02-03 17:09:12.910178: step 55450, loss = 0.67 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 17:09:17.626453: step 55460, loss = 0.74 (291.2 examples/sec; 0.440 sec/batch)
2016-02-03 17:09:22.365996: step 55470, loss = 0.81 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 17:09:27.080196: step 55480, loss = 0.86 (270.9 examples/sec; 0.472 sec/batch)
2016-02-03 17:09:31.785015: step 55490, loss = 0.81 (253.1 examples/sec; 0.506 sec/batch)
2016-02-03 17:09:36.490156: step 55500, loss = 0.84 (259.4 examples/sec; 0.493 sec/batch)
2016-02-03 17:09:41.717695: step 55510, loss = 1.01 (252.9 examples/sec; 0.506 sec/batch)
2016-02-03 17:09:46.361717: step 55520, loss = 0.83 (276.6 examples/sec; 0.463 sec/batch)
2016-02-03 17:09:51.100246: step 55530, loss = 0.79 (259.2 examples/sec; 0.494 sec/batch)
2016-02-03 17:09:55.762494: step 55540, loss = 0.71 (296.2 examples/sec; 0.432 sec/batch)
2016-02-03 17:10:00.492619: step 55550, loss = 0.93 (282.4 examples/sec; 0.453 sec/batch)
2016-02-03 17:10:05.305395: step 55560, loss = 0.70 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 17:10:10.068019: step 55570, loss = 0.94 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 17:10:14.716552: step 55580, loss = 0.70 (289.3 examples/sec; 0.442 sec/batch)
2016-02-03 17:10:19.380929: step 55590, loss = 0.81 (282.4 examples/sec; 0.453 sec/batch)
2016-02-03 17:10:24.069842: step 55600, loss = 0.84 (298.8 examples/sec; 0.428 sec/batch)
2016-02-03 17:10:29.261563: step 55610, loss = 0.79 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 17:10:33.980064: step 55620, loss = 0.62 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 17:10:38.567987: step 55630, loss = 0.76 (283.8 examples/sec; 0.451 sec/batch)
2016-02-03 17:10:43.292023: step 55640, loss = 0.76 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 17:10:48.008578: step 55650, loss = 0.65 (246.5 examples/sec; 0.519 sec/batch)
2016-02-03 17:10:52.718671: step 55660, loss = 0.79 (267.5 examples/sec; 0.479 sec/batch)
2016-02-03 17:10:57.421368: step 55670, loss = 0.80 (262.6 examples/sec; 0.487 sec/batch)
2016-02-03 17:11:02.055672: step 55680, loss = 0.81 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 17:11:06.774708: step 55690, loss = 0.69 (262.9 examples/sec; 0.487 sec/batch)
2016-02-03 17:11:11.458617: step 55700, loss = 0.72 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 17:11:16.597006: step 55710, loss = 0.86 (299.0 examples/sec; 0.428 sec/batch)
2016-02-03 17:11:21.326552: step 55720, loss = 0.68 (259.5 examples/sec; 0.493 sec/batch)
2016-02-03 17:11:25.982660: step 55730, loss = 0.82 (283.5 examples/sec; 0.451 sec/batch)
2016-02-03 17:11:30.631042: step 55740, loss = 0.78 (282.8 examples/sec; 0.453 sec/batch)
2016-02-03 17:11:35.303129: step 55750, loss = 0.73 (263.1 examples/sec; 0.486 sec/batch)
2016-02-03 17:11:40.085077: step 55760, loss = 0.56 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 17:11:44.798294: step 55770, loss = 0.93 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 17:11:49.498485: step 55780, loss = 0.79 (262.9 examples/sec; 0.487 sec/batch)
2016-02-03 17:11:54.214364: step 55790, loss = 0.68 (292.2 examples/sec; 0.438 sec/batch)
2016-02-03 17:11:58.957792: step 55800, loss = 0.79 (249.1 examples/sec; 0.514 sec/batch)
2016-02-03 17:12:04.123820: step 55810, loss = 0.84 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 17:12:08.861421: step 55820, loss = 0.98 (270.9 examples/sec; 0.473 sec/batch)
2016-02-03 17:12:13.603575: step 55830, loss = 0.49 (285.0 examples/sec; 0.449 sec/batch)
2016-02-03 17:12:18.239609: step 55840, loss = 0.78 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 17:12:22.893172: step 55850, loss = 0.80 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 17:12:27.592121: step 55860, loss = 0.79 (255.9 examples/sec; 0.500 sec/batch)
2016-02-03 17:12:32.319672: step 55870, loss = 0.76 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 17:12:37.049467: step 55880, loss = 0.98 (277.2 examples/sec; 0.462 sec/batch)
2016-02-03 17:12:41.838473: step 55890, loss = 0.74 (281.4 examples/sec; 0.455 sec/batch)
2016-02-03 17:12:46.588084: step 55900, loss = 0.59 (259.4 examples/sec; 0.493 sec/batch)
2016-02-03 17:12:51.850800: step 55910, loss = 0.86 (265.8 examples/sec; 0.482 sec/batch)
2016-02-03 17:12:56.556096: step 55920, loss = 0.85 (265.0 examples/sec; 0.483 sec/batch)
2016-02-03 17:13:01.282410: step 55930, loss = 0.78 (260.5 examples/sec; 0.491 sec/batch)
2016-02-03 17:13:05.941371: step 55940, loss = 0.62 (303.7 examples/sec; 0.421 sec/batch)
2016-02-03 17:13:10.605041: step 55950, loss = 0.64 (310.0 examples/sec; 0.413 sec/batch)
2016-02-03 17:13:15.365781: step 55960, loss = 0.71 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 17:13:20.060504: step 55970, loss = 1.00 (286.8 examples/sec; 0.446 sec/batch)
2016-02-03 17:13:24.716902: step 55980, loss = 0.71 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 17:13:29.489883: step 55990, loss = 0.69 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 17:13:34.300971: step 56000, loss = 0.80 (259.0 examples/sec; 0.494 sec/batch)
2016-02-03 17:13:39.459375: step 56010, loss = 0.69 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 17:13:44.180003: step 56020, loss = 0.67 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 17:13:48.903183: step 56030, loss = 0.76 (267.5 examples/sec; 0.478 sec/batch)
2016-02-03 17:13:53.600539: step 56040, loss = 0.68 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 17:13:58.336445: step 56050, loss = 0.80 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 17:14:03.112640: step 56060, loss = 0.60 (266.9 examples/sec; 0.480 sec/batch)
2016-02-03 17:14:07.819014: step 56070, loss = 0.83 (255.1 examples/sec; 0.502 sec/batch)
2016-02-03 17:14:12.491364: step 56080, loss = 0.79 (299.1 examples/sec; 0.428 sec/batch)
2016-02-03 17:14:17.251899: step 56090, loss = 1.01 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 17:14:21.936394: step 56100, loss = 0.73 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 17:14:27.150190: step 56110, loss = 0.74 (259.2 examples/sec; 0.494 sec/batch)
2016-02-03 17:14:31.872040: step 56120, loss = 0.74 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 17:14:36.595713: step 56130, loss = 0.68 (299.5 examples/sec; 0.427 sec/batch)
2016-02-03 17:14:41.352634: step 56140, loss = 0.83 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 17:14:46.103161: step 56150, loss = 0.83 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 17:14:50.753733: step 56160, loss = 0.76 (281.7 examples/sec; 0.454 sec/batch)
2016-02-03 17:14:55.428751: step 56170, loss = 0.93 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 17:15:00.121609: step 56180, loss = 0.70 (299.7 examples/sec; 0.427 sec/batch)
2016-02-03 17:15:04.763475: step 56190, loss = 0.65 (293.1 examples/sec; 0.437 sec/batch)
2016-02-03 17:15:09.538416: step 56200, loss = 0.65 (253.4 examples/sec; 0.505 sec/batch)
2016-02-03 17:15:14.701872: step 56210, loss = 0.64 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 17:15:19.419718: step 56220, loss = 0.72 (259.5 examples/sec; 0.493 sec/batch)
2016-02-03 17:15:24.153803: step 56230, loss = 0.74 (268.1 examples/sec; 0.478 sec/batch)
2016-02-03 17:15:28.898575: step 56240, loss = 0.81 (250.4 examples/sec; 0.511 sec/batch)
2016-02-03 17:15:33.562662: step 56250, loss = 0.73 (289.2 examples/sec; 0.443 sec/batch)
2016-02-03 17:15:38.309113: step 56260, loss = 0.62 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 17:15:43.073630: step 56270, loss = 0.72 (277.8 examples/sec; 0.461 sec/batch)
2016-02-03 17:15:47.752240: step 56280, loss = 0.72 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 17:15:52.440769: step 56290, loss = 0.80 (292.8 examples/sec; 0.437 sec/batch)
2016-02-03 17:15:57.127964: step 56300, loss = 0.57 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 17:16:02.346229: step 56310, loss = 0.80 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 17:16:06.959467: step 56320, loss = 0.78 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 17:16:11.587358: step 56330, loss = 0.77 (289.5 examples/sec; 0.442 sec/batch)
2016-02-03 17:16:16.266060: step 56340, loss = 0.90 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 17:16:21.020373: step 56350, loss = 0.70 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 17:16:25.814748: step 56360, loss = 0.60 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 17:16:30.537274: step 56370, loss = 0.79 (289.4 examples/sec; 0.442 sec/batch)
2016-02-03 17:16:35.293671: step 56380, loss = 0.61 (261.3 examples/sec; 0.490 sec/batch)
2016-02-03 17:16:39.948648: step 56390, loss = 0.81 (292.3 examples/sec; 0.438 sec/batch)
2016-02-03 17:16:44.670384: step 56400, loss = 0.72 (281.7 examples/sec; 0.454 sec/batch)
2016-02-03 17:16:49.903336: step 56410, loss = 0.73 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 17:16:54.555052: step 56420, loss = 0.86 (263.1 examples/sec; 0.486 sec/batch)
2016-02-03 17:16:59.241153: step 56430, loss = 0.65 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 17:17:03.896029: step 56440, loss = 0.82 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 17:17:08.594174: step 56450, loss = 0.80 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 17:17:13.303808: step 56460, loss = 0.67 (288.2 examples/sec; 0.444 sec/batch)
2016-02-03 17:17:18.119201: step 56470, loss = 0.74 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 17:17:22.734708: step 56480, loss = 0.73 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 17:17:27.431322: step 56490, loss = 0.70 (263.6 examples/sec; 0.486 sec/batch)
2016-02-03 17:17:32.076844: step 56500, loss = 0.73 (279.2 examples/sec; 0.458 sec/batch)
2016-02-03 17:17:37.349505: step 56510, loss = 0.69 (261.4 examples/sec; 0.490 sec/batch)
2016-02-03 17:17:42.091586: step 56520, loss = 0.81 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 17:17:46.944446: step 56530, loss = 0.75 (232.4 examples/sec; 0.551 sec/batch)
2016-02-03 17:17:51.609215: step 56540, loss = 0.76 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 17:17:56.305820: step 56550, loss = 0.64 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 17:18:01.014232: step 56560, loss = 0.71 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 17:18:05.790424: step 56570, loss = 0.72 (243.1 examples/sec; 0.526 sec/batch)
2016-02-03 17:18:10.461108: step 56580, loss = 0.80 (285.0 examples/sec; 0.449 sec/batch)
2016-02-03 17:18:15.272592: step 56590, loss = 0.72 (235.5 examples/sec; 0.543 sec/batch)
2016-02-03 17:18:19.919429: step 56600, loss = 0.73 (275.6 examples/sec; 0.464 sec/batch)
2016-02-03 17:18:25.190153: step 56610, loss = 0.74 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 17:18:29.939365: step 56620, loss = 0.80 (255.0 examples/sec; 0.502 sec/batch)
2016-02-03 17:18:34.625402: step 56630, loss = 0.72 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 17:18:39.278074: step 56640, loss = 0.86 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 17:18:43.962556: step 56650, loss = 0.60 (253.0 examples/sec; 0.506 sec/batch)
2016-02-03 17:18:48.711018: step 56660, loss = 0.69 (276.3 examples/sec; 0.463 sec/batch)
2016-02-03 17:18:53.386940: step 56670, loss = 0.76 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 17:18:58.069882: step 56680, loss = 0.65 (295.2 examples/sec; 0.434 sec/batch)
2016-02-03 17:19:02.818030: step 56690, loss = 0.82 (256.8 examples/sec; 0.498 sec/batch)
2016-02-03 17:19:07.471306: step 56700, loss = 0.76 (281.4 examples/sec; 0.455 sec/batch)
2016-02-03 17:19:12.624115: step 56710, loss = 0.64 (296.5 examples/sec; 0.432 sec/batch)
2016-02-03 17:19:17.351499: step 56720, loss = 0.85 (249.0 examples/sec; 0.514 sec/batch)
2016-02-03 17:19:22.021320: step 56730, loss = 0.77 (286.6 examples/sec; 0.447 sec/batch)
2016-02-03 17:19:26.706886: step 56740, loss = 0.63 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 17:19:31.448408: step 56750, loss = 0.93 (284.7 examples/sec; 0.450 sec/batch)
2016-02-03 17:19:36.167013: step 56760, loss = 0.64 (253.9 examples/sec; 0.504 sec/batch)
2016-02-03 17:19:40.844355: step 56770, loss = 0.75 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 17:19:45.497443: step 56780, loss = 0.50 (259.3 examples/sec; 0.494 sec/batch)
2016-02-03 17:19:50.196769: step 56790, loss = 0.85 (264.1 examples/sec; 0.485 sec/batch)
2016-02-03 17:19:54.924024: step 56800, loss = 0.87 (268.6 examples/sec; 0.476 sec/batch)
2016-02-03 17:20:00.114726: step 56810, loss = 0.72 (257.0 examples/sec; 0.498 sec/batch)
2016-02-03 17:20:04.792752: step 56820, loss = 0.67 (261.7 examples/sec; 0.489 sec/batch)
2016-02-03 17:20:09.476436: step 56830, loss = 0.79 (253.1 examples/sec; 0.506 sec/batch)
2016-02-03 17:20:14.046714: step 56840, loss = 0.61 (286.5 examples/sec; 0.447 sec/batch)
2016-02-03 17:20:18.687966: step 56850, loss = 0.77 (305.2 examples/sec; 0.419 sec/batch)
2016-02-03 17:20:23.331507: step 56860, loss = 0.76 (255.0 examples/sec; 0.502 sec/batch)
2016-02-03 17:20:28.004699: step 56870, loss = 0.60 (290.6 examples/sec; 0.440 sec/batch)
2016-02-03 17:20:32.687926: step 56880, loss = 0.69 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 17:20:37.397413: step 56890, loss = 0.73 (260.7 examples/sec; 0.491 sec/batch)
2016-02-03 17:20:42.118859: step 56900, loss = 0.69 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 17:20:47.331837: step 56910, loss = 0.80 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 17:20:51.990481: step 56920, loss = 0.68 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 17:20:56.666236: step 56930, loss = 0.95 (286.2 examples/sec; 0.447 sec/batch)
2016-02-03 17:21:01.395569: step 56940, loss = 0.70 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 17:21:06.067396: step 56950, loss = 0.68 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 17:21:10.700123: step 56960, loss = 0.76 (299.6 examples/sec; 0.427 sec/batch)
2016-02-03 17:21:15.356462: step 56970, loss = 0.66 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 17:21:20.006977: step 56980, loss = 0.88 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 17:21:24.691208: step 56990, loss = 0.92 (296.4 examples/sec; 0.432 sec/batch)
2016-02-03 17:21:29.377616: step 57000, loss = 0.86 (246.3 examples/sec; 0.520 sec/batch)
2016-02-03 17:21:34.535975: step 57010, loss = 0.61 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 17:21:39.254854: step 57020, loss = 0.72 (285.2 examples/sec; 0.449 sec/batch)
2016-02-03 17:21:44.000632: step 57030, loss = 0.87 (247.5 examples/sec; 0.517 sec/batch)
2016-02-03 17:21:48.630517: step 57040, loss = 0.75 (249.5 examples/sec; 0.513 sec/batch)
2016-02-03 17:21:53.297553: step 57050, loss = 0.89 (266.9 examples/sec; 0.480 sec/batch)
2016-02-03 17:21:58.034956: step 57060, loss = 0.77 (259.3 examples/sec; 0.494 sec/batch)
2016-02-03 17:22:02.734641: step 57070, loss = 0.77 (257.0 examples/sec; 0.498 sec/batch)
2016-02-03 17:22:07.373602: step 57080, loss = 0.78 (296.3 examples/sec; 0.432 sec/batch)
2016-02-03 17:22:12.057702: step 57090, loss = 0.73 (298.5 examples/sec; 0.429 sec/batch)
2016-02-03 17:22:16.646425: step 57100, loss = 0.77 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 17:22:21.812203: step 57110, loss = 0.83 (292.5 examples/sec; 0.438 sec/batch)
2016-02-03 17:22:26.418279: step 57120, loss = 0.76 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 17:22:30.997409: step 57130, loss = 0.73 (302.2 examples/sec; 0.424 sec/batch)
2016-02-03 17:22:35.741950: step 57140, loss = 0.63 (256.6 examples/sec; 0.499 sec/batch)
2016-02-03 17:22:40.290871: step 57150, loss = 0.80 (276.9 examples/sec; 0.462 sec/batch)
2016-02-03 17:22:45.032098: step 57160, loss = 0.81 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 17:22:49.709811: step 57170, loss = 0.70 (299.2 examples/sec; 0.428 sec/batch)
2016-02-03 17:22:54.416522: step 57180, loss = 0.78 (287.9 examples/sec; 0.445 sec/batch)
2016-02-03 17:22:59.131589: step 57190, loss = 0.61 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 17:23:03.763884: step 57200, loss = 0.69 (284.7 examples/sec; 0.450 sec/batch)
2016-02-03 17:23:08.942851: step 57210, loss = 0.70 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 17:23:13.633982: step 57220, loss = 0.78 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 17:23:18.336164: step 57230, loss = 0.68 (277.6 examples/sec; 0.461 sec/batch)
2016-02-03 17:23:23.093984: step 57240, loss = 0.74 (252.2 examples/sec; 0.508 sec/batch)
2016-02-03 17:23:27.713164: step 57250, loss = 0.73 (265.8 examples/sec; 0.482 sec/batch)
2016-02-03 17:23:32.462017: step 57260, loss = 0.62 (283.6 examples/sec; 0.451 sec/batch)
2016-02-03 17:23:37.215647: step 57270, loss = 0.66 (254.8 examples/sec; 0.502 sec/batch)
2016-02-03 17:23:41.965969: step 57280, loss = 0.73 (275.0 examples/sec; 0.465 sec/batch)
2016-02-03 17:23:46.722101: step 57290, loss = 0.78 (254.5 examples/sec; 0.503 sec/batch)
2016-02-03 17:23:51.387622: step 57300, loss = 0.82 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 17:23:56.592584: step 57310, loss = 0.72 (289.7 examples/sec; 0.442 sec/batch)
2016-02-03 17:24:01.227683: step 57320, loss = 0.91 (250.1 examples/sec; 0.512 sec/batch)
2016-02-03 17:24:05.965201: step 57330, loss = 0.83 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 17:24:10.595598: step 57340, loss = 0.68 (269.3 examples/sec; 0.475 sec/batch)
2016-02-03 17:24:15.301238: step 57350, loss = 0.75 (262.6 examples/sec; 0.488 sec/batch)
2016-02-03 17:24:20.009506: step 57360, loss = 0.61 (258.9 examples/sec; 0.494 sec/batch)
2016-02-03 17:24:24.685901: step 57370, loss = 0.75 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 17:24:29.325589: step 57380, loss = 0.66 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 17:24:34.095353: step 57390, loss = 0.80 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 17:24:38.756523: step 57400, loss = 0.62 (285.2 examples/sec; 0.449 sec/batch)
2016-02-03 17:24:43.983747: step 57410, loss = 0.67 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 17:24:48.708705: step 57420, loss = 0.89 (283.4 examples/sec; 0.452 sec/batch)
2016-02-03 17:24:53.384869: step 57430, loss = 0.67 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 17:24:58.073144: step 57440, loss = 0.64 (304.0 examples/sec; 0.421 sec/batch)
2016-02-03 17:25:02.765535: step 57450, loss = 0.96 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 17:25:07.497446: step 57460, loss = 0.73 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 17:25:12.214693: step 57470, loss = 0.82 (306.5 examples/sec; 0.418 sec/batch)
2016-02-03 17:25:16.970802: step 57480, loss = 0.87 (255.2 examples/sec; 0.502 sec/batch)
2016-02-03 17:25:21.713118: step 57490, loss = 0.83 (290.8 examples/sec; 0.440 sec/batch)
2016-02-03 17:25:26.412296: step 57500, loss = 0.75 (254.4 examples/sec; 0.503 sec/batch)
2016-02-03 17:25:31.569793: step 57510, loss = 0.88 (296.6 examples/sec; 0.432 sec/batch)
2016-02-03 17:25:36.234507: step 57520, loss = 0.79 (308.4 examples/sec; 0.415 sec/batch)
2016-02-03 17:25:40.978634: step 57530, loss = 0.60 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 17:25:45.660253: step 57540, loss = 0.79 (303.9 examples/sec; 0.421 sec/batch)
2016-02-03 17:25:50.386655: step 57550, loss = 0.82 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 17:25:55.081999: step 57560, loss = 0.83 (251.0 examples/sec; 0.510 sec/batch)
2016-02-03 17:25:59.655714: step 57570, loss = 0.70 (279.8 examples/sec; 0.457 sec/batch)
2016-02-03 17:26:04.378532: step 57580, loss = 0.67 (252.6 examples/sec; 0.507 sec/batch)
2016-02-03 17:26:09.082050: step 57590, loss = 0.68 (302.2 examples/sec; 0.424 sec/batch)
2016-02-03 17:26:13.782283: step 57600, loss = 0.78 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 17:26:18.988344: step 57610, loss = 0.77 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 17:26:23.605241: step 57620, loss = 0.77 (304.3 examples/sec; 0.421 sec/batch)
2016-02-03 17:26:28.404394: step 57630, loss = 0.65 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 17:26:33.198218: step 57640, loss = 0.67 (258.3 examples/sec; 0.495 sec/batch)
2016-02-03 17:26:37.866706: step 57650, loss = 0.76 (276.6 examples/sec; 0.463 sec/batch)
2016-02-03 17:26:42.589675: step 57660, loss = 0.67 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 17:26:47.331957: step 57670, loss = 0.80 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 17:26:52.004499: step 57680, loss = 0.76 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 17:26:56.695209: step 57690, loss = 0.56 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 17:27:01.403827: step 57700, loss = 0.87 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 17:27:06.652027: step 57710, loss = 0.84 (289.4 examples/sec; 0.442 sec/batch)
2016-02-03 17:27:11.359483: step 57720, loss = 0.84 (267.5 examples/sec; 0.478 sec/batch)
2016-02-03 17:27:16.135631: step 57730, loss = 0.75 (245.6 examples/sec; 0.521 sec/batch)
2016-02-03 17:27:20.886583: step 57740, loss = 0.81 (259.5 examples/sec; 0.493 sec/batch)
2016-02-03 17:27:25.615953: step 57750, loss = 0.70 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 17:27:30.406530: step 57760, loss = 0.75 (254.9 examples/sec; 0.502 sec/batch)
2016-02-03 17:27:35.036457: step 57770, loss = 0.81 (281.7 examples/sec; 0.454 sec/batch)
2016-02-03 17:27:39.772399: step 57780, loss = 0.68 (287.5 examples/sec; 0.445 sec/batch)
2016-02-03 17:27:44.499508: step 57790, loss = 0.69 (263.1 examples/sec; 0.486 sec/batch)
2016-02-03 17:27:49.214722: step 57800, loss = 0.66 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 17:27:54.433698: step 57810, loss = 0.58 (264.7 examples/sec; 0.484 sec/batch)
2016-02-03 17:27:59.131139: step 57820, loss = 0.79 (259.9 examples/sec; 0.492 sec/batch)
2016-02-03 17:28:03.879286: step 57830, loss = 0.64 (256.7 examples/sec; 0.499 sec/batch)
2016-02-03 17:28:08.657804: step 57840, loss = 0.69 (262.2 examples/sec; 0.488 sec/batch)
2016-02-03 17:28:13.300639: step 57850, loss = 0.87 (283.0 examples/sec; 0.452 sec/batch)
2016-02-03 17:28:18.033176: step 57860, loss = 0.76 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 17:28:22.722550: step 57870, loss = 0.63 (284.4 examples/sec; 0.450 sec/batch)
2016-02-03 17:28:27.470118: step 57880, loss = 0.60 (267.5 examples/sec; 0.478 sec/batch)
2016-02-03 17:28:32.200855: step 57890, loss = 0.69 (265.2 examples/sec; 0.483 sec/batch)
2016-02-03 17:28:36.993715: step 57900, loss = 0.66 (248.0 examples/sec; 0.516 sec/batch)
2016-02-03 17:28:42.330915: step 57910, loss = 0.82 (269.2 examples/sec; 0.476 sec/batch)
2016-02-03 17:28:47.080702: step 57920, loss = 0.63 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 17:28:51.814192: step 57930, loss = 0.64 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 17:28:56.561728: step 57940, loss = 0.79 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 17:29:01.287043: step 57950, loss = 0.69 (259.3 examples/sec; 0.494 sec/batch)
2016-02-03 17:29:05.984637: step 57960, loss = 0.73 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 17:29:10.688488: step 57970, loss = 0.81 (251.0 examples/sec; 0.510 sec/batch)
2016-02-03 17:29:15.401660: step 57980, loss = 0.89 (254.1 examples/sec; 0.504 sec/batch)
2016-02-03 17:29:20.134163: step 57990, loss = 0.81 (248.6 examples/sec; 0.515 sec/batch)
2016-02-03 17:29:24.804004: step 58000, loss = 0.83 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 17:29:30.059840: step 58010, loss = 0.63 (283.1 examples/sec; 0.452 sec/batch)
2016-02-03 17:29:34.690553: step 58020, loss = 0.66 (310.5 examples/sec; 0.412 sec/batch)
2016-02-03 17:29:39.346815: step 58030, loss = 0.70 (273.2 examples/sec; 0.468 sec/batch)
2016-02-03 17:29:44.021529: step 58040, loss = 0.73 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 17:29:48.715786: step 58050, loss = 0.73 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 17:29:53.434005: step 58060, loss = 0.74 (258.2 examples/sec; 0.496 sec/batch)
2016-02-03 17:29:58.138780: step 58070, loss = 0.68 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 17:30:02.778434: step 58080, loss = 0.67 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 17:30:07.529518: step 58090, loss = 0.76 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 17:30:12.276814: step 58100, loss = 0.79 (244.8 examples/sec; 0.523 sec/batch)
2016-02-03 17:30:17.436557: step 58110, loss = 0.83 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 17:30:22.214101: step 58120, loss = 0.69 (260.4 examples/sec; 0.491 sec/batch)
2016-02-03 17:30:27.028706: step 58130, loss = 0.81 (240.8 examples/sec; 0.531 sec/batch)
2016-02-03 17:30:31.652935: step 58140, loss = 0.65 (299.5 examples/sec; 0.427 sec/batch)
2016-02-03 17:30:36.391929: step 58150, loss = 0.65 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 17:30:41.097739: step 58160, loss = 0.84 (276.9 examples/sec; 0.462 sec/batch)
2016-02-03 17:30:45.759998: step 58170, loss = 0.81 (304.5 examples/sec; 0.420 sec/batch)
2016-02-03 17:30:50.360451: step 58180, loss = 0.66 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 17:30:55.088484: step 58190, loss = 0.76 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 17:30:59.716054: step 58200, loss = 0.57 (290.9 examples/sec; 0.440 sec/batch)
2016-02-03 17:31:04.925200: step 58210, loss = 0.75 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 17:31:09.595675: step 58220, loss = 0.78 (287.9 examples/sec; 0.445 sec/batch)
2016-02-03 17:31:14.232517: step 58230, loss = 0.82 (316.5 examples/sec; 0.404 sec/batch)
2016-02-03 17:31:19.066161: step 58240, loss = 0.81 (245.2 examples/sec; 0.522 sec/batch)
2016-02-03 17:31:23.820304: step 58250, loss = 0.70 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 17:31:28.520878: step 58260, loss = 0.63 (258.7 examples/sec; 0.495 sec/batch)
2016-02-03 17:31:33.230394: step 58270, loss = 0.68 (285.7 examples/sec; 0.448 sec/batch)
2016-02-03 17:31:38.000612: step 58280, loss = 0.92 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 17:31:42.577338: step 58290, loss = 0.65 (311.1 examples/sec; 0.411 sec/batch)
2016-02-03 17:31:47.242072: step 58300, loss = 0.72 (254.8 examples/sec; 0.502 sec/batch)
2016-02-03 17:31:52.460783: step 58310, loss = 0.93 (255.2 examples/sec; 0.501 sec/batch)
2016-02-03 17:31:57.191751: step 58320, loss = 0.78 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 17:32:01.967556: step 58330, loss = 0.80 (258.8 examples/sec; 0.495 sec/batch)
2016-02-03 17:32:06.634622: step 58340, loss = 0.66 (290.0 examples/sec; 0.441 sec/batch)
2016-02-03 17:32:11.439991: step 58350, loss = 0.74 (275.6 examples/sec; 0.464 sec/batch)
2016-02-03 17:32:16.155546: step 58360, loss = 0.71 (260.7 examples/sec; 0.491 sec/batch)
2016-02-03 17:32:20.808154: step 58370, loss = 0.67 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 17:32:25.452537: step 58380, loss = 0.71 (312.9 examples/sec; 0.409 sec/batch)
2016-02-03 17:32:30.147745: step 58390, loss = 0.78 (251.6 examples/sec; 0.509 sec/batch)
2016-02-03 17:32:34.864879: step 58400, loss = 0.78 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 17:32:39.996609: step 58410, loss = 0.84 (306.5 examples/sec; 0.418 sec/batch)
2016-02-03 17:32:44.662854: step 58420, loss = 0.75 (301.2 examples/sec; 0.425 sec/batch)
2016-02-03 17:32:49.390793: step 58430, loss = 0.79 (252.3 examples/sec; 0.507 sec/batch)
2016-02-03 17:32:54.049441: step 58440, loss = 0.71 (280.3 examples/sec; 0.457 sec/batch)
2016-02-03 17:32:58.753255: step 58450, loss = 0.63 (304.7 examples/sec; 0.420 sec/batch)
2016-02-03 17:33:03.543200: step 58460, loss = 0.63 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 17:33:08.235343: step 58470, loss = 0.66 (275.7 examples/sec; 0.464 sec/batch)
2016-02-03 17:33:12.935378: step 58480, loss = 0.64 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 17:33:17.615507: step 58490, loss = 0.59 (287.4 examples/sec; 0.445 sec/batch)
2016-02-03 17:33:22.303890: step 58500, loss = 0.82 (285.3 examples/sec; 0.449 sec/batch)
2016-02-03 17:33:27.540980: step 58510, loss = 0.64 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 17:33:32.275517: step 58520, loss = 0.61 (275.0 examples/sec; 0.466 sec/batch)
2016-02-03 17:33:36.993999: step 58530, loss = 0.67 (282.5 examples/sec; 0.453 sec/batch)
2016-02-03 17:33:41.703016: step 58540, loss = 0.63 (282.4 examples/sec; 0.453 sec/batch)
2016-02-03 17:33:46.357411: step 58550, loss = 0.72 (285.5 examples/sec; 0.448 sec/batch)
2016-02-03 17:33:50.985270: step 58560, loss = 0.68 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 17:33:55.669042: step 58570, loss = 0.89 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 17:34:00.386140: step 58580, loss = 0.74 (268.6 examples/sec; 0.477 sec/batch)
2016-02-03 17:34:05.167688: step 58590, loss = 0.69 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 17:34:09.836553: step 58600, loss = 0.64 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 17:34:15.117120: step 58610, loss = 0.67 (292.3 examples/sec; 0.438 sec/batch)
2016-02-03 17:34:19.805268: step 58620, loss = 0.74 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 17:34:24.501698: step 58630, loss = 0.67 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 17:34:29.303819: step 58640, loss = 0.71 (243.9 examples/sec; 0.525 sec/batch)
2016-02-03 17:34:34.044848: step 58650, loss = 0.67 (245.4 examples/sec; 0.522 sec/batch)
2016-02-03 17:34:38.757916: step 58660, loss = 0.79 (277.7 examples/sec; 0.461 sec/batch)
2016-02-03 17:34:43.465757: step 58670, loss = 0.87 (302.9 examples/sec; 0.423 sec/batch)
2016-02-03 17:34:48.185704: step 58680, loss = 0.83 (280.1 examples/sec; 0.457 sec/batch)
2016-02-03 17:34:52.991235: step 58690, loss = 0.66 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 17:34:57.686706: step 58700, loss = 0.66 (260.7 examples/sec; 0.491 sec/batch)
2016-02-03 17:35:02.868094: step 58710, loss = 0.84 (257.2 examples/sec; 0.498 sec/batch)
2016-02-03 17:35:07.569762: step 58720, loss = 0.64 (265.2 examples/sec; 0.483 sec/batch)
2016-02-03 17:35:12.327215: step 58730, loss = 0.75 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 17:35:17.013655: step 58740, loss = 0.79 (272.6 examples/sec; 0.470 sec/batch)
2016-02-03 17:35:21.741466: step 58750, loss = 0.56 (279.7 examples/sec; 0.458 sec/batch)
2016-02-03 17:35:26.418712: step 58760, loss = 0.65 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 17:35:31.109478: step 58770, loss = 0.72 (262.2 examples/sec; 0.488 sec/batch)
2016-02-03 17:35:35.878682: step 58780, loss = 0.70 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 17:35:40.585371: step 58790, loss = 0.62 (299.1 examples/sec; 0.428 sec/batch)
2016-02-03 17:35:45.309258: step 58800, loss = 0.76 (255.6 examples/sec; 0.501 sec/batch)
2016-02-03 17:35:50.552846: step 58810, loss = 0.76 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 17:35:55.256952: step 58820, loss = 0.80 (289.8 examples/sec; 0.442 sec/batch)
2016-02-03 17:36:00.092799: step 58830, loss = 0.73 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 17:36:04.757353: step 58840, loss = 0.78 (287.3 examples/sec; 0.446 sec/batch)
2016-02-03 17:36:09.558015: step 58850, loss = 0.81 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 17:36:14.298063: step 58860, loss = 0.71 (285.5 examples/sec; 0.448 sec/batch)
2016-02-03 17:36:19.028751: step 58870, loss = 0.65 (255.5 examples/sec; 0.501 sec/batch)
2016-02-03 17:36:23.714084: step 58880, loss = 0.75 (293.4 examples/sec; 0.436 sec/batch)
2016-02-03 17:36:28.535175: step 58890, loss = 0.78 (251.6 examples/sec; 0.509 sec/batch)
2016-02-03 17:36:33.348788: step 58900, loss = 0.70 (248.1 examples/sec; 0.516 sec/batch)
2016-02-03 17:36:38.553565: step 58910, loss = 0.72 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 17:36:43.316145: step 58920, loss = 0.89 (253.3 examples/sec; 0.505 sec/batch)
2016-02-03 17:36:47.978944: step 58930, loss = 0.64 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 17:36:52.759046: step 58940, loss = 0.74 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 17:36:57.404485: step 58950, loss = 0.88 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 17:37:02.052848: step 58960, loss = 0.67 (320.7 examples/sec; 0.399 sec/batch)
2016-02-03 17:37:06.738469: step 58970, loss = 0.77 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 17:37:11.438867: step 58980, loss = 0.68 (282.9 examples/sec; 0.452 sec/batch)
2016-02-03 17:37:16.180762: step 58990, loss = 0.70 (258.5 examples/sec; 0.495 sec/batch)
2016-02-03 17:37:20.936819: step 59000, loss = 0.83 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 17:37:26.149867: step 59010, loss = 0.73 (259.5 examples/sec; 0.493 sec/batch)
2016-02-03 17:37:30.834379: step 59020, loss = 0.70 (255.8 examples/sec; 0.500 sec/batch)
2016-02-03 17:37:35.599628: step 59030, loss = 0.79 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 17:37:40.323822: step 59040, loss = 0.58 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 17:37:45.100024: step 59050, loss = 0.74 (251.6 examples/sec; 0.509 sec/batch)
2016-02-03 17:37:49.781583: step 59060, loss = 0.86 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 17:37:54.407075: step 59070, loss = 0.69 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 17:37:59.022714: step 59080, loss = 0.70 (292.7 examples/sec; 0.437 sec/batch)
2016-02-03 17:38:03.718776: step 59090, loss = 0.72 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 17:38:08.417903: step 59100, loss = 0.62 (309.5 examples/sec; 0.414 sec/batch)
2016-02-03 17:38:13.610978: step 59110, loss = 0.91 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 17:38:18.347595: step 59120, loss = 0.70 (253.1 examples/sec; 0.506 sec/batch)
2016-02-03 17:38:22.990198: step 59130, loss = 0.78 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 17:38:27.688726: step 59140, loss = 0.79 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 17:38:32.349871: step 59150, loss = 0.68 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 17:38:37.021493: step 59160, loss = 0.67 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 17:38:41.759048: step 59170, loss = 0.68 (298.4 examples/sec; 0.429 sec/batch)
2016-02-03 17:38:46.466978: step 59180, loss = 0.57 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 17:38:51.268539: step 59190, loss = 0.80 (240.5 examples/sec; 0.532 sec/batch)
2016-02-03 17:38:55.974171: step 59200, loss = 0.70 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 17:39:01.210985: step 59210, loss = 0.79 (306.0 examples/sec; 0.418 sec/batch)
2016-02-03 17:39:05.892960: step 59220, loss = 0.73 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 17:39:10.648323: step 59230, loss = 0.85 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 17:39:15.398559: step 59240, loss = 0.78 (249.2 examples/sec; 0.514 sec/batch)
2016-02-03 17:39:20.097203: step 59250, loss = 0.75 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 17:39:24.908961: step 59260, loss = 0.70 (252.2 examples/sec; 0.508 sec/batch)
2016-02-03 17:39:29.676910: step 59270, loss = 0.67 (262.1 examples/sec; 0.488 sec/batch)
2016-02-03 17:39:34.382215: step 59280, loss = 0.74 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 17:39:39.046825: step 59290, loss = 0.89 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 17:39:43.723917: step 59300, loss = 0.86 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 17:39:48.961621: step 59310, loss = 0.71 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 17:39:53.657569: step 59320, loss = 0.69 (291.1 examples/sec; 0.440 sec/batch)
2016-02-03 17:39:58.417354: step 59330, loss = 0.77 (251.3 examples/sec; 0.509 sec/batch)
2016-02-03 17:40:03.042365: step 59340, loss = 0.77 (279.2 examples/sec; 0.459 sec/batch)
2016-02-03 17:40:07.707105: step 59350, loss = 0.88 (281.2 examples/sec; 0.455 sec/batch)
2016-02-03 17:40:12.431796: step 59360, loss = 0.87 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 17:40:17.160290: step 59370, loss = 0.61 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 17:40:21.942257: step 59380, loss = 0.79 (249.2 examples/sec; 0.514 sec/batch)
2016-02-03 17:40:26.691314: step 59390, loss = 0.73 (278.5 examples/sec; 0.460 sec/batch)
2016-02-03 17:40:31.452007: step 59400, loss = 0.74 (250.8 examples/sec; 0.510 sec/batch)
2016-02-03 17:40:36.705970: step 59410, loss = 0.57 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 17:40:41.406207: step 59420, loss = 0.89 (253.2 examples/sec; 0.506 sec/batch)
2016-02-03 17:40:46.105785: step 59430, loss = 0.68 (256.7 examples/sec; 0.499 sec/batch)
2016-02-03 17:40:50.859230: step 59440, loss = 0.64 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 17:40:55.604544: step 59450, loss = 0.77 (278.6 examples/sec; 0.460 sec/batch)
2016-02-03 17:41:00.288523: step 59460, loss = 0.82 (283.8 examples/sec; 0.451 sec/batch)
2016-02-03 17:41:05.084399: step 59470, loss = 0.66 (263.0 examples/sec; 0.487 sec/batch)
2016-02-03 17:41:09.750066: step 59480, loss = 0.89 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 17:41:14.543627: step 59490, loss = 0.66 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 17:41:19.199636: step 59500, loss = 0.70 (252.8 examples/sec; 0.506 sec/batch)
2016-02-03 17:41:24.384365: step 59510, loss = 0.76 (290.1 examples/sec; 0.441 sec/batch)
2016-02-03 17:41:29.014358: step 59520, loss = 0.77 (297.4 examples/sec; 0.430 sec/batch)
2016-02-03 17:41:33.724552: step 59530, loss = 0.80 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 17:41:38.413246: step 59540, loss = 0.63 (278.0 examples/sec; 0.460 sec/batch)
2016-02-03 17:41:43.159896: step 59550, loss = 0.89 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 17:41:47.809862: step 59560, loss = 0.78 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 17:41:52.539134: step 59570, loss = 0.82 (299.4 examples/sec; 0.427 sec/batch)
2016-02-03 17:41:57.293584: step 59580, loss = 0.74 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 17:42:01.997760: step 59590, loss = 0.87 (281.1 examples/sec; 0.455 sec/batch)
2016-02-03 17:42:06.743443: step 59600, loss = 0.81 (291.6 examples/sec; 0.439 sec/batch)
2016-02-03 17:42:12.029181: step 59610, loss = 0.78 (244.9 examples/sec; 0.523 sec/batch)
2016-02-03 17:42:16.702273: step 59620, loss = 0.78 (251.9 examples/sec; 0.508 sec/batch)
2016-02-03 17:42:21.438368: step 59630, loss = 0.79 (249.4 examples/sec; 0.513 sec/batch)
2016-02-03 17:42:26.096946: step 59640, loss = 0.68 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 17:42:30.726627: step 59650, loss = 0.74 (259.9 examples/sec; 0.493 sec/batch)
2016-02-03 17:42:35.438774: step 59660, loss = 0.61 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 17:42:40.071043: step 59670, loss = 0.77 (289.9 examples/sec; 0.442 sec/batch)
2016-02-03 17:42:44.797088: step 59680, loss = 0.55 (286.4 examples/sec; 0.447 sec/batch)
2016-02-03 17:42:49.513862: step 59690, loss = 0.63 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 17:42:54.218095: step 59700, loss = 0.65 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 17:42:59.394937: step 59710, loss = 0.81 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 17:43:03.987412: step 59720, loss = 0.66 (286.3 examples/sec; 0.447 sec/batch)
2016-02-03 17:43:08.592400: step 59730, loss = 0.76 (276.3 examples/sec; 0.463 sec/batch)
2016-02-03 17:43:13.271881: step 59740, loss = 0.71 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 17:43:17.980684: step 59750, loss = 0.76 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 17:43:22.670146: step 59760, loss = 0.66 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 17:43:27.271233: step 59770, loss = 0.73 (277.9 examples/sec; 0.461 sec/batch)
2016-02-03 17:43:32.004590: step 59780, loss = 0.77 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 17:43:36.743907: step 59790, loss = 0.65 (269.3 examples/sec; 0.475 sec/batch)
2016-02-03 17:43:41.462569: step 59800, loss = 0.75 (270.9 examples/sec; 0.473 sec/batch)
2016-02-03 17:43:46.679959: step 59810, loss = 0.83 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 17:43:51.386689: step 59820, loss = 0.87 (256.3 examples/sec; 0.499 sec/batch)
2016-02-03 17:43:56.106131: step 59830, loss = 0.68 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 17:44:00.824466: step 59840, loss = 0.68 (259.4 examples/sec; 0.493 sec/batch)
2016-02-03 17:44:05.575929: step 59850, loss = 0.75 (263.2 examples/sec; 0.486 sec/batch)
2016-02-03 17:44:10.225405: step 59860, loss = 0.83 (298.6 examples/sec; 0.429 sec/batch)
2016-02-03 17:44:14.973226: step 59870, loss = 0.76 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 17:44:19.734152: step 59880, loss = 0.78 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 17:44:24.518845: step 59890, loss = 0.75 (235.9 examples/sec; 0.542 sec/batch)
2016-02-03 17:44:29.232758: step 59900, loss = 0.84 (260.5 examples/sec; 0.491 sec/batch)
2016-02-03 17:44:34.511560: step 59910, loss = 0.78 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 17:44:39.156130: step 59920, loss = 0.82 (279.8 examples/sec; 0.458 sec/batch)
2016-02-03 17:44:43.903766: step 59930, loss = 0.58 (253.0 examples/sec; 0.506 sec/batch)
2016-02-03 17:44:48.515473: step 59940, loss = 0.66 (281.2 examples/sec; 0.455 sec/batch)
2016-02-03 17:44:53.139007: step 59950, loss = 0.61 (290.2 examples/sec; 0.441 sec/batch)
2016-02-03 17:44:57.854287: step 59960, loss = 0.72 (280.6 examples/sec; 0.456 sec/batch)
2016-02-03 17:45:02.528659: step 59970, loss = 0.64 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 17:45:07.210398: step 59980, loss = 0.59 (273.2 examples/sec; 0.469 sec/batch)
2016-02-03 17:45:11.904969: step 59990, loss = 0.57 (282.9 examples/sec; 0.452 sec/batch)
2016-02-03 17:45:16.582085: step 60000, loss = 0.79 (257.9 examples/sec; 0.496 sec/batch)
2016-02-03 17:45:21.805238: step 60010, loss = 0.77 (289.4 examples/sec; 0.442 sec/batch)
2016-02-03 17:45:26.556940: step 60020, loss = 0.73 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 17:45:31.228490: step 60030, loss = 0.76 (302.8 examples/sec; 0.423 sec/batch)
2016-02-03 17:45:35.915749: step 60040, loss = 0.81 (291.1 examples/sec; 0.440 sec/batch)
2016-02-03 17:45:40.636620: step 60050, loss = 0.66 (252.1 examples/sec; 0.508 sec/batch)
2016-02-03 17:45:45.361191: step 60060, loss = 0.71 (276.4 examples/sec; 0.463 sec/batch)
2016-02-03 17:45:50.053534: step 60070, loss = 0.79 (264.5 examples/sec; 0.484 sec/batch)
2016-02-03 17:45:54.816688: step 60080, loss = 0.66 (255.2 examples/sec; 0.502 sec/batch)
2016-02-03 17:45:59.544648: step 60090, loss = 0.78 (290.8 examples/sec; 0.440 sec/batch)
2016-02-03 17:46:04.222441: step 60100, loss = 0.74 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 17:46:09.432851: step 60110, loss = 0.67 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 17:46:14.125205: step 60120, loss = 0.79 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 17:46:18.800735: step 60130, loss = 0.69 (251.5 examples/sec; 0.509 sec/batch)
2016-02-03 17:46:23.472911: step 60140, loss = 0.72 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 17:46:28.232986: step 60150, loss = 0.69 (254.9 examples/sec; 0.502 sec/batch)
2016-02-03 17:46:33.015622: step 60160, loss = 0.83 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 17:46:37.664139: step 60170, loss = 0.66 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 17:46:42.268661: step 60180, loss = 0.95 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 17:46:46.979840: step 60190, loss = 0.81 (263.6 examples/sec; 0.486 sec/batch)
2016-02-03 17:46:51.628582: step 60200, loss = 0.77 (259.3 examples/sec; 0.494 sec/batch)
2016-02-03 17:46:56.838986: step 60210, loss = 0.77 (260.0 examples/sec; 0.492 sec/batch)
2016-02-03 17:47:01.572161: step 60220, loss = 0.69 (276.4 examples/sec; 0.463 sec/batch)
2016-02-03 17:47:06.228032: step 60230, loss = 0.63 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 17:47:10.846946: step 60240, loss = 0.78 (279.4 examples/sec; 0.458 sec/batch)
2016-02-03 17:47:15.607236: step 60250, loss = 0.65 (289.3 examples/sec; 0.442 sec/batch)
2016-02-03 17:47:20.286126: step 60260, loss = 0.59 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 17:47:24.969283: step 60270, loss = 0.78 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 17:47:29.657677: step 60280, loss = 0.85 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 17:47:34.378059: step 60290, loss = 0.84 (305.0 examples/sec; 0.420 sec/batch)
2016-02-03 17:47:39.115779: step 60300, loss = 0.78 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 17:47:44.348630: step 60310, loss = 0.63 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 17:47:49.010031: step 60320, loss = 0.78 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 17:47:53.723065: step 60330, loss = 0.65 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 17:47:58.386249: step 60340, loss = 0.74 (293.4 examples/sec; 0.436 sec/batch)
2016-02-03 17:48:03.073214: step 60350, loss = 0.60 (311.5 examples/sec; 0.411 sec/batch)
2016-02-03 17:48:07.811635: step 60360, loss = 0.71 (288.4 examples/sec; 0.444 sec/batch)
2016-02-03 17:48:12.520972: step 60370, loss = 0.63 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 17:48:17.183389: step 60380, loss = 0.75 (248.9 examples/sec; 0.514 sec/batch)
2016-02-03 17:48:21.913100: step 60390, loss = 0.68 (248.1 examples/sec; 0.516 sec/batch)
2016-02-03 17:48:26.620770: step 60400, loss = 0.62 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 17:48:31.834721: step 60410, loss = 0.63 (281.4 examples/sec; 0.455 sec/batch)
2016-02-03 17:48:36.589242: step 60420, loss = 0.74 (268.0 examples/sec; 0.478 sec/batch)
2016-02-03 17:48:41.223067: step 60430, loss = 0.89 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 17:48:45.912998: step 60440, loss = 0.72 (293.3 examples/sec; 0.436 sec/batch)
2016-02-03 17:48:50.611048: step 60450, loss = 0.68 (290.2 examples/sec; 0.441 sec/batch)
2016-02-03 17:48:55.330051: step 60460, loss = 0.73 (258.3 examples/sec; 0.496 sec/batch)
2016-02-03 17:49:00.037071: step 60470, loss = 0.68 (289.4 examples/sec; 0.442 sec/batch)
2016-02-03 17:49:04.857887: step 60480, loss = 0.68 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 17:49:09.596442: step 60490, loss = 0.85 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 17:49:14.265879: step 60500, loss = 0.82 (274.4 examples/sec; 0.467 sec/batch)
2016-02-03 17:49:19.420850: step 60510, loss = 0.66 (285.9 examples/sec; 0.448 sec/batch)
2016-02-03 17:49:24.154614: step 60520, loss = 0.76 (296.2 examples/sec; 0.432 sec/batch)
2016-02-03 17:49:28.899730: step 60530, loss = 0.68 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 17:49:33.722077: step 60540, loss = 0.78 (250.8 examples/sec; 0.510 sec/batch)
2016-02-03 17:49:38.402723: step 60550, loss = 0.76 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 17:49:43.121572: step 60560, loss = 0.78 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 17:49:47.806492: step 60570, loss = 0.64 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 17:49:52.592413: step 60580, loss = 0.80 (282.6 examples/sec; 0.453 sec/batch)
2016-02-03 17:49:57.286312: step 60590, loss = 0.68 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 17:50:02.103132: step 60600, loss = 0.75 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 17:50:07.262259: step 60610, loss = 0.70 (265.8 examples/sec; 0.482 sec/batch)
2016-02-03 17:50:12.047606: step 60620, loss = 0.86 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 17:50:16.771233: step 60630, loss = 0.81 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 17:50:21.490341: step 60640, loss = 0.67 (255.6 examples/sec; 0.501 sec/batch)
2016-02-03 17:50:26.289273: step 60650, loss = 0.73 (252.2 examples/sec; 0.508 sec/batch)
2016-02-03 17:50:30.979016: step 60660, loss = 0.90 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 17:50:35.686195: step 60670, loss = 0.60 (253.7 examples/sec; 0.505 sec/batch)
2016-02-03 17:50:40.408592: step 60680, loss = 0.79 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 17:50:45.161847: step 60690, loss = 0.68 (255.7 examples/sec; 0.501 sec/batch)
2016-02-03 17:50:49.893853: step 60700, loss = 0.68 (292.9 examples/sec; 0.437 sec/batch)
2016-02-03 17:50:55.160624: step 60710, loss = 0.90 (263.9 examples/sec; 0.485 sec/batch)
2016-02-03 17:50:59.889870: step 60720, loss = 0.67 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 17:51:04.595635: step 60730, loss = 0.63 (251.3 examples/sec; 0.509 sec/batch)
2016-02-03 17:51:09.344725: step 60740, loss = 0.74 (255.2 examples/sec; 0.502 sec/batch)
2016-02-03 17:51:14.076894: step 60750, loss = 0.88 (275.6 examples/sec; 0.464 sec/batch)
2016-02-03 17:51:18.822811: step 60760, loss = 0.84 (250.5 examples/sec; 0.511 sec/batch)
2016-02-03 17:51:23.603383: step 60770, loss = 0.80 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 17:51:28.292417: step 60780, loss = 0.72 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 17:51:32.991136: step 60790, loss = 0.70 (300.4 examples/sec; 0.426 sec/batch)
2016-02-03 17:51:37.738212: step 60800, loss = 0.66 (246.8 examples/sec; 0.519 sec/batch)
2016-02-03 17:51:42.959629: step 60810, loss = 0.61 (265.2 examples/sec; 0.483 sec/batch)
2016-02-03 17:51:47.596254: step 60820, loss = 0.57 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 17:51:52.339721: step 60830, loss = 0.77 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 17:51:57.002740: step 60840, loss = 0.97 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 17:52:01.693419: step 60850, loss = 0.68 (268.6 examples/sec; 0.476 sec/batch)
2016-02-03 17:52:06.433149: step 60860, loss = 0.70 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 17:52:11.162559: step 60870, loss = 0.73 (264.7 examples/sec; 0.483 sec/batch)
2016-02-03 17:52:15.773615: step 60880, loss = 0.78 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 17:52:20.543298: step 60890, loss = 0.72 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 17:52:25.163139: step 60900, loss = 0.78 (287.0 examples/sec; 0.446 sec/batch)
2016-02-03 17:52:30.419397: step 60910, loss = 0.74 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 17:52:35.091431: step 60920, loss = 0.78 (279.4 examples/sec; 0.458 sec/batch)
2016-02-03 17:52:39.810123: step 60930, loss = 0.87 (258.1 examples/sec; 0.496 sec/batch)
2016-02-03 17:52:44.613128: step 60940, loss = 0.66 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 17:52:49.372135: step 60950, loss = 0.81 (288.8 examples/sec; 0.443 sec/batch)
2016-02-03 17:52:54.029724: step 60960, loss = 0.69 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 17:52:58.694174: step 60970, loss = 0.69 (261.7 examples/sec; 0.489 sec/batch)
2016-02-03 17:53:03.395091: step 60980, loss = 0.71 (281.2 examples/sec; 0.455 sec/batch)
2016-02-03 17:53:08.015698: step 60990, loss = 0.55 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 17:53:12.689189: step 61000, loss = 0.86 (259.3 examples/sec; 0.494 sec/batch)
2016-02-03 17:53:17.923580: step 61010, loss = 0.66 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 17:53:22.607666: step 61020, loss = 0.53 (275.6 examples/sec; 0.464 sec/batch)
2016-02-03 17:53:27.345103: step 61030, loss = 0.81 (282.9 examples/sec; 0.452 sec/batch)
2016-02-03 17:53:32.113896: step 61040, loss = 0.78 (266.3 examples/sec; 0.481 sec/batch)
2016-02-03 17:53:36.813037: step 61050, loss = 0.71 (290.7 examples/sec; 0.440 sec/batch)
2016-02-03 17:53:41.435128: step 61060, loss = 0.80 (298.0 examples/sec; 0.429 sec/batch)
2016-02-03 17:53:46.172225: step 61070, loss = 0.87 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 17:53:50.901340: step 61080, loss = 0.81 (247.1 examples/sec; 0.518 sec/batch)
2016-02-03 17:53:55.626971: step 61090, loss = 0.80 (261.0 examples/sec; 0.490 sec/batch)
2016-02-03 17:54:00.286430: step 61100, loss = 0.60 (259.2 examples/sec; 0.494 sec/batch)
2016-02-03 17:54:05.452224: step 61110, loss = 0.76 (286.3 examples/sec; 0.447 sec/batch)
2016-02-03 17:54:10.095192: step 61120, loss = 0.59 (278.5 examples/sec; 0.460 sec/batch)
2016-02-03 17:54:14.835310: step 61130, loss = 0.64 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 17:54:19.458202: step 61140, loss = 0.83 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 17:54:24.140719: step 61150, loss = 0.65 (297.3 examples/sec; 0.430 sec/batch)
2016-02-03 17:54:28.836508: step 61160, loss = 0.79 (258.2 examples/sec; 0.496 sec/batch)
2016-02-03 17:54:33.490749: step 61170, loss = 0.63 (280.3 examples/sec; 0.457 sec/batch)
2016-02-03 17:54:38.138445: step 61180, loss = 0.65 (293.6 examples/sec; 0.436 sec/batch)
2016-02-03 17:54:42.843793: step 61190, loss = 0.72 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 17:54:47.602242: step 61200, loss = 0.63 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 17:54:52.801854: step 61210, loss = 0.78 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 17:54:57.539591: step 61220, loss = 0.81 (252.0 examples/sec; 0.508 sec/batch)
2016-02-03 17:55:02.190549: step 61230, loss = 0.61 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 17:55:06.949072: step 61240, loss = 0.72 (259.0 examples/sec; 0.494 sec/batch)
2016-02-03 17:55:11.689407: step 61250, loss = 0.77 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 17:55:16.363498: step 61260, loss = 0.66 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 17:55:21.072887: step 61270, loss = 0.68 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 17:55:25.786442: step 61280, loss = 0.75 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 17:55:30.430552: step 61290, loss = 0.63 (322.9 examples/sec; 0.396 sec/batch)
2016-02-03 17:55:35.256497: step 61300, loss = 0.76 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 17:55:40.519089: step 61310, loss = 0.75 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 17:55:45.167563: step 61320, loss = 0.76 (279.4 examples/sec; 0.458 sec/batch)
2016-02-03 17:55:49.928957: step 61330, loss = 0.90 (262.0 examples/sec; 0.489 sec/batch)
2016-02-03 17:55:54.630472: step 61340, loss = 0.79 (283.0 examples/sec; 0.452 sec/batch)
2016-02-03 17:55:59.413245: step 61350, loss = 0.68 (275.0 examples/sec; 0.465 sec/batch)
2016-02-03 17:56:04.185318: step 61360, loss = 0.66 (260.8 examples/sec; 0.491 sec/batch)
2016-02-03 17:56:08.903474: step 61370, loss = 0.73 (253.6 examples/sec; 0.505 sec/batch)
2016-02-03 17:56:13.537620: step 61380, loss = 0.71 (296.8 examples/sec; 0.431 sec/batch)
2016-02-03 17:56:18.292612: step 61390, loss = 0.77 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 17:56:22.982937: step 61400, loss = 0.80 (268.2 examples/sec; 0.477 sec/batch)
2016-02-03 17:56:28.154685: step 61410, loss = 0.89 (262.2 examples/sec; 0.488 sec/batch)
2016-02-03 17:56:32.846489: step 61420, loss = 0.85 (256.8 examples/sec; 0.498 sec/batch)
2016-02-03 17:56:37.478153: step 61430, loss = 0.76 (272.6 examples/sec; 0.470 sec/batch)
2016-02-03 17:56:42.252640: step 61440, loss = 0.68 (290.3 examples/sec; 0.441 sec/batch)
2016-02-03 17:56:47.053975: step 61450, loss = 0.76 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 17:56:51.844992: step 61460, loss = 0.72 (253.3 examples/sec; 0.505 sec/batch)
2016-02-03 17:56:56.606585: step 61470, loss = 0.68 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 17:57:01.325470: step 61480, loss = 0.65 (288.8 examples/sec; 0.443 sec/batch)
2016-02-03 17:57:06.095494: step 61490, loss = 0.59 (258.7 examples/sec; 0.495 sec/batch)
2016-02-03 17:57:10.820809: step 61500, loss = 0.93 (285.3 examples/sec; 0.449 sec/batch)
2016-02-03 17:57:16.111256: step 61510, loss = 0.77 (246.2 examples/sec; 0.520 sec/batch)
2016-02-03 17:57:20.832465: step 61520, loss = 0.87 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 17:57:25.577391: step 61530, loss = 0.91 (256.7 examples/sec; 0.499 sec/batch)
2016-02-03 17:57:30.310086: step 61540, loss = 0.70 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 17:57:35.074055: step 61550, loss = 0.64 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 17:57:39.825693: step 61560, loss = 0.72 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 17:57:44.506767: step 61570, loss = 0.68 (253.5 examples/sec; 0.505 sec/batch)
2016-02-03 17:57:49.256037: step 61580, loss = 0.77 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 17:57:53.952146: step 61590, loss = 0.70 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 17:57:58.679800: step 61600, loss = 0.64 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 17:58:03.832457: step 61610, loss = 0.83 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 17:58:08.570196: step 61620, loss = 0.73 (253.0 examples/sec; 0.506 sec/batch)
2016-02-03 17:58:13.291636: step 61630, loss = 0.64 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 17:58:18.037916: step 61640, loss = 0.83 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 17:58:22.667575: step 61650, loss = 0.59 (297.6 examples/sec; 0.430 sec/batch)
2016-02-03 17:58:27.462228: step 61660, loss = 0.80 (263.1 examples/sec; 0.487 sec/batch)
2016-02-03 17:58:32.195518: step 61670, loss = 0.69 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 17:58:36.888219: step 61680, loss = 0.80 (284.2 examples/sec; 0.450 sec/batch)
2016-02-03 17:58:41.633230: step 61690, loss = 0.70 (267.0 examples/sec; 0.479 sec/batch)
2016-02-03 17:58:46.328539: step 61700, loss = 0.66 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 17:58:51.532880: step 61710, loss = 0.89 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 17:58:56.248057: step 61720, loss = 0.78 (296.4 examples/sec; 0.432 sec/batch)
2016-02-03 17:59:00.990948: step 61730, loss = 0.80 (265.2 examples/sec; 0.483 sec/batch)
2016-02-03 17:59:05.646654: step 61740, loss = 0.84 (280.6 examples/sec; 0.456 sec/batch)
2016-02-03 17:59:10.362719: step 61750, loss = 0.79 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 17:59:15.020998: step 61760, loss = 0.70 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 17:59:19.749046: step 61770, loss = 0.70 (260.1 examples/sec; 0.492 sec/batch)
2016-02-03 17:59:24.430159: step 61780, loss = 0.67 (296.8 examples/sec; 0.431 sec/batch)
2016-02-03 17:59:29.156255: step 61790, loss = 0.76 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 17:59:33.903450: step 61800, loss = 0.78 (257.9 examples/sec; 0.496 sec/batch)
2016-02-03 17:59:39.056664: step 61810, loss = 0.95 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 17:59:43.853933: step 61820, loss = 0.75 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 17:59:48.583083: step 61830, loss = 0.83 (275.6 examples/sec; 0.464 sec/batch)
2016-02-03 17:59:53.233683: step 61840, loss = 0.77 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 17:59:57.928140: step 61850, loss = 0.58 (291.7 examples/sec; 0.439 sec/batch)
2016-02-03 18:00:02.602701: step 61860, loss = 0.82 (260.5 examples/sec; 0.491 sec/batch)
2016-02-03 18:00:07.296963: step 61870, loss = 0.69 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 18:00:12.031920: step 61880, loss = 0.68 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 18:00:16.679029: step 61890, loss = 0.77 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 18:00:21.452115: step 61900, loss = 0.63 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 18:00:26.678582: step 61910, loss = 0.78 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 18:00:31.403297: step 61920, loss = 0.66 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 18:00:36.168246: step 61930, loss = 0.66 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 18:00:40.938799: step 61940, loss = 0.66 (246.3 examples/sec; 0.520 sec/batch)
2016-02-03 18:00:45.670215: step 61950, loss = 0.88 (279.4 examples/sec; 0.458 sec/batch)
2016-02-03 18:00:50.318823: step 61960, loss = 0.81 (290.3 examples/sec; 0.441 sec/batch)
2016-02-03 18:00:55.077254: step 61970, loss = 0.80 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 18:00:59.709842: step 61980, loss = 0.66 (280.1 examples/sec; 0.457 sec/batch)
2016-02-03 18:01:04.515409: step 61990, loss = 0.70 (264.1 examples/sec; 0.485 sec/batch)
2016-02-03 18:01:09.218140: step 62000, loss = 0.66 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 18:01:14.469717: step 62010, loss = 0.74 (282.2 examples/sec; 0.454 sec/batch)
2016-02-03 18:01:19.232014: step 62020, loss = 0.69 (250.3 examples/sec; 0.511 sec/batch)
2016-02-03 18:01:23.907213: step 62030, loss = 0.72 (294.7 examples/sec; 0.434 sec/batch)
2016-02-03 18:01:28.642248: step 62040, loss = 0.80 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 18:01:33.378717: step 62050, loss = 0.79 (251.5 examples/sec; 0.509 sec/batch)
2016-02-03 18:01:38.087538: step 62060, loss = 0.69 (268.2 examples/sec; 0.477 sec/batch)
2016-02-03 18:01:42.746195: step 62070, loss = 0.66 (275.0 examples/sec; 0.465 sec/batch)
2016-02-03 18:01:47.424253: step 62080, loss = 0.72 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 18:01:52.196547: step 62090, loss = 0.90 (267.2 examples/sec; 0.479 sec/batch)
2016-02-03 18:01:56.841894: step 62100, loss = 0.79 (285.9 examples/sec; 0.448 sec/batch)
2016-02-03 18:02:02.083859: step 62110, loss = 0.78 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 18:02:06.831319: step 62120, loss = 0.65 (263.0 examples/sec; 0.487 sec/batch)
2016-02-03 18:02:11.561180: step 62130, loss = 0.77 (244.0 examples/sec; 0.525 sec/batch)
2016-02-03 18:02:16.261019: step 62140, loss = 0.79 (292.2 examples/sec; 0.438 sec/batch)
2016-02-03 18:02:21.052386: step 62150, loss = 0.70 (253.8 examples/sec; 0.504 sec/batch)
2016-02-03 18:02:25.794176: step 62160, loss = 0.58 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 18:02:30.442850: step 62170, loss = 0.73 (283.8 examples/sec; 0.451 sec/batch)
2016-02-03 18:02:35.132420: step 62180, loss = 0.61 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 18:02:39.845597: step 62190, loss = 0.61 (254.1 examples/sec; 0.504 sec/batch)
2016-02-03 18:02:44.466316: step 62200, loss = 0.68 (282.2 examples/sec; 0.454 sec/batch)
2016-02-03 18:02:49.749486: step 62210, loss = 0.77 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 18:02:54.433430: step 62220, loss = 0.64 (304.2 examples/sec; 0.421 sec/batch)
2016-02-03 18:02:59.188502: step 62230, loss = 0.71 (273.2 examples/sec; 0.468 sec/batch)
2016-02-03 18:03:03.918689: step 62240, loss = 0.64 (259.4 examples/sec; 0.493 sec/batch)
2016-02-03 18:03:08.644747: step 62250, loss = 0.84 (245.4 examples/sec; 0.522 sec/batch)
2016-02-03 18:03:13.363102: step 62260, loss = 0.62 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 18:03:18.062502: step 62270, loss = 0.64 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 18:03:22.789692: step 62280, loss = 0.70 (281.0 examples/sec; 0.455 sec/batch)
2016-02-03 18:03:27.471532: step 62290, loss = 0.78 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 18:03:32.243179: step 62300, loss = 0.68 (277.2 examples/sec; 0.462 sec/batch)
2016-02-03 18:03:37.390720: step 62310, loss = 0.72 (280.6 examples/sec; 0.456 sec/batch)
2016-02-03 18:03:42.101027: step 62320, loss = 0.79 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 18:03:46.701590: step 62330, loss = 0.70 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 18:03:51.369330: step 62340, loss = 0.75 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 18:03:56.157100: step 62350, loss = 0.82 (256.7 examples/sec; 0.499 sec/batch)
2016-02-03 18:04:00.830846: step 62360, loss = 0.71 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 18:04:05.568953: step 62370, loss = 0.92 (247.5 examples/sec; 0.517 sec/batch)
2016-02-03 18:04:10.276529: step 62380, loss = 0.76 (291.9 examples/sec; 0.438 sec/batch)
2016-02-03 18:04:14.968543: step 62390, loss = 0.74 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 18:04:19.713283: step 62400, loss = 0.68 (247.6 examples/sec; 0.517 sec/batch)
2016-02-03 18:04:24.924539: step 62410, loss = 0.64 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 18:04:29.690418: step 62420, loss = 0.75 (252.9 examples/sec; 0.506 sec/batch)
2016-02-03 18:04:34.403233: step 62430, loss = 0.63 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 18:04:39.071623: step 62440, loss = 0.60 (277.2 examples/sec; 0.462 sec/batch)
2016-02-03 18:04:43.718432: step 62450, loss = 0.68 (289.9 examples/sec; 0.441 sec/batch)
2016-02-03 18:04:48.411710: step 62460, loss = 0.71 (261.9 examples/sec; 0.489 sec/batch)
2016-02-03 18:04:53.108312: step 62470, loss = 0.74 (272.6 examples/sec; 0.469 sec/batch)
2016-02-03 18:04:57.901012: step 62480, loss = 0.75 (256.7 examples/sec; 0.499 sec/batch)
2016-02-03 18:05:02.629181: step 62490, loss = 0.51 (285.8 examples/sec; 0.448 sec/batch)
2016-02-03 18:05:07.384716: step 62500, loss = 0.74 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 18:05:12.572889: step 62510, loss = 0.77 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 18:05:17.415048: step 62520, loss = 0.65 (252.5 examples/sec; 0.507 sec/batch)
2016-02-03 18:05:22.092585: step 62530, loss = 0.70 (287.4 examples/sec; 0.445 sec/batch)
2016-02-03 18:05:26.815066: step 62540, loss = 0.70 (259.3 examples/sec; 0.494 sec/batch)
2016-02-03 18:05:31.511201: step 62550, loss = 0.60 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 18:05:36.204183: step 62560, loss = 0.73 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 18:05:40.917645: step 62570, loss = 0.71 (255.5 examples/sec; 0.501 sec/batch)
2016-02-03 18:05:45.586635: step 62580, loss = 0.75 (261.7 examples/sec; 0.489 sec/batch)
2016-02-03 18:05:50.247560: step 62590, loss = 0.64 (283.9 examples/sec; 0.451 sec/batch)
2016-02-03 18:05:54.875446: step 62600, loss = 0.84 (284.7 examples/sec; 0.450 sec/batch)
2016-02-03 18:06:00.108287: step 62610, loss = 0.74 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 18:06:04.774721: step 62620, loss = 0.60 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 18:06:09.455814: step 62630, loss = 0.68 (291.6 examples/sec; 0.439 sec/batch)
2016-02-03 18:06:14.145206: step 62640, loss = 0.77 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 18:06:18.838538: step 62650, loss = 0.88 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 18:06:23.525601: step 62660, loss = 0.75 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 18:06:28.191317: step 62670, loss = 0.71 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 18:06:32.907725: step 62680, loss = 0.93 (254.8 examples/sec; 0.502 sec/batch)
2016-02-03 18:06:37.588194: step 62690, loss = 0.86 (268.6 examples/sec; 0.477 sec/batch)
2016-02-03 18:06:42.310769: step 62700, loss = 0.70 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 18:06:47.506094: step 62710, loss = 0.77 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 18:06:52.244639: step 62720, loss = 0.63 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 18:06:56.893518: step 62730, loss = 0.69 (287.7 examples/sec; 0.445 sec/batch)
2016-02-03 18:07:01.543864: step 62740, loss = 0.62 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 18:07:06.232417: step 62750, loss = 0.67 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 18:07:10.951291: step 62760, loss = 0.82 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 18:07:15.649981: step 62770, loss = 0.89 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 18:07:20.421934: step 62780, loss = 0.69 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 18:07:25.121747: step 62790, loss = 0.89 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 18:07:29.848170: step 62800, loss = 0.58 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 18:07:35.080133: step 62810, loss = 0.70 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 18:07:39.848409: step 62820, loss = 0.83 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 18:07:44.552982: step 62830, loss = 0.67 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 18:07:49.329870: step 62840, loss = 0.79 (245.5 examples/sec; 0.521 sec/batch)
2016-02-03 18:07:54.010209: step 62850, loss = 0.75 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 18:07:58.774814: step 62860, loss = 0.77 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 18:08:03.467261: step 62870, loss = 0.79 (272.7 examples/sec; 0.469 sec/batch)
2016-02-03 18:08:08.136011: step 62880, loss = 0.80 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 18:08:12.904531: step 62890, loss = 0.76 (261.7 examples/sec; 0.489 sec/batch)
2016-02-03 18:08:17.644302: step 62900, loss = 0.74 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 18:08:22.893200: step 62910, loss = 0.86 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 18:08:27.596280: step 62920, loss = 0.90 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 18:08:32.353248: step 62930, loss = 0.62 (244.6 examples/sec; 0.523 sec/batch)
2016-02-03 18:08:37.082920: step 62940, loss = 0.81 (295.3 examples/sec; 0.433 sec/batch)
2016-02-03 18:08:41.776512: step 62950, loss = 0.62 (250.8 examples/sec; 0.510 sec/batch)
2016-02-03 18:08:46.426118: step 62960, loss = 0.85 (269.2 examples/sec; 0.475 sec/batch)
2016-02-03 18:08:51.182403: step 62970, loss = 0.77 (254.8 examples/sec; 0.502 sec/batch)
2016-02-03 18:08:55.994983: step 62980, loss = 0.55 (267.0 examples/sec; 0.479 sec/batch)
2016-02-03 18:09:00.734030: step 62990, loss = 0.69 (256.5 examples/sec; 0.499 sec/batch)
2016-02-03 18:09:05.495671: step 63000, loss = 0.68 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 18:09:10.738034: step 63010, loss = 0.83 (298.7 examples/sec; 0.429 sec/batch)
2016-02-03 18:09:15.433649: step 63020, loss = 0.80 (284.0 examples/sec; 0.451 sec/batch)
2016-02-03 18:09:20.130014: step 63030, loss = 0.70 (257.5 examples/sec; 0.497 sec/batch)
2016-02-03 18:09:24.812710: step 63040, loss = 0.77 (294.7 examples/sec; 0.434 sec/batch)
2016-02-03 18:09:29.568218: step 63050, loss = 1.00 (259.7 examples/sec; 0.493 sec/batch)
2016-02-03 18:09:34.230485: step 63060, loss = 0.80 (260.8 examples/sec; 0.491 sec/batch)
2016-02-03 18:09:38.844027: step 63070, loss = 0.73 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 18:09:43.562298: step 63080, loss = 0.70 (288.5 examples/sec; 0.444 sec/batch)
2016-02-03 18:09:48.284064: step 63090, loss = 0.88 (260.4 examples/sec; 0.491 sec/batch)
2016-02-03 18:09:53.038223: step 63100, loss = 0.89 (254.5 examples/sec; 0.503 sec/batch)
2016-02-03 18:09:58.263873: step 63110, loss = 0.68 (276.7 examples/sec; 0.463 sec/batch)
2016-02-03 18:10:03.038755: step 63120, loss = 0.78 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 18:10:07.785822: step 63130, loss = 0.81 (259.3 examples/sec; 0.494 sec/batch)
2016-02-03 18:10:12.458609: step 63140, loss = 0.74 (276.7 examples/sec; 0.463 sec/batch)
2016-02-03 18:10:17.202538: step 63150, loss = 0.63 (254.5 examples/sec; 0.503 sec/batch)
2016-02-03 18:10:21.982267: step 63160, loss = 0.71 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 18:10:26.716592: step 63170, loss = 0.56 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 18:10:31.421338: step 63180, loss = 0.78 (278.6 examples/sec; 0.460 sec/batch)
2016-02-03 18:10:36.203377: step 63190, loss = 0.80 (281.0 examples/sec; 0.456 sec/batch)
2016-02-03 18:10:40.996040: step 63200, loss = 0.66 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 18:10:46.160281: step 63210, loss = 0.84 (290.6 examples/sec; 0.441 sec/batch)
2016-02-03 18:10:50.933488: step 63220, loss = 0.89 (251.8 examples/sec; 0.508 sec/batch)
2016-02-03 18:10:55.647533: step 63230, loss = 0.72 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 18:11:00.422083: step 63240, loss = 0.55 (302.1 examples/sec; 0.424 sec/batch)
2016-02-03 18:11:05.072226: step 63250, loss = 0.95 (261.9 examples/sec; 0.489 sec/batch)
2016-02-03 18:11:09.850001: step 63260, loss = 0.79 (247.6 examples/sec; 0.517 sec/batch)
2016-02-03 18:11:14.520951: step 63270, loss = 0.71 (256.4 examples/sec; 0.499 sec/batch)
2016-02-03 18:11:19.178464: step 63280, loss = 0.65 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 18:11:23.845235: step 63290, loss = 0.67 (278.0 examples/sec; 0.460 sec/batch)
2016-02-03 18:11:28.538938: step 63300, loss = 0.80 (272.6 examples/sec; 0.470 sec/batch)
2016-02-03 18:11:33.802993: step 63310, loss = 0.64 (276.3 examples/sec; 0.463 sec/batch)
2016-02-03 18:11:38.496265: step 63320, loss = 0.63 (284.6 examples/sec; 0.450 sec/batch)
2016-02-03 18:11:43.152539: step 63330, loss = 0.73 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 18:11:47.876744: step 63340, loss = 0.78 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 18:11:52.594014: step 63350, loss = 0.81 (257.5 examples/sec; 0.497 sec/batch)
2016-02-03 18:11:57.333644: step 63360, loss = 0.71 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 18:12:02.061962: step 63370, loss = 0.71 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 18:12:06.807003: step 63380, loss = 0.73 (264.9 examples/sec; 0.483 sec/batch)
2016-02-03 18:12:11.544270: step 63390, loss = 0.72 (279.8 examples/sec; 0.457 sec/batch)
2016-02-03 18:12:16.221126: step 63400, loss = 0.80 (286.3 examples/sec; 0.447 sec/batch)
2016-02-03 18:12:21.526520: step 63410, loss = 0.75 (262.6 examples/sec; 0.487 sec/batch)
2016-02-03 18:12:26.248125: step 63420, loss = 0.74 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 18:12:30.974518: step 63430, loss = 0.72 (286.7 examples/sec; 0.446 sec/batch)
2016-02-03 18:12:35.687376: step 63440, loss = 0.61 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 18:12:40.328249: step 63450, loss = 0.70 (281.7 examples/sec; 0.454 sec/batch)
2016-02-03 18:12:45.068022: step 63460, loss = 0.76 (270.5 examples/sec; 0.473 sec/batch)
2016-02-03 18:12:49.871337: step 63470, loss = 0.70 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 18:12:54.573322: step 63480, loss = 0.61 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 18:12:59.332983: step 63490, loss = 0.77 (257.5 examples/sec; 0.497 sec/batch)
2016-02-03 18:13:04.097782: step 63500, loss = 0.74 (251.6 examples/sec; 0.509 sec/batch)
2016-02-03 18:13:09.250197: step 63510, loss = 0.71 (292.6 examples/sec; 0.438 sec/batch)
2016-02-03 18:13:13.957963: step 63520, loss = 0.75 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 18:13:18.744782: step 63530, loss = 0.67 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 18:13:23.494276: step 63540, loss = 0.59 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 18:13:28.268376: step 63550, loss = 0.73 (293.7 examples/sec; 0.436 sec/batch)
2016-02-03 18:13:32.923799: step 63560, loss = 0.72 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 18:13:37.667529: step 63570, loss = 0.81 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 18:13:42.360342: step 63580, loss = 0.74 (283.1 examples/sec; 0.452 sec/batch)
2016-02-03 18:13:47.082777: step 63590, loss = 0.65 (263.9 examples/sec; 0.485 sec/batch)
2016-02-03 18:13:51.844621: step 63600, loss = 0.69 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 18:13:56.997234: step 63610, loss = 0.74 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 18:14:01.744238: step 63620, loss = 0.85 (282.2 examples/sec; 0.454 sec/batch)
2016-02-03 18:14:06.494040: step 63630, loss = 0.76 (259.5 examples/sec; 0.493 sec/batch)
2016-02-03 18:14:11.228036: step 63640, loss = 0.87 (277.6 examples/sec; 0.461 sec/batch)
2016-02-03 18:14:15.875110: step 63650, loss = 0.68 (282.8 examples/sec; 0.453 sec/batch)
2016-02-03 18:14:20.589103: step 63660, loss = 0.81 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 18:14:25.323896: step 63670, loss = 0.78 (263.1 examples/sec; 0.486 sec/batch)
2016-02-03 18:14:29.999416: step 63680, loss = 0.80 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 18:14:34.660692: step 63690, loss = 0.51 (282.8 examples/sec; 0.453 sec/batch)
2016-02-03 18:14:39.372394: step 63700, loss = 0.80 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 18:14:44.687479: step 63710, loss = 0.85 (234.9 examples/sec; 0.545 sec/batch)
2016-02-03 18:14:49.374454: step 63720, loss = 0.82 (281.0 examples/sec; 0.455 sec/batch)
2016-02-03 18:14:54.137611: step 63730, loss = 0.96 (247.6 examples/sec; 0.517 sec/batch)
2016-02-03 18:14:58.817406: step 63740, loss = 0.70 (283.7 examples/sec; 0.451 sec/batch)
2016-02-03 18:15:03.474612: step 63750, loss = 0.87 (276.4 examples/sec; 0.463 sec/batch)
2016-02-03 18:15:08.199157: step 63760, loss = 0.82 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 18:15:12.977197: step 63770, loss = 0.55 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 18:15:17.763254: step 63780, loss = 0.83 (257.0 examples/sec; 0.498 sec/batch)
2016-02-03 18:15:22.491910: step 63790, loss = 0.78 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 18:15:27.178939: step 63800, loss = 0.63 (260.4 examples/sec; 0.492 sec/batch)
2016-02-03 18:15:32.421837: step 63810, loss = 0.89 (289.5 examples/sec; 0.442 sec/batch)
2016-02-03 18:15:37.235600: step 63820, loss = 0.69 (280.5 examples/sec; 0.456 sec/batch)
2016-02-03 18:15:41.957251: step 63830, loss = 0.88 (261.3 examples/sec; 0.490 sec/batch)
2016-02-03 18:15:46.600724: step 63840, loss = 0.63 (287.0 examples/sec; 0.446 sec/batch)
2016-02-03 18:15:51.276786: step 63850, loss = 0.77 (295.9 examples/sec; 0.433 sec/batch)
2016-02-03 18:15:56.035310: step 63860, loss = 0.71 (259.1 examples/sec; 0.494 sec/batch)
2016-02-03 18:16:00.702920: step 63870, loss = 0.63 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 18:16:05.347815: step 63880, loss = 0.75 (256.7 examples/sec; 0.499 sec/batch)
2016-02-03 18:16:10.039673: step 63890, loss = 0.79 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 18:16:14.789526: step 63900, loss = 0.63 (285.4 examples/sec; 0.449 sec/batch)
2016-02-03 18:16:20.027255: step 63910, loss = 0.76 (257.7 examples/sec; 0.497 sec/batch)
2016-02-03 18:16:24.745177: step 63920, loss = 0.98 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 18:16:29.510121: step 63930, loss = 0.57 (254.6 examples/sec; 0.503 sec/batch)
2016-02-03 18:16:34.177258: step 63940, loss = 0.64 (284.6 examples/sec; 0.450 sec/batch)
2016-02-03 18:16:38.906248: step 63950, loss = 0.74 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 18:16:43.582122: step 63960, loss = 0.58 (289.7 examples/sec; 0.442 sec/batch)
2016-02-03 18:16:48.229224: step 63970, loss = 0.82 (290.7 examples/sec; 0.440 sec/batch)
2016-02-03 18:16:53.058092: step 63980, loss = 0.78 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 18:16:57.728330: step 63990, loss = 0.73 (282.2 examples/sec; 0.454 sec/batch)
2016-02-03 18:17:02.356147: step 64000, loss = 0.60 (285.6 examples/sec; 0.448 sec/batch)
2016-02-03 18:17:07.649890: step 64010, loss = 0.78 (258.2 examples/sec; 0.496 sec/batch)
2016-02-03 18:17:12.391120: step 64020, loss = 0.76 (257.4 examples/sec; 0.497 sec/batch)
2016-02-03 18:17:17.148826: step 64030, loss = 0.63 (253.2 examples/sec; 0.505 sec/batch)
2016-02-03 18:17:21.887975: step 64040, loss = 0.83 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 18:17:26.621794: step 64050, loss = 0.82 (266.4 examples/sec; 0.480 sec/batch)
2016-02-03 18:17:31.470245: step 64060, loss = 0.79 (244.4 examples/sec; 0.524 sec/batch)
2016-02-03 18:17:36.093340: step 64070, loss = 0.69 (286.1 examples/sec; 0.447 sec/batch)
2016-02-03 18:17:40.870972: step 64080, loss = 0.76 (260.3 examples/sec; 0.492 sec/batch)
2016-02-03 18:17:45.616784: step 64090, loss = 0.84 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 18:17:50.344550: step 64100, loss = 0.73 (283.1 examples/sec; 0.452 sec/batch)
2016-02-03 18:17:55.551372: step 64110, loss = 0.66 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 18:18:00.265804: step 64120, loss = 0.75 (257.9 examples/sec; 0.496 sec/batch)
2016-02-03 18:18:04.982626: step 64130, loss = 0.70 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 18:18:09.670143: step 64140, loss = 0.73 (292.9 examples/sec; 0.437 sec/batch)
2016-02-03 18:18:14.454419: step 64150, loss = 0.65 (237.6 examples/sec; 0.539 sec/batch)
2016-02-03 18:18:19.195190: step 64160, loss = 0.76 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 18:18:23.972460: step 64170, loss = 0.65 (254.0 examples/sec; 0.504 sec/batch)
2016-02-03 18:18:28.564957: step 64180, loss = 0.83 (305.2 examples/sec; 0.419 sec/batch)
2016-02-03 18:18:33.368559: step 64190, loss = 0.83 (257.6 examples/sec; 0.497 sec/batch)
2016-02-03 18:18:38.081693: step 64200, loss = 0.87 (248.3 examples/sec; 0.515 sec/batch)
2016-02-03 18:18:43.272136: step 64210, loss = 0.77 (258.2 examples/sec; 0.496 sec/batch)
2016-02-03 18:18:48.037233: step 64220, loss = 0.83 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 18:18:52.673004: step 64230, loss = 0.83 (273.2 examples/sec; 0.468 sec/batch)
2016-02-03 18:18:57.400675: step 64240, loss = 0.65 (301.7 examples/sec; 0.424 sec/batch)
2016-02-03 18:19:02.078107: step 64250, loss = 0.90 (264.5 examples/sec; 0.484 sec/batch)
2016-02-03 18:19:06.758173: step 64260, loss = 0.81 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 18:19:11.405040: step 64270, loss = 0.69 (277.2 examples/sec; 0.462 sec/batch)
2016-02-03 18:19:16.105643: step 64280, loss = 0.69 (300.2 examples/sec; 0.426 sec/batch)
2016-02-03 18:19:20.825195: step 64290, loss = 0.71 (295.4 examples/sec; 0.433 sec/batch)
2016-02-03 18:19:25.539724: step 64300, loss = 0.65 (261.1 examples/sec; 0.490 sec/batch)
2016-02-03 18:19:30.765121: step 64310, loss = 0.85 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 18:19:35.410776: step 64320, loss = 0.68 (287.3 examples/sec; 0.446 sec/batch)
2016-02-03 18:19:40.156213: step 64330, loss = 0.65 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 18:19:44.910539: step 64340, loss = 0.78 (283.3 examples/sec; 0.452 sec/batch)
2016-02-03 18:19:49.645123: step 64350, loss = 0.72 (281.5 examples/sec; 0.455 sec/batch)
2016-02-03 18:19:54.328276: step 64360, loss = 0.74 (300.3 examples/sec; 0.426 sec/batch)
2016-02-03 18:19:59.034368: step 64370, loss = 0.69 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 18:20:03.831042: step 64380, loss = 0.76 (255.0 examples/sec; 0.502 sec/batch)
2016-02-03 18:20:08.544468: step 64390, loss = 0.53 (265.3 examples/sec; 0.482 sec/batch)
2016-02-03 18:20:13.242375: step 64400, loss = 0.77 (263.6 examples/sec; 0.486 sec/batch)
2016-02-03 18:20:18.482325: step 64410, loss = 0.62 (252.0 examples/sec; 0.508 sec/batch)
2016-02-03 18:20:23.212668: step 64420, loss = 0.73 (250.0 examples/sec; 0.512 sec/batch)
2016-02-03 18:20:27.969502: step 64430, loss = 0.99 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 18:20:32.673262: step 64440, loss = 0.68 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 18:20:37.389761: step 64450, loss = 0.64 (285.3 examples/sec; 0.449 sec/batch)
2016-02-03 18:20:42.056373: step 64460, loss = 0.77 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 18:20:46.837384: step 64470, loss = 0.62 (268.0 examples/sec; 0.478 sec/batch)
2016-02-03 18:20:51.509335: step 64480, loss = 0.69 (280.4 examples/sec; 0.456 sec/batch)
2016-02-03 18:20:56.242661: step 64490, loss = 0.94 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 18:21:00.996123: step 64500, loss = 0.74 (259.9 examples/sec; 0.493 sec/batch)
2016-02-03 18:21:06.156379: step 64510, loss = 0.80 (261.3 examples/sec; 0.490 sec/batch)
2016-02-03 18:21:10.865227: step 64520, loss = 0.74 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 18:21:15.582077: step 64530, loss = 0.80 (267.0 examples/sec; 0.479 sec/batch)
2016-02-03 18:21:20.273318: step 64540, loss = 0.78 (255.5 examples/sec; 0.501 sec/batch)
2016-02-03 18:21:24.986751: step 64550, loss = 0.63 (286.1 examples/sec; 0.447 sec/batch)
2016-02-03 18:21:29.622210: step 64560, loss = 0.62 (307.2 examples/sec; 0.417 sec/batch)
2016-02-03 18:21:34.219629: step 64570, loss = 0.72 (299.5 examples/sec; 0.427 sec/batch)
2016-02-03 18:21:38.865017: step 64580, loss = 0.68 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 18:21:43.585505: step 64590, loss = 0.68 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 18:21:48.263154: step 64600, loss = 0.65 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 18:21:53.506665: step 64610, loss = 0.54 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 18:21:58.232586: step 64620, loss = 0.66 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 18:22:03.001381: step 64630, loss = 0.70 (285.5 examples/sec; 0.448 sec/batch)
2016-02-03 18:22:07.730967: step 64640, loss = 0.64 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 18:22:12.497123: step 64650, loss = 0.72 (247.1 examples/sec; 0.518 sec/batch)
2016-02-03 18:22:17.296116: step 64660, loss = 0.83 (269.3 examples/sec; 0.475 sec/batch)
2016-02-03 18:22:21.908234: step 64670, loss = 0.66 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 18:22:26.493726: step 64680, loss = 0.68 (307.5 examples/sec; 0.416 sec/batch)
2016-02-03 18:22:31.268797: step 64690, loss = 0.73 (287.7 examples/sec; 0.445 sec/batch)
2016-02-03 18:22:35.963314: step 64700, loss = 0.62 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 18:22:41.156520: step 64710, loss = 0.86 (285.4 examples/sec; 0.448 sec/batch)
2016-02-03 18:22:45.905150: step 64720, loss = 0.64 (281.0 examples/sec; 0.455 sec/batch)
2016-02-03 18:22:50.648918: step 64730, loss = 0.94 (272.6 examples/sec; 0.469 sec/batch)
2016-02-03 18:22:55.390697: step 64740, loss = 0.72 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 18:23:00.096042: step 64750, loss = 0.63 (261.4 examples/sec; 0.490 sec/batch)
2016-02-03 18:23:04.747348: step 64760, loss = 0.80 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 18:23:09.492130: step 64770, loss = 0.58 (253.4 examples/sec; 0.505 sec/batch)
2016-02-03 18:23:14.072781: step 64780, loss = 0.68 (288.2 examples/sec; 0.444 sec/batch)
2016-02-03 18:23:18.798322: step 64790, loss = 0.72 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 18:23:23.565178: step 64800, loss = 0.82 (254.4 examples/sec; 0.503 sec/batch)
2016-02-03 18:23:28.738012: step 64810, loss = 0.70 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 18:23:33.408123: step 64820, loss = 0.98 (250.4 examples/sec; 0.511 sec/batch)
2016-02-03 18:23:38.047060: step 64830, loss = 0.73 (280.4 examples/sec; 0.457 sec/batch)
2016-02-03 18:23:42.710410: step 64840, loss = 0.73 (267.3 examples/sec; 0.479 sec/batch)
2016-02-03 18:23:47.498048: step 64850, loss = 0.61 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 18:23:52.194764: step 64860, loss = 0.73 (269.3 examples/sec; 0.475 sec/batch)
2016-02-03 18:23:56.850912: step 64870, loss = 0.68 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 18:24:01.593602: step 64880, loss = 1.13 (267.2 examples/sec; 0.479 sec/batch)
2016-02-03 18:24:06.298788: step 64890, loss = 0.71 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 18:24:11.068465: step 64900, loss = 0.72 (246.7 examples/sec; 0.519 sec/batch)
2016-02-03 18:24:16.258854: step 64910, loss = 0.74 (275.6 examples/sec; 0.465 sec/batch)
2016-02-03 18:24:20.931340: step 64920, loss = 0.76 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 18:24:25.580393: step 64930, loss = 0.70 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 18:24:30.355150: step 64940, loss = 0.66 (254.0 examples/sec; 0.504 sec/batch)
2016-02-03 18:24:34.999419: step 64950, loss = 0.72 (284.2 examples/sec; 0.450 sec/batch)
2016-02-03 18:24:39.661551: step 64960, loss = 0.66 (256.7 examples/sec; 0.499 sec/batch)
2016-02-03 18:24:44.384387: step 64970, loss = 0.83 (251.3 examples/sec; 0.509 sec/batch)
2016-02-03 18:24:49.156050: step 64980, loss = 0.83 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 18:24:53.946231: step 64990, loss = 0.92 (256.6 examples/sec; 0.499 sec/batch)
2016-02-03 18:24:58.605041: step 65000, loss = 0.71 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 18:25:03.802946: step 65010, loss = 0.83 (263.1 examples/sec; 0.486 sec/batch)
2016-02-03 18:25:08.498247: step 65020, loss = 0.85 (252.9 examples/sec; 0.506 sec/batch)
2016-02-03 18:25:13.164949: step 65030, loss = 0.64 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 18:25:17.944786: step 65040, loss = 0.60 (255.0 examples/sec; 0.502 sec/batch)
2016-02-03 18:25:22.631875: step 65050, loss = 0.75 (318.1 examples/sec; 0.402 sec/batch)
2016-02-03 18:25:27.393836: step 65060, loss = 0.79 (259.0 examples/sec; 0.494 sec/batch)
2016-02-03 18:25:32.096066: step 65070, loss = 0.87 (289.7 examples/sec; 0.442 sec/batch)
2016-02-03 18:25:36.781254: step 65080, loss = 0.68 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 18:25:41.340488: step 65090, loss = 0.75 (281.7 examples/sec; 0.454 sec/batch)
2016-02-03 18:25:46.017500: step 65100, loss = 0.79 (280.4 examples/sec; 0.457 sec/batch)
2016-02-03 18:25:51.166448: step 65110, loss = 0.75 (276.9 examples/sec; 0.462 sec/batch)
2016-02-03 18:25:55.830370: step 65120, loss = 0.76 (279.7 examples/sec; 0.458 sec/batch)
2016-02-03 18:26:00.352923: step 65130, loss = 0.84 (286.3 examples/sec; 0.447 sec/batch)
2016-02-03 18:26:04.839584: step 65140, loss = 0.76 (291.1 examples/sec; 0.440 sec/batch)
2016-02-03 18:26:09.563741: step 65150, loss = 0.82 (290.2 examples/sec; 0.441 sec/batch)
2016-02-03 18:26:14.307838: step 65160, loss = 0.75 (285.3 examples/sec; 0.449 sec/batch)
2016-02-03 18:26:19.012622: step 65170, loss = 0.74 (259.3 examples/sec; 0.494 sec/batch)
2016-02-03 18:26:23.564031: step 65180, loss = 0.64 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 18:26:28.285537: step 65190, loss = 0.79 (267.2 examples/sec; 0.479 sec/batch)
2016-02-03 18:26:32.887154: step 65200, loss = 0.57 (286.9 examples/sec; 0.446 sec/batch)
2016-02-03 18:26:37.827086: step 65210, loss = 0.77 (289.4 examples/sec; 0.442 sec/batch)
2016-02-03 18:26:42.496423: step 65220, loss = 0.72 (265.2 examples/sec; 0.483 sec/batch)
2016-02-03 18:26:47.177119: step 65230, loss = 0.79 (251.8 examples/sec; 0.508 sec/batch)
2016-02-03 18:26:51.878261: step 65240, loss = 0.57 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 18:26:56.586171: step 65250, loss = 0.62 (283.1 examples/sec; 0.452 sec/batch)
2016-02-03 18:27:01.271381: step 65260, loss = 0.77 (260.4 examples/sec; 0.491 sec/batch)
2016-02-03 18:27:05.967489: step 65270, loss = 0.64 (256.8 examples/sec; 0.498 sec/batch)
2016-02-03 18:27:10.645447: step 65280, loss = 0.74 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 18:27:15.422518: step 65290, loss = 0.69 (249.9 examples/sec; 0.512 sec/batch)
2016-02-03 18:27:20.080469: step 65300, loss = 0.80 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 18:27:25.316798: step 65310, loss = 0.68 (314.4 examples/sec; 0.407 sec/batch)
2016-02-03 18:27:30.052394: step 65320, loss = 0.69 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 18:27:34.705924: step 65330, loss = 0.64 (295.3 examples/sec; 0.434 sec/batch)
2016-02-03 18:27:39.513534: step 65340, loss = 0.64 (260.1 examples/sec; 0.492 sec/batch)
2016-02-03 18:27:44.149652: step 65350, loss = 0.80 (281.7 examples/sec; 0.454 sec/batch)
2016-02-03 18:27:48.847503: step 65360, loss = 0.68 (267.2 examples/sec; 0.479 sec/batch)
2016-02-03 18:27:53.558589: step 65370, loss = 0.69 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 18:27:58.287110: step 65380, loss = 0.84 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 18:28:02.920455: step 65390, loss = 0.79 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 18:28:07.632548: step 65400, loss = 0.78 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 18:28:12.918843: step 65410, loss = 0.62 (293.9 examples/sec; 0.436 sec/batch)
2016-02-03 18:28:17.707701: step 65420, loss = 0.80 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 18:28:22.456200: step 65430, loss = 0.73 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 18:28:27.147770: step 65440, loss = 0.75 (283.9 examples/sec; 0.451 sec/batch)
2016-02-03 18:28:31.766468: step 65450, loss = 0.63 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 18:28:36.470073: step 65460, loss = 0.76 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 18:28:41.186195: step 65470, loss = 0.67 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 18:28:45.888894: step 65480, loss = 0.68 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 18:28:50.601943: step 65490, loss = 0.84 (284.8 examples/sec; 0.449 sec/batch)
2016-02-03 18:28:55.241416: step 65500, loss = 0.64 (295.5 examples/sec; 0.433 sec/batch)
2016-02-03 18:29:00.415386: step 65510, loss = 0.78 (302.6 examples/sec; 0.423 sec/batch)
2016-02-03 18:29:05.110932: step 65520, loss = 0.76 (276.7 examples/sec; 0.463 sec/batch)
2016-02-03 18:29:09.734828: step 65530, loss = 0.64 (306.6 examples/sec; 0.418 sec/batch)
2016-02-03 18:29:14.441333: step 65540, loss = 0.81 (285.0 examples/sec; 0.449 sec/batch)
2016-02-03 18:29:19.228106: step 65550, loss = 0.70 (252.2 examples/sec; 0.508 sec/batch)
2016-02-03 18:29:23.959900: step 65560, loss = 0.71 (267.5 examples/sec; 0.479 sec/batch)
2016-02-03 18:29:28.650710: step 65570, loss = 0.60 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 18:29:33.325557: step 65580, loss = 0.73 (285.1 examples/sec; 0.449 sec/batch)
2016-02-03 18:29:38.073905: step 65590, loss = 0.61 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 18:29:42.694826: step 65600, loss = 0.83 (301.2 examples/sec; 0.425 sec/batch)
2016-02-03 18:29:47.859198: step 65610, loss = 0.73 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 18:29:52.549306: step 65620, loss = 0.71 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 18:29:57.250394: step 65630, loss = 0.77 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 18:30:01.981896: step 65640, loss = 0.53 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 18:30:06.705163: step 65650, loss = 0.56 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 18:30:11.488986: step 65660, loss = 0.65 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 18:30:16.216583: step 65670, loss = 0.73 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 18:30:20.958620: step 65680, loss = 0.86 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 18:30:25.680014: step 65690, loss = 0.64 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 18:30:30.442137: step 65700, loss = 0.75 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 18:30:35.758395: step 65710, loss = 0.87 (281.4 examples/sec; 0.455 sec/batch)
2016-02-03 18:30:40.558589: step 65720, loss = 0.78 (255.1 examples/sec; 0.502 sec/batch)
2016-02-03 18:30:45.292772: step 65730, loss = 0.87 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 18:30:50.031150: step 65740, loss = 0.82 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 18:30:54.770924: step 65750, loss = 0.62 (286.0 examples/sec; 0.448 sec/batch)
2016-02-03 18:30:59.533751: step 65760, loss = 0.79 (279.7 examples/sec; 0.458 sec/batch)
2016-02-03 18:31:04.317983: step 65770, loss = 0.69 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 18:31:09.031097: step 65780, loss = 0.79 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 18:31:13.722987: step 65790, loss = 0.82 (286.8 examples/sec; 0.446 sec/batch)
2016-02-03 18:31:18.504472: step 65800, loss = 0.73 (265.2 examples/sec; 0.483 sec/batch)
2016-02-03 18:31:23.763913: step 65810, loss = 0.80 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 18:31:28.436644: step 65820, loss = 0.66 (292.4 examples/sec; 0.438 sec/batch)
2016-02-03 18:31:33.133827: step 65830, loss = 0.65 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 18:31:37.925665: step 65840, loss = 0.66 (252.6 examples/sec; 0.507 sec/batch)
2016-02-03 18:31:42.583454: step 65850, loss = 0.90 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 18:31:47.245431: step 65860, loss = 0.76 (259.9 examples/sec; 0.492 sec/batch)
2016-02-03 18:31:51.914316: step 65870, loss = 0.80 (286.3 examples/sec; 0.447 sec/batch)
2016-02-03 18:31:56.708299: step 65880, loss = 0.71 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 18:32:01.468603: step 65890, loss = 0.75 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 18:32:06.223274: step 65900, loss = 0.66 (262.6 examples/sec; 0.487 sec/batch)
2016-02-03 18:32:11.473371: step 65910, loss = 0.83 (282.9 examples/sec; 0.452 sec/batch)
2016-02-03 18:32:16.221634: step 65920, loss = 0.64 (252.0 examples/sec; 0.508 sec/batch)
2016-02-03 18:32:20.953362: step 65930, loss = 0.72 (259.1 examples/sec; 0.494 sec/batch)
2016-02-03 18:32:25.670459: step 65940, loss = 0.65 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 18:32:30.384214: step 65950, loss = 0.74 (309.2 examples/sec; 0.414 sec/batch)
2016-02-03 18:32:35.075836: step 65960, loss = 0.71 (280.3 examples/sec; 0.457 sec/batch)
2016-02-03 18:32:39.834600: step 65970, loss = 0.70 (286.3 examples/sec; 0.447 sec/batch)
2016-02-03 18:32:44.653072: step 65980, loss = 0.68 (252.3 examples/sec; 0.507 sec/batch)
2016-02-03 18:32:49.377773: step 65990, loss = 0.91 (246.2 examples/sec; 0.520 sec/batch)
2016-02-03 18:32:54.047694: step 66000, loss = 0.73 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 18:32:59.266348: step 66010, loss = 0.70 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 18:33:03.956615: step 66020, loss = 0.56 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 18:33:08.658586: step 66030, loss = 0.90 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 18:33:13.383083: step 66040, loss = 0.79 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 18:33:18.088973: step 66050, loss = 0.83 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 18:33:22.829074: step 66060, loss = 0.77 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 18:33:27.545433: step 66070, loss = 0.67 (296.9 examples/sec; 0.431 sec/batch)
2016-02-03 18:33:32.230821: step 66080, loss = 0.58 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 18:33:36.980292: step 66090, loss = 0.98 (257.8 examples/sec; 0.496 sec/batch)
2016-02-03 18:33:41.601254: step 66100, loss = 0.78 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 18:33:46.940868: step 66110, loss = 0.76 (252.5 examples/sec; 0.507 sec/batch)
2016-02-03 18:33:51.698457: step 66120, loss = 0.78 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 18:33:56.459124: step 66130, loss = 0.72 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 18:34:01.183995: step 66140, loss = 0.85 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 18:34:05.888233: step 66150, loss = 0.63 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 18:34:10.670984: step 66160, loss = 0.71 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 18:34:15.431299: step 66170, loss = 0.53 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 18:34:20.126999: step 66180, loss = 0.65 (286.0 examples/sec; 0.448 sec/batch)
2016-02-03 18:34:24.782437: step 66190, loss = 0.71 (311.9 examples/sec; 0.410 sec/batch)
2016-02-03 18:34:29.570047: step 66200, loss = 0.78 (260.4 examples/sec; 0.492 sec/batch)
2016-02-03 18:34:34.789476: step 66210, loss = 0.56 (288.4 examples/sec; 0.444 sec/batch)
2016-02-03 18:34:39.499343: step 66220, loss = 0.94 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 18:34:44.230358: step 66230, loss = 0.58 (254.7 examples/sec; 0.503 sec/batch)
2016-02-03 18:34:48.954812: step 66240, loss = 0.74 (258.3 examples/sec; 0.496 sec/batch)
2016-02-03 18:34:53.723772: step 66250, loss = 0.76 (295.1 examples/sec; 0.434 sec/batch)
2016-02-03 18:34:58.510765: step 66260, loss = 0.86 (289.3 examples/sec; 0.442 sec/batch)
2016-02-03 18:35:03.305457: step 66270, loss = 0.74 (289.3 examples/sec; 0.443 sec/batch)
2016-02-03 18:35:08.013812: step 66280, loss = 0.63 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 18:35:12.713208: step 66290, loss = 0.76 (274.4 examples/sec; 0.466 sec/batch)
2016-02-03 18:35:17.446260: step 66300, loss = 0.62 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 18:35:22.644713: step 66310, loss = 1.02 (262.2 examples/sec; 0.488 sec/batch)
2016-02-03 18:35:27.397080: step 66320, loss = 0.65 (256.0 examples/sec; 0.500 sec/batch)
2016-02-03 18:35:32.060801: step 66330, loss = 0.73 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 18:35:36.862866: step 66340, loss = 0.69 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 18:35:41.597662: step 66350, loss = 0.80 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 18:35:46.438052: step 66360, loss = 0.76 (250.5 examples/sec; 0.511 sec/batch)
2016-02-03 18:35:51.173127: step 66370, loss = 0.66 (256.1 examples/sec; 0.500 sec/batch)
2016-02-03 18:35:55.897608: step 66380, loss = 0.65 (248.2 examples/sec; 0.516 sec/batch)
2016-02-03 18:36:00.563381: step 66390, loss = 0.78 (260.8 examples/sec; 0.491 sec/batch)
2016-02-03 18:36:05.301838: step 66400, loss = 0.64 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 18:36:10.462493: step 66410, loss = 0.65 (303.3 examples/sec; 0.422 sec/batch)
2016-02-03 18:36:15.156845: step 66420, loss = 0.69 (279.6 examples/sec; 0.458 sec/batch)
2016-02-03 18:36:19.914171: step 66430, loss = 0.73 (261.5 examples/sec; 0.489 sec/batch)
2016-02-03 18:36:24.561126: step 66440, loss = 0.64 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 18:36:29.350310: step 66450, loss = 0.88 (295.0 examples/sec; 0.434 sec/batch)
2016-02-03 18:36:34.081177: step 66460, loss = 0.70 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 18:36:38.753685: step 66470, loss = 0.88 (288.5 examples/sec; 0.444 sec/batch)
2016-02-03 18:36:43.449698: step 66480, loss = 0.77 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 18:36:48.123847: step 66490, loss = 0.71 (265.8 examples/sec; 0.482 sec/batch)
2016-02-03 18:36:52.752322: step 66500, loss = 0.69 (285.6 examples/sec; 0.448 sec/batch)
2016-02-03 18:36:57.989622: step 66510, loss = 0.76 (280.4 examples/sec; 0.456 sec/batch)
2016-02-03 18:37:02.696536: step 66520, loss = 0.70 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 18:37:07.354723: step 66530, loss = 0.65 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 18:37:12.058872: step 66540, loss = 0.83 (288.4 examples/sec; 0.444 sec/batch)
2016-02-03 18:37:16.842717: step 66550, loss = 0.72 (260.9 examples/sec; 0.491 sec/batch)
2016-02-03 18:37:21.531725: step 66560, loss = 0.66 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 18:37:26.230710: step 66570, loss = 0.70 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 18:37:30.957047: step 66580, loss = 0.81 (267.2 examples/sec; 0.479 sec/batch)
2016-02-03 18:37:35.681052: step 66590, loss = 0.76 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 18:37:40.362913: step 66600, loss = 0.86 (242.0 examples/sec; 0.529 sec/batch)
2016-02-03 18:37:45.536601: step 66610, loss = 0.87 (283.9 examples/sec; 0.451 sec/batch)
2016-02-03 18:37:50.272804: step 66620, loss = 0.59 (313.9 examples/sec; 0.408 sec/batch)
2016-02-03 18:37:55.015803: step 66630, loss = 0.78 (249.8 examples/sec; 0.512 sec/batch)
2016-02-03 18:37:59.742520: step 66640, loss = 0.84 (252.0 examples/sec; 0.508 sec/batch)
2016-02-03 18:38:04.433890: step 66650, loss = 0.72 (230.9 examples/sec; 0.554 sec/batch)
2016-02-03 18:38:09.093863: step 66660, loss = 0.80 (283.0 examples/sec; 0.452 sec/batch)
2016-02-03 18:38:13.790790: step 66670, loss = 0.68 (266.9 examples/sec; 0.480 sec/batch)
2016-02-03 18:38:18.555753: step 66680, loss = 0.66 (266.3 examples/sec; 0.481 sec/batch)
2016-02-03 18:38:23.148227: step 66690, loss = 0.59 (277.2 examples/sec; 0.462 sec/batch)
2016-02-03 18:38:27.843731: step 66700, loss = 0.70 (272.6 examples/sec; 0.470 sec/batch)
2016-02-03 18:38:33.006491: step 66710, loss = 0.68 (299.5 examples/sec; 0.427 sec/batch)
2016-02-03 18:38:37.630153: step 66720, loss = 0.65 (316.3 examples/sec; 0.405 sec/batch)
2016-02-03 18:38:42.370860: step 66730, loss = 0.81 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 18:38:46.927449: step 66740, loss = 0.90 (284.0 examples/sec; 0.451 sec/batch)
2016-02-03 18:38:51.644997: step 66750, loss = 0.56 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 18:38:56.231492: step 66760, loss = 0.64 (286.3 examples/sec; 0.447 sec/batch)
2016-02-03 18:39:00.931857: step 66770, loss = 0.75 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 18:39:05.654196: step 66780, loss = 0.70 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 18:39:10.276664: step 66790, loss = 0.82 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 18:39:15.061269: step 66800, loss = 0.70 (250.6 examples/sec; 0.511 sec/batch)
2016-02-03 18:39:20.275053: step 66810, loss = 0.62 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 18:39:24.957982: step 66820, loss = 0.73 (256.3 examples/sec; 0.499 sec/batch)
2016-02-03 18:39:29.644522: step 66830, loss = 0.70 (288.8 examples/sec; 0.443 sec/batch)
2016-02-03 18:39:34.315998: step 66840, loss = 0.55 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 18:39:39.025420: step 66850, loss = 0.69 (254.5 examples/sec; 0.503 sec/batch)
2016-02-03 18:39:43.733118: step 66860, loss = 0.66 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 18:39:48.423526: step 66870, loss = 0.69 (261.3 examples/sec; 0.490 sec/batch)
2016-02-03 18:39:53.065472: step 66880, loss = 0.81 (244.3 examples/sec; 0.524 sec/batch)
2016-02-03 18:39:57.702878: step 66890, loss = 0.79 (285.5 examples/sec; 0.448 sec/batch)
2016-02-03 18:40:02.425134: step 66900, loss = 0.64 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 18:40:07.662452: step 66910, loss = 0.65 (292.7 examples/sec; 0.437 sec/batch)
2016-02-03 18:40:12.396470: step 66920, loss = 0.68 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 18:40:17.102226: step 66930, loss = 0.71 (276.6 examples/sec; 0.463 sec/batch)
2016-02-03 18:40:21.793409: step 66940, loss = 0.75 (283.1 examples/sec; 0.452 sec/batch)
2016-02-03 18:40:26.455774: step 66950, loss = 0.67 (281.4 examples/sec; 0.455 sec/batch)
2016-02-03 18:40:31.112279: step 66960, loss = 0.80 (261.1 examples/sec; 0.490 sec/batch)
2016-02-03 18:40:35.850744: step 66970, loss = 0.77 (249.5 examples/sec; 0.513 sec/batch)
2016-02-03 18:40:40.622524: step 66980, loss = 0.76 (254.0 examples/sec; 0.504 sec/batch)
2016-02-03 18:40:45.344131: step 66990, loss = 0.79 (244.6 examples/sec; 0.523 sec/batch)
2016-02-03 18:40:50.038153: step 67000, loss = 0.70 (278.5 examples/sec; 0.460 sec/batch)
2016-02-03 18:40:55.377286: step 67010, loss = 0.65 (239.7 examples/sec; 0.534 sec/batch)
2016-02-03 18:41:00.043211: step 67020, loss = 0.86 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 18:41:04.738279: step 67030, loss = 0.73 (291.6 examples/sec; 0.439 sec/batch)
2016-02-03 18:41:09.418433: step 67040, loss = 0.64 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 18:41:14.135197: step 67050, loss = 0.81 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 18:41:18.752389: step 67060, loss = 0.62 (295.6 examples/sec; 0.433 sec/batch)
2016-02-03 18:41:23.529967: step 67070, loss = 0.67 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 18:41:28.151095: step 67080, loss = 0.82 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 18:41:32.911854: step 67090, loss = 0.62 (250.1 examples/sec; 0.512 sec/batch)
2016-02-03 18:41:37.608971: step 67100, loss = 0.65 (258.3 examples/sec; 0.496 sec/batch)
2016-02-03 18:41:42.761257: step 67110, loss = 0.73 (263.1 examples/sec; 0.487 sec/batch)
2016-02-03 18:41:47.457297: step 67120, loss = 0.75 (285.1 examples/sec; 0.449 sec/batch)
2016-02-03 18:41:52.137874: step 67130, loss = 0.74 (287.5 examples/sec; 0.445 sec/batch)
2016-02-03 18:41:56.839292: step 67140, loss = 0.72 (280.1 examples/sec; 0.457 sec/batch)
2016-02-03 18:42:01.564655: step 67150, loss = 0.58 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 18:42:06.135122: step 67160, loss = 0.71 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 18:42:10.791139: step 67170, loss = 0.70 (267.5 examples/sec; 0.479 sec/batch)
2016-02-03 18:42:15.387668: step 67180, loss = 0.71 (280.6 examples/sec; 0.456 sec/batch)
2016-02-03 18:42:20.053606: step 67190, loss = 0.80 (320.2 examples/sec; 0.400 sec/batch)
2016-02-03 18:42:24.842090: step 67200, loss = 0.69 (276.6 examples/sec; 0.463 sec/batch)
2016-02-03 18:42:30.122641: step 67210, loss = 0.72 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 18:42:34.885301: step 67220, loss = 0.82 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 18:42:39.544451: step 67230, loss = 0.80 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 18:42:44.249999: step 67240, loss = 0.70 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 18:42:48.898858: step 67250, loss = 0.76 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 18:42:53.579536: step 67260, loss = 0.77 (277.9 examples/sec; 0.461 sec/batch)
2016-02-03 18:42:58.065804: step 67270, loss = 0.65 (287.6 examples/sec; 0.445 sec/batch)
2016-02-03 18:43:02.691898: step 67280, loss = 0.65 (253.5 examples/sec; 0.505 sec/batch)
2016-02-03 18:43:07.265528: step 67290, loss = 0.70 (288.1 examples/sec; 0.444 sec/batch)
2016-02-03 18:43:11.870655: step 67300, loss = 0.74 (293.6 examples/sec; 0.436 sec/batch)
2016-02-03 18:43:16.885505: step 67310, loss = 0.64 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 18:43:21.482855: step 67320, loss = 0.70 (267.2 examples/sec; 0.479 sec/batch)
2016-02-03 18:43:26.084441: step 67330, loss = 0.84 (256.2 examples/sec; 0.500 sec/batch)
2016-02-03 18:43:30.685675: step 67340, loss = 0.77 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 18:43:35.252033: step 67350, loss = 0.70 (286.6 examples/sec; 0.447 sec/batch)
2016-02-03 18:43:39.950706: step 67360, loss = 0.77 (297.6 examples/sec; 0.430 sec/batch)
2016-02-03 18:43:44.658173: step 67370, loss = 0.60 (260.4 examples/sec; 0.492 sec/batch)
2016-02-03 18:43:49.295449: step 67380, loss = 0.77 (257.5 examples/sec; 0.497 sec/batch)
2016-02-03 18:43:53.949147: step 67390, loss = 0.77 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 18:43:58.585791: step 67400, loss = 0.78 (265.0 examples/sec; 0.483 sec/batch)
2016-02-03 18:44:03.746620: step 67410, loss = 0.76 (282.6 examples/sec; 0.453 sec/batch)
2016-02-03 18:44:08.398793: step 67420, loss = 0.82 (283.5 examples/sec; 0.451 sec/batch)
2016-02-03 18:44:13.062798: step 67430, loss = 0.80 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 18:44:17.713234: step 67440, loss = 0.57 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 18:44:22.287950: step 67450, loss = 0.73 (308.9 examples/sec; 0.414 sec/batch)
2016-02-03 18:44:26.997850: step 67460, loss = 0.61 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 18:44:31.717304: step 67470, loss = 0.70 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 18:44:36.449612: step 67480, loss = 0.66 (258.9 examples/sec; 0.494 sec/batch)
2016-02-03 18:44:41.045563: step 67490, loss = 0.74 (287.7 examples/sec; 0.445 sec/batch)
2016-02-03 18:44:45.694568: step 67500, loss = 0.75 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 18:44:50.842316: step 67510, loss = 0.78 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 18:44:55.553063: step 67520, loss = 0.65 (282.6 examples/sec; 0.453 sec/batch)
2016-02-03 18:45:00.240435: step 67530, loss = 0.69 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 18:45:04.930585: step 67540, loss = 0.84 (254.6 examples/sec; 0.503 sec/batch)
2016-02-03 18:45:09.641365: step 67550, loss = 0.75 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 18:45:14.349014: step 67560, loss = 0.60 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 18:45:18.984903: step 67570, loss = 0.64 (263.2 examples/sec; 0.486 sec/batch)
2016-02-03 18:45:23.639642: step 67580, loss = 0.67 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 18:45:28.317280: step 67590, loss = 0.74 (256.6 examples/sec; 0.499 sec/batch)
2016-02-03 18:45:32.967607: step 67600, loss = 0.74 (298.7 examples/sec; 0.428 sec/batch)
2016-02-03 18:45:38.204969: step 67610, loss = 0.80 (283.3 examples/sec; 0.452 sec/batch)
2016-02-03 18:45:42.902944: step 67620, loss = 0.75 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 18:45:47.582819: step 67630, loss = 0.73 (327.1 examples/sec; 0.391 sec/batch)
2016-02-03 18:45:52.237987: step 67640, loss = 0.68 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 18:45:56.874596: step 67650, loss = 0.74 (259.0 examples/sec; 0.494 sec/batch)
2016-02-03 18:46:01.553430: step 67660, loss = 0.79 (283.5 examples/sec; 0.451 sec/batch)
2016-02-03 18:46:06.307253: step 67670, loss = 0.72 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 18:46:11.012633: step 67680, loss = 0.70 (259.9 examples/sec; 0.493 sec/batch)
2016-02-03 18:46:15.738915: step 67690, loss = 0.80 (285.8 examples/sec; 0.448 sec/batch)
2016-02-03 18:46:20.432796: step 67700, loss = 0.67 (305.7 examples/sec; 0.419 sec/batch)
2016-02-03 18:46:25.631623: step 67710, loss = 0.67 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 18:46:30.297685: step 67720, loss = 0.77 (258.8 examples/sec; 0.495 sec/batch)
2016-02-03 18:46:34.980120: step 67730, loss = 0.57 (245.0 examples/sec; 0.522 sec/batch)
2016-02-03 18:46:39.641945: step 67740, loss = 0.68 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 18:46:44.369260: step 67750, loss = 0.81 (267.3 examples/sec; 0.479 sec/batch)
2016-02-03 18:46:49.047943: step 67760, loss = 0.86 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 18:46:53.812142: step 67770, loss = 0.72 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 18:46:58.490997: step 67780, loss = 0.70 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 18:47:03.186202: step 67790, loss = 0.78 (277.7 examples/sec; 0.461 sec/batch)
2016-02-03 18:47:07.893584: step 67800, loss = 0.80 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 18:47:13.188694: step 67810, loss = 0.78 (264.2 examples/sec; 0.485 sec/batch)
2016-02-03 18:47:17.954487: step 67820, loss = 0.71 (268.1 examples/sec; 0.478 sec/batch)
2016-02-03 18:47:22.603175: step 67830, loss = 0.86 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 18:47:27.200354: step 67840, loss = 0.57 (284.7 examples/sec; 0.450 sec/batch)
2016-02-03 18:47:31.979915: step 67850, loss = 0.70 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 18:47:36.622441: step 67860, loss = 0.76 (264.2 examples/sec; 0.484 sec/batch)
2016-02-03 18:47:41.247717: step 67870, loss = 0.71 (291.9 examples/sec; 0.438 sec/batch)
2016-02-03 18:47:45.921922: step 67880, loss = 0.67 (267.3 examples/sec; 0.479 sec/batch)
2016-02-03 18:47:50.695324: step 67890, loss = 0.64 (264.9 examples/sec; 0.483 sec/batch)
2016-02-03 18:47:55.326933: step 67900, loss = 0.77 (288.8 examples/sec; 0.443 sec/batch)
2016-02-03 18:48:00.461857: step 67910, loss = 0.78 (277.7 examples/sec; 0.461 sec/batch)
2016-02-03 18:48:05.131871: step 67920, loss = 0.80 (276.8 examples/sec; 0.463 sec/batch)
2016-02-03 18:48:09.765392: step 67930, loss = 0.58 (280.4 examples/sec; 0.457 sec/batch)
2016-02-03 18:48:14.440199: step 67940, loss = 0.61 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 18:48:19.126093: step 67950, loss = 0.56 (298.4 examples/sec; 0.429 sec/batch)
2016-02-03 18:48:23.787675: step 67960, loss = 0.71 (299.3 examples/sec; 0.428 sec/batch)
2016-02-03 18:48:28.455105: step 67970, loss = 0.72 (290.1 examples/sec; 0.441 sec/batch)
2016-02-03 18:48:33.136390: step 67980, loss = 0.69 (283.5 examples/sec; 0.452 sec/batch)
2016-02-03 18:48:37.886493: step 67990, loss = 0.60 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 18:48:42.575719: step 68000, loss = 0.74 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 18:48:47.699208: step 68010, loss = 0.74 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 18:48:52.407374: step 68020, loss = 0.68 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 18:48:57.152529: step 68030, loss = 0.68 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 18:49:01.806759: step 68040, loss = 0.73 (305.0 examples/sec; 0.420 sec/batch)
2016-02-03 18:49:06.527157: step 68050, loss = 0.85 (267.0 examples/sec; 0.479 sec/batch)
2016-02-03 18:49:11.216171: step 68060, loss = 0.59 (279.4 examples/sec; 0.458 sec/batch)
2016-02-03 18:49:15.860639: step 68070, loss = 0.73 (252.7 examples/sec; 0.507 sec/batch)
2016-02-03 18:49:20.611931: step 68080, loss = 0.86 (255.2 examples/sec; 0.502 sec/batch)
2016-02-03 18:49:25.241124: step 68090, loss = 0.70 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 18:49:29.977242: step 68100, loss = 0.78 (289.6 examples/sec; 0.442 sec/batch)
2016-02-03 18:49:35.236437: step 68110, loss = 0.69 (304.3 examples/sec; 0.421 sec/batch)
2016-02-03 18:49:40.075114: step 68120, loss = 0.75 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 18:49:44.851874: step 68130, loss = 0.69 (258.0 examples/sec; 0.496 sec/batch)
2016-02-03 18:49:49.463026: step 68140, loss = 0.86 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 18:49:54.135370: step 68150, loss = 0.61 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 18:49:58.940954: step 68160, loss = 0.72 (253.4 examples/sec; 0.505 sec/batch)
2016-02-03 18:50:03.616504: step 68170, loss = 0.80 (294.5 examples/sec; 0.435 sec/batch)
2016-02-03 18:50:08.237268: step 68180, loss = 0.73 (288.6 examples/sec; 0.444 sec/batch)
2016-02-03 18:50:12.934114: step 68190, loss = 0.80 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 18:50:17.624050: step 68200, loss = 0.71 (267.0 examples/sec; 0.479 sec/batch)
2016-02-03 18:50:22.843678: step 68210, loss = 0.79 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 18:50:27.597065: step 68220, loss = 0.72 (255.7 examples/sec; 0.501 sec/batch)
2016-02-03 18:50:32.256178: step 68230, loss = 0.68 (244.6 examples/sec; 0.523 sec/batch)
2016-02-03 18:50:36.896420: step 68240, loss = 0.78 (274.4 examples/sec; 0.467 sec/batch)
2016-02-03 18:50:41.612511: step 68250, loss = 0.80 (286.5 examples/sec; 0.447 sec/batch)
2016-02-03 18:50:46.291749: step 68260, loss = 0.84 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 18:50:50.939753: step 68270, loss = 0.64 (277.2 examples/sec; 0.462 sec/batch)
2016-02-03 18:50:55.684331: step 68280, loss = 0.66 (260.9 examples/sec; 0.491 sec/batch)
2016-02-03 18:51:00.413652: step 68290, loss = 0.60 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 18:51:05.068824: step 68300, loss = 0.60 (295.6 examples/sec; 0.433 sec/batch)
2016-02-03 18:51:10.254801: step 68310, loss = 0.78 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 18:51:14.944135: step 68320, loss = 0.71 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 18:51:19.664993: step 68330, loss = 0.88 (262.0 examples/sec; 0.489 sec/batch)
2016-02-03 18:51:24.389105: step 68340, loss = 0.69 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 18:51:29.131795: step 68350, loss = 0.69 (258.0 examples/sec; 0.496 sec/batch)
2016-02-03 18:51:33.812293: step 68360, loss = 0.82 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 18:51:38.522679: step 68370, loss = 0.75 (295.6 examples/sec; 0.433 sec/batch)
2016-02-03 18:51:43.191108: step 68380, loss = 0.71 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 18:51:47.821677: step 68390, loss = 0.76 (279.7 examples/sec; 0.458 sec/batch)
2016-02-03 18:51:52.575558: step 68400, loss = 0.87 (280.3 examples/sec; 0.457 sec/batch)
2016-02-03 18:51:57.783768: step 68410, loss = 0.77 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 18:52:02.547473: step 68420, loss = 0.62 (276.6 examples/sec; 0.463 sec/batch)
2016-02-03 18:52:07.185717: step 68430, loss = 0.81 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 18:52:11.754948: step 68440, loss = 0.73 (245.7 examples/sec; 0.521 sec/batch)
2016-02-03 18:52:16.499460: step 68450, loss = 0.77 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 18:52:21.081160: step 68460, loss = 0.72 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 18:52:25.762899: step 68470, loss = 0.73 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 18:52:30.394080: step 68480, loss = 0.61 (298.9 examples/sec; 0.428 sec/batch)
2016-02-03 18:52:35.092040: step 68490, loss = 0.79 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 18:52:39.778489: step 68500, loss = 0.71 (285.2 examples/sec; 0.449 sec/batch)
2016-02-03 18:52:45.045348: step 68510, loss = 0.72 (300.3 examples/sec; 0.426 sec/batch)
2016-02-03 18:52:49.769104: step 68520, loss = 0.64 (249.1 examples/sec; 0.514 sec/batch)
2016-02-03 18:52:54.447270: step 68530, loss = 0.66 (257.2 examples/sec; 0.498 sec/batch)
2016-02-03 18:52:59.169601: step 68540, loss = 0.77 (250.8 examples/sec; 0.510 sec/batch)
2016-02-03 18:53:03.835794: step 68550, loss = 0.63 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 18:53:08.552646: step 68560, loss = 0.82 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 18:53:13.274966: step 68570, loss = 0.74 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 18:53:17.960429: step 68580, loss = 0.82 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 18:53:22.574909: step 68590, loss = 0.79 (290.7 examples/sec; 0.440 sec/batch)
2016-02-03 18:53:27.225561: step 68600, loss = 0.75 (260.9 examples/sec; 0.491 sec/batch)
2016-02-03 18:53:32.494952: step 68610, loss = 0.74 (274.4 examples/sec; 0.466 sec/batch)
2016-02-03 18:53:37.182090: step 68620, loss = 0.79 (263.1 examples/sec; 0.486 sec/batch)
2016-02-03 18:53:41.915805: step 68630, loss = 0.58 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 18:53:46.629427: step 68640, loss = 0.73 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 18:53:51.374946: step 68650, loss = 0.73 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 18:53:56.040303: step 68660, loss = 0.86 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 18:54:00.724633: step 68670, loss = 0.69 (259.4 examples/sec; 0.494 sec/batch)
2016-02-03 18:54:05.443753: step 68680, loss = 0.76 (248.4 examples/sec; 0.515 sec/batch)
2016-02-03 18:54:10.029825: step 68690, loss = 0.62 (294.8 examples/sec; 0.434 sec/batch)
2016-02-03 18:54:14.709953: step 68700, loss = 0.79 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 18:54:19.866418: step 68710, loss = 0.70 (281.0 examples/sec; 0.456 sec/batch)
2016-02-03 18:54:24.456611: step 68720, loss = 0.85 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 18:54:29.141790: step 68730, loss = 0.74 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 18:54:33.839026: step 68740, loss = 0.76 (264.9 examples/sec; 0.483 sec/batch)
2016-02-03 18:54:38.445945: step 68750, loss = 0.69 (281.2 examples/sec; 0.455 sec/batch)
2016-02-03 18:54:43.110094: step 68760, loss = 0.61 (280.9 examples/sec; 0.456 sec/batch)
2016-02-03 18:54:47.699900: step 68770, loss = 0.72 (299.2 examples/sec; 0.428 sec/batch)
2016-02-03 18:54:52.524611: step 68780, loss = 0.73 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 18:54:57.162045: step 68790, loss = 0.82 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 18:55:01.827812: step 68800, loss = 0.65 (307.0 examples/sec; 0.417 sec/batch)
2016-02-03 18:55:07.011899: step 68810, loss = 0.58 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 18:55:11.641963: step 68820, loss = 0.66 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 18:55:16.372762: step 68830, loss = 0.66 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 18:55:20.966201: step 68840, loss = 0.61 (279.6 examples/sec; 0.458 sec/batch)
2016-02-03 18:55:25.691930: step 68850, loss = 0.69 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 18:55:30.400620: step 68860, loss = 0.78 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 18:55:35.031866: step 68870, loss = 0.51 (250.0 examples/sec; 0.512 sec/batch)
2016-02-03 18:55:39.718240: step 68880, loss = 0.82 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 18:55:44.383951: step 68890, loss = 0.77 (294.1 examples/sec; 0.435 sec/batch)
2016-02-03 18:55:49.038786: step 68900, loss = 0.77 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 18:55:54.271254: step 68910, loss = 0.69 (308.7 examples/sec; 0.415 sec/batch)
2016-02-03 18:55:58.917671: step 68920, loss = 0.77 (260.1 examples/sec; 0.492 sec/batch)
2016-02-03 18:56:03.563658: step 68930, loss = 0.64 (289.6 examples/sec; 0.442 sec/batch)
2016-02-03 18:56:08.289095: step 68940, loss = 0.70 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 18:56:13.010560: step 68950, loss = 0.91 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 18:56:17.685927: step 68960, loss = 0.76 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 18:56:22.373616: step 68970, loss = 0.75 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 18:56:27.130721: step 68980, loss = 0.65 (243.8 examples/sec; 0.525 sec/batch)
2016-02-03 18:56:31.838851: step 68990, loss = 0.67 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 18:56:36.600424: step 69000, loss = 0.85 (260.5 examples/sec; 0.491 sec/batch)
2016-02-03 18:56:41.824748: step 69010, loss = 0.62 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 18:56:46.523895: step 69020, loss = 0.63 (294.7 examples/sec; 0.434 sec/batch)
2016-02-03 18:56:51.214033: step 69030, loss = 0.59 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 18:56:55.906053: step 69040, loss = 0.77 (268.0 examples/sec; 0.478 sec/batch)
2016-02-03 18:57:00.562567: step 69050, loss = 0.66 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 18:57:05.342144: step 69060, loss = 0.71 (246.3 examples/sec; 0.520 sec/batch)
2016-02-03 18:57:10.124450: step 69070, loss = 0.83 (243.4 examples/sec; 0.526 sec/batch)
2016-02-03 18:57:14.801657: step 69080, loss = 0.78 (302.8 examples/sec; 0.423 sec/batch)
2016-02-03 18:57:19.525284: step 69090, loss = 0.73 (281.2 examples/sec; 0.455 sec/batch)
2016-02-03 18:57:24.300662: step 69100, loss = 0.66 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 18:57:29.559013: step 69110, loss = 0.64 (270.9 examples/sec; 0.472 sec/batch)
2016-02-03 18:57:34.303172: step 69120, loss = 0.79 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 18:57:38.997364: step 69130, loss = 0.71 (288.4 examples/sec; 0.444 sec/batch)
2016-02-03 18:57:43.727457: step 69140, loss = 0.78 (265.0 examples/sec; 0.483 sec/batch)
2016-02-03 18:57:48.400361: step 69150, loss = 0.76 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 18:57:53.127995: step 69160, loss = 0.75 (277.8 examples/sec; 0.461 sec/batch)
2016-02-03 18:57:57.817135: step 69170, loss = 0.83 (265.2 examples/sec; 0.483 sec/batch)
2016-02-03 18:58:02.499094: step 69180, loss = 0.64 (287.8 examples/sec; 0.445 sec/batch)
2016-02-03 18:58:07.232303: step 69190, loss = 0.71 (288.9 examples/sec; 0.443 sec/batch)
2016-02-03 18:58:11.921124: step 69200, loss = 0.69 (280.5 examples/sec; 0.456 sec/batch)
2016-02-03 18:58:17.145157: step 69210, loss = 0.63 (290.4 examples/sec; 0.441 sec/batch)
2016-02-03 18:58:21.907712: step 69220, loss = 0.74 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 18:58:26.596977: step 69230, loss = 0.86 (260.7 examples/sec; 0.491 sec/batch)
2016-02-03 18:58:31.289478: step 69240, loss = 0.77 (263.0 examples/sec; 0.487 sec/batch)
2016-02-03 18:58:36.080772: step 69250, loss = 0.86 (260.0 examples/sec; 0.492 sec/batch)
2016-02-03 18:58:40.781969: step 69260, loss = 0.72 (261.1 examples/sec; 0.490 sec/batch)
2016-02-03 18:58:45.539444: step 69270, loss = 0.66 (264.3 examples/sec; 0.484 sec/batch)
2016-02-03 18:58:50.242797: step 69280, loss = 0.75 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 18:58:54.796944: step 69290, loss = 0.71 (295.7 examples/sec; 0.433 sec/batch)
2016-02-03 18:58:59.552561: step 69300, loss = 0.78 (287.3 examples/sec; 0.446 sec/batch)
2016-02-03 18:59:04.882272: step 69310, loss = 0.73 (257.9 examples/sec; 0.496 sec/batch)
2016-02-03 18:59:09.679376: step 69320, loss = 0.67 (257.1 examples/sec; 0.498 sec/batch)
2016-02-03 18:59:14.383790: step 69330, loss = 0.70 (294.9 examples/sec; 0.434 sec/batch)
2016-02-03 18:59:19.107890: step 69340, loss = 0.83 (308.3 examples/sec; 0.415 sec/batch)
2016-02-03 18:59:23.844809: step 69350, loss = 0.65 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 18:59:28.512525: step 69360, loss = 0.71 (278.6 examples/sec; 0.460 sec/batch)
2016-02-03 18:59:33.219266: step 69370, loss = 0.84 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 18:59:37.873803: step 69380, loss = 0.76 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 18:59:42.541992: step 69390, loss = 0.72 (290.1 examples/sec; 0.441 sec/batch)
2016-02-03 18:59:47.250084: step 69400, loss = 0.74 (243.3 examples/sec; 0.526 sec/batch)
2016-02-03 18:59:52.401728: step 69410, loss = 0.70 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 18:59:57.070308: step 69420, loss = 0.75 (257.6 examples/sec; 0.497 sec/batch)
2016-02-03 19:00:01.789167: step 69430, loss = 0.74 (278.5 examples/sec; 0.460 sec/batch)
2016-02-03 19:00:06.524131: step 69440, loss = 0.75 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 19:00:11.218905: step 69450, loss = 0.77 (285.5 examples/sec; 0.448 sec/batch)
2016-02-03 19:00:15.983860: step 69460, loss = 0.81 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 19:00:20.671749: step 69470, loss = 0.59 (289.8 examples/sec; 0.442 sec/batch)
2016-02-03 19:00:25.359645: step 69480, loss = 0.64 (255.6 examples/sec; 0.501 sec/batch)
2016-02-03 19:00:30.014790: step 69490, loss = 0.74 (256.8 examples/sec; 0.499 sec/batch)
2016-02-03 19:00:34.755861: step 69500, loss = 0.90 (248.9 examples/sec; 0.514 sec/batch)
2016-02-03 19:00:40.086552: step 69510, loss = 0.70 (255.8 examples/sec; 0.500 sec/batch)
2016-02-03 19:00:44.767311: step 69520, loss = 0.80 (257.6 examples/sec; 0.497 sec/batch)
2016-02-03 19:00:49.553993: step 69530, loss = 0.63 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 19:00:54.147902: step 69540, loss = 0.74 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 19:00:58.823270: step 69550, loss = 0.64 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 19:01:03.514469: step 69560, loss = 0.72 (258.2 examples/sec; 0.496 sec/batch)
2016-02-03 19:01:08.194616: step 69570, loss = 0.70 (295.0 examples/sec; 0.434 sec/batch)
2016-02-03 19:01:12.921844: step 69580, loss = 0.78 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 19:01:17.623622: step 69590, loss = 0.68 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 19:01:22.263725: step 69600, loss = 0.78 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 19:01:27.551323: step 69610, loss = 0.69 (253.6 examples/sec; 0.505 sec/batch)
2016-02-03 19:01:32.217261: step 69620, loss = 0.68 (280.1 examples/sec; 0.457 sec/batch)
2016-02-03 19:01:36.926989: step 69630, loss = 0.67 (242.0 examples/sec; 0.529 sec/batch)
2016-02-03 19:01:41.660688: step 69640, loss = 0.82 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 19:01:46.358685: step 69650, loss = 0.90 (283.9 examples/sec; 0.451 sec/batch)
2016-02-03 19:01:51.049206: step 69660, loss = 0.68 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 19:01:55.766317: step 69670, loss = 0.74 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 19:02:00.469223: step 69680, loss = 0.58 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 19:02:05.214440: step 69690, loss = 0.83 (287.9 examples/sec; 0.445 sec/batch)
2016-02-03 19:02:09.978196: step 69700, loss = 0.65 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 19:02:15.276317: step 69710, loss = 0.76 (259.5 examples/sec; 0.493 sec/batch)
2016-02-03 19:02:19.976444: step 69720, loss = 0.66 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 19:02:24.719063: step 69730, loss = 0.67 (264.5 examples/sec; 0.484 sec/batch)
2016-02-03 19:02:29.471666: step 69740, loss = 0.66 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 19:02:34.133420: step 69750, loss = 0.78 (285.6 examples/sec; 0.448 sec/batch)
2016-02-03 19:02:38.888155: step 69760, loss = 0.77 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 19:02:43.622765: step 69770, loss = 0.66 (254.5 examples/sec; 0.503 sec/batch)
2016-02-03 19:02:48.253111: step 69780, loss = 0.70 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 19:02:52.996913: step 69790, loss = 0.59 (261.9 examples/sec; 0.489 sec/batch)
2016-02-03 19:02:57.755667: step 69800, loss = 1.02 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 19:03:03.014899: step 69810, loss = 0.76 (257.2 examples/sec; 0.498 sec/batch)
2016-02-03 19:03:07.773979: step 69820, loss = 0.64 (259.8 examples/sec; 0.493 sec/batch)
2016-02-03 19:03:12.496360: step 69830, loss = 0.67 (253.2 examples/sec; 0.506 sec/batch)
2016-02-03 19:03:17.153030: step 69840, loss = 0.71 (301.6 examples/sec; 0.424 sec/batch)
2016-02-03 19:03:21.863956: step 69850, loss = 0.87 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 19:03:26.492219: step 69860, loss = 0.81 (294.3 examples/sec; 0.435 sec/batch)
2016-02-03 19:03:31.229106: step 69870, loss = 0.80 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 19:03:35.975755: step 69880, loss = 0.63 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 19:03:40.718527: step 69890, loss = 0.84 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 19:03:45.413865: step 69900, loss = 0.81 (262.2 examples/sec; 0.488 sec/batch)
2016-02-03 19:03:50.532684: step 69910, loss = 0.62 (305.1 examples/sec; 0.420 sec/batch)
2016-02-03 19:03:55.283768: step 69920, loss = 0.64 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 19:03:59.990032: step 69930, loss = 0.75 (284.9 examples/sec; 0.449 sec/batch)
2016-02-03 19:04:04.666444: step 69940, loss = 0.72 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 19:04:09.372801: step 69950, loss = 0.77 (283.5 examples/sec; 0.451 sec/batch)
2016-02-03 19:04:14.063555: step 69960, loss = 0.84 (288.5 examples/sec; 0.444 sec/batch)
2016-02-03 19:04:18.724859: step 69970, loss = 0.69 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 19:04:23.447977: step 69980, loss = 0.84 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 19:04:28.080477: step 69990, loss = 0.82 (273.2 examples/sec; 0.469 sec/batch)
2016-02-03 19:04:32.737021: step 70000, loss = 0.93 (286.2 examples/sec; 0.447 sec/batch)
2016-02-03 19:04:38.031662: step 70010, loss = 0.74 (247.4 examples/sec; 0.517 sec/batch)
2016-02-03 19:04:42.716180: step 70020, loss = 0.65 (258.9 examples/sec; 0.494 sec/batch)
2016-02-03 19:04:47.424380: step 70030, loss = 0.82 (285.6 examples/sec; 0.448 sec/batch)
2016-02-03 19:04:52.169388: step 70040, loss = 0.65 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 19:04:56.883593: step 70050, loss = 0.73 (252.8 examples/sec; 0.506 sec/batch)
2016-02-03 19:05:01.526740: step 70060, loss = 0.68 (244.5 examples/sec; 0.523 sec/batch)
2016-02-03 19:05:06.129321: step 70070, loss = 0.74 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 19:05:10.845770: step 70080, loss = 0.66 (252.9 examples/sec; 0.506 sec/batch)
2016-02-03 19:05:15.563248: step 70090, loss = 0.82 (247.2 examples/sec; 0.518 sec/batch)
2016-02-03 19:05:20.147573: step 70100, loss = 0.83 (292.7 examples/sec; 0.437 sec/batch)
2016-02-03 19:05:25.441808: step 70110, loss = 0.73 (249.0 examples/sec; 0.514 sec/batch)
2016-02-03 19:05:30.030866: step 70120, loss = 0.70 (280.3 examples/sec; 0.457 sec/batch)
2016-02-03 19:05:34.771833: step 70130, loss = 0.80 (281.4 examples/sec; 0.455 sec/batch)
2016-02-03 19:05:39.497293: step 70140, loss = 0.75 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 19:05:44.254537: step 70150, loss = 0.78 (254.5 examples/sec; 0.503 sec/batch)
2016-02-03 19:05:48.940164: step 70160, loss = 0.75 (260.6 examples/sec; 0.491 sec/batch)
2016-02-03 19:05:53.632821: step 70170, loss = 0.69 (259.9 examples/sec; 0.492 sec/batch)
2016-02-03 19:05:58.347288: step 70180, loss = 0.74 (280.1 examples/sec; 0.457 sec/batch)
2016-02-03 19:06:02.951484: step 70190, loss = 0.78 (289.8 examples/sec; 0.442 sec/batch)
2016-02-03 19:06:07.639111: step 70200, loss = 0.77 (319.2 examples/sec; 0.401 sec/batch)
2016-02-03 19:06:12.842575: step 70210, loss = 0.87 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 19:06:17.418542: step 70220, loss = 0.74 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 19:06:22.022689: step 70230, loss = 0.71 (285.2 examples/sec; 0.449 sec/batch)
2016-02-03 19:06:26.688396: step 70240, loss = 0.72 (297.4 examples/sec; 0.430 sec/batch)
2016-02-03 19:06:31.401912: step 70250, loss = 0.58 (260.6 examples/sec; 0.491 sec/batch)
2016-02-03 19:06:36.051797: step 70260, loss = 0.66 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 19:06:40.712107: step 70270, loss = 0.72 (281.2 examples/sec; 0.455 sec/batch)
2016-02-03 19:06:45.364317: step 70280, loss = 0.76 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 19:06:50.065185: step 70290, loss = 0.70 (292.1 examples/sec; 0.438 sec/batch)
2016-02-03 19:06:54.775933: step 70300, loss = 0.77 (264.9 examples/sec; 0.483 sec/batch)
2016-02-03 19:06:59.882058: step 70310, loss = 0.76 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 19:07:04.517022: step 70320, loss = 0.59 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 19:07:09.128492: step 70330, loss = 0.73 (285.7 examples/sec; 0.448 sec/batch)
2016-02-03 19:07:13.833097: step 70340, loss = 0.79 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 19:07:18.534994: step 70350, loss = 0.65 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 19:07:23.229937: step 70360, loss = 0.62 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 19:07:27.908053: step 70370, loss = 0.73 (287.2 examples/sec; 0.446 sec/batch)
2016-02-03 19:07:32.693868: step 70380, loss = 0.78 (258.0 examples/sec; 0.496 sec/batch)
2016-02-03 19:07:37.412087: step 70390, loss = 0.73 (276.3 examples/sec; 0.463 sec/batch)
2016-02-03 19:07:42.054480: step 70400, loss = 0.71 (289.7 examples/sec; 0.442 sec/batch)
2016-02-03 19:07:47.256964: step 70410, loss = 0.83 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 19:07:51.935307: step 70420, loss = 0.79 (287.9 examples/sec; 0.445 sec/batch)
2016-02-03 19:07:56.669791: step 70430, loss = 0.66 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 19:08:01.401319: step 70440, loss = 0.88 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 19:08:06.133844: step 70450, loss = 0.63 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 19:08:10.821295: step 70460, loss = 0.81 (301.1 examples/sec; 0.425 sec/batch)
2016-02-03 19:08:15.576917: step 70470, loss = 0.68 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 19:08:20.307791: step 70480, loss = 0.82 (254.0 examples/sec; 0.504 sec/batch)
2016-02-03 19:08:25.050617: step 70490, loss = 0.69 (248.6 examples/sec; 0.515 sec/batch)
2016-02-03 19:08:29.809987: step 70500, loss = 0.69 (254.1 examples/sec; 0.504 sec/batch)
2016-02-03 19:08:35.062255: step 70510, loss = 0.61 (261.7 examples/sec; 0.489 sec/batch)
2016-02-03 19:08:39.803587: step 70520, loss = 0.72 (287.0 examples/sec; 0.446 sec/batch)
2016-02-03 19:08:44.565397: step 70530, loss = 0.73 (252.1 examples/sec; 0.508 sec/batch)
2016-02-03 19:08:49.262174: step 70540, loss = 0.70 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 19:08:53.992938: step 70550, loss = 0.69 (248.3 examples/sec; 0.516 sec/batch)
2016-02-03 19:08:58.735919: step 70560, loss = 0.63 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 19:09:03.426873: step 70570, loss = 0.75 (295.6 examples/sec; 0.433 sec/batch)
2016-02-03 19:09:08.066082: step 70580, loss = 0.54 (298.9 examples/sec; 0.428 sec/batch)
2016-02-03 19:09:12.928338: step 70590, loss = 0.80 (252.1 examples/sec; 0.508 sec/batch)
2016-02-03 19:09:17.613791: step 70600, loss = 0.81 (254.7 examples/sec; 0.503 sec/batch)
2016-02-03 19:09:22.863827: step 70610, loss = 0.54 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 19:09:27.628756: step 70620, loss = 0.71 (248.0 examples/sec; 0.516 sec/batch)
2016-02-03 19:09:32.300468: step 70630, loss = 0.73 (276.9 examples/sec; 0.462 sec/batch)
2016-02-03 19:09:36.963161: step 70640, loss = 0.72 (258.2 examples/sec; 0.496 sec/batch)
2016-02-03 19:09:41.676383: step 70650, loss = 0.85 (279.8 examples/sec; 0.458 sec/batch)
2016-02-03 19:09:46.411684: step 70660, loss = 0.72 (280.9 examples/sec; 0.456 sec/batch)
2016-02-03 19:09:51.073832: step 70670, loss = 0.62 (283.3 examples/sec; 0.452 sec/batch)
2016-02-03 19:09:55.703102: step 70680, loss = 0.67 (314.1 examples/sec; 0.407 sec/batch)
2016-02-03 19:10:00.528028: step 70690, loss = 0.72 (254.1 examples/sec; 0.504 sec/batch)
2016-02-03 19:10:05.167653: step 70700, loss = 0.77 (275.0 examples/sec; 0.465 sec/batch)
2016-02-03 19:10:10.424224: step 70710, loss = 0.85 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 19:10:15.118734: step 70720, loss = 0.70 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 19:10:19.915345: step 70730, loss = 0.82 (261.4 examples/sec; 0.490 sec/batch)
2016-02-03 19:10:24.570063: step 70740, loss = 0.69 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 19:10:29.316503: step 70750, loss = 1.00 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 19:10:33.966176: step 70760, loss = 0.77 (297.4 examples/sec; 0.430 sec/batch)
2016-02-03 19:10:38.684047: step 70770, loss = 0.82 (264.7 examples/sec; 0.483 sec/batch)
2016-02-03 19:10:43.424235: step 70780, loss = 0.75 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 19:10:48.190996: step 70790, loss = 0.82 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 19:10:52.855656: step 70800, loss = 0.84 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 19:10:58.133891: step 70810, loss = 0.70 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 19:11:02.900427: step 70820, loss = 0.67 (265.8 examples/sec; 0.482 sec/batch)
2016-02-03 19:11:07.561726: step 70830, loss = 0.81 (261.7 examples/sec; 0.489 sec/batch)
2016-02-03 19:11:12.185357: step 70840, loss = 0.67 (291.2 examples/sec; 0.440 sec/batch)
2016-02-03 19:11:16.917420: step 70850, loss = 0.81 (287.3 examples/sec; 0.446 sec/batch)
2016-02-03 19:11:21.634650: step 70860, loss = 0.72 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 19:11:26.317115: step 70870, loss = 0.63 (299.2 examples/sec; 0.428 sec/batch)
2016-02-03 19:11:31.037946: step 70880, loss = 0.66 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 19:11:35.749440: step 70890, loss = 0.71 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 19:11:40.521729: step 70900, loss = 0.75 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 19:11:45.813520: step 70910, loss = 0.61 (293.0 examples/sec; 0.437 sec/batch)
2016-02-03 19:11:50.511610: step 70920, loss = 0.78 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 19:11:55.212227: step 70930, loss = 0.80 (254.3 examples/sec; 0.503 sec/batch)
2016-02-03 19:11:59.945432: step 70940, loss = 0.76 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 19:12:04.660703: step 70950, loss = 0.74 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 19:12:09.404721: step 70960, loss = 0.70 (277.8 examples/sec; 0.461 sec/batch)
2016-02-03 19:12:14.067206: step 70970, loss = 0.85 (294.2 examples/sec; 0.435 sec/batch)
2016-02-03 19:12:18.825409: step 70980, loss = 0.76 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 19:12:23.539084: step 70990, loss = 0.59 (287.8 examples/sec; 0.445 sec/batch)
2016-02-03 19:12:28.315658: step 71000, loss = 0.77 (292.4 examples/sec; 0.438 sec/batch)
2016-02-03 19:12:33.593988: step 71010, loss = 0.71 (258.3 examples/sec; 0.495 sec/batch)
2016-02-03 19:12:38.375003: step 71020, loss = 0.71 (258.2 examples/sec; 0.496 sec/batch)
2016-02-03 19:12:43.041747: step 71030, loss = 0.75 (294.5 examples/sec; 0.435 sec/batch)
2016-02-03 19:12:47.731585: step 71040, loss = 0.63 (284.9 examples/sec; 0.449 sec/batch)
2016-02-03 19:12:52.435688: step 71050, loss = 0.87 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 19:12:57.095276: step 71060, loss = 0.73 (252.3 examples/sec; 0.507 sec/batch)
2016-02-03 19:13:01.809148: step 71070, loss = 0.88 (255.4 examples/sec; 0.501 sec/batch)
2016-02-03 19:13:06.571136: step 71080, loss = 0.73 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 19:13:11.359268: step 71090, loss = 0.87 (278.5 examples/sec; 0.460 sec/batch)
2016-02-03 19:13:16.103383: step 71100, loss = 0.67 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 19:13:21.367606: step 71110, loss = 0.81 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 19:13:26.163131: step 71120, loss = 0.63 (247.7 examples/sec; 0.517 sec/batch)
2016-02-03 19:13:30.781002: step 71130, loss = 0.64 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 19:13:35.560737: step 71140, loss = 0.75 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 19:13:40.325125: step 71150, loss = 0.77 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 19:13:45.088377: step 71160, loss = 0.71 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 19:13:49.774877: step 71170, loss = 0.72 (273.2 examples/sec; 0.468 sec/batch)
2016-02-03 19:13:54.528678: step 71180, loss = 0.78 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 19:13:59.195952: step 71190, loss = 0.67 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 19:14:03.850063: step 71200, loss = 0.72 (278.0 examples/sec; 0.460 sec/batch)
2016-02-03 19:14:09.121182: step 71210, loss = 0.77 (265.2 examples/sec; 0.483 sec/batch)
2016-02-03 19:14:13.928052: step 71220, loss = 0.83 (264.1 examples/sec; 0.485 sec/batch)
2016-02-03 19:14:18.588474: step 71230, loss = 0.79 (284.1 examples/sec; 0.451 sec/batch)
2016-02-03 19:14:23.304499: step 71240, loss = 0.64 (296.3 examples/sec; 0.432 sec/batch)
2016-02-03 19:14:28.013830: step 71250, loss = 0.64 (270.9 examples/sec; 0.472 sec/batch)
2016-02-03 19:14:32.694319: step 71260, loss = 0.91 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 19:14:37.404924: step 71270, loss = 0.87 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 19:14:42.061787: step 71280, loss = 0.68 (289.9 examples/sec; 0.442 sec/batch)
2016-02-03 19:14:46.770380: step 71290, loss = 0.78 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 19:14:51.528342: step 71300, loss = 0.51 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 19:14:56.752961: step 71310, loss = 0.78 (279.7 examples/sec; 0.458 sec/batch)
2016-02-03 19:15:01.449547: step 71320, loss = 0.76 (273.2 examples/sec; 0.469 sec/batch)
2016-02-03 19:15:06.143891: step 71330, loss = 0.66 (256.7 examples/sec; 0.499 sec/batch)
2016-02-03 19:15:10.822098: step 71340, loss = 0.71 (301.8 examples/sec; 0.424 sec/batch)
2016-02-03 19:15:15.545671: step 71350, loss = 0.81 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 19:15:20.263578: step 71360, loss = 0.69 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 19:15:24.939515: step 71370, loss = 0.81 (260.3 examples/sec; 0.492 sec/batch)
2016-02-03 19:15:29.612669: step 71380, loss = 0.76 (267.5 examples/sec; 0.478 sec/batch)
2016-02-03 19:15:34.336698: step 71390, loss = 0.88 (257.4 examples/sec; 0.497 sec/batch)
2016-02-03 19:15:39.068672: step 71400, loss = 0.83 (298.2 examples/sec; 0.429 sec/batch)
2016-02-03 19:15:44.337950: step 71410, loss = 0.74 (280.3 examples/sec; 0.457 sec/batch)
2016-02-03 19:15:49.070904: step 71420, loss = 0.78 (255.5 examples/sec; 0.501 sec/batch)
2016-02-03 19:15:53.725564: step 71430, loss = 0.69 (291.9 examples/sec; 0.439 sec/batch)
2016-02-03 19:15:58.344520: step 71440, loss = 0.71 (306.2 examples/sec; 0.418 sec/batch)
2016-02-03 19:16:03.034959: step 71450, loss = 0.65 (301.9 examples/sec; 0.424 sec/batch)
2016-02-03 19:16:07.860765: step 71460, loss = 0.70 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 19:16:12.596364: step 71470, loss = 0.60 (276.7 examples/sec; 0.463 sec/batch)
2016-02-03 19:16:17.333455: step 71480, loss = 0.69 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 19:16:22.078202: step 71490, loss = 0.61 (287.0 examples/sec; 0.446 sec/batch)
2016-02-03 19:16:26.807313: step 71500, loss = 0.60 (281.7 examples/sec; 0.454 sec/batch)
2016-02-03 19:16:32.031985: step 71510, loss = 0.67 (281.7 examples/sec; 0.454 sec/batch)
2016-02-03 19:16:36.821594: step 71520, loss = 0.76 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 19:16:41.543197: step 71530, loss = 0.78 (262.6 examples/sec; 0.487 sec/batch)
2016-02-03 19:16:46.306414: step 71540, loss = 0.65 (282.8 examples/sec; 0.453 sec/batch)
2016-02-03 19:16:51.062205: step 71550, loss = 0.72 (297.2 examples/sec; 0.431 sec/batch)
2016-02-03 19:16:55.795021: step 71560, loss = 0.83 (283.0 examples/sec; 0.452 sec/batch)
2016-02-03 19:17:00.434768: step 71570, loss = 0.69 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 19:17:05.135006: step 71580, loss = 0.82 (258.3 examples/sec; 0.496 sec/batch)
2016-02-03 19:17:09.805483: step 71590, loss = 0.68 (280.4 examples/sec; 0.457 sec/batch)
2016-02-03 19:17:14.560582: step 71600, loss = 0.73 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 19:17:19.733402: step 71610, loss = 0.73 (264.5 examples/sec; 0.484 sec/batch)
2016-02-03 19:17:24.444145: step 71620, loss = 0.74 (262.9 examples/sec; 0.487 sec/batch)
2016-02-03 19:17:29.245857: step 71630, loss = 0.78 (252.8 examples/sec; 0.506 sec/batch)
2016-02-03 19:17:33.934526: step 71640, loss = 0.90 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 19:17:38.650981: step 71650, loss = 0.61 (286.7 examples/sec; 0.446 sec/batch)
2016-02-03 19:17:43.385383: step 71660, loss = 0.77 (285.1 examples/sec; 0.449 sec/batch)
2016-02-03 19:17:48.112786: step 71670, loss = 0.79 (260.8 examples/sec; 0.491 sec/batch)
2016-02-03 19:17:52.779953: step 71680, loss = 0.64 (283.0 examples/sec; 0.452 sec/batch)
2016-02-03 19:17:57.514148: step 71690, loss = 0.73 (280.6 examples/sec; 0.456 sec/batch)
2016-02-03 19:18:02.177189: step 71700, loss = 0.62 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 19:18:07.441475: step 71710, loss = 0.69 (258.4 examples/sec; 0.495 sec/batch)
2016-02-03 19:18:12.165949: step 71720, loss = 0.89 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 19:18:16.924962: step 71730, loss = 0.63 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 19:18:21.584380: step 71740, loss = 0.63 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 19:18:26.322745: step 71750, loss = 0.62 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 19:18:30.989840: step 71760, loss = 0.71 (277.7 examples/sec; 0.461 sec/batch)
2016-02-03 19:18:35.714680: step 71770, loss = 0.82 (297.6 examples/sec; 0.430 sec/batch)
2016-02-03 19:18:40.419143: step 71780, loss = 0.80 (285.3 examples/sec; 0.449 sec/batch)
2016-02-03 19:18:45.016400: step 71790, loss = 0.67 (299.2 examples/sec; 0.428 sec/batch)
2016-02-03 19:18:49.783171: step 71800, loss = 0.67 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 19:18:55.032944: step 71810, loss = 0.71 (284.0 examples/sec; 0.451 sec/batch)
2016-02-03 19:18:59.738128: step 71820, loss = 0.66 (252.0 examples/sec; 0.508 sec/batch)
2016-02-03 19:19:04.332006: step 71830, loss = 0.73 (285.4 examples/sec; 0.448 sec/batch)
2016-02-03 19:19:09.040909: step 71840, loss = 0.91 (292.7 examples/sec; 0.437 sec/batch)
2016-02-03 19:19:13.699397: step 71850, loss = 0.84 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 19:19:18.456424: step 71860, loss = 0.77 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 19:19:23.210782: step 71870, loss = 0.67 (257.6 examples/sec; 0.497 sec/batch)
2016-02-03 19:19:27.883568: step 71880, loss = 0.69 (297.1 examples/sec; 0.431 sec/batch)
2016-02-03 19:19:32.632119: step 71890, loss = 0.67 (286.9 examples/sec; 0.446 sec/batch)
2016-02-03 19:19:37.392561: step 71900, loss = 0.70 (267.0 examples/sec; 0.479 sec/batch)
2016-02-03 19:19:42.625695: step 71910, loss = 0.71 (275.7 examples/sec; 0.464 sec/batch)
2016-02-03 19:19:47.316496: step 71920, loss = 0.78 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 19:19:52.110183: step 71930, loss = 0.67 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 19:19:56.843311: step 71940, loss = 0.61 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 19:20:01.554121: step 71950, loss = 0.68 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 19:20:06.190346: step 71960, loss = 0.76 (281.4 examples/sec; 0.455 sec/batch)
2016-02-03 19:20:10.921881: step 71970, loss = 0.72 (270.9 examples/sec; 0.473 sec/batch)
2016-02-03 19:20:15.666065: step 71980, loss = 0.66 (289.3 examples/sec; 0.442 sec/batch)
2016-02-03 19:20:20.390212: step 71990, loss = 0.75 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 19:20:25.117070: step 72000, loss = 0.77 (272.7 examples/sec; 0.469 sec/batch)
2016-02-03 19:20:30.402080: step 72010, loss = 0.79 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 19:20:35.031497: step 72020, loss = 0.60 (286.1 examples/sec; 0.447 sec/batch)
2016-02-03 19:20:39.795773: step 72030, loss = 0.75 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 19:20:44.529862: step 72040, loss = 0.71 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 19:20:49.284245: step 72050, loss = 0.90 (245.4 examples/sec; 0.522 sec/batch)
2016-02-03 19:20:53.964461: step 72060, loss = 0.66 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 19:20:58.562267: step 72070, loss = 0.71 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 19:21:03.262121: step 72080, loss = 0.95 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 19:21:08.055118: step 72090, loss = 0.62 (263.6 examples/sec; 0.486 sec/batch)
2016-02-03 19:21:12.805728: step 72100, loss = 0.71 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 19:21:18.052746: step 72110, loss = 0.85 (279.4 examples/sec; 0.458 sec/batch)
2016-02-03 19:21:22.824738: step 72120, loss = 0.65 (277.6 examples/sec; 0.461 sec/batch)
2016-02-03 19:21:27.545881: step 72130, loss = 0.72 (259.8 examples/sec; 0.493 sec/batch)
2016-02-03 19:21:32.278242: step 72140, loss = 0.76 (289.2 examples/sec; 0.443 sec/batch)
2016-02-03 19:21:36.971422: step 72150, loss = 0.66 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 19:21:41.732096: step 72160, loss = 0.78 (253.2 examples/sec; 0.505 sec/batch)
2016-02-03 19:21:46.412009: step 72170, loss = 0.68 (237.3 examples/sec; 0.539 sec/batch)
2016-02-03 19:21:51.102856: step 72180, loss = 0.70 (289.2 examples/sec; 0.443 sec/batch)
2016-02-03 19:21:55.877386: step 72190, loss = 0.68 (291.5 examples/sec; 0.439 sec/batch)
2016-02-03 19:22:00.556439: step 72200, loss = 0.70 (286.4 examples/sec; 0.447 sec/batch)
2016-02-03 19:22:05.822553: step 72210, loss = 0.85 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 19:22:10.502096: step 72220, loss = 0.80 (298.2 examples/sec; 0.429 sec/batch)
2016-02-03 19:22:15.170573: step 72230, loss = 0.86 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 19:22:19.943950: step 72240, loss = 0.82 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 19:22:24.635678: step 72250, loss = 0.68 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 19:22:29.334536: step 72260, loss = 0.74 (285.0 examples/sec; 0.449 sec/batch)
2016-02-03 19:22:34.025352: step 72270, loss = 0.77 (276.7 examples/sec; 0.463 sec/batch)
2016-02-03 19:22:38.764050: step 72280, loss = 0.69 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 19:22:43.519446: step 72290, loss = 0.70 (294.9 examples/sec; 0.434 sec/batch)
2016-02-03 19:22:48.235554: step 72300, loss = 0.80 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 19:22:53.459610: step 72310, loss = 0.62 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 19:22:58.198908: step 72320, loss = 0.85 (257.5 examples/sec; 0.497 sec/batch)
2016-02-03 19:23:02.862447: step 72330, loss = 0.70 (290.1 examples/sec; 0.441 sec/batch)
2016-02-03 19:23:07.548459: step 72340, loss = 0.67 (285.6 examples/sec; 0.448 sec/batch)
2016-02-03 19:23:12.262056: step 72350, loss = 0.80 (254.6 examples/sec; 0.503 sec/batch)
2016-02-03 19:23:16.978071: step 72360, loss = 0.64 (266.9 examples/sec; 0.480 sec/batch)
2016-02-03 19:23:21.700865: step 72370, loss = 0.61 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 19:23:26.421991: step 72380, loss = 0.85 (268.0 examples/sec; 0.478 sec/batch)
2016-02-03 19:23:31.190765: step 72390, loss = 0.60 (253.4 examples/sec; 0.505 sec/batch)
2016-02-03 19:23:35.945653: step 72400, loss = 0.69 (276.4 examples/sec; 0.463 sec/batch)
2016-02-03 19:23:41.215409: step 72410, loss = 0.77 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 19:23:46.006750: step 72420, loss = 0.68 (246.6 examples/sec; 0.519 sec/batch)
2016-02-03 19:23:50.679536: step 72430, loss = 0.72 (280.9 examples/sec; 0.456 sec/batch)
2016-02-03 19:23:55.419957: step 72440, loss = 0.59 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 19:24:00.102824: step 72450, loss = 0.75 (300.2 examples/sec; 0.426 sec/batch)
2016-02-03 19:24:04.849131: step 72460, loss = 0.76 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 19:24:09.611095: step 72470, loss = 0.85 (252.9 examples/sec; 0.506 sec/batch)
2016-02-03 19:24:14.315774: step 72480, loss = 0.83 (263.2 examples/sec; 0.486 sec/batch)
2016-02-03 19:24:18.926537: step 72490, loss = 0.85 (293.3 examples/sec; 0.436 sec/batch)
2016-02-03 19:24:23.672115: step 72500, loss = 0.73 (282.2 examples/sec; 0.454 sec/batch)
2016-02-03 19:24:28.916642: step 72510, loss = 0.74 (300.7 examples/sec; 0.426 sec/batch)
2016-02-03 19:24:33.683577: step 72520, loss = 0.72 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 19:24:38.315805: step 72530, loss = 0.72 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 19:24:42.937614: step 72540, loss = 0.79 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 19:24:47.628117: step 72550, loss = 0.71 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 19:24:52.268859: step 72560, loss = 0.69 (284.6 examples/sec; 0.450 sec/batch)
2016-02-03 19:24:57.025931: step 72570, loss = 0.66 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 19:25:01.757117: step 72580, loss = 0.68 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 19:25:06.464798: step 72590, loss = 0.78 (258.3 examples/sec; 0.496 sec/batch)
2016-02-03 19:25:11.231681: step 72600, loss = 0.78 (255.8 examples/sec; 0.500 sec/batch)
2016-02-03 19:25:16.502935: step 72610, loss = 0.86 (249.3 examples/sec; 0.513 sec/batch)
2016-02-03 19:25:21.212817: step 72620, loss = 0.67 (289.0 examples/sec; 0.443 sec/batch)
2016-02-03 19:25:25.856193: step 72630, loss = 0.78 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 19:25:30.617198: step 72640, loss = 0.67 (253.3 examples/sec; 0.505 sec/batch)
2016-02-03 19:25:35.296096: step 72650, loss = 0.95 (290.7 examples/sec; 0.440 sec/batch)
2016-02-03 19:25:39.986354: step 72660, loss = 0.62 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 19:25:44.810874: step 72670, loss = 0.63 (253.7 examples/sec; 0.505 sec/batch)
2016-02-03 19:25:49.500490: step 72680, loss = 0.69 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 19:25:54.230132: step 72690, loss = 0.78 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 19:25:58.956148: step 72700, loss = 0.63 (288.5 examples/sec; 0.444 sec/batch)
2016-02-03 19:26:04.139624: step 72710, loss = 0.72 (284.4 examples/sec; 0.450 sec/batch)
2016-02-03 19:26:08.742862: step 72720, loss = 0.78 (288.4 examples/sec; 0.444 sec/batch)
2016-02-03 19:26:13.541481: step 72730, loss = 0.64 (253.2 examples/sec; 0.505 sec/batch)
2016-02-03 19:26:18.240109: step 72740, loss = 0.60 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 19:26:22.949962: step 72750, loss = 0.82 (258.7 examples/sec; 0.495 sec/batch)
2016-02-03 19:26:27.598306: step 72760, loss = 0.64 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 19:26:32.320855: step 72770, loss = 0.68 (289.0 examples/sec; 0.443 sec/batch)
2016-02-03 19:26:37.077046: step 72780, loss = 0.72 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 19:26:41.705516: step 72790, loss = 0.78 (284.8 examples/sec; 0.449 sec/batch)
2016-02-03 19:26:46.468864: step 72800, loss = 0.67 (275.6 examples/sec; 0.464 sec/batch)
2016-02-03 19:26:51.671631: step 72810, loss = 0.92 (290.0 examples/sec; 0.441 sec/batch)
2016-02-03 19:26:56.392473: step 72820, loss = 0.64 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 19:27:01.173239: step 72830, loss = 0.86 (258.4 examples/sec; 0.495 sec/batch)
2016-02-03 19:27:05.898524: step 72840, loss = 0.81 (260.6 examples/sec; 0.491 sec/batch)
2016-02-03 19:27:10.594926: step 72850, loss = 0.77 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 19:27:15.234377: step 72860, loss = 0.83 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 19:27:19.929817: step 72870, loss = 0.72 (280.4 examples/sec; 0.456 sec/batch)
2016-02-03 19:27:24.603909: step 72880, loss = 0.76 (275.0 examples/sec; 0.465 sec/batch)
2016-02-03 19:27:29.319771: step 72890, loss = 0.80 (261.7 examples/sec; 0.489 sec/batch)
2016-02-03 19:27:34.055369: step 72900, loss = 0.63 (255.0 examples/sec; 0.502 sec/batch)
2016-02-03 19:27:39.278878: step 72910, loss = 0.78 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 19:27:44.013774: step 72920, loss = 0.66 (252.2 examples/sec; 0.508 sec/batch)
2016-02-03 19:27:48.670385: step 72930, loss = 0.63 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 19:27:53.379325: step 72940, loss = 0.81 (300.8 examples/sec; 0.426 sec/batch)
2016-02-03 19:27:58.200572: step 72950, loss = 0.87 (233.7 examples/sec; 0.548 sec/batch)
2016-02-03 19:28:02.803818: step 72960, loss = 0.82 (280.5 examples/sec; 0.456 sec/batch)
2016-02-03 19:28:07.562564: step 72970, loss = 0.74 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 19:28:12.221537: step 72980, loss = 0.71 (260.6 examples/sec; 0.491 sec/batch)
2016-02-03 19:28:16.820794: step 72990, loss = 0.81 (304.4 examples/sec; 0.420 sec/batch)
2016-02-03 19:28:21.520982: step 73000, loss = 0.76 (288.2 examples/sec; 0.444 sec/batch)
2016-02-03 19:28:26.757883: step 73010, loss = 0.64 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 19:28:31.414997: step 73020, loss = 0.57 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 19:28:36.076359: step 73030, loss = 0.71 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 19:28:40.798572: step 73040, loss = 0.66 (238.4 examples/sec; 0.537 sec/batch)
2016-02-03 19:28:45.453846: step 73050, loss = 0.74 (298.7 examples/sec; 0.428 sec/batch)
2016-02-03 19:28:50.185838: step 73060, loss = 0.76 (261.7 examples/sec; 0.489 sec/batch)
2016-02-03 19:28:54.903612: step 73070, loss = 0.76 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 19:28:59.569298: step 73080, loss = 0.63 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 19:29:04.246995: step 73090, loss = 0.66 (289.2 examples/sec; 0.443 sec/batch)
2016-02-03 19:29:08.969369: step 73100, loss = 0.66 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 19:29:14.233210: step 73110, loss = 0.80 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 19:29:18.873233: step 73120, loss = 0.62 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 19:29:23.536838: step 73130, loss = 0.73 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 19:29:28.256087: step 73140, loss = 0.71 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 19:29:33.043720: step 73150, loss = 0.70 (259.4 examples/sec; 0.493 sec/batch)
2016-02-03 19:29:37.695500: step 73160, loss = 0.63 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 19:29:42.389607: step 73170, loss = 0.76 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 19:29:47.028285: step 73180, loss = 0.78 (264.3 examples/sec; 0.484 sec/batch)
2016-02-03 19:29:51.719378: step 73190, loss = 0.71 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 19:29:56.408277: step 73200, loss = 0.68 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 19:30:01.656286: step 73210, loss = 0.72 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 19:30:06.409434: step 73220, loss = 0.73 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 19:30:11.086595: step 73230, loss = 0.78 (286.7 examples/sec; 0.446 sec/batch)
2016-02-03 19:30:15.793996: step 73240, loss = 0.71 (290.6 examples/sec; 0.441 sec/batch)
2016-02-03 19:30:20.485597: step 73250, loss = 0.70 (260.9 examples/sec; 0.491 sec/batch)
2016-02-03 19:30:25.165751: step 73260, loss = 0.65 (256.2 examples/sec; 0.500 sec/batch)
2016-02-03 19:30:29.826560: step 73270, loss = 0.70 (277.9 examples/sec; 0.461 sec/batch)
2016-02-03 19:30:34.535279: step 73280, loss = 0.78 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 19:30:39.259477: step 73290, loss = 0.68 (257.1 examples/sec; 0.498 sec/batch)
2016-02-03 19:30:43.957268: step 73300, loss = 0.83 (257.8 examples/sec; 0.497 sec/batch)
2016-02-03 19:30:49.238677: step 73310, loss = 0.72 (263.6 examples/sec; 0.486 sec/batch)
2016-02-03 19:30:54.011149: step 73320, loss = 0.77 (249.4 examples/sec; 0.513 sec/batch)
2016-02-03 19:30:58.727745: step 73330, loss = 0.63 (254.5 examples/sec; 0.503 sec/batch)
2016-02-03 19:31:03.450179: step 73340, loss = 0.74 (264.1 examples/sec; 0.485 sec/batch)
2016-02-03 19:31:08.128695: step 73350, loss = 0.89 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 19:31:12.852006: step 73360, loss = 0.84 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 19:31:17.569628: step 73370, loss = 0.66 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 19:31:22.305321: step 73380, loss = 0.77 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 19:31:26.989844: step 73390, loss = 0.78 (312.5 examples/sec; 0.410 sec/batch)
2016-02-03 19:31:31.734903: step 73400, loss = 0.66 (261.9 examples/sec; 0.489 sec/batch)
2016-02-03 19:31:37.005704: step 73410, loss = 0.83 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 19:31:41.737349: step 73420, loss = 0.80 (294.2 examples/sec; 0.435 sec/batch)
2016-02-03 19:31:46.421494: step 73430, loss = 0.84 (291.2 examples/sec; 0.440 sec/batch)
2016-02-03 19:31:51.225269: step 73440, loss = 0.67 (238.9 examples/sec; 0.536 sec/batch)
2016-02-03 19:31:55.922939: step 73450, loss = 0.83 (257.1 examples/sec; 0.498 sec/batch)
2016-02-03 19:32:00.655004: step 73460, loss = 0.77 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 19:32:05.354088: step 73470, loss = 0.66 (287.5 examples/sec; 0.445 sec/batch)
2016-02-03 19:32:09.995125: step 73480, loss = 0.66 (285.8 examples/sec; 0.448 sec/batch)
2016-02-03 19:32:14.772090: step 73490, loss = 0.73 (289.3 examples/sec; 0.442 sec/batch)
2016-02-03 19:32:19.565071: step 73500, loss = 0.85 (253.2 examples/sec; 0.506 sec/batch)
2016-02-03 19:32:24.887475: step 73510, loss = 0.74 (285.8 examples/sec; 0.448 sec/batch)
2016-02-03 19:32:29.611937: step 73520, loss = 0.65 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 19:32:34.353289: step 73530, loss = 0.60 (259.1 examples/sec; 0.494 sec/batch)
2016-02-03 19:32:39.083640: step 73540, loss = 0.80 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 19:32:43.873978: step 73550, loss = 0.78 (259.7 examples/sec; 0.493 sec/batch)
2016-02-03 19:32:48.580479: step 73560, loss = 0.74 (282.9 examples/sec; 0.452 sec/batch)
2016-02-03 19:32:53.377399: step 73570, loss = 0.74 (285.8 examples/sec; 0.448 sec/batch)
2016-02-03 19:32:58.091113: step 73580, loss = 0.71 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 19:33:02.829917: step 73590, loss = 0.74 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 19:33:07.572983: step 73600, loss = 0.75 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 19:33:12.783329: step 73610, loss = 0.78 (281.6 examples/sec; 0.454 sec/batch)
2016-02-03 19:33:17.526466: step 73620, loss = 0.67 (258.3 examples/sec; 0.496 sec/batch)
2016-02-03 19:33:22.158864: step 73630, loss = 0.73 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 19:33:26.874906: step 73640, loss = 0.69 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 19:33:31.705537: step 73650, loss = 0.64 (257.5 examples/sec; 0.497 sec/batch)
2016-02-03 19:33:36.401998: step 73660, loss = 0.71 (251.5 examples/sec; 0.509 sec/batch)
2016-02-03 19:33:41.091763: step 73670, loss = 0.65 (285.9 examples/sec; 0.448 sec/batch)
2016-02-03 19:33:45.818740: step 73680, loss = 0.72 (258.0 examples/sec; 0.496 sec/batch)
2016-02-03 19:33:50.562313: step 73690, loss = 0.65 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 19:33:55.304512: step 73700, loss = 0.64 (260.0 examples/sec; 0.492 sec/batch)
2016-02-03 19:34:00.568771: step 73710, loss = 0.66 (282.6 examples/sec; 0.453 sec/batch)
2016-02-03 19:34:05.229312: step 73720, loss = 0.77 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 19:34:09.918155: step 73730, loss = 0.68 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 19:34:14.613368: step 73740, loss = 0.69 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 19:34:19.242698: step 73750, loss = 0.92 (284.8 examples/sec; 0.449 sec/batch)
2016-02-03 19:34:23.933341: step 73760, loss = 0.74 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 19:34:28.693207: step 73770, loss = 0.70 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 19:34:33.353465: step 73780, loss = 0.91 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 19:34:38.041284: step 73790, loss = 0.70 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 19:34:42.815005: step 73800, loss = 0.64 (255.5 examples/sec; 0.501 sec/batch)
2016-02-03 19:34:47.990598: step 73810, loss = 0.88 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 19:34:52.694803: step 73820, loss = 0.73 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 19:34:57.387891: step 73830, loss = 0.66 (284.9 examples/sec; 0.449 sec/batch)
2016-02-03 19:35:02.095836: step 73840, loss = 0.80 (296.4 examples/sec; 0.432 sec/batch)
2016-02-03 19:35:06.816067: step 73850, loss = 0.81 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 19:35:11.503075: step 73860, loss = 0.82 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 19:35:16.215938: step 73870, loss = 0.71 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 19:35:20.976352: step 73880, loss = 0.79 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 19:35:25.713003: step 73890, loss = 0.83 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 19:35:30.402189: step 73900, loss = 0.68 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 19:35:35.640504: step 73910, loss = 0.97 (261.9 examples/sec; 0.489 sec/batch)
2016-02-03 19:35:40.371439: step 73920, loss = 0.66 (251.3 examples/sec; 0.509 sec/batch)
2016-02-03 19:35:45.052400: step 73930, loss = 0.73 (268.1 examples/sec; 0.478 sec/batch)
2016-02-03 19:35:49.762313: step 73940, loss = 0.80 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 19:35:54.406236: step 73950, loss = 0.62 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 19:35:59.130648: step 73960, loss = 0.65 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 19:36:03.824965: step 73970, loss = 0.75 (260.1 examples/sec; 0.492 sec/batch)
2016-02-03 19:36:08.480557: step 73980, loss = 0.65 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 19:36:13.164844: step 73990, loss = 0.79 (276.4 examples/sec; 0.463 sec/batch)
2016-02-03 19:36:17.817682: step 74000, loss = 0.74 (286.6 examples/sec; 0.447 sec/batch)
2016-02-03 19:36:23.108606: step 74010, loss = 0.64 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 19:36:27.751907: step 74020, loss = 0.64 (287.9 examples/sec; 0.445 sec/batch)
2016-02-03 19:36:32.440205: step 74030, loss = 0.93 (292.3 examples/sec; 0.438 sec/batch)
2016-02-03 19:36:37.168788: step 74040, loss = 0.88 (263.2 examples/sec; 0.486 sec/batch)
2016-02-03 19:36:41.855958: step 74050, loss = 0.85 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 19:36:46.511551: step 74060, loss = 0.75 (286.2 examples/sec; 0.447 sec/batch)
2016-02-03 19:36:51.184773: step 74070, loss = 0.88 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 19:36:55.875429: step 74080, loss = 0.80 (274.4 examples/sec; 0.467 sec/batch)
2016-02-03 19:37:00.522298: step 74090, loss = 0.71 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 19:37:05.151664: step 74100, loss = 0.82 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 19:37:10.364441: step 74110, loss = 0.69 (258.6 examples/sec; 0.495 sec/batch)
2016-02-03 19:37:15.009559: step 74120, loss = 0.68 (266.9 examples/sec; 0.479 sec/batch)
2016-02-03 19:37:19.822317: step 74130, loss = 0.70 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 19:37:24.483483: step 74140, loss = 0.59 (305.0 examples/sec; 0.420 sec/batch)
2016-02-03 19:37:29.200693: step 74150, loss = 0.81 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 19:37:33.856311: step 74160, loss = 0.66 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 19:37:38.559811: step 74170, loss = 0.63 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 19:37:43.357951: step 74180, loss = 0.70 (243.7 examples/sec; 0.525 sec/batch)
2016-02-03 19:37:48.052986: step 74190, loss = 0.79 (256.8 examples/sec; 0.499 sec/batch)
2016-02-03 19:37:52.732400: step 74200, loss = 0.75 (297.3 examples/sec; 0.431 sec/batch)
2016-02-03 19:37:57.965958: step 74210, loss = 0.65 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 19:38:02.651779: step 74220, loss = 0.73 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 19:38:07.344319: step 74230, loss = 0.67 (287.5 examples/sec; 0.445 sec/batch)
2016-02-03 19:38:12.020547: step 74240, loss = 0.65 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 19:38:16.783740: step 74250, loss = 0.81 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 19:38:21.514410: step 74260, loss = 0.74 (261.9 examples/sec; 0.489 sec/batch)
2016-02-03 19:38:26.268592: step 74270, loss = 0.75 (286.3 examples/sec; 0.447 sec/batch)
2016-02-03 19:38:30.920930: step 74280, loss = 0.73 (284.2 examples/sec; 0.450 sec/batch)
2016-02-03 19:38:35.596611: step 74290, loss = 0.69 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 19:38:40.322042: step 74300, loss = 0.87 (250.5 examples/sec; 0.511 sec/batch)
2016-02-03 19:38:45.497444: step 74310, loss = 0.85 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 19:38:50.212339: step 74320, loss = 0.76 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 19:38:54.914736: step 74330, loss = 0.63 (288.0 examples/sec; 0.444 sec/batch)
2016-02-03 19:38:59.756696: step 74340, loss = 0.92 (247.9 examples/sec; 0.516 sec/batch)
2016-02-03 19:39:04.440617: step 74350, loss = 0.65 (259.2 examples/sec; 0.494 sec/batch)
2016-02-03 19:39:09.177647: step 74360, loss = 0.82 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 19:39:13.858449: step 74370, loss = 0.69 (287.2 examples/sec; 0.446 sec/batch)
2016-02-03 19:39:18.545670: step 74380, loss = 0.93 (255.6 examples/sec; 0.501 sec/batch)
2016-02-03 19:39:23.276949: step 74390, loss = 0.81 (284.7 examples/sec; 0.450 sec/batch)
2016-02-03 19:39:27.973591: step 74400, loss = 0.72 (259.0 examples/sec; 0.494 sec/batch)
2016-02-03 19:39:33.100367: step 74410, loss = 0.65 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 19:39:37.732215: step 74420, loss = 0.65 (276.7 examples/sec; 0.463 sec/batch)
2016-02-03 19:39:42.396046: step 74430, loss = 0.71 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 19:39:47.073209: step 74440, loss = 0.70 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 19:39:51.767780: step 74450, loss = 0.92 (254.5 examples/sec; 0.503 sec/batch)
2016-02-03 19:39:56.528534: step 74460, loss = 0.68 (259.3 examples/sec; 0.494 sec/batch)
2016-02-03 19:40:01.222748: step 74470, loss = 0.71 (293.3 examples/sec; 0.436 sec/batch)
2016-02-03 19:40:06.038705: step 74480, loss = 0.80 (258.3 examples/sec; 0.496 sec/batch)
2016-02-03 19:40:10.724301: step 74490, loss = 0.75 (288.6 examples/sec; 0.444 sec/batch)
2016-02-03 19:40:15.449150: step 74500, loss = 0.59 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 19:40:20.683599: step 74510, loss = 0.77 (276.7 examples/sec; 0.463 sec/batch)
2016-02-03 19:40:25.358817: step 74520, loss = 0.53 (291.0 examples/sec; 0.440 sec/batch)
2016-02-03 19:40:30.149918: step 74530, loss = 0.80 (293.0 examples/sec; 0.437 sec/batch)
2016-02-03 19:40:34.905851: step 74540, loss = 0.75 (255.3 examples/sec; 0.501 sec/batch)
2016-02-03 19:40:39.624477: step 74550, loss = 0.71 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 19:40:44.313365: step 74560, loss = 0.69 (267.2 examples/sec; 0.479 sec/batch)
2016-02-03 19:40:48.983260: step 74570, loss = 0.68 (295.4 examples/sec; 0.433 sec/batch)
2016-02-03 19:40:53.719299: step 74580, loss = 0.66 (298.9 examples/sec; 0.428 sec/batch)
2016-02-03 19:40:58.439683: step 74590, loss = 0.79 (287.5 examples/sec; 0.445 sec/batch)
2016-02-03 19:41:03.160590: step 74600, loss = 0.52 (243.7 examples/sec; 0.525 sec/batch)
2016-02-03 19:41:08.297506: step 74610, loss = 0.68 (275.0 examples/sec; 0.465 sec/batch)
2016-02-03 19:41:12.914373: step 74620, loss = 0.73 (284.4 examples/sec; 0.450 sec/batch)
2016-02-03 19:41:17.589945: step 74630, loss = 0.82 (273.2 examples/sec; 0.469 sec/batch)
2016-02-03 19:41:22.315950: step 74640, loss = 0.86 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 19:41:26.977582: step 74650, loss = 0.75 (258.9 examples/sec; 0.494 sec/batch)
2016-02-03 19:41:31.656790: step 74660, loss = 0.76 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 19:41:36.373488: step 74670, loss = 0.69 (262.9 examples/sec; 0.487 sec/batch)
2016-02-03 19:41:41.121629: step 74680, loss = 0.65 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 19:41:45.806057: step 74690, loss = 0.70 (287.4 examples/sec; 0.445 sec/batch)
2016-02-03 19:41:50.574022: step 74700, loss = 0.66 (273.2 examples/sec; 0.469 sec/batch)
2016-02-03 19:41:55.751180: step 74710, loss = 0.70 (279.6 examples/sec; 0.458 sec/batch)
2016-02-03 19:42:00.501866: step 74720, loss = 0.81 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 19:42:05.224670: step 74730, loss = 0.72 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 19:42:09.902387: step 74740, loss = 0.85 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 19:42:14.551243: step 74750, loss = 0.61 (289.2 examples/sec; 0.443 sec/batch)
2016-02-03 19:42:19.302963: step 74760, loss = 0.88 (281.5 examples/sec; 0.455 sec/batch)
2016-02-03 19:42:24.082771: step 74770, loss = 0.66 (270.5 examples/sec; 0.473 sec/batch)
2016-02-03 19:42:28.798951: step 74780, loss = 0.62 (264.9 examples/sec; 0.483 sec/batch)
2016-02-03 19:42:33.500004: step 74790, loss = 0.65 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 19:42:38.150018: step 74800, loss = 0.67 (290.0 examples/sec; 0.441 sec/batch)
2016-02-03 19:42:43.349125: step 74810, loss = 0.62 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 19:42:47.965840: step 74820, loss = 0.81 (293.3 examples/sec; 0.436 sec/batch)
2016-02-03 19:42:52.749524: step 74830, loss = 0.71 (250.7 examples/sec; 0.511 sec/batch)
2016-02-03 19:42:57.434568: step 74840, loss = 0.83 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 19:43:02.132825: step 74850, loss = 0.69 (270.9 examples/sec; 0.473 sec/batch)
2016-02-03 19:43:06.827099: step 74860, loss = 0.73 (287.9 examples/sec; 0.445 sec/batch)
2016-02-03 19:43:11.493389: step 74870, loss = 0.79 (276.4 examples/sec; 0.463 sec/batch)
2016-02-03 19:43:16.214485: step 74880, loss = 0.67 (249.1 examples/sec; 0.514 sec/batch)
2016-02-03 19:43:20.838346: step 74890, loss = 0.63 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 19:43:25.650948: step 74900, loss = 0.78 (243.6 examples/sec; 0.526 sec/batch)
2016-02-03 19:43:30.933136: step 74910, loss = 0.76 (254.8 examples/sec; 0.502 sec/batch)
2016-02-03 19:43:35.724642: step 74920, loss = 0.64 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 19:43:40.437423: step 74930, loss = 0.71 (262.1 examples/sec; 0.488 sec/batch)
2016-02-03 19:43:45.191255: step 74940, loss = 0.74 (297.7 examples/sec; 0.430 sec/batch)
2016-02-03 19:43:49.929139: step 74950, loss = 0.80 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 19:43:54.688062: step 74960, loss = 0.70 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 19:43:59.352902: step 74970, loss = 0.81 (293.4 examples/sec; 0.436 sec/batch)
2016-02-03 19:44:04.047934: step 74980, loss = 0.66 (308.8 examples/sec; 0.415 sec/batch)
2016-02-03 19:44:08.735035: step 74990, loss = 0.62 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 19:44:13.456933: step 75000, loss = 0.80 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 19:44:18.675675: step 75010, loss = 0.75 (299.7 examples/sec; 0.427 sec/batch)
2016-02-03 19:44:23.351372: step 75020, loss = 0.56 (260.9 examples/sec; 0.491 sec/batch)
2016-02-03 19:44:27.993094: step 75030, loss = 0.91 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 19:44:32.715307: step 75040, loss = 0.63 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 19:44:37.454153: step 75050, loss = 0.70 (254.6 examples/sec; 0.503 sec/batch)
2016-02-03 19:44:42.197477: step 75060, loss = 0.86 (249.1 examples/sec; 0.514 sec/batch)
2016-02-03 19:44:46.875241: step 75070, loss = 0.72 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 19:44:51.516896: step 75080, loss = 0.75 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 19:44:56.182214: step 75090, loss = 0.76 (285.2 examples/sec; 0.449 sec/batch)
2016-02-03 19:45:00.846622: step 75100, loss = 0.72 (294.4 examples/sec; 0.435 sec/batch)
2016-02-03 19:45:06.140127: step 75110, loss = 0.78 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 19:45:10.803812: step 75120, loss = 0.93 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 19:45:15.547728: step 75130, loss = 0.63 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 19:45:20.288457: step 75140, loss = 0.71 (244.9 examples/sec; 0.523 sec/batch)
2016-02-03 19:45:24.944908: step 75150, loss = 0.93 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 19:45:29.602498: step 75160, loss = 0.70 (280.3 examples/sec; 0.457 sec/batch)
2016-02-03 19:45:34.357404: step 75170, loss = 0.63 (258.7 examples/sec; 0.495 sec/batch)
2016-02-03 19:45:39.100728: step 75180, loss = 0.67 (248.5 examples/sec; 0.515 sec/batch)
2016-02-03 19:45:43.799813: step 75190, loss = 0.64 (263.1 examples/sec; 0.487 sec/batch)
2016-02-03 19:45:48.533647: step 75200, loss = 0.71 (263.2 examples/sec; 0.486 sec/batch)
2016-02-03 19:45:53.773498: step 75210, loss = 0.62 (251.9 examples/sec; 0.508 sec/batch)
2016-02-03 19:45:58.526386: step 75220, loss = 0.86 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 19:46:03.260985: step 75230, loss = 0.72 (254.9 examples/sec; 0.502 sec/batch)
2016-02-03 19:46:07.904630: step 75240, loss = 0.88 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 19:46:12.625874: step 75250, loss = 0.60 (284.2 examples/sec; 0.450 sec/batch)
2016-02-03 19:46:17.271115: step 75260, loss = 0.56 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 19:46:22.079481: step 75270, loss = 0.68 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 19:46:26.794312: step 75280, loss = 0.66 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 19:46:31.575328: step 75290, loss = 0.56 (254.1 examples/sec; 0.504 sec/batch)
2016-02-03 19:46:36.277593: step 75300, loss = 0.74 (257.2 examples/sec; 0.498 sec/batch)
2016-02-03 19:46:41.492836: step 75310, loss = 0.74 (287.6 examples/sec; 0.445 sec/batch)
2016-02-03 19:46:46.173052: step 75320, loss = 0.83 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 19:46:50.818824: step 75330, loss = 0.68 (284.1 examples/sec; 0.451 sec/batch)
2016-02-03 19:46:55.509490: step 75340, loss = 0.77 (335.6 examples/sec; 0.381 sec/batch)
2016-02-03 19:47:00.148336: step 75350, loss = 0.71 (290.2 examples/sec; 0.441 sec/batch)
2016-02-03 19:47:04.894160: step 75360, loss = 0.57 (277.9 examples/sec; 0.461 sec/batch)
2016-02-03 19:47:09.656677: step 75370, loss = 0.77 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 19:47:14.411465: step 75380, loss = 0.83 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 19:47:19.146895: step 75390, loss = 0.64 (245.5 examples/sec; 0.521 sec/batch)
2016-02-03 19:47:23.874036: step 75400, loss = 0.67 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 19:47:29.149291: step 75410, loss = 0.67 (277.2 examples/sec; 0.462 sec/batch)
2016-02-03 19:47:33.807665: step 75420, loss = 0.58 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 19:47:38.532048: step 75430, loss = 0.59 (283.5 examples/sec; 0.451 sec/batch)
2016-02-03 19:47:43.300909: step 75440, loss = 0.67 (264.2 examples/sec; 0.484 sec/batch)
2016-02-03 19:47:47.948923: step 75450, loss = 0.95 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 19:47:52.625224: step 75460, loss = 0.77 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 19:47:57.344255: step 75470, loss = 0.86 (255.9 examples/sec; 0.500 sec/batch)
2016-02-03 19:48:02.049873: step 75480, loss = 0.65 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 19:48:06.717060: step 75490, loss = 0.60 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 19:48:11.479347: step 75500, loss = 0.63 (290.0 examples/sec; 0.441 sec/batch)
2016-02-03 19:48:16.729477: step 75510, loss = 0.83 (262.0 examples/sec; 0.489 sec/batch)
2016-02-03 19:48:21.404093: step 75520, loss = 0.69 (287.2 examples/sec; 0.446 sec/batch)
2016-02-03 19:48:26.083072: step 75530, loss = 0.65 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 19:48:30.782852: step 75540, loss = 0.61 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 19:48:35.450282: step 75550, loss = 0.84 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 19:48:40.087883: step 75560, loss = 0.72 (295.1 examples/sec; 0.434 sec/batch)
2016-02-03 19:48:44.794452: step 75570, loss = 0.71 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 19:48:49.480926: step 75580, loss = 0.76 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 19:48:54.208303: step 75590, loss = 0.58 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 19:48:58.870055: step 75600, loss = 0.57 (268.2 examples/sec; 0.477 sec/batch)
2016-02-03 19:49:04.105328: step 75610, loss = 0.73 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 19:49:08.833091: step 75620, loss = 0.64 (275.0 examples/sec; 0.465 sec/batch)
2016-02-03 19:49:13.456752: step 75630, loss = 0.78 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 19:49:18.104535: step 75640, loss = 0.86 (284.9 examples/sec; 0.449 sec/batch)
2016-02-03 19:49:22.920068: step 75650, loss = 0.76 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 19:49:27.620167: step 75660, loss = 0.68 (257.0 examples/sec; 0.498 sec/batch)
2016-02-03 19:49:32.218194: step 75670, loss = 0.64 (293.4 examples/sec; 0.436 sec/batch)
2016-02-03 19:49:36.990937: step 75680, loss = 0.81 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 19:49:41.678377: step 75690, loss = 0.86 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 19:49:46.318213: step 75700, loss = 0.74 (278.0 examples/sec; 0.460 sec/batch)
2016-02-03 19:49:51.482630: step 75710, loss = 0.75 (283.7 examples/sec; 0.451 sec/batch)
2016-02-03 19:49:56.298667: step 75720, loss = 0.71 (268.1 examples/sec; 0.478 sec/batch)
2016-02-03 19:50:00.989928: step 75730, loss = 0.57 (264.3 examples/sec; 0.484 sec/batch)
2016-02-03 19:50:05.455084: step 75740, loss = 0.73 (291.4 examples/sec; 0.439 sec/batch)
2016-02-03 19:50:10.154497: step 75750, loss = 0.69 (291.9 examples/sec; 0.439 sec/batch)
2016-02-03 19:50:14.876345: step 75760, loss = 0.71 (256.3 examples/sec; 0.499 sec/batch)
2016-02-03 19:50:19.575637: step 75770, loss = 0.74 (294.4 examples/sec; 0.435 sec/batch)
2016-02-03 19:50:24.308553: step 75780, loss = 0.70 (247.3 examples/sec; 0.518 sec/batch)
2016-02-03 19:50:28.902325: step 75790, loss = 0.77 (282.4 examples/sec; 0.453 sec/batch)
2016-02-03 19:50:33.541356: step 75800, loss = 0.65 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 19:50:38.770689: step 75810, loss = 0.89 (256.5 examples/sec; 0.499 sec/batch)
2016-02-03 19:50:43.362474: step 75820, loss = 0.81 (259.1 examples/sec; 0.494 sec/batch)
2016-02-03 19:50:48.109285: step 75830, loss = 0.64 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 19:50:52.808912: step 75840, loss = 0.91 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 19:50:57.485269: step 75850, loss = 0.79 (263.2 examples/sec; 0.486 sec/batch)
2016-02-03 19:51:02.204478: step 75860, loss = 0.70 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 19:51:06.894613: step 75870, loss = 0.64 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 19:51:11.628114: step 75880, loss = 0.65 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 19:51:16.239751: step 75890, loss = 0.73 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 19:51:20.841312: step 75900, loss = 0.76 (300.1 examples/sec; 0.427 sec/batch)
2016-02-03 19:51:26.072725: step 75910, loss = 0.75 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 19:51:30.682895: step 75920, loss = 0.63 (317.4 examples/sec; 0.403 sec/batch)
2016-02-03 19:51:35.464986: step 75930, loss = 0.62 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 19:51:40.120359: step 75940, loss = 0.61 (269.3 examples/sec; 0.475 sec/batch)
2016-02-03 19:51:44.768097: step 75950, loss = 0.71 (283.9 examples/sec; 0.451 sec/batch)
2016-02-03 19:51:49.513189: step 75960, loss = 0.83 (281.5 examples/sec; 0.455 sec/batch)
2016-02-03 19:51:54.188979: step 75970, loss = 0.68 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 19:51:58.866133: step 75980, loss = 0.75 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 19:52:03.596941: step 75990, loss = 0.80 (294.6 examples/sec; 0.435 sec/batch)
2016-02-03 19:52:08.337927: step 76000, loss = 0.73 (286.2 examples/sec; 0.447 sec/batch)
2016-02-03 19:52:13.587157: step 76010, loss = 0.98 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 19:52:18.280258: step 76020, loss = 0.64 (290.8 examples/sec; 0.440 sec/batch)
2016-02-03 19:52:22.990646: step 76030, loss = 0.68 (255.7 examples/sec; 0.501 sec/batch)
2016-02-03 19:52:27.680206: step 76040, loss = 0.76 (264.3 examples/sec; 0.484 sec/batch)
2016-02-03 19:52:32.456695: step 76050, loss = 0.74 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 19:52:37.160057: step 76060, loss = 0.70 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 19:52:41.910609: step 76070, loss = 0.78 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 19:52:46.681163: step 76080, loss = 0.68 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 19:52:51.409727: step 76090, loss = 0.60 (250.1 examples/sec; 0.512 sec/batch)
2016-02-03 19:52:56.102721: step 76100, loss = 0.69 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 19:53:01.310140: step 76110, loss = 0.71 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 19:53:06.106086: step 76120, loss = 0.66 (245.1 examples/sec; 0.522 sec/batch)
2016-02-03 19:53:10.847215: step 76130, loss = 0.63 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 19:53:15.536107: step 76140, loss = 0.67 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 19:53:20.209756: step 76150, loss = 0.67 (254.4 examples/sec; 0.503 sec/batch)
2016-02-03 19:53:24.904336: step 76160, loss = 0.69 (249.8 examples/sec; 0.512 sec/batch)
2016-02-03 19:53:29.649640: step 76170, loss = 0.67 (254.1 examples/sec; 0.504 sec/batch)
2016-02-03 19:53:34.376976: step 76180, loss = 0.76 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 19:53:39.006605: step 76190, loss = 0.74 (272.6 examples/sec; 0.470 sec/batch)
2016-02-03 19:53:43.661053: step 76200, loss = 0.69 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 19:53:48.920161: step 76210, loss = 0.69 (297.7 examples/sec; 0.430 sec/batch)
2016-02-03 19:53:53.704901: step 76220, loss = 0.57 (294.6 examples/sec; 0.434 sec/batch)
2016-02-03 19:53:58.457901: step 76230, loss = 0.70 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 19:54:03.182342: step 76240, loss = 0.70 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 19:54:07.928275: step 76250, loss = 0.63 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 19:54:12.518201: step 76260, loss = 0.70 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 19:54:17.201818: step 76270, loss = 0.80 (304.5 examples/sec; 0.420 sec/batch)
2016-02-03 19:54:22.010364: step 76280, loss = 0.77 (252.2 examples/sec; 0.508 sec/batch)
2016-02-03 19:54:26.741631: step 76290, loss = 0.82 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 19:54:31.424019: step 76300, loss = 0.76 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 19:54:36.629726: step 76310, loss = 0.63 (285.6 examples/sec; 0.448 sec/batch)
2016-02-03 19:54:41.293542: step 76320, loss = 0.71 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 19:54:45.944098: step 76330, loss = 0.74 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 19:54:50.568753: step 76340, loss = 0.65 (306.5 examples/sec; 0.418 sec/batch)
2016-02-03 19:54:55.166981: step 76350, loss = 0.74 (293.4 examples/sec; 0.436 sec/batch)
2016-02-03 19:54:59.832882: step 76360, loss = 0.76 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 19:55:04.569217: step 76370, loss = 0.82 (261.5 examples/sec; 0.490 sec/batch)
2016-02-03 19:55:09.327257: step 76380, loss = 0.73 (254.2 examples/sec; 0.504 sec/batch)
2016-02-03 19:55:14.007999: step 76390, loss = 0.79 (255.2 examples/sec; 0.502 sec/batch)
2016-02-03 19:55:18.662864: step 76400, loss = 0.80 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 19:55:23.887914: step 76410, loss = 0.55 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 19:55:28.533594: step 76420, loss = 0.67 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 19:55:33.275237: step 76430, loss = 0.77 (286.9 examples/sec; 0.446 sec/batch)
2016-02-03 19:55:37.991142: step 76440, loss = 0.64 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 19:55:42.700527: step 76450, loss = 0.61 (300.6 examples/sec; 0.426 sec/batch)
2016-02-03 19:55:47.390872: step 76460, loss = 0.83 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 19:55:52.085139: step 76470, loss = 0.69 (246.0 examples/sec; 0.520 sec/batch)
2016-02-03 19:55:56.764058: step 76480, loss = 0.70 (284.0 examples/sec; 0.451 sec/batch)
2016-02-03 19:56:01.475465: step 76490, loss = 0.61 (254.5 examples/sec; 0.503 sec/batch)
2016-02-03 19:56:06.246111: step 76500, loss = 0.58 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 19:56:11.473910: step 76510, loss = 0.81 (264.9 examples/sec; 0.483 sec/batch)
2016-02-03 19:56:16.314668: step 76520, loss = 0.63 (236.8 examples/sec; 0.541 sec/batch)
2016-02-03 19:56:21.001532: step 76530, loss = 0.75 (258.2 examples/sec; 0.496 sec/batch)
2016-02-03 19:56:25.688821: step 76540, loss = 0.82 (259.5 examples/sec; 0.493 sec/batch)
2016-02-03 19:56:30.325160: step 76550, loss = 0.87 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 19:56:34.903910: step 76560, loss = 0.66 (283.3 examples/sec; 0.452 sec/batch)
2016-02-03 19:56:39.587319: step 76570, loss = 0.71 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 19:56:44.367011: step 76580, loss = 0.65 (258.5 examples/sec; 0.495 sec/batch)
2016-02-03 19:56:49.043622: step 76590, loss = 0.81 (274.4 examples/sec; 0.466 sec/batch)
2016-02-03 19:56:53.752400: step 76600, loss = 0.69 (289.0 examples/sec; 0.443 sec/batch)
2016-02-03 19:56:59.025795: step 76610, loss = 0.76 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 19:57:03.673477: step 76620, loss = 0.74 (299.9 examples/sec; 0.427 sec/batch)
2016-02-03 19:57:08.400641: step 76630, loss = 0.70 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 19:57:13.101172: step 76640, loss = 0.75 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 19:57:17.832721: step 76650, loss = 0.65 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 19:57:22.605263: step 76660, loss = 0.80 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 19:57:27.358459: step 76670, loss = 0.77 (287.2 examples/sec; 0.446 sec/batch)
2016-02-03 19:57:32.101879: step 76680, loss = 0.77 (282.5 examples/sec; 0.453 sec/batch)
2016-02-03 19:57:36.805942: step 76690, loss = 0.71 (290.8 examples/sec; 0.440 sec/batch)
2016-02-03 19:57:41.622251: step 76700, loss = 0.71 (241.0 examples/sec; 0.531 sec/batch)
2016-02-03 19:57:46.829547: step 76710, loss = 0.79 (284.0 examples/sec; 0.451 sec/batch)
2016-02-03 19:57:51.602300: step 76720, loss = 0.62 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 19:57:56.273270: step 76730, loss = 0.79 (268.6 examples/sec; 0.477 sec/batch)
2016-02-03 19:58:00.986984: step 76740, loss = 0.59 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 19:58:05.697822: step 76750, loss = 0.65 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 19:58:10.414413: step 76760, loss = 0.68 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 19:58:15.082593: step 76770, loss = 0.62 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 19:58:19.833408: step 76780, loss = 0.68 (254.7 examples/sec; 0.503 sec/batch)
2016-02-03 19:58:24.586062: step 76790, loss = 0.58 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 19:58:29.302780: step 76800, loss = 0.74 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 19:58:34.528073: step 76810, loss = 0.70 (292.4 examples/sec; 0.438 sec/batch)
2016-02-03 19:58:39.240958: step 76820, loss = 0.71 (265.2 examples/sec; 0.483 sec/batch)
2016-02-03 19:58:43.970758: step 76830, loss = 0.74 (288.0 examples/sec; 0.444 sec/batch)
2016-02-03 19:58:48.711351: step 76840, loss = 0.66 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 19:58:53.397405: step 76850, loss = 0.67 (285.1 examples/sec; 0.449 sec/batch)
2016-02-03 19:58:58.135162: step 76860, loss = 0.71 (284.0 examples/sec; 0.451 sec/batch)
2016-02-03 19:59:02.839946: step 76870, loss = 0.61 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 19:59:07.561748: step 76880, loss = 0.85 (264.7 examples/sec; 0.484 sec/batch)
2016-02-03 19:59:12.318140: step 76890, loss = 0.81 (264.7 examples/sec; 0.483 sec/batch)
2016-02-03 19:59:17.045222: step 76900, loss = 0.73 (260.3 examples/sec; 0.492 sec/batch)
2016-02-03 19:59:22.248326: step 76910, loss = 0.70 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 19:59:26.978378: step 76920, loss = 0.77 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 19:59:31.703622: step 76930, loss = 0.77 (254.9 examples/sec; 0.502 sec/batch)
2016-02-03 19:59:36.415383: step 76940, loss = 0.62 (291.2 examples/sec; 0.440 sec/batch)
2016-02-03 19:59:41.137402: step 76950, loss = 0.57 (291.8 examples/sec; 0.439 sec/batch)
2016-02-03 19:59:45.823372: step 76960, loss = 0.60 (275.0 examples/sec; 0.465 sec/batch)
2016-02-03 19:59:50.581004: step 76970, loss = 0.63 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 19:59:55.298923: step 76980, loss = 0.72 (276.9 examples/sec; 0.462 sec/batch)
2016-02-03 20:00:00.062996: step 76990, loss = 0.80 (258.8 examples/sec; 0.495 sec/batch)
2016-02-03 20:00:04.712555: step 77000, loss = 0.82 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 20:00:09.828636: step 77010, loss = 0.80 (289.2 examples/sec; 0.443 sec/batch)
2016-02-03 20:00:14.506879: step 77020, loss = 0.71 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 20:00:19.178890: step 77030, loss = 0.69 (300.9 examples/sec; 0.425 sec/batch)
2016-02-03 20:00:23.892466: step 77040, loss = 0.73 (261.5 examples/sec; 0.489 sec/batch)
2016-02-03 20:00:28.669558: step 77050, loss = 0.70 (289.6 examples/sec; 0.442 sec/batch)
2016-02-03 20:00:33.401912: step 77060, loss = 0.73 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 20:00:38.065323: step 77070, loss = 0.80 (288.4 examples/sec; 0.444 sec/batch)
2016-02-03 20:00:42.787102: step 77080, loss = 0.72 (257.5 examples/sec; 0.497 sec/batch)
2016-02-03 20:00:47.556314: step 77090, loss = 0.72 (242.7 examples/sec; 0.527 sec/batch)
2016-02-03 20:00:52.203363: step 77100, loss = 0.75 (283.8 examples/sec; 0.451 sec/batch)
2016-02-03 20:00:57.523833: step 77110, loss = 0.74 (257.7 examples/sec; 0.497 sec/batch)
2016-02-03 20:01:02.169992: step 77120, loss = 0.86 (292.6 examples/sec; 0.437 sec/batch)
2016-02-03 20:01:06.900314: step 77130, loss = 0.73 (262.2 examples/sec; 0.488 sec/batch)
2016-02-03 20:01:11.580950: step 77140, loss = 0.77 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 20:01:16.257108: step 77150, loss = 0.81 (252.1 examples/sec; 0.508 sec/batch)
2016-02-03 20:01:20.885834: step 77160, loss = 0.83 (278.5 examples/sec; 0.460 sec/batch)
2016-02-03 20:01:25.553577: step 77170, loss = 0.69 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 20:01:30.227702: step 77180, loss = 0.70 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 20:01:34.953587: step 77190, loss = 0.72 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 20:01:39.675272: step 77200, loss = 0.71 (284.4 examples/sec; 0.450 sec/batch)
2016-02-03 20:01:44.868792: step 77210, loss = 0.79 (307.4 examples/sec; 0.416 sec/batch)
2016-02-03 20:01:49.597907: step 77220, loss = 0.70 (255.7 examples/sec; 0.501 sec/batch)
2016-02-03 20:01:54.249881: step 77230, loss = 0.76 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 20:01:58.928659: step 77240, loss = 0.68 (265.8 examples/sec; 0.482 sec/batch)
2016-02-03 20:02:03.673227: step 77250, loss = 0.66 (248.2 examples/sec; 0.516 sec/batch)
2016-02-03 20:02:08.384322: step 77260, loss = 0.63 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 20:02:13.068792: step 77270, loss = 0.79 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 20:02:17.800283: step 77280, loss = 0.62 (287.9 examples/sec; 0.445 sec/batch)
2016-02-03 20:02:22.441632: step 77290, loss = 0.87 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 20:02:27.160509: step 77300, loss = 0.65 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 20:02:32.399277: step 77310, loss = 0.62 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 20:02:37.104636: step 77320, loss = 0.63 (255.9 examples/sec; 0.500 sec/batch)
2016-02-03 20:02:41.853510: step 77330, loss = 0.73 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 20:02:46.526934: step 77340, loss = 0.80 (289.8 examples/sec; 0.442 sec/batch)
2016-02-03 20:02:51.240780: step 77350, loss = 0.66 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 20:02:55.930899: step 77360, loss = 0.66 (291.2 examples/sec; 0.440 sec/batch)
2016-02-03 20:03:00.652623: step 77370, loss = 0.72 (261.3 examples/sec; 0.490 sec/batch)
2016-02-03 20:03:05.328418: step 77380, loss = 0.67 (293.2 examples/sec; 0.437 sec/batch)
2016-02-03 20:03:10.002066: step 77390, loss = 0.79 (289.1 examples/sec; 0.443 sec/batch)
2016-02-03 20:03:14.717803: step 77400, loss = 0.72 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 20:03:19.942904: step 77410, loss = 0.69 (249.2 examples/sec; 0.514 sec/batch)
2016-02-03 20:03:24.658953: step 77420, loss = 0.62 (284.9 examples/sec; 0.449 sec/batch)
2016-02-03 20:03:29.389013: step 77430, loss = 0.70 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 20:03:34.081074: step 77440, loss = 0.80 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 20:03:38.782912: step 77450, loss = 0.64 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 20:03:43.468115: step 77460, loss = 0.64 (276.9 examples/sec; 0.462 sec/batch)
2016-02-03 20:03:48.178310: step 77470, loss = 0.74 (279.6 examples/sec; 0.458 sec/batch)
2016-02-03 20:03:52.904912: step 77480, loss = 0.69 (290.8 examples/sec; 0.440 sec/batch)
2016-02-03 20:03:57.597468: step 77490, loss = 0.68 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 20:04:02.396590: step 77500, loss = 0.66 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 20:04:07.568049: step 77510, loss = 0.77 (259.5 examples/sec; 0.493 sec/batch)
2016-02-03 20:04:12.252423: step 77520, loss = 0.69 (274.4 examples/sec; 0.466 sec/batch)
2016-02-03 20:04:16.858349: step 77530, loss = 0.73 (293.9 examples/sec; 0.436 sec/batch)
2016-02-03 20:04:21.478304: step 77540, loss = 0.72 (304.3 examples/sec; 0.421 sec/batch)
2016-02-03 20:04:26.076339: step 77550, loss = 0.77 (303.0 examples/sec; 0.422 sec/batch)
2016-02-03 20:04:30.729559: step 77560, loss = 0.70 (287.0 examples/sec; 0.446 sec/batch)
2016-02-03 20:04:35.353925: step 77570, loss = 0.73 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 20:04:40.032275: step 77580, loss = 0.64 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 20:04:44.789830: step 77590, loss = 0.79 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 20:04:49.472911: step 77600, loss = 0.74 (276.4 examples/sec; 0.463 sec/batch)
2016-02-03 20:04:54.576441: step 77610, loss = 0.72 (294.4 examples/sec; 0.435 sec/batch)
2016-02-03 20:04:59.378381: step 77620, loss = 0.65 (256.5 examples/sec; 0.499 sec/batch)
2016-02-03 20:05:04.055779: step 77630, loss = 0.79 (266.9 examples/sec; 0.480 sec/batch)
2016-02-03 20:05:08.710968: step 77640, loss = 0.79 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 20:05:13.467274: step 77650, loss = 0.80 (287.2 examples/sec; 0.446 sec/batch)
2016-02-03 20:05:18.202159: step 77660, loss = 0.72 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 20:05:22.941051: step 77670, loss = 0.81 (283.4 examples/sec; 0.452 sec/batch)
2016-02-03 20:05:27.708143: step 77680, loss = 0.62 (249.8 examples/sec; 0.513 sec/batch)
2016-02-03 20:05:32.415961: step 77690, loss = 0.80 (239.9 examples/sec; 0.534 sec/batch)
2016-02-03 20:05:37.085694: step 77700, loss = 0.70 (258.6 examples/sec; 0.495 sec/batch)
2016-02-03 20:05:42.405554: step 77710, loss = 0.60 (264.3 examples/sec; 0.484 sec/batch)
2016-02-03 20:05:47.085888: step 77720, loss = 0.83 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 20:05:51.751906: step 77730, loss = 0.73 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 20:05:56.512691: step 77740, loss = 0.69 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 20:06:01.266670: step 77750, loss = 0.66 (277.7 examples/sec; 0.461 sec/batch)
2016-02-03 20:06:06.008309: step 77760, loss = 0.68 (256.1 examples/sec; 0.500 sec/batch)
2016-02-03 20:06:10.795753: step 77770, loss = 0.73 (258.8 examples/sec; 0.495 sec/batch)
2016-02-03 20:06:15.459195: step 77780, loss = 0.77 (283.1 examples/sec; 0.452 sec/batch)
2016-02-03 20:06:20.235889: step 77790, loss = 0.54 (249.3 examples/sec; 0.513 sec/batch)
2016-02-03 20:06:24.944005: step 77800, loss = 0.64 (248.7 examples/sec; 0.515 sec/batch)
2016-02-03 20:06:30.080150: step 77810, loss = 0.57 (289.2 examples/sec; 0.443 sec/batch)
2016-02-03 20:06:34.815980: step 77820, loss = 0.83 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 20:06:39.460448: step 77830, loss = 0.85 (274.4 examples/sec; 0.467 sec/batch)
2016-02-03 20:06:44.145770: step 77840, loss = 0.70 (287.4 examples/sec; 0.445 sec/batch)
2016-02-03 20:06:48.860915: step 77850, loss = 0.75 (290.2 examples/sec; 0.441 sec/batch)
2016-02-03 20:06:53.503942: step 77860, loss = 0.76 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 20:06:58.179142: step 77870, loss = 0.78 (257.2 examples/sec; 0.498 sec/batch)
2016-02-03 20:07:02.908333: step 77880, loss = 0.76 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 20:07:07.579210: step 77890, loss = 0.70 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 20:07:12.224722: step 77900, loss = 0.76 (251.7 examples/sec; 0.509 sec/batch)
2016-02-03 20:07:17.479116: step 77910, loss = 0.67 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 20:07:22.139922: step 77920, loss = 0.63 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 20:07:26.792136: step 77930, loss = 0.85 (298.4 examples/sec; 0.429 sec/batch)
2016-02-03 20:07:31.463325: step 77940, loss = 0.67 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 20:07:36.196412: step 77950, loss = 0.72 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 20:07:40.891568: step 77960, loss = 0.74 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 20:07:45.610906: step 77970, loss = 0.80 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 20:07:50.292741: step 77980, loss = 0.72 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 20:07:54.935074: step 77990, loss = 0.64 (269.2 examples/sec; 0.475 sec/batch)
2016-02-03 20:07:59.659353: step 78000, loss = 0.70 (264.7 examples/sec; 0.484 sec/batch)
2016-02-03 20:08:04.856226: step 78010, loss = 0.74 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 20:08:09.540167: step 78020, loss = 0.70 (256.1 examples/sec; 0.500 sec/batch)
2016-02-03 20:08:14.240250: step 78030, loss = 0.72 (255.3 examples/sec; 0.501 sec/batch)
2016-02-03 20:08:18.858121: step 78040, loss = 0.66 (259.2 examples/sec; 0.494 sec/batch)
2016-02-03 20:08:23.551498: step 78050, loss = 0.78 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 20:08:28.215545: step 78060, loss = 0.79 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 20:08:32.984232: step 78070, loss = 0.82 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 20:08:37.771947: step 78080, loss = 0.74 (268.0 examples/sec; 0.478 sec/batch)
2016-02-03 20:08:42.455033: step 78090, loss = 0.60 (263.1 examples/sec; 0.486 sec/batch)
2016-02-03 20:08:47.150482: step 78100, loss = 0.71 (263.1 examples/sec; 0.486 sec/batch)
2016-02-03 20:08:52.398740: step 78110, loss = 0.76 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 20:08:57.163246: step 78120, loss = 0.58 (266.4 examples/sec; 0.480 sec/batch)
2016-02-03 20:09:01.892310: step 78130, loss = 0.77 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 20:09:06.589043: step 78140, loss = 0.68 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 20:09:11.261161: step 78150, loss = 0.78 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 20:09:15.898968: step 78160, loss = 0.71 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 20:09:20.628204: step 78170, loss = 0.81 (285.0 examples/sec; 0.449 sec/batch)
2016-02-03 20:09:25.409481: step 78180, loss = 0.67 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 20:09:30.172191: step 78190, loss = 0.67 (296.8 examples/sec; 0.431 sec/batch)
2016-02-03 20:09:34.837543: step 78200, loss = 0.76 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 20:09:40.084570: step 78210, loss = 0.63 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 20:09:44.836073: step 78220, loss = 0.69 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 20:09:49.486805: step 78230, loss = 0.67 (290.0 examples/sec; 0.441 sec/batch)
2016-02-03 20:09:54.254219: step 78240, loss = 0.88 (255.7 examples/sec; 0.501 sec/batch)
2016-02-03 20:09:58.889716: step 78250, loss = 0.65 (285.0 examples/sec; 0.449 sec/batch)
2016-02-03 20:10:03.596104: step 78260, loss = 0.86 (284.0 examples/sec; 0.451 sec/batch)
2016-02-03 20:10:08.273357: step 78270, loss = 0.65 (285.7 examples/sec; 0.448 sec/batch)
2016-02-03 20:10:12.898084: step 78280, loss = 0.92 (285.2 examples/sec; 0.449 sec/batch)
2016-02-03 20:10:17.530245: step 78290, loss = 0.65 (281.7 examples/sec; 0.454 sec/batch)
2016-02-03 20:10:22.264479: step 78300, loss = 0.65 (284.7 examples/sec; 0.450 sec/batch)
2016-02-03 20:10:27.402860: step 78310, loss = 0.96 (293.9 examples/sec; 0.435 sec/batch)
2016-02-03 20:10:32.093994: step 78320, loss = 0.63 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 20:10:36.885337: step 78330, loss = 0.68 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 20:10:41.580067: step 78340, loss = 0.70 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 20:10:46.286592: step 78350, loss = 0.55 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 20:10:51.027918: step 78360, loss = 0.69 (245.7 examples/sec; 0.521 sec/batch)
2016-02-03 20:10:55.678264: step 78370, loss = 0.83 (253.9 examples/sec; 0.504 sec/batch)
2016-02-03 20:11:00.457991: step 78380, loss = 0.63 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 20:11:05.318050: step 78390, loss = 0.76 (238.6 examples/sec; 0.536 sec/batch)
2016-02-03 20:11:09.972934: step 78400, loss = 0.67 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 20:11:15.203995: step 78410, loss = 0.69 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 20:11:19.892085: step 78420, loss = 0.81 (282.9 examples/sec; 0.452 sec/batch)
2016-02-03 20:11:24.586635: step 78430, loss = 0.86 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 20:11:29.319542: step 78440, loss = 0.71 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 20:11:34.071760: step 78450, loss = 0.73 (257.7 examples/sec; 0.497 sec/batch)
2016-02-03 20:11:38.725953: step 78460, loss = 0.73 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 20:11:43.439135: step 78470, loss = 0.81 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 20:11:48.184777: step 78480, loss = 0.75 (279.2 examples/sec; 0.458 sec/batch)
2016-02-03 20:11:52.945276: step 78490, loss = 0.75 (254.3 examples/sec; 0.503 sec/batch)
2016-02-03 20:11:57.606918: step 78500, loss = 0.69 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 20:12:02.861005: step 78510, loss = 0.77 (265.8 examples/sec; 0.482 sec/batch)
2016-02-03 20:12:07.590114: step 78520, loss = 0.94 (262.0 examples/sec; 0.489 sec/batch)
2016-02-03 20:12:12.391817: step 78530, loss = 0.66 (240.6 examples/sec; 0.532 sec/batch)
2016-02-03 20:12:17.121861: step 78540, loss = 0.86 (300.7 examples/sec; 0.426 sec/batch)
2016-02-03 20:12:21.837890: step 78550, loss = 0.74 (240.8 examples/sec; 0.532 sec/batch)
2016-02-03 20:12:26.552826: step 78560, loss = 0.83 (286.7 examples/sec; 0.446 sec/batch)
2016-02-03 20:12:31.262626: step 78570, loss = 0.68 (261.1 examples/sec; 0.490 sec/batch)
2016-02-03 20:12:35.945642: step 78580, loss = 0.78 (290.5 examples/sec; 0.441 sec/batch)
2016-02-03 20:12:40.545456: step 78590, loss = 0.63 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 20:12:45.229888: step 78600, loss = 0.93 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 20:12:50.369280: step 78610, loss = 0.67 (283.8 examples/sec; 0.451 sec/batch)
2016-02-03 20:12:54.988346: step 78620, loss = 0.65 (294.7 examples/sec; 0.434 sec/batch)
2016-02-03 20:12:59.766630: step 78630, loss = 0.71 (255.7 examples/sec; 0.501 sec/batch)
2016-02-03 20:13:04.482662: step 78640, loss = 0.63 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 20:13:09.090713: step 78650, loss = 0.85 (274.4 examples/sec; 0.466 sec/batch)
2016-02-03 20:13:13.671846: step 78660, loss = 0.73 (301.7 examples/sec; 0.424 sec/batch)
2016-02-03 20:13:18.427878: step 78670, loss = 0.74 (255.2 examples/sec; 0.502 sec/batch)
2016-02-03 20:13:23.103507: step 78680, loss = 0.77 (283.9 examples/sec; 0.451 sec/batch)
2016-02-03 20:13:27.724601: step 78690, loss = 0.81 (251.1 examples/sec; 0.510 sec/batch)
2016-02-03 20:13:32.365804: step 78700, loss = 0.67 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 20:13:37.456801: step 78710, loss = 0.64 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 20:13:42.138122: step 78720, loss = 0.71 (279.4 examples/sec; 0.458 sec/batch)
2016-02-03 20:13:46.669448: step 78730, loss = 0.75 (292.7 examples/sec; 0.437 sec/batch)
2016-02-03 20:13:51.293932: step 78740, loss = 0.65 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 20:13:55.920313: step 78750, loss = 0.67 (299.0 examples/sec; 0.428 sec/batch)
2016-02-03 20:14:00.502647: step 78760, loss = 0.87 (291.0 examples/sec; 0.440 sec/batch)
2016-02-03 20:14:05.249750: step 78770, loss = 0.69 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 20:14:09.968749: step 78780, loss = 0.80 (296.9 examples/sec; 0.431 sec/batch)
2016-02-03 20:14:14.667537: step 78790, loss = 0.61 (291.2 examples/sec; 0.440 sec/batch)
2016-02-03 20:14:19.458540: step 78800, loss = 0.76 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 20:14:24.688943: step 78810, loss = 0.69 (274.4 examples/sec; 0.467 sec/batch)
2016-02-03 20:14:29.366631: step 78820, loss = 0.73 (283.3 examples/sec; 0.452 sec/batch)
2016-02-03 20:14:34.136016: step 78830, loss = 0.67 (257.4 examples/sec; 0.497 sec/batch)
2016-02-03 20:14:38.818657: step 78840, loss = 0.74 (255.8 examples/sec; 0.500 sec/batch)
2016-02-03 20:14:43.384031: step 78850, loss = 0.66 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 20:14:47.963531: step 78860, loss = 0.87 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 20:14:52.701006: step 78870, loss = 0.85 (245.2 examples/sec; 0.522 sec/batch)
2016-02-03 20:14:57.348418: step 78880, loss = 0.83 (279.4 examples/sec; 0.458 sec/batch)
2016-02-03 20:15:01.988351: step 78890, loss = 0.65 (278.0 examples/sec; 0.460 sec/batch)
2016-02-03 20:15:06.723971: step 78900, loss = 0.71 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 20:15:11.988852: step 78910, loss = 0.86 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 20:15:16.732063: step 78920, loss = 0.59 (262.6 examples/sec; 0.488 sec/batch)
2016-02-03 20:15:21.440350: step 78930, loss = 0.74 (284.8 examples/sec; 0.449 sec/batch)
2016-02-03 20:15:26.138498: step 78940, loss = 0.58 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 20:15:30.803054: step 78950, loss = 0.60 (290.2 examples/sec; 0.441 sec/batch)
2016-02-03 20:15:35.496839: step 78960, loss = 0.68 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 20:15:40.228751: step 78970, loss = 0.66 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 20:15:44.955181: step 78980, loss = 0.68 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 20:15:49.658501: step 78990, loss = 0.61 (269.2 examples/sec; 0.476 sec/batch)
2016-02-03 20:15:54.361092: step 79000, loss = 0.67 (285.3 examples/sec; 0.449 sec/batch)
2016-02-03 20:15:59.529093: step 79010, loss = 0.59 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 20:16:04.258582: step 79020, loss = 0.72 (264.9 examples/sec; 0.483 sec/batch)
2016-02-03 20:16:08.922823: step 79030, loss = 0.73 (296.4 examples/sec; 0.432 sec/batch)
2016-02-03 20:16:13.565597: step 79040, loss = 0.68 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 20:16:18.338758: step 79050, loss = 0.62 (253.3 examples/sec; 0.505 sec/batch)
2016-02-03 20:16:23.036791: step 79060, loss = 0.64 (253.6 examples/sec; 0.505 sec/batch)
2016-02-03 20:16:27.688936: step 79070, loss = 0.62 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 20:16:32.416182: step 79080, loss = 0.87 (256.8 examples/sec; 0.498 sec/batch)
2016-02-03 20:16:37.067528: step 79090, loss = 0.82 (292.6 examples/sec; 0.437 sec/batch)
2016-02-03 20:16:41.777383: step 79100, loss = 0.73 (293.9 examples/sec; 0.436 sec/batch)
2016-02-03 20:16:47.004190: step 79110, loss = 0.71 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 20:16:51.675971: step 79120, loss = 0.76 (301.5 examples/sec; 0.425 sec/batch)
2016-02-03 20:16:56.405119: step 79130, loss = 0.64 (280.6 examples/sec; 0.456 sec/batch)
2016-02-03 20:17:01.177676: step 79140, loss = 0.75 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 20:17:05.944012: step 79150, loss = 0.61 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 20:17:10.675771: step 79160, loss = 0.70 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 20:17:15.333801: step 79170, loss = 0.73 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 20:17:20.069778: step 79180, loss = 0.62 (256.5 examples/sec; 0.499 sec/batch)
2016-02-03 20:17:24.734763: step 79190, loss = 0.64 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 20:17:29.352739: step 79200, loss = 0.77 (297.6 examples/sec; 0.430 sec/batch)
2016-02-03 20:17:34.458280: step 79210, loss = 0.66 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 20:17:39.163587: step 79220, loss = 0.66 (291.2 examples/sec; 0.440 sec/batch)
2016-02-03 20:17:43.916458: step 79230, loss = 0.73 (252.5 examples/sec; 0.507 sec/batch)
2016-02-03 20:17:48.690078: step 79240, loss = 0.69 (248.2 examples/sec; 0.516 sec/batch)
2016-02-03 20:17:53.406501: step 79250, loss = 0.67 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 20:17:58.197299: step 79260, loss = 0.75 (253.5 examples/sec; 0.505 sec/batch)
2016-02-03 20:18:02.898746: step 79270, loss = 0.69 (256.6 examples/sec; 0.499 sec/batch)
2016-02-03 20:18:07.604921: step 79280, loss = 0.71 (279.4 examples/sec; 0.458 sec/batch)
2016-02-03 20:18:12.332173: step 79290, loss = 0.62 (259.9 examples/sec; 0.493 sec/batch)
2016-02-03 20:18:17.080710: step 79300, loss = 0.85 (255.0 examples/sec; 0.502 sec/batch)
2016-02-03 20:18:22.263609: step 79310, loss = 0.63 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 20:18:26.975161: step 79320, loss = 0.72 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 20:18:31.699453: step 79330, loss = 0.67 (256.3 examples/sec; 0.499 sec/batch)
2016-02-03 20:18:36.297837: step 79340, loss = 0.78 (290.0 examples/sec; 0.441 sec/batch)
2016-02-03 20:18:40.945168: step 79350, loss = 0.72 (276.4 examples/sec; 0.463 sec/batch)
2016-02-03 20:18:45.630194: step 79360, loss = 0.72 (259.7 examples/sec; 0.493 sec/batch)
2016-02-03 20:18:50.338148: step 79370, loss = 0.75 (265.2 examples/sec; 0.483 sec/batch)
2016-02-03 20:18:54.977267: step 79380, loss = 0.74 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 20:18:59.672664: step 79390, loss = 0.65 (280.3 examples/sec; 0.457 sec/batch)
2016-02-03 20:19:04.322273: step 79400, loss = 0.63 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 20:19:09.371264: step 79410, loss = 0.71 (331.7 examples/sec; 0.386 sec/batch)
2016-02-03 20:19:13.952205: step 79420, loss = 0.76 (322.0 examples/sec; 0.397 sec/batch)
2016-02-03 20:19:18.667058: step 79430, loss = 0.81 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 20:19:23.284406: step 79440, loss = 0.74 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 20:19:27.960575: step 79450, loss = 0.70 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 20:19:32.646430: step 79460, loss = 0.67 (289.2 examples/sec; 0.443 sec/batch)
2016-02-03 20:19:37.342901: step 79470, loss = 0.72 (255.8 examples/sec; 0.500 sec/batch)
2016-02-03 20:19:42.079820: step 79480, loss = 0.74 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 20:19:46.711737: step 79490, loss = 0.63 (276.7 examples/sec; 0.463 sec/batch)
2016-02-03 20:19:51.353365: step 79500, loss = 0.63 (283.6 examples/sec; 0.451 sec/batch)
2016-02-03 20:19:56.605998: step 79510, loss = 0.72 (268.0 examples/sec; 0.478 sec/batch)
2016-02-03 20:20:01.263303: step 79520, loss = 0.62 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 20:20:05.932540: step 79530, loss = 0.67 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 20:20:10.615687: step 79540, loss = 0.57 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 20:20:15.277777: step 79550, loss = 0.73 (272.7 examples/sec; 0.469 sec/batch)
2016-02-03 20:20:19.966782: step 79560, loss = 0.64 (268.0 examples/sec; 0.478 sec/batch)
2016-02-03 20:20:24.664570: step 79570, loss = 0.81 (254.9 examples/sec; 0.502 sec/batch)
2016-02-03 20:20:29.265987: step 79580, loss = 0.66 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 20:20:33.878569: step 79590, loss = 0.72 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 20:20:38.509341: step 79600, loss = 0.68 (254.3 examples/sec; 0.503 sec/batch)
2016-02-03 20:20:43.602431: step 79610, loss = 0.70 (283.0 examples/sec; 0.452 sec/batch)
2016-02-03 20:20:48.325663: step 79620, loss = 0.72 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 20:20:52.994808: step 79630, loss = 0.71 (291.3 examples/sec; 0.439 sec/batch)
2016-02-03 20:20:57.637803: step 79640, loss = 0.77 (292.1 examples/sec; 0.438 sec/batch)
2016-02-03 20:21:02.346314: step 79650, loss = 0.74 (284.1 examples/sec; 0.451 sec/batch)
2016-02-03 20:21:06.990217: step 79660, loss = 0.83 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 20:21:11.678172: step 79670, loss = 0.69 (277.8 examples/sec; 0.461 sec/batch)
2016-02-03 20:21:16.370442: step 79680, loss = 0.81 (259.9 examples/sec; 0.493 sec/batch)
2016-02-03 20:21:21.120921: step 79690, loss = 0.70 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 20:21:25.777861: step 79700, loss = 0.75 (285.1 examples/sec; 0.449 sec/batch)
2016-02-03 20:21:31.032407: step 79710, loss = 0.76 (260.3 examples/sec; 0.492 sec/batch)
2016-02-03 20:21:35.723375: step 79720, loss = 0.75 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 20:21:40.515566: step 79730, loss = 0.71 (256.0 examples/sec; 0.500 sec/batch)
2016-02-03 20:21:45.217663: step 79740, loss = 0.64 (261.0 examples/sec; 0.490 sec/batch)
2016-02-03 20:21:49.920716: step 79750, loss = 0.67 (259.8 examples/sec; 0.493 sec/batch)
2016-02-03 20:21:54.702500: step 79760, loss = 0.75 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 20:21:59.378272: step 79770, loss = 0.75 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 20:22:04.091028: step 79780, loss = 0.75 (275.7 examples/sec; 0.464 sec/batch)
2016-02-03 20:22:08.769736: step 79790, loss = 0.60 (265.8 examples/sec; 0.482 sec/batch)
2016-02-03 20:22:13.502742: step 79800, loss = 0.68 (289.1 examples/sec; 0.443 sec/batch)
2016-02-03 20:22:18.812525: step 79810, loss = 0.68 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 20:22:23.595566: step 79820, loss = 0.68 (264.1 examples/sec; 0.485 sec/batch)
2016-02-03 20:22:28.322105: step 79830, loss = 0.66 (272.6 examples/sec; 0.470 sec/batch)
2016-02-03 20:22:33.116284: step 79840, loss = 0.66 (259.4 examples/sec; 0.494 sec/batch)
2016-02-03 20:22:37.859454: step 79850, loss = 0.83 (278.5 examples/sec; 0.460 sec/batch)
2016-02-03 20:22:42.554417: step 79860, loss = 0.65 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 20:22:47.262738: step 79870, loss = 0.76 (249.0 examples/sec; 0.514 sec/batch)
2016-02-03 20:22:51.947837: step 79880, loss = 0.78 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 20:22:56.714426: step 79890, loss = 0.78 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 20:23:01.393875: step 79900, loss = 0.75 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 20:23:06.564446: step 79910, loss = 0.80 (264.7 examples/sec; 0.484 sec/batch)
2016-02-03 20:23:11.185157: step 79920, loss = 0.62 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 20:23:15.818845: step 79930, loss = 0.77 (291.4 examples/sec; 0.439 sec/batch)
2016-02-03 20:23:20.463886: step 79940, loss = 0.75 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 20:23:25.150226: step 79950, loss = 0.81 (291.4 examples/sec; 0.439 sec/batch)
2016-02-03 20:23:29.843585: step 79960, loss = 0.66 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 20:23:34.507154: step 79970, loss = 0.82 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 20:23:39.209527: step 79980, loss = 0.73 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 20:23:43.876742: step 79990, loss = 0.79 (279.2 examples/sec; 0.459 sec/batch)
2016-02-03 20:23:48.558785: step 80000, loss = 0.67 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 20:23:53.762419: step 80010, loss = 0.68 (287.3 examples/sec; 0.446 sec/batch)
2016-02-03 20:23:58.450512: step 80020, loss = 0.67 (262.0 examples/sec; 0.489 sec/batch)
2016-02-03 20:24:03.144665: step 80030, loss = 0.69 (276.9 examples/sec; 0.462 sec/batch)
2016-02-03 20:24:07.863978: step 80040, loss = 0.79 (256.1 examples/sec; 0.500 sec/batch)
2016-02-03 20:24:12.537391: step 80050, loss = 0.72 (284.2 examples/sec; 0.450 sec/batch)
2016-02-03 20:24:17.142589: step 80060, loss = 0.72 (291.2 examples/sec; 0.440 sec/batch)
2016-02-03 20:24:21.812175: step 80070, loss = 0.70 (261.4 examples/sec; 0.490 sec/batch)
2016-02-03 20:24:26.463806: step 80080, loss = 0.81 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 20:24:31.146086: step 80090, loss = 0.72 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 20:24:35.824261: step 80100, loss = 0.69 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 20:24:41.094877: step 80110, loss = 0.74 (269.2 examples/sec; 0.475 sec/batch)
2016-02-03 20:24:45.894010: step 80120, loss = 0.60 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 20:24:50.605947: step 80130, loss = 0.73 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 20:24:55.276695: step 80140, loss = 0.75 (292.6 examples/sec; 0.437 sec/batch)
2016-02-03 20:25:00.064079: step 80150, loss = 0.79 (253.4 examples/sec; 0.505 sec/batch)
2016-02-03 20:25:04.773849: step 80160, loss = 0.65 (262.2 examples/sec; 0.488 sec/batch)
2016-02-03 20:25:09.521633: step 80170, loss = 0.71 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 20:25:14.217515: step 80180, loss = 0.82 (265.8 examples/sec; 0.481 sec/batch)
2016-02-03 20:25:18.950800: step 80190, loss = 0.70 (260.6 examples/sec; 0.491 sec/batch)
2016-02-03 20:25:23.634641: step 80200, loss = 0.61 (293.5 examples/sec; 0.436 sec/batch)
2016-02-03 20:25:28.975811: step 80210, loss = 0.65 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 20:25:33.687104: step 80220, loss = 0.66 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 20:25:38.460078: step 80230, loss = 0.73 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 20:25:43.147299: step 80240, loss = 0.82 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 20:25:47.866368: step 80250, loss = 0.77 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 20:25:52.654045: step 80260, loss = 0.72 (248.6 examples/sec; 0.515 sec/batch)
2016-02-03 20:25:57.368359: step 80270, loss = 0.68 (240.2 examples/sec; 0.533 sec/batch)
2016-02-03 20:26:02.008057: step 80280, loss = 0.77 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 20:26:06.738124: step 80290, loss = 0.70 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 20:26:11.396379: step 80300, loss = 0.77 (262.2 examples/sec; 0.488 sec/batch)
2016-02-03 20:26:16.639528: step 80310, loss = 0.66 (261.0 examples/sec; 0.490 sec/batch)
2016-02-03 20:26:21.335167: step 80320, loss = 0.70 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 20:26:26.098017: step 80330, loss = 0.62 (241.1 examples/sec; 0.531 sec/batch)
2016-02-03 20:26:30.790351: step 80340, loss = 0.89 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 20:26:35.411925: step 80350, loss = 0.62 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 20:26:40.115514: step 80360, loss = 0.81 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 20:26:44.782257: step 80370, loss = 0.83 (267.3 examples/sec; 0.479 sec/batch)
2016-02-03 20:26:49.445298: step 80380, loss = 0.68 (244.8 examples/sec; 0.523 sec/batch)
2016-02-03 20:26:54.023188: step 80390, loss = 0.82 (259.7 examples/sec; 0.493 sec/batch)
2016-02-03 20:26:58.786876: step 80400, loss = 0.61 (255.1 examples/sec; 0.502 sec/batch)
2016-02-03 20:27:03.989784: step 80410, loss = 0.56 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 20:27:08.671202: step 80420, loss = 0.76 (264.7 examples/sec; 0.484 sec/batch)
2016-02-03 20:27:13.337169: step 80430, loss = 0.75 (278.5 examples/sec; 0.460 sec/batch)
2016-02-03 20:27:18.076444: step 80440, loss = 0.71 (304.3 examples/sec; 0.421 sec/batch)
2016-02-03 20:27:22.803421: step 80450, loss = 0.69 (285.7 examples/sec; 0.448 sec/batch)
2016-02-03 20:27:27.578502: step 80460, loss = 0.66 (255.1 examples/sec; 0.502 sec/batch)
2016-02-03 20:27:32.328434: step 80470, loss = 0.65 (252.8 examples/sec; 0.506 sec/batch)
2016-02-03 20:27:36.998158: step 80480, loss = 0.66 (261.0 examples/sec; 0.490 sec/batch)
2016-02-03 20:27:41.709266: step 80490, loss = 0.71 (286.4 examples/sec; 0.447 sec/batch)
2016-02-03 20:27:46.490715: step 80500, loss = 0.73 (250.7 examples/sec; 0.511 sec/batch)
2016-02-03 20:27:51.742237: step 80510, loss = 0.64 (292.4 examples/sec; 0.438 sec/batch)
2016-02-03 20:27:56.468372: step 80520, loss = 0.68 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 20:28:01.185333: step 80530, loss = 0.84 (268.0 examples/sec; 0.478 sec/batch)
2016-02-03 20:28:05.922619: step 80540, loss = 0.66 (282.8 examples/sec; 0.453 sec/batch)
2016-02-03 20:28:10.660153: step 80550, loss = 0.80 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 20:28:15.428203: step 80560, loss = 0.70 (242.4 examples/sec; 0.528 sec/batch)
2016-02-03 20:28:20.163462: step 80570, loss = 0.66 (264.9 examples/sec; 0.483 sec/batch)
2016-02-03 20:28:24.841813: step 80580, loss = 0.73 (259.9 examples/sec; 0.493 sec/batch)
2016-02-03 20:28:29.577746: step 80590, loss = 0.68 (280.4 examples/sec; 0.456 sec/batch)
2016-02-03 20:28:34.284430: step 80600, loss = 0.80 (310.1 examples/sec; 0.413 sec/batch)
2016-02-03 20:28:39.533319: step 80610, loss = 0.65 (301.2 examples/sec; 0.425 sec/batch)
2016-02-03 20:28:44.249716: step 80620, loss = 0.78 (285.6 examples/sec; 0.448 sec/batch)
2016-02-03 20:28:48.956484: step 80630, loss = 0.65 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 20:28:53.690310: step 80640, loss = 0.76 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 20:28:58.369083: step 80650, loss = 0.75 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 20:29:02.953457: step 80660, loss = 0.63 (286.0 examples/sec; 0.448 sec/batch)
2016-02-03 20:29:07.729276: step 80670, loss = 0.69 (261.9 examples/sec; 0.489 sec/batch)
2016-02-03 20:29:12.452817: step 80680, loss = 0.73 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 20:29:17.123601: step 80690, loss = 0.70 (290.6 examples/sec; 0.440 sec/batch)
2016-02-03 20:29:21.898690: step 80700, loss = 0.58 (242.6 examples/sec; 0.528 sec/batch)
2016-02-03 20:29:27.124569: step 80710, loss = 0.71 (258.3 examples/sec; 0.496 sec/batch)
2016-02-03 20:29:31.889333: step 80720, loss = 0.73 (296.8 examples/sec; 0.431 sec/batch)
2016-02-03 20:29:36.532076: step 80730, loss = 0.85 (287.8 examples/sec; 0.445 sec/batch)
2016-02-03 20:29:41.126415: step 80740, loss = 0.74 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 20:29:45.828560: step 80750, loss = 0.65 (251.9 examples/sec; 0.508 sec/batch)
2016-02-03 20:29:50.468810: step 80760, loss = 0.64 (249.7 examples/sec; 0.513 sec/batch)
2016-02-03 20:29:55.168894: step 80770, loss = 0.83 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 20:29:59.958186: step 80780, loss = 0.71 (251.5 examples/sec; 0.509 sec/batch)
2016-02-03 20:30:04.644426: step 80790, loss = 0.74 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 20:30:09.352116: step 80800, loss = 0.78 (288.3 examples/sec; 0.444 sec/batch)
2016-02-03 20:30:14.638864: step 80810, loss = 0.78 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 20:30:19.297220: step 80820, loss = 0.69 (277.6 examples/sec; 0.461 sec/batch)
2016-02-03 20:30:24.046295: step 80830, loss = 0.76 (251.5 examples/sec; 0.509 sec/batch)
2016-02-03 20:30:28.695947: step 80840, loss = 0.66 (251.9 examples/sec; 0.508 sec/batch)
2016-02-03 20:30:33.427924: step 80850, loss = 0.62 (251.7 examples/sec; 0.508 sec/batch)
2016-02-03 20:30:38.123058: step 80860, loss = 0.77 (261.4 examples/sec; 0.490 sec/batch)
2016-02-03 20:30:42.830149: step 80870, loss = 0.67 (293.4 examples/sec; 0.436 sec/batch)
2016-02-03 20:30:47.503328: step 80880, loss = 0.65 (280.9 examples/sec; 0.456 sec/batch)
2016-02-03 20:30:52.251991: step 80890, loss = 0.61 (290.4 examples/sec; 0.441 sec/batch)
2016-02-03 20:30:57.000667: step 80900, loss = 0.77 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 20:31:02.151141: step 80910, loss = 0.66 (258.3 examples/sec; 0.496 sec/batch)
2016-02-03 20:31:06.822648: step 80920, loss = 0.58 (285.3 examples/sec; 0.449 sec/batch)
2016-02-03 20:31:11.475540: step 80930, loss = 0.71 (295.9 examples/sec; 0.433 sec/batch)
2016-02-03 20:31:16.150710: step 80940, loss = 0.69 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 20:31:20.987011: step 80950, loss = 0.92 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 20:31:25.771880: step 80960, loss = 0.59 (262.1 examples/sec; 0.488 sec/batch)
2016-02-03 20:31:30.441393: step 80970, loss = 0.61 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 20:31:35.149393: step 80980, loss = 0.69 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 20:31:39.970659: step 80990, loss = 0.75 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 20:31:44.768052: step 81000, loss = 0.75 (264.3 examples/sec; 0.484 sec/batch)
2016-02-03 20:31:50.080554: step 81010, loss = 0.78 (259.2 examples/sec; 0.494 sec/batch)
2016-02-03 20:31:54.731218: step 81020, loss = 0.69 (293.1 examples/sec; 0.437 sec/batch)
2016-02-03 20:31:59.479127: step 81030, loss = 0.61 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 20:32:04.179124: step 81040, loss = 0.61 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 20:32:08.914715: step 81050, loss = 0.83 (266.9 examples/sec; 0.480 sec/batch)
2016-02-03 20:32:13.654568: step 81060, loss = 0.62 (289.7 examples/sec; 0.442 sec/batch)
2016-02-03 20:32:18.338703: step 81070, loss = 0.73 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 20:32:23.152670: step 81080, loss = 0.80 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 20:32:27.835642: step 81090, loss = 0.77 (283.7 examples/sec; 0.451 sec/batch)
2016-02-03 20:32:32.627481: step 81100, loss = 0.69 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 20:32:37.873188: step 81110, loss = 0.63 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 20:32:42.545972: step 81120, loss = 0.79 (251.9 examples/sec; 0.508 sec/batch)
2016-02-03 20:32:47.281348: step 81130, loss = 0.65 (259.4 examples/sec; 0.493 sec/batch)
2016-02-03 20:32:52.021726: step 81140, loss = 0.76 (249.0 examples/sec; 0.514 sec/batch)
2016-02-03 20:32:56.719179: step 81150, loss = 0.62 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 20:33:01.438510: step 81160, loss = 0.84 (254.6 examples/sec; 0.503 sec/batch)
2016-02-03 20:33:06.105318: step 81170, loss = 0.62 (278.0 examples/sec; 0.460 sec/batch)
2016-02-03 20:33:10.835800: step 81180, loss = 0.73 (298.0 examples/sec; 0.430 sec/batch)
2016-02-03 20:33:15.486207: step 81190, loss = 0.74 (287.8 examples/sec; 0.445 sec/batch)
2016-02-03 20:33:20.195878: step 81200, loss = 0.78 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 20:33:25.508246: step 81210, loss = 0.76 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 20:33:30.192634: step 81220, loss = 0.62 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 20:33:34.872832: step 81230, loss = 0.80 (284.7 examples/sec; 0.450 sec/batch)
2016-02-03 20:33:39.601248: step 81240, loss = 0.70 (290.9 examples/sec; 0.440 sec/batch)
2016-02-03 20:33:44.289248: step 81250, loss = 0.87 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 20:33:48.967308: step 81260, loss = 0.68 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 20:33:53.671563: step 81270, loss = 0.84 (257.7 examples/sec; 0.497 sec/batch)
2016-02-03 20:33:58.368137: step 81280, loss = 0.73 (287.2 examples/sec; 0.446 sec/batch)
2016-02-03 20:34:03.158041: step 81290, loss = 0.62 (251.6 examples/sec; 0.509 sec/batch)
2016-02-03 20:34:07.925561: step 81300, loss = 0.72 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 20:34:13.155108: step 81310, loss = 0.66 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 20:34:17.831338: step 81320, loss = 0.72 (285.5 examples/sec; 0.448 sec/batch)
2016-02-03 20:34:22.544190: step 81330, loss = 0.73 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 20:34:27.301798: step 81340, loss = 0.76 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 20:34:32.047739: step 81350, loss = 1.02 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 20:34:36.728732: step 81360, loss = 0.84 (250.3 examples/sec; 0.511 sec/batch)
2016-02-03 20:34:41.363685: step 81370, loss = 0.63 (308.0 examples/sec; 0.416 sec/batch)
2016-02-03 20:34:46.103474: step 81380, loss = 0.64 (258.1 examples/sec; 0.496 sec/batch)
2016-02-03 20:34:50.726519: step 81390, loss = 0.80 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 20:34:55.417943: step 81400, loss = 0.78 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 20:35:00.617082: step 81410, loss = 0.61 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 20:35:05.277315: step 81420, loss = 0.70 (286.1 examples/sec; 0.447 sec/batch)
2016-02-03 20:35:10.003366: step 81430, loss = 0.66 (253.4 examples/sec; 0.505 sec/batch)
2016-02-03 20:35:14.702802: step 81440, loss = 0.78 (262.6 examples/sec; 0.487 sec/batch)
2016-02-03 20:35:19.477820: step 81450, loss = 0.76 (269.2 examples/sec; 0.475 sec/batch)
2016-02-03 20:35:24.095588: step 81460, loss = 0.76 (287.0 examples/sec; 0.446 sec/batch)
2016-02-03 20:35:28.778343: step 81470, loss = 0.65 (285.9 examples/sec; 0.448 sec/batch)
2016-02-03 20:35:33.551160: step 81480, loss = 0.71 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 20:35:38.322607: step 81490, loss = 0.74 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 20:35:43.004331: step 81500, loss = 0.63 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 20:35:48.177204: step 81510, loss = 0.72 (289.8 examples/sec; 0.442 sec/batch)
2016-02-03 20:35:52.920665: step 81520, loss = 0.68 (295.1 examples/sec; 0.434 sec/batch)
2016-02-03 20:35:57.590595: step 81530, loss = 0.88 (257.3 examples/sec; 0.497 sec/batch)
2016-02-03 20:36:02.278956: step 81540, loss = 0.73 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 20:36:06.931535: step 81550, loss = 0.78 (282.2 examples/sec; 0.454 sec/batch)
2016-02-03 20:36:11.698910: step 81560, loss = 0.73 (247.3 examples/sec; 0.518 sec/batch)
2016-02-03 20:36:16.427408: step 81570, loss = 0.79 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 20:36:21.183505: step 81580, loss = 0.81 (270.5 examples/sec; 0.473 sec/batch)
2016-02-03 20:36:25.913398: step 81590, loss = 0.73 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 20:36:30.620748: step 81600, loss = 0.69 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 20:36:35.893055: step 81610, loss = 0.79 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 20:36:40.608412: step 81620, loss = 0.72 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 20:36:45.288892: step 81630, loss = 0.82 (286.4 examples/sec; 0.447 sec/batch)
2016-02-03 20:36:50.035230: step 81640, loss = 0.67 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 20:36:54.726016: step 81650, loss = 0.71 (277.7 examples/sec; 0.461 sec/batch)
2016-02-03 20:36:59.474446: step 81660, loss = 0.79 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 20:37:04.230403: step 81670, loss = 0.79 (252.7 examples/sec; 0.507 sec/batch)
2016-02-03 20:37:08.985968: step 81680, loss = 0.65 (256.9 examples/sec; 0.498 sec/batch)
2016-02-03 20:37:13.742545: step 81690, loss = 0.64 (254.5 examples/sec; 0.503 sec/batch)
2016-02-03 20:37:18.460430: step 81700, loss = 0.74 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 20:37:23.714239: step 81710, loss = 0.69 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 20:37:28.457951: step 81720, loss = 0.91 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 20:37:33.064556: step 81730, loss = 0.73 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 20:37:37.704798: step 81740, loss = 0.73 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 20:37:42.449036: step 81750, loss = 0.87 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 20:37:47.182469: step 81760, loss = 0.64 (253.7 examples/sec; 0.505 sec/batch)
2016-02-03 20:37:51.823384: step 81770, loss = 0.74 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 20:37:56.534521: step 81780, loss = 0.78 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 20:38:01.268493: step 81790, loss = 0.64 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 20:38:05.990238: step 81800, loss = 0.76 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 20:38:11.188377: step 81810, loss = 0.75 (290.5 examples/sec; 0.441 sec/batch)
2016-02-03 20:38:15.909373: step 81820, loss = 0.62 (283.1 examples/sec; 0.452 sec/batch)
2016-02-03 20:38:20.608700: step 81830, loss = 0.64 (258.4 examples/sec; 0.495 sec/batch)
2016-02-03 20:38:25.302754: step 81840, loss = 0.64 (254.7 examples/sec; 0.503 sec/batch)
2016-02-03 20:38:29.950357: step 81850, loss = 0.79 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 20:38:34.640661: step 81860, loss = 0.77 (292.4 examples/sec; 0.438 sec/batch)
2016-02-03 20:38:39.411176: step 81870, loss = 0.78 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 20:38:44.060331: step 81880, loss = 0.83 (291.9 examples/sec; 0.439 sec/batch)
2016-02-03 20:38:48.804399: step 81890, loss = 0.72 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 20:38:53.527882: step 81900, loss = 0.90 (284.0 examples/sec; 0.451 sec/batch)
2016-02-03 20:38:58.686558: step 81910, loss = 0.76 (286.6 examples/sec; 0.447 sec/batch)
2016-02-03 20:39:03.376206: step 81920, loss = 0.57 (282.8 examples/sec; 0.453 sec/batch)
2016-02-03 20:39:08.074166: step 81930, loss = 0.67 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 20:39:12.805059: step 81940, loss = 0.78 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 20:39:17.513920: step 81950, loss = 0.67 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 20:39:22.214683: step 81960, loss = 0.79 (286.5 examples/sec; 0.447 sec/batch)
2016-02-03 20:39:26.925962: step 81970, loss = 0.70 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 20:39:31.636327: step 81980, loss = 0.80 (260.6 examples/sec; 0.491 sec/batch)
2016-02-03 20:39:36.278398: step 81990, loss = 0.61 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 20:39:40.903150: step 82000, loss = 0.64 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 20:39:46.200056: step 82010, loss = 0.60 (257.9 examples/sec; 0.496 sec/batch)
2016-02-03 20:39:51.006141: step 82020, loss = 0.76 (253.8 examples/sec; 0.504 sec/batch)
2016-02-03 20:39:55.711560: step 82030, loss = 0.82 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 20:40:00.450301: step 82040, loss = 0.80 (290.3 examples/sec; 0.441 sec/batch)
2016-02-03 20:40:05.214018: step 82050, loss = 0.70 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 20:40:09.955831: step 82060, loss = 0.77 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 20:40:14.698888: step 82070, loss = 0.72 (246.4 examples/sec; 0.519 sec/batch)
2016-02-03 20:40:19.346166: step 82080, loss = 0.74 (280.1 examples/sec; 0.457 sec/batch)
2016-02-03 20:40:24.118443: step 82090, loss = 0.63 (288.1 examples/sec; 0.444 sec/batch)
2016-02-03 20:40:28.915645: step 82100, loss = 0.68 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 20:40:34.130224: step 82110, loss = 0.75 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 20:40:38.806120: step 82120, loss = 0.70 (277.9 examples/sec; 0.461 sec/batch)
2016-02-03 20:40:43.425255: step 82130, loss = 0.79 (295.6 examples/sec; 0.433 sec/batch)
2016-02-03 20:40:48.127200: step 82140, loss = 0.73 (254.9 examples/sec; 0.502 sec/batch)
2016-02-03 20:40:52.879755: step 82150, loss = 0.64 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 20:40:57.587551: step 82160, loss = 0.69 (289.3 examples/sec; 0.442 sec/batch)
2016-02-03 20:41:02.259201: step 82170, loss = 0.79 (249.8 examples/sec; 0.512 sec/batch)
2016-02-03 20:41:06.928065: step 82180, loss = 0.90 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 20:41:11.697551: step 82190, loss = 0.71 (264.9 examples/sec; 0.483 sec/batch)
2016-02-03 20:41:16.358901: step 82200, loss = 0.76 (298.6 examples/sec; 0.429 sec/batch)
2016-02-03 20:41:21.655767: step 82210, loss = 0.72 (257.2 examples/sec; 0.498 sec/batch)
2016-02-03 20:41:26.373371: step 82220, loss = 0.78 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 20:41:31.070895: step 82230, loss = 0.75 (264.3 examples/sec; 0.484 sec/batch)
2016-02-03 20:41:35.701548: step 82240, loss = 0.59 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 20:41:40.410538: step 82250, loss = 0.80 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 20:41:45.133618: step 82260, loss = 0.74 (290.9 examples/sec; 0.440 sec/batch)
2016-02-03 20:41:49.861224: step 82270, loss = 0.71 (257.1 examples/sec; 0.498 sec/batch)
2016-02-03 20:41:54.597076: step 82280, loss = 0.51 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 20:41:59.326030: step 82290, loss = 0.86 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 20:42:04.073255: step 82300, loss = 0.80 (295.1 examples/sec; 0.434 sec/batch)
2016-02-03 20:42:09.407389: step 82310, loss = 0.71 (246.9 examples/sec; 0.519 sec/batch)
2016-02-03 20:42:14.119660: step 82320, loss = 0.75 (270.9 examples/sec; 0.472 sec/batch)
2016-02-03 20:42:18.823830: step 82330, loss = 0.70 (272.7 examples/sec; 0.469 sec/batch)
2016-02-03 20:42:23.557820: step 82340, loss = 0.55 (298.7 examples/sec; 0.429 sec/batch)
2016-02-03 20:42:28.336008: step 82350, loss = 0.75 (261.0 examples/sec; 0.490 sec/batch)
2016-02-03 20:42:33.079929: step 82360, loss = 0.68 (260.5 examples/sec; 0.491 sec/batch)
2016-02-03 20:42:37.703468: step 82370, loss = 0.80 (295.7 examples/sec; 0.433 sec/batch)
2016-02-03 20:42:42.368774: step 82380, loss = 0.65 (281.2 examples/sec; 0.455 sec/batch)
2016-02-03 20:42:47.054938: step 82390, loss = 0.68 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 20:42:51.742765: step 82400, loss = 0.79 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 20:42:57.022299: step 82410, loss = 0.69 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 20:43:01.678297: step 82420, loss = 0.76 (285.3 examples/sec; 0.449 sec/batch)
2016-02-03 20:43:06.436120: step 82430, loss = 0.75 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 20:43:11.224699: step 82440, loss = 0.74 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 20:43:16.001801: step 82450, loss = 0.75 (257.4 examples/sec; 0.497 sec/batch)
2016-02-03 20:43:20.821313: step 82460, loss = 0.74 (261.7 examples/sec; 0.489 sec/batch)
2016-02-03 20:43:25.511223: step 82470, loss = 0.68 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 20:43:30.198708: step 82480, loss = 0.76 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 20:43:34.994534: step 82490, loss = 0.77 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 20:43:39.691812: step 82500, loss = 0.75 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 20:43:44.966549: step 82510, loss = 0.72 (263.2 examples/sec; 0.486 sec/batch)
2016-02-03 20:43:49.755611: step 82520, loss = 0.98 (251.9 examples/sec; 0.508 sec/batch)
2016-02-03 20:43:54.424973: step 82530, loss = 0.66 (293.8 examples/sec; 0.436 sec/batch)
2016-02-03 20:43:59.154123: step 82540, loss = 0.82 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 20:44:03.985175: step 82550, loss = 0.73 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 20:44:08.748572: step 82560, loss = 0.66 (240.4 examples/sec; 0.532 sec/batch)
2016-02-03 20:44:13.423161: step 82570, loss = 0.64 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 20:44:18.158708: step 82580, loss = 0.80 (294.1 examples/sec; 0.435 sec/batch)
2016-02-03 20:44:22.821234: step 82590, loss = 0.61 (279.6 examples/sec; 0.458 sec/batch)
2016-02-03 20:44:27.465625: step 82600, loss = 0.73 (288.3 examples/sec; 0.444 sec/batch)
2016-02-03 20:44:32.744557: step 82610, loss = 0.81 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 20:44:37.389383: step 82620, loss = 0.67 (252.0 examples/sec; 0.508 sec/batch)
2016-02-03 20:44:42.192628: step 82630, loss = 0.64 (256.1 examples/sec; 0.500 sec/batch)
2016-02-03 20:44:46.925458: step 82640, loss = 0.63 (255.3 examples/sec; 0.501 sec/batch)
2016-02-03 20:44:51.574163: step 82650, loss = 0.65 (281.5 examples/sec; 0.455 sec/batch)
2016-02-03 20:44:56.170525: step 82660, loss = 0.65 (288.1 examples/sec; 0.444 sec/batch)
2016-02-03 20:45:00.840104: step 82670, loss = 0.77 (285.4 examples/sec; 0.449 sec/batch)
2016-02-03 20:45:05.438954: step 82680, loss = 0.58 (301.8 examples/sec; 0.424 sec/batch)
2016-02-03 20:45:10.162070: step 82690, loss = 0.60 (283.7 examples/sec; 0.451 sec/batch)
2016-02-03 20:45:14.846926: step 82700, loss = 0.85 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 20:45:20.086900: step 82710, loss = 0.66 (280.6 examples/sec; 0.456 sec/batch)
2016-02-03 20:45:24.793696: step 82720, loss = 0.67 (286.1 examples/sec; 0.447 sec/batch)
2016-02-03 20:45:29.494618: step 82730, loss = 0.67 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 20:45:34.186900: step 82740, loss = 0.85 (303.7 examples/sec; 0.422 sec/batch)
2016-02-03 20:45:38.934714: step 82750, loss = 0.90 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 20:45:43.619237: step 82760, loss = 0.73 (287.4 examples/sec; 0.445 sec/batch)
2016-02-03 20:45:48.297383: step 82770, loss = 0.62 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 20:45:53.039199: step 82780, loss = 0.88 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 20:45:57.816930: step 82790, loss = 0.60 (247.3 examples/sec; 0.518 sec/batch)
2016-02-03 20:46:02.557086: step 82800, loss = 0.75 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 20:46:07.871570: step 82810, loss = 0.67 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 20:46:12.548495: step 82820, loss = 0.68 (273.8 examples/sec; 0.468 sec/batch)
2016-02-03 20:46:17.294250: step 82830, loss = 0.61 (246.9 examples/sec; 0.518 sec/batch)
2016-02-03 20:46:22.094193: step 82840, loss = 0.74 (259.0 examples/sec; 0.494 sec/batch)
2016-02-03 20:46:26.713206: step 82850, loss = 0.72 (308.3 examples/sec; 0.415 sec/batch)
2016-02-03 20:46:31.498544: step 82860, loss = 0.68 (264.7 examples/sec; 0.484 sec/batch)
2016-02-03 20:46:36.173239: step 82870, loss = 0.73 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 20:46:40.830440: step 82880, loss = 0.68 (283.9 examples/sec; 0.451 sec/batch)
2016-02-03 20:46:45.496427: step 82890, loss = 0.81 (267.3 examples/sec; 0.479 sec/batch)
2016-02-03 20:46:50.179895: step 82900, loss = 0.65 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 20:46:55.173241: step 82910, loss = 0.60 (267.3 examples/sec; 0.479 sec/batch)
2016-02-03 20:46:59.837795: step 82920, loss = 0.68 (267.0 examples/sec; 0.479 sec/batch)
2016-02-03 20:47:04.524661: step 82930, loss = 0.69 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 20:47:09.158975: step 82940, loss = 0.68 (287.0 examples/sec; 0.446 sec/batch)
2016-02-03 20:47:13.899349: step 82950, loss = 0.70 (243.4 examples/sec; 0.526 sec/batch)
2016-02-03 20:47:18.523073: step 82960, loss = 0.62 (302.6 examples/sec; 0.423 sec/batch)
2016-02-03 20:47:23.266543: step 82970, loss = 0.70 (247.9 examples/sec; 0.516 sec/batch)
2016-02-03 20:47:27.914441: step 82980, loss = 0.65 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 20:47:32.607993: step 82990, loss = 0.53 (272.7 examples/sec; 0.469 sec/batch)
2016-02-03 20:47:37.334559: step 83000, loss = 0.65 (256.4 examples/sec; 0.499 sec/batch)
2016-02-03 20:47:42.520642: step 83010, loss = 0.63 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 20:47:47.200414: step 83020, loss = 0.83 (287.9 examples/sec; 0.445 sec/batch)
2016-02-03 20:47:51.916413: step 83030, loss = 0.68 (263.2 examples/sec; 0.486 sec/batch)
2016-02-03 20:47:56.610900: step 83040, loss = 0.76 (286.7 examples/sec; 0.447 sec/batch)
2016-02-03 20:48:01.329863: step 83050, loss = 0.72 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 20:48:06.051645: step 83060, loss = 0.68 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 20:48:10.635593: step 83070, loss = 0.69 (286.0 examples/sec; 0.448 sec/batch)
2016-02-03 20:48:15.292989: step 83080, loss = 0.74 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 20:48:19.983860: step 83090, loss = 0.63 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 20:48:24.619217: step 83100, loss = 0.62 (287.5 examples/sec; 0.445 sec/batch)
2016-02-03 20:48:29.816471: step 83110, loss = 0.73 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 20:48:34.382022: step 83120, loss = 0.70 (289.3 examples/sec; 0.443 sec/batch)
2016-02-03 20:48:38.891515: step 83130, loss = 0.69 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 20:48:43.439624: step 83140, loss = 0.79 (286.6 examples/sec; 0.447 sec/batch)
2016-02-03 20:48:48.061834: step 83150, loss = 0.78 (287.9 examples/sec; 0.445 sec/batch)
2016-02-03 20:48:52.697414: step 83160, loss = 0.81 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 20:48:57.323473: step 83170, loss = 0.61 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 20:49:01.929904: step 83180, loss = 0.79 (248.8 examples/sec; 0.514 sec/batch)
2016-02-03 20:49:06.604625: step 83190, loss = 0.72 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 20:49:11.365770: step 83200, loss = 0.57 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 20:49:16.555787: step 83210, loss = 0.70 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 20:49:21.238960: step 83220, loss = 0.82 (289.2 examples/sec; 0.443 sec/batch)
2016-02-03 20:49:25.999610: step 83230, loss = 0.71 (238.4 examples/sec; 0.537 sec/batch)
2016-02-03 20:49:30.618821: step 83240, loss = 0.63 (283.3 examples/sec; 0.452 sec/batch)
2016-02-03 20:49:35.378812: step 83250, loss = 0.73 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 20:49:40.105505: step 83260, loss = 0.72 (291.3 examples/sec; 0.439 sec/batch)
2016-02-03 20:49:44.797671: step 83270, loss = 0.67 (275.6 examples/sec; 0.464 sec/batch)
2016-02-03 20:49:49.634810: step 83280, loss = 0.70 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 20:49:54.286867: step 83290, loss = 0.60 (303.2 examples/sec; 0.422 sec/batch)
2016-02-03 20:49:58.937932: step 83300, loss = 0.79 (288.4 examples/sec; 0.444 sec/batch)
2016-02-03 20:50:04.051671: step 83310, loss = 0.60 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 20:50:08.743258: step 83320, loss = 0.73 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 20:50:13.423844: step 83330, loss = 0.67 (256.0 examples/sec; 0.500 sec/batch)
2016-02-03 20:50:18.089950: step 83340, loss = 0.66 (282.2 examples/sec; 0.454 sec/batch)
2016-02-03 20:50:22.821456: step 83350, loss = 0.68 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 20:50:27.465820: step 83360, loss = 0.74 (263.6 examples/sec; 0.486 sec/batch)
2016-02-03 20:50:32.171246: step 83370, loss = 0.74 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 20:50:36.782406: step 83380, loss = 0.61 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 20:50:41.468395: step 83390, loss = 0.86 (278.5 examples/sec; 0.460 sec/batch)
2016-02-03 20:50:46.249219: step 83400, loss = 0.68 (262.0 examples/sec; 0.489 sec/batch)
2016-02-03 20:50:51.427224: step 83410, loss = 0.70 (287.5 examples/sec; 0.445 sec/batch)
2016-02-03 20:50:56.177515: step 83420, loss = 0.86 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 20:51:00.915433: step 83430, loss = 0.61 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 20:51:05.718986: step 83440, loss = 0.67 (259.1 examples/sec; 0.494 sec/batch)
2016-02-03 20:51:10.381547: step 83450, loss = 0.68 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 20:51:15.104733: step 83460, loss = 0.94 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 20:51:19.782384: step 83470, loss = 0.67 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 20:51:24.451789: step 83480, loss = 0.83 (283.9 examples/sec; 0.451 sec/batch)
2016-02-03 20:51:29.179237: step 83490, loss = 0.69 (269.3 examples/sec; 0.475 sec/batch)
2016-02-03 20:51:33.958123: step 83500, loss = 0.67 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 20:51:39.259136: step 83510, loss = 0.78 (258.6 examples/sec; 0.495 sec/batch)
2016-02-03 20:51:44.006705: step 83520, loss = 0.65 (280.9 examples/sec; 0.456 sec/batch)
2016-02-03 20:51:48.766991: step 83530, loss = 0.76 (241.2 examples/sec; 0.531 sec/batch)
2016-02-03 20:51:53.395575: step 83540, loss = 0.72 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 20:51:58.118763: step 83550, loss = 0.74 (280.4 examples/sec; 0.456 sec/batch)
2016-02-03 20:52:02.850475: step 83560, loss = 0.64 (258.3 examples/sec; 0.496 sec/batch)
2016-02-03 20:52:07.510717: step 83570, loss = 0.81 (259.2 examples/sec; 0.494 sec/batch)
2016-02-03 20:52:12.179828: step 83580, loss = 0.71 (287.3 examples/sec; 0.446 sec/batch)
2016-02-03 20:52:16.933520: step 83590, loss = 0.82 (248.7 examples/sec; 0.515 sec/batch)
2016-02-03 20:52:21.679703: step 83600, loss = 0.68 (254.0 examples/sec; 0.504 sec/batch)
2016-02-03 20:52:26.956567: step 83610, loss = 0.66 (291.8 examples/sec; 0.439 sec/batch)
2016-02-03 20:52:31.658019: step 83620, loss = 0.54 (283.1 examples/sec; 0.452 sec/batch)
2016-02-03 20:52:36.362277: step 83630, loss = 0.74 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 20:52:41.126743: step 83640, loss = 0.62 (262.9 examples/sec; 0.487 sec/batch)
2016-02-03 20:52:45.854281: step 83650, loss = 0.73 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 20:52:50.498466: step 83660, loss = 0.55 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 20:52:55.267301: step 83670, loss = 0.86 (256.1 examples/sec; 0.500 sec/batch)
2016-02-03 20:52:59.980247: step 83680, loss = 0.78 (267.2 examples/sec; 0.479 sec/batch)
2016-02-03 20:53:04.702708: step 83690, loss = 0.77 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 20:53:09.490739: step 83700, loss = 0.83 (246.6 examples/sec; 0.519 sec/batch)
2016-02-03 20:53:14.683047: step 83710, loss = 0.64 (309.5 examples/sec; 0.414 sec/batch)
2016-02-03 20:53:19.433969: step 83720, loss = 0.69 (267.5 examples/sec; 0.478 sec/batch)
2016-02-03 20:53:24.183446: step 83730, loss = 0.51 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 20:53:28.878859: step 83740, loss = 0.85 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 20:53:33.612662: step 83750, loss = 0.62 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 20:53:38.321291: step 83760, loss = 0.69 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 20:53:43.045799: step 83770, loss = 0.73 (291.0 examples/sec; 0.440 sec/batch)
2016-02-03 20:53:47.742825: step 83780, loss = 0.57 (258.0 examples/sec; 0.496 sec/batch)
2016-02-03 20:53:52.349467: step 83790, loss = 0.69 (266.4 examples/sec; 0.480 sec/batch)
2016-02-03 20:53:57.086796: step 83800, loss = 0.65 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 20:54:02.312904: step 83810, loss = 0.71 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 20:54:06.956257: step 83820, loss = 0.67 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 20:54:11.630241: step 83830, loss = 0.66 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 20:54:16.313127: step 83840, loss = 0.63 (287.3 examples/sec; 0.446 sec/batch)
2016-02-03 20:54:21.071762: step 83850, loss = 0.86 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 20:54:25.823032: step 83860, loss = 0.84 (284.9 examples/sec; 0.449 sec/batch)
2016-02-03 20:54:30.488319: step 83870, loss = 0.58 (293.7 examples/sec; 0.436 sec/batch)
2016-02-03 20:54:35.182063: step 83880, loss = 0.82 (283.0 examples/sec; 0.452 sec/batch)
2016-02-03 20:54:39.901545: step 83890, loss = 0.69 (283.9 examples/sec; 0.451 sec/batch)
2016-02-03 20:54:44.588243: step 83900, loss = 0.66 (301.7 examples/sec; 0.424 sec/batch)
2016-02-03 20:54:49.807674: step 83910, loss = 0.87 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 20:54:54.428124: step 83920, loss = 0.71 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 20:54:59.092889: step 83930, loss = 0.55 (285.1 examples/sec; 0.449 sec/batch)
2016-02-03 20:55:03.817663: step 83940, loss = 0.75 (290.9 examples/sec; 0.440 sec/batch)
2016-02-03 20:55:08.487582: step 83950, loss = 0.67 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 20:55:13.200135: step 83960, loss = 0.65 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 20:55:17.933844: step 83970, loss = 0.88 (279.6 examples/sec; 0.458 sec/batch)
2016-02-03 20:55:22.597243: step 83980, loss = 0.84 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 20:55:27.387633: step 83990, loss = 0.76 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 20:55:32.084966: step 84000, loss = 0.58 (306.0 examples/sec; 0.418 sec/batch)
2016-02-03 20:55:37.369552: step 84010, loss = 0.77 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 20:55:41.971464: step 84020, loss = 0.69 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 20:55:46.653410: step 84030, loss = 0.62 (275.7 examples/sec; 0.464 sec/batch)
2016-02-03 20:55:51.394625: step 84040, loss = 0.80 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 20:55:56.005084: step 84050, loss = 0.85 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 20:56:00.727424: step 84060, loss = 0.78 (312.3 examples/sec; 0.410 sec/batch)
2016-02-03 20:56:05.446627: step 84070, loss = 0.75 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 20:56:10.087532: step 84080, loss = 0.60 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 20:56:14.843432: step 84090, loss = 0.69 (256.9 examples/sec; 0.498 sec/batch)
2016-02-03 20:56:19.515720: step 84100, loss = 0.62 (268.0 examples/sec; 0.478 sec/batch)
2016-02-03 20:56:24.726912: step 84110, loss = 0.73 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 20:56:29.445777: step 84120, loss = 0.77 (284.8 examples/sec; 0.449 sec/batch)
2016-02-03 20:56:34.113500: step 84130, loss = 0.86 (263.1 examples/sec; 0.487 sec/batch)
2016-02-03 20:56:38.740332: step 84140, loss = 0.74 (294.0 examples/sec; 0.435 sec/batch)
2016-02-03 20:56:43.466892: step 84150, loss = 0.76 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 20:56:48.073709: step 84160, loss = 0.67 (288.9 examples/sec; 0.443 sec/batch)
2016-02-03 20:56:52.790342: step 84170, loss = 0.69 (257.5 examples/sec; 0.497 sec/batch)
2016-02-03 20:56:57.441008: step 84180, loss = 0.67 (259.8 examples/sec; 0.493 sec/batch)
2016-02-03 20:57:02.142330: step 84190, loss = 0.82 (262.0 examples/sec; 0.489 sec/batch)
2016-02-03 20:57:06.968983: step 84200, loss = 0.62 (256.1 examples/sec; 0.500 sec/batch)
2016-02-03 20:57:12.206085: step 84210, loss = 0.83 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 20:57:16.756129: step 84220, loss = 0.72 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 20:57:21.456471: step 84230, loss = 0.68 (266.3 examples/sec; 0.481 sec/batch)
2016-02-03 20:57:26.147466: step 84240, loss = 0.72 (290.1 examples/sec; 0.441 sec/batch)
2016-02-03 20:57:30.799066: step 84250, loss = 0.71 (280.3 examples/sec; 0.457 sec/batch)
2016-02-03 20:57:35.574885: step 84260, loss = 0.63 (262.9 examples/sec; 0.487 sec/batch)
2016-02-03 20:57:40.246499: step 84270, loss = 0.59 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 20:57:44.945569: step 84280, loss = 0.76 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 20:57:49.695410: step 84290, loss = 0.83 (267.3 examples/sec; 0.479 sec/batch)
2016-02-03 20:57:54.380897: step 84300, loss = 0.71 (298.9 examples/sec; 0.428 sec/batch)
2016-02-03 20:57:59.647013: step 84310, loss = 0.63 (257.3 examples/sec; 0.497 sec/batch)
2016-02-03 20:58:04.256638: step 84320, loss = 0.71 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 20:58:08.918570: step 84330, loss = 0.71 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 20:58:13.663990: step 84340, loss = 0.74 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 20:58:18.405182: step 84350, loss = 0.72 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 20:58:23.059828: step 84360, loss = 0.69 (291.0 examples/sec; 0.440 sec/batch)
2016-02-03 20:58:27.793082: step 84370, loss = 0.74 (288.5 examples/sec; 0.444 sec/batch)
2016-02-03 20:58:32.522892: step 84380, loss = 0.66 (303.7 examples/sec; 0.421 sec/batch)
2016-02-03 20:58:37.263910: step 84390, loss = 0.77 (260.7 examples/sec; 0.491 sec/batch)
2016-02-03 20:58:41.868282: step 84400, loss = 0.71 (264.7 examples/sec; 0.484 sec/batch)
2016-02-03 20:58:47.112032: step 84410, loss = 0.67 (289.7 examples/sec; 0.442 sec/batch)
2016-02-03 20:58:51.848758: step 84420, loss = 0.78 (247.7 examples/sec; 0.517 sec/batch)
2016-02-03 20:58:56.546808: step 84430, loss = 0.66 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 20:59:01.239232: step 84440, loss = 0.78 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 20:59:05.916469: step 84450, loss = 0.84 (276.3 examples/sec; 0.463 sec/batch)
2016-02-03 20:59:10.583469: step 84460, loss = 0.81 (280.6 examples/sec; 0.456 sec/batch)
2016-02-03 20:59:15.347830: step 84470, loss = 0.69 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 20:59:19.960496: step 84480, loss = 0.71 (288.3 examples/sec; 0.444 sec/batch)
2016-02-03 20:59:24.719916: step 84490, loss = 0.68 (276.2 examples/sec; 0.464 sec/batch)
2016-02-03 20:59:29.415167: step 84500, loss = 0.63 (291.3 examples/sec; 0.439 sec/batch)
2016-02-03 20:59:34.598934: step 84510, loss = 0.73 (286.5 examples/sec; 0.447 sec/batch)
2016-02-03 20:59:39.361620: step 84520, loss = 0.67 (248.5 examples/sec; 0.515 sec/batch)
2016-02-03 20:59:44.163781: step 84530, loss = 0.72 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 20:59:48.760204: step 84540, loss = 0.68 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 20:59:53.526365: step 84550, loss = 0.81 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 20:59:58.217170: step 84560, loss = 0.68 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 21:00:02.974705: step 84570, loss = 0.68 (253.9 examples/sec; 0.504 sec/batch)
2016-02-03 21:00:07.647843: step 84580, loss = 0.72 (259.1 examples/sec; 0.494 sec/batch)
2016-02-03 21:00:12.382279: step 84590, loss = 0.76 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 21:00:17.024038: step 84600, loss = 0.60 (276.9 examples/sec; 0.462 sec/batch)
2016-02-03 21:00:22.281421: step 84610, loss = 0.78 (267.5 examples/sec; 0.478 sec/batch)
2016-02-03 21:00:26.968361: step 84620, loss = 0.55 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 21:00:31.594507: step 84630, loss = 0.72 (284.2 examples/sec; 0.450 sec/batch)
2016-02-03 21:00:36.356948: step 84640, loss = 0.71 (253.9 examples/sec; 0.504 sec/batch)
2016-02-03 21:00:41.079164: step 84650, loss = 0.69 (270.9 examples/sec; 0.472 sec/batch)
2016-02-03 21:00:45.806299: step 84660, loss = 0.66 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 21:00:50.545265: step 84670, loss = 1.03 (260.9 examples/sec; 0.491 sec/batch)
2016-02-03 21:00:55.269298: step 84680, loss = 0.69 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 21:00:59.937979: step 84690, loss = 0.73 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 21:01:04.716994: step 84700, loss = 0.81 (245.1 examples/sec; 0.522 sec/batch)
2016-02-03 21:01:09.922845: step 84710, loss = 0.87 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 21:01:14.633921: step 84720, loss = 0.68 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 21:01:19.292918: step 84730, loss = 0.67 (259.8 examples/sec; 0.493 sec/batch)
2016-02-03 21:01:23.994301: step 84740, loss = 0.71 (268.6 examples/sec; 0.477 sec/batch)
2016-02-03 21:01:28.596445: step 84750, loss = 0.62 (298.0 examples/sec; 0.430 sec/batch)
2016-02-03 21:01:33.195066: step 84760, loss = 0.69 (283.8 examples/sec; 0.451 sec/batch)
2016-02-03 21:01:38.017569: step 84770, loss = 0.87 (248.9 examples/sec; 0.514 sec/batch)
2016-02-03 21:01:42.726339: step 84780, loss = 0.64 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 21:01:47.387283: step 84790, loss = 0.55 (269.3 examples/sec; 0.475 sec/batch)
2016-02-03 21:01:51.955007: step 84800, loss = 0.73 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 21:01:57.192659: step 84810, loss = 0.66 (283.8 examples/sec; 0.451 sec/batch)
2016-02-03 21:02:01.924882: step 84820, loss = 0.61 (284.6 examples/sec; 0.450 sec/batch)
2016-02-03 21:02:06.534962: step 84830, loss = 0.66 (329.6 examples/sec; 0.388 sec/batch)
2016-02-03 21:02:11.234856: step 84840, loss = 0.65 (259.2 examples/sec; 0.494 sec/batch)
2016-02-03 21:02:15.961453: step 84850, loss = 0.75 (291.6 examples/sec; 0.439 sec/batch)
2016-02-03 21:02:20.705788: step 84860, loss = 0.77 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 21:02:25.425659: step 84870, loss = 0.53 (278.5 examples/sec; 0.460 sec/batch)
2016-02-03 21:02:30.146176: step 84880, loss = 0.76 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 21:02:34.793674: step 84890, loss = 0.64 (295.3 examples/sec; 0.433 sec/batch)
2016-02-03 21:02:39.519137: step 84900, loss = 0.58 (276.4 examples/sec; 0.463 sec/batch)
2016-02-03 21:02:44.741011: step 84910, loss = 0.79 (256.1 examples/sec; 0.500 sec/batch)
2016-02-03 21:02:49.379999: step 84920, loss = 0.70 (309.6 examples/sec; 0.413 sec/batch)
2016-02-03 21:02:54.193536: step 84930, loss = 0.67 (283.3 examples/sec; 0.452 sec/batch)
2016-02-03 21:02:58.809668: step 84940, loss = 0.79 (289.0 examples/sec; 0.443 sec/batch)
2016-02-03 21:03:03.472898: step 84950, loss = 0.67 (253.3 examples/sec; 0.505 sec/batch)
2016-02-03 21:03:08.080012: step 84960, loss = 0.75 (270.9 examples/sec; 0.472 sec/batch)
2016-02-03 21:03:12.643458: step 84970, loss = 0.64 (267.3 examples/sec; 0.479 sec/batch)
2016-02-03 21:03:17.336786: step 84980, loss = 0.69 (265.3 examples/sec; 0.482 sec/batch)
2016-02-03 21:03:22.025398: step 84990, loss = 0.64 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 21:03:26.650554: step 85000, loss = 0.68 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 21:03:31.848903: step 85010, loss = 0.72 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 21:03:36.559521: step 85020, loss = 0.74 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 21:03:41.173726: step 85030, loss = 0.83 (285.9 examples/sec; 0.448 sec/batch)
2016-02-03 21:03:45.868520: step 85040, loss = 0.65 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 21:03:50.499373: step 85050, loss = 0.76 (285.0 examples/sec; 0.449 sec/batch)
2016-02-03 21:03:55.201670: step 85060, loss = 0.74 (284.0 examples/sec; 0.451 sec/batch)
2016-02-03 21:03:59.826665: step 85070, loss = 0.82 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 21:04:04.560395: step 85080, loss = 0.82 (250.6 examples/sec; 0.511 sec/batch)
2016-02-03 21:04:09.318747: step 85090, loss = 0.73 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 21:04:13.979255: step 85100, loss = 0.65 (249.5 examples/sec; 0.513 sec/batch)
2016-02-03 21:04:19.237925: step 85110, loss = 0.72 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 21:04:23.995125: step 85120, loss = 0.70 (308.9 examples/sec; 0.414 sec/batch)
2016-02-03 21:04:28.732127: step 85130, loss = 0.77 (257.5 examples/sec; 0.497 sec/batch)
2016-02-03 21:04:33.452207: step 85140, loss = 0.99 (276.4 examples/sec; 0.463 sec/batch)
2016-02-03 21:04:38.156430: step 85150, loss = 0.74 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 21:04:42.801366: step 85160, loss = 0.73 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 21:04:47.525033: step 85170, loss = 0.58 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 21:04:52.322559: step 85180, loss = 0.85 (269.3 examples/sec; 0.475 sec/batch)
2016-02-03 21:04:57.068237: step 85190, loss = 0.66 (241.7 examples/sec; 0.529 sec/batch)
2016-02-03 21:05:01.691849: step 85200, loss = 0.70 (299.4 examples/sec; 0.428 sec/batch)
2016-02-03 21:05:06.883567: step 85210, loss = 0.84 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 21:05:11.554999: step 85220, loss = 0.82 (260.8 examples/sec; 0.491 sec/batch)
2016-02-03 21:05:16.316271: step 85230, loss = 0.69 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 21:05:21.000178: step 85240, loss = 0.87 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 21:05:25.729447: step 85250, loss = 0.70 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 21:05:30.400359: step 85260, loss = 0.65 (305.1 examples/sec; 0.420 sec/batch)
2016-02-03 21:05:35.104456: step 85270, loss = 0.63 (258.1 examples/sec; 0.496 sec/batch)
2016-02-03 21:05:39.805093: step 85280, loss = 0.65 (283.1 examples/sec; 0.452 sec/batch)
2016-02-03 21:05:44.497553: step 85290, loss = 0.75 (277.7 examples/sec; 0.461 sec/batch)
2016-02-03 21:05:49.121312: step 85300, loss = 0.75 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 21:05:54.396113: step 85310, loss = 0.85 (281.2 examples/sec; 0.455 sec/batch)
2016-02-03 21:05:59.102472: step 85320, loss = 0.64 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 21:06:03.825901: step 85330, loss = 0.69 (242.9 examples/sec; 0.527 sec/batch)
2016-02-03 21:06:08.464999: step 85340, loss = 0.72 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 21:06:13.104105: step 85350, loss = 0.73 (308.2 examples/sec; 0.415 sec/batch)
2016-02-03 21:06:17.788727: step 85360, loss = 0.69 (288.0 examples/sec; 0.444 sec/batch)
2016-02-03 21:06:22.506379: step 85370, loss = 0.73 (281.0 examples/sec; 0.456 sec/batch)
2016-02-03 21:06:27.279218: step 85380, loss = 0.75 (259.2 examples/sec; 0.494 sec/batch)
2016-02-03 21:06:31.977666: step 85390, loss = 0.61 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 21:06:36.704993: step 85400, loss = 0.82 (303.6 examples/sec; 0.422 sec/batch)
2016-02-03 21:06:41.950023: step 85410, loss = 0.70 (254.6 examples/sec; 0.503 sec/batch)
2016-02-03 21:06:46.654138: step 85420, loss = 0.67 (284.7 examples/sec; 0.450 sec/batch)
2016-02-03 21:06:51.243290: step 85430, loss = 0.76 (277.9 examples/sec; 0.461 sec/batch)
2016-02-03 21:06:55.908494: step 85440, loss = 0.63 (263.2 examples/sec; 0.486 sec/batch)
2016-02-03 21:07:00.639204: step 85450, loss = 0.76 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 21:07:05.381876: step 85460, loss = 0.82 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 21:07:10.054327: step 85470, loss = 0.66 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 21:07:14.828101: step 85480, loss = 0.72 (259.9 examples/sec; 0.493 sec/batch)
2016-02-03 21:07:19.454241: step 85490, loss = 0.64 (286.5 examples/sec; 0.447 sec/batch)
2016-02-03 21:07:24.105510: step 85500, loss = 0.60 (264.1 examples/sec; 0.485 sec/batch)
2016-02-03 21:07:29.322277: step 85510, loss = 0.62 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 21:07:34.020363: step 85520, loss = 0.69 (277.2 examples/sec; 0.462 sec/batch)
2016-02-03 21:07:38.703223: step 85530, loss = 0.68 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 21:07:43.407566: step 85540, loss = 0.97 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 21:07:48.106290: step 85550, loss = 0.79 (255.9 examples/sec; 0.500 sec/batch)
2016-02-03 21:07:52.757443: step 85560, loss = 0.79 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 21:07:57.402994: step 85570, loss = 0.54 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 21:08:02.113940: step 85580, loss = 0.70 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 21:08:06.854658: step 85590, loss = 0.78 (282.4 examples/sec; 0.453 sec/batch)
2016-02-03 21:08:11.615220: step 85600, loss = 0.62 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 21:08:16.877800: step 85610, loss = 0.68 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 21:08:21.662383: step 85620, loss = 0.78 (245.8 examples/sec; 0.521 sec/batch)
2016-02-03 21:08:26.322824: step 85630, loss = 0.63 (296.5 examples/sec; 0.432 sec/batch)
2016-02-03 21:08:31.055985: step 85640, loss = 0.69 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 21:08:35.782380: step 85650, loss = 0.82 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 21:08:40.461997: step 85660, loss = 0.74 (252.4 examples/sec; 0.507 sec/batch)
2016-02-03 21:08:45.146263: step 85670, loss = 0.72 (251.1 examples/sec; 0.510 sec/batch)
2016-02-03 21:08:49.784971: step 85680, loss = 0.68 (244.3 examples/sec; 0.524 sec/batch)
2016-02-03 21:08:54.508066: step 85690, loss = 0.60 (265.8 examples/sec; 0.482 sec/batch)
2016-02-03 21:08:59.222167: step 85700, loss = 0.67 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 21:09:04.442598: step 85710, loss = 0.66 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 21:09:09.190975: step 85720, loss = 0.72 (286.2 examples/sec; 0.447 sec/batch)
2016-02-03 21:09:13.901376: step 85730, loss = 0.71 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 21:09:18.597996: step 85740, loss = 0.58 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 21:09:23.301595: step 85750, loss = 0.70 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 21:09:28.084103: step 85760, loss = 0.95 (285.1 examples/sec; 0.449 sec/batch)
2016-02-03 21:09:32.780553: step 85770, loss = 0.64 (264.9 examples/sec; 0.483 sec/batch)
2016-02-03 21:09:37.443276: step 85780, loss = 0.73 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 21:09:42.184081: step 85790, loss = 0.69 (276.3 examples/sec; 0.463 sec/batch)
2016-02-03 21:09:46.908187: step 85800, loss = 0.62 (252.3 examples/sec; 0.507 sec/batch)
2016-02-03 21:09:52.073495: step 85810, loss = 0.79 (257.5 examples/sec; 0.497 sec/batch)
2016-02-03 21:09:56.783127: step 85820, loss = 0.70 (294.4 examples/sec; 0.435 sec/batch)
2016-02-03 21:10:01.465041: step 85830, loss = 0.61 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 21:10:06.160625: step 85840, loss = 0.72 (280.5 examples/sec; 0.456 sec/batch)
2016-02-03 21:10:10.895882: step 85850, loss = 0.82 (266.3 examples/sec; 0.481 sec/batch)
2016-02-03 21:10:15.592556: step 85860, loss = 0.84 (258.8 examples/sec; 0.495 sec/batch)
2016-02-03 21:10:20.283526: step 85870, loss = 0.65 (292.3 examples/sec; 0.438 sec/batch)
2016-02-03 21:10:25.028379: step 85880, loss = 0.60 (293.2 examples/sec; 0.437 sec/batch)
2016-02-03 21:10:29.770884: step 85890, loss = 0.67 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 21:10:34.474010: step 85900, loss = 0.77 (282.2 examples/sec; 0.454 sec/batch)
2016-02-03 21:10:39.663868: step 85910, loss = 0.72 (285.2 examples/sec; 0.449 sec/batch)
2016-02-03 21:10:44.370254: step 85920, loss = 0.68 (264.2 examples/sec; 0.484 sec/batch)
2016-02-03 21:10:49.091412: step 85930, loss = 0.76 (283.9 examples/sec; 0.451 sec/batch)
2016-02-03 21:10:53.747798: step 85940, loss = 0.69 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 21:10:58.340989: step 85950, loss = 0.68 (295.5 examples/sec; 0.433 sec/batch)
2016-02-03 21:11:02.995057: step 85960, loss = 0.64 (283.8 examples/sec; 0.451 sec/batch)
2016-02-03 21:11:07.671541: step 85970, loss = 0.89 (257.3 examples/sec; 0.497 sec/batch)
2016-02-03 21:11:12.383340: step 85980, loss = 0.72 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 21:11:17.020565: step 85990, loss = 0.71 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 21:11:21.796206: step 86000, loss = 0.64 (263.6 examples/sec; 0.486 sec/batch)
2016-02-03 21:11:26.861198: step 86010, loss = 0.70 (299.5 examples/sec; 0.427 sec/batch)
2016-02-03 21:11:31.550948: step 86020, loss = 0.65 (309.1 examples/sec; 0.414 sec/batch)
2016-02-03 21:11:36.265834: step 86030, loss = 0.68 (294.1 examples/sec; 0.435 sec/batch)
2016-02-03 21:11:40.976972: step 86040, loss = 0.90 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 21:11:45.672453: step 86050, loss = 0.79 (287.8 examples/sec; 0.445 sec/batch)
2016-02-03 21:11:50.386790: step 86060, loss = 0.94 (259.0 examples/sec; 0.494 sec/batch)
2016-02-03 21:11:55.014295: step 86070, loss = 0.91 (255.9 examples/sec; 0.500 sec/batch)
2016-02-03 21:11:59.739315: step 86080, loss = 0.77 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 21:12:04.515417: step 86090, loss = 0.70 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 21:12:09.181925: step 86100, loss = 0.60 (294.2 examples/sec; 0.435 sec/batch)
2016-02-03 21:12:14.372323: step 86110, loss = 0.70 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 21:12:19.039617: step 86120, loss = 0.64 (255.1 examples/sec; 0.502 sec/batch)
2016-02-03 21:12:23.764953: step 86130, loss = 0.61 (260.7 examples/sec; 0.491 sec/batch)
2016-02-03 21:12:28.431351: step 86140, loss = 0.76 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 21:12:33.192278: step 86150, loss = 0.79 (245.5 examples/sec; 0.521 sec/batch)
2016-02-03 21:12:37.793500: step 86160, loss = 0.89 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 21:12:42.480987: step 86170, loss = 0.76 (277.8 examples/sec; 0.461 sec/batch)
2016-02-03 21:12:47.277234: step 86180, loss = 0.80 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 21:12:51.991633: step 86190, loss = 0.65 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 21:12:56.756415: step 86200, loss = 0.73 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 21:13:01.896089: step 86210, loss = 0.78 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 21:13:06.574484: step 86220, loss = 0.77 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 21:13:11.240999: step 86230, loss = 0.67 (259.1 examples/sec; 0.494 sec/batch)
2016-02-03 21:13:15.930052: step 86240, loss = 0.85 (250.0 examples/sec; 0.512 sec/batch)
2016-02-03 21:13:20.684549: step 86250, loss = 0.82 (282.2 examples/sec; 0.454 sec/batch)
2016-02-03 21:13:25.426453: step 86260, loss = 0.79 (258.9 examples/sec; 0.494 sec/batch)
2016-02-03 21:13:30.143971: step 86270, loss = 0.67 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 21:13:34.914237: step 86280, loss = 0.67 (265.0 examples/sec; 0.483 sec/batch)
2016-02-03 21:13:39.563590: step 86290, loss = 0.76 (285.4 examples/sec; 0.449 sec/batch)
2016-02-03 21:13:44.300981: step 86300, loss = 0.68 (258.2 examples/sec; 0.496 sec/batch)
2016-02-03 21:13:49.486634: step 86310, loss = 0.69 (255.9 examples/sec; 0.500 sec/batch)
2016-02-03 21:13:54.094666: step 86320, loss = 0.71 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 21:13:58.807238: step 86330, loss = 0.58 (276.6 examples/sec; 0.463 sec/batch)
2016-02-03 21:14:03.524258: step 86340, loss = 0.70 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 21:14:08.220330: step 86350, loss = 0.53 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 21:14:12.952891: step 86360, loss = 0.65 (292.5 examples/sec; 0.438 sec/batch)
2016-02-03 21:14:17.691073: step 86370, loss = 0.69 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 21:14:22.343566: step 86380, loss = 0.78 (277.8 examples/sec; 0.461 sec/batch)
2016-02-03 21:14:27.089167: step 86390, loss = 0.87 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 21:14:31.701791: step 86400, loss = 0.68 (288.8 examples/sec; 0.443 sec/batch)
2016-02-03 21:14:37.079956: step 86410, loss = 0.62 (254.7 examples/sec; 0.503 sec/batch)
2016-02-03 21:14:41.805123: step 86420, loss = 0.59 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 21:14:46.507755: step 86430, loss = 0.89 (296.6 examples/sec; 0.432 sec/batch)
2016-02-03 21:14:51.273844: step 86440, loss = 0.77 (292.4 examples/sec; 0.438 sec/batch)
2016-02-03 21:14:56.038032: step 86450, loss = 0.61 (248.4 examples/sec; 0.515 sec/batch)
2016-02-03 21:15:00.769693: step 86460, loss = 0.74 (286.5 examples/sec; 0.447 sec/batch)
2016-02-03 21:15:05.539298: step 86470, loss = 0.77 (252.4 examples/sec; 0.507 sec/batch)
2016-02-03 21:15:10.210593: step 86480, loss = 0.76 (293.3 examples/sec; 0.436 sec/batch)
2016-02-03 21:15:14.894957: step 86490, loss = 0.54 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 21:15:19.637737: step 86500, loss = 0.71 (282.9 examples/sec; 0.452 sec/batch)
2016-02-03 21:15:24.936455: step 86510, loss = 0.69 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 21:15:29.618882: step 86520, loss = 0.59 (288.6 examples/sec; 0.444 sec/batch)
2016-02-03 21:15:34.315050: step 86530, loss = 0.81 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 21:15:38.958087: step 86540, loss = 0.68 (282.5 examples/sec; 0.453 sec/batch)
2016-02-03 21:15:43.585014: step 86550, loss = 0.66 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 21:15:48.291971: step 86560, loss = 0.91 (249.4 examples/sec; 0.513 sec/batch)
2016-02-03 21:15:52.997281: step 86570, loss = 0.60 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 21:15:57.745001: step 86580, loss = 0.66 (255.2 examples/sec; 0.501 sec/batch)
2016-02-03 21:16:02.417208: step 86590, loss = 0.60 (284.6 examples/sec; 0.450 sec/batch)
2016-02-03 21:16:07.205417: step 86600, loss = 0.59 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 21:16:12.494939: step 86610, loss = 0.82 (242.6 examples/sec; 0.528 sec/batch)
2016-02-03 21:16:17.209961: step 86620, loss = 0.68 (270.9 examples/sec; 0.472 sec/batch)
2016-02-03 21:16:21.929634: step 86630, loss = 0.69 (264.3 examples/sec; 0.484 sec/batch)
2016-02-03 21:16:26.578420: step 86640, loss = 0.72 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 21:16:31.316937: step 86650, loss = 0.78 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 21:16:36.019909: step 86660, loss = 0.63 (263.9 examples/sec; 0.485 sec/batch)
2016-02-03 21:16:40.644620: step 86670, loss = 0.83 (285.2 examples/sec; 0.449 sec/batch)
2016-02-03 21:16:45.348541: step 86680, loss = 0.74 (295.0 examples/sec; 0.434 sec/batch)
2016-02-03 21:16:50.046195: step 86690, loss = 0.62 (295.4 examples/sec; 0.433 sec/batch)
2016-02-03 21:16:54.778750: step 86700, loss = 0.78 (261.0 examples/sec; 0.490 sec/batch)
2016-02-03 21:17:00.042018: step 86710, loss = 0.62 (243.6 examples/sec; 0.525 sec/batch)
2016-02-03 21:17:04.685609: step 86720, loss = 0.68 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 21:17:09.460615: step 86730, loss = 0.78 (284.4 examples/sec; 0.450 sec/batch)
2016-02-03 21:17:14.155728: step 86740, loss = 0.67 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 21:17:18.824438: step 86750, loss = 0.63 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 21:17:23.558499: step 86760, loss = 0.70 (303.2 examples/sec; 0.422 sec/batch)
2016-02-03 21:17:28.310638: step 86770, loss = 0.66 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 21:17:32.944765: step 86780, loss = 0.82 (292.4 examples/sec; 0.438 sec/batch)
2016-02-03 21:17:37.626023: step 86790, loss = 0.67 (290.0 examples/sec; 0.441 sec/batch)
2016-02-03 21:17:42.312239: step 86800, loss = 0.69 (285.6 examples/sec; 0.448 sec/batch)
2016-02-03 21:17:47.549591: step 86810, loss = 0.88 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 21:17:52.309606: step 86820, loss = 0.85 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 21:17:56.975970: step 86830, loss = 0.64 (291.3 examples/sec; 0.439 sec/batch)
2016-02-03 21:18:01.746157: step 86840, loss = 0.85 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 21:18:06.384949: step 86850, loss = 0.71 (274.4 examples/sec; 0.466 sec/batch)
2016-02-03 21:18:11.051884: step 86860, loss = 0.68 (260.0 examples/sec; 0.492 sec/batch)
2016-02-03 21:18:15.729097: step 86870, loss = 0.78 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 21:18:20.404450: step 86880, loss = 0.71 (282.6 examples/sec; 0.453 sec/batch)
2016-02-03 21:18:25.026065: step 86890, loss = 0.77 (298.0 examples/sec; 0.430 sec/batch)
2016-02-03 21:18:29.731508: step 86900, loss = 0.72 (287.0 examples/sec; 0.446 sec/batch)
2016-02-03 21:18:34.872254: step 86910, loss = 0.66 (275.0 examples/sec; 0.465 sec/batch)
2016-02-03 21:18:39.593244: step 86920, loss = 0.59 (288.2 examples/sec; 0.444 sec/batch)
2016-02-03 21:18:44.359655: step 86930, loss = 0.58 (255.7 examples/sec; 0.501 sec/batch)
2016-02-03 21:18:48.999522: step 86940, loss = 0.61 (284.2 examples/sec; 0.450 sec/batch)
2016-02-03 21:18:53.726242: step 86950, loss = 0.64 (292.5 examples/sec; 0.438 sec/batch)
2016-02-03 21:18:58.451884: step 86960, loss = 0.73 (288.1 examples/sec; 0.444 sec/batch)
2016-02-03 21:19:03.140405: step 86970, loss = 0.74 (288.2 examples/sec; 0.444 sec/batch)
2016-02-03 21:19:07.834893: step 86980, loss = 0.79 (287.2 examples/sec; 0.446 sec/batch)
2016-02-03 21:19:12.558294: step 86990, loss = 0.86 (299.0 examples/sec; 0.428 sec/batch)
2016-02-03 21:19:17.338125: step 87000, loss = 0.65 (276.7 examples/sec; 0.463 sec/batch)
2016-02-03 21:19:22.553181: step 87010, loss = 0.74 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 21:19:27.224037: step 87020, loss = 0.80 (279.2 examples/sec; 0.458 sec/batch)
2016-02-03 21:19:31.862054: step 87030, loss = 0.62 (296.9 examples/sec; 0.431 sec/batch)
2016-02-03 21:19:36.622633: step 87040, loss = 0.76 (289.2 examples/sec; 0.443 sec/batch)
2016-02-03 21:19:41.320202: step 87050, loss = 0.74 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 21:19:46.022728: step 87060, loss = 0.69 (263.6 examples/sec; 0.486 sec/batch)
2016-02-03 21:19:50.680370: step 87070, loss = 0.67 (280.5 examples/sec; 0.456 sec/batch)
2016-02-03 21:19:55.429990: step 87080, loss = 0.72 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 21:20:00.010609: step 87090, loss = 0.78 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 21:20:04.697539: step 87100, loss = 0.77 (286.2 examples/sec; 0.447 sec/batch)
2016-02-03 21:20:09.926373: step 87110, loss = 0.80 (280.9 examples/sec; 0.456 sec/batch)
2016-02-03 21:20:14.633077: step 87120, loss = 0.71 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 21:20:19.322402: step 87130, loss = 0.70 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 21:20:24.042547: step 87140, loss = 0.69 (288.4 examples/sec; 0.444 sec/batch)
2016-02-03 21:20:28.724233: step 87150, loss = 0.54 (297.0 examples/sec; 0.431 sec/batch)
2016-02-03 21:20:33.508679: step 87160, loss = 0.73 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 21:20:38.241675: step 87170, loss = 0.61 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 21:20:42.979935: step 87180, loss = 0.67 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 21:20:47.724555: step 87190, loss = 0.76 (260.8 examples/sec; 0.491 sec/batch)
2016-02-03 21:20:52.368463: step 87200, loss = 0.72 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 21:20:57.734185: step 87210, loss = 0.83 (280.4 examples/sec; 0.456 sec/batch)
2016-02-03 21:21:02.455374: step 87220, loss = 0.74 (301.7 examples/sec; 0.424 sec/batch)
2016-02-03 21:21:07.145237: step 87230, loss = 0.69 (279.7 examples/sec; 0.458 sec/batch)
2016-02-03 21:21:11.918534: step 87240, loss = 0.72 (259.7 examples/sec; 0.493 sec/batch)
2016-02-03 21:21:16.671824: step 87250, loss = 0.78 (254.7 examples/sec; 0.503 sec/batch)
2016-02-03 21:21:21.338640: step 87260, loss = 0.62 (281.7 examples/sec; 0.454 sec/batch)
2016-02-03 21:21:26.003170: step 87270, loss = 0.51 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 21:21:30.739490: step 87280, loss = 0.76 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 21:21:35.512830: step 87290, loss = 0.70 (265.2 examples/sec; 0.483 sec/batch)
2016-02-03 21:21:40.265929: step 87300, loss = 0.59 (296.6 examples/sec; 0.432 sec/batch)
2016-02-03 21:21:45.617162: step 87310, loss = 0.67 (258.0 examples/sec; 0.496 sec/batch)
2016-02-03 21:21:50.275777: step 87320, loss = 0.67 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 21:21:54.926698: step 87330, loss = 0.82 (291.7 examples/sec; 0.439 sec/batch)
2016-02-03 21:21:59.666113: step 87340, loss = 0.64 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 21:22:04.315354: step 87350, loss = 0.94 (267.0 examples/sec; 0.479 sec/batch)
2016-02-03 21:22:09.018769: step 87360, loss = 0.69 (253.3 examples/sec; 0.505 sec/batch)
2016-02-03 21:22:13.726540: step 87370, loss = 0.78 (287.3 examples/sec; 0.445 sec/batch)
2016-02-03 21:22:18.455595: step 87380, loss = 0.65 (286.5 examples/sec; 0.447 sec/batch)
2016-02-03 21:22:23.179640: step 87390, loss = 0.70 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 21:22:27.864931: step 87400, loss = 0.67 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 21:22:33.116853: step 87410, loss = 0.70 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 21:22:37.885338: step 87420, loss = 0.62 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 21:22:42.667584: step 87430, loss = 0.77 (282.2 examples/sec; 0.454 sec/batch)
2016-02-03 21:22:47.396690: step 87440, loss = 0.82 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 21:22:52.131648: step 87450, loss = 0.63 (278.0 examples/sec; 0.460 sec/batch)
2016-02-03 21:22:56.851707: step 87460, loss = 0.75 (248.5 examples/sec; 0.515 sec/batch)
2016-02-03 21:23:01.551497: step 87470, loss = 0.74 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 21:23:06.230579: step 87480, loss = 0.52 (291.2 examples/sec; 0.440 sec/batch)
2016-02-03 21:23:10.956258: step 87490, loss = 0.69 (280.5 examples/sec; 0.456 sec/batch)
2016-02-03 21:23:15.676187: step 87500, loss = 0.62 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 21:23:20.958896: step 87510, loss = 0.72 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 21:23:25.610977: step 87520, loss = 0.77 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 21:23:30.340156: step 87530, loss = 0.84 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 21:23:34.990032: step 87540, loss = 0.60 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 21:23:39.692414: step 87550, loss = 0.95 (288.7 examples/sec; 0.443 sec/batch)
2016-02-03 21:23:44.479101: step 87560, loss = 0.73 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 21:23:49.283679: step 87570, loss = 1.09 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 21:23:53.941777: step 87580, loss = 0.71 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 21:23:58.643185: step 87590, loss = 0.84 (292.2 examples/sec; 0.438 sec/batch)
2016-02-03 21:24:03.258149: step 87600, loss = 0.55 (253.0 examples/sec; 0.506 sec/batch)
2016-02-03 21:24:08.434463: step 87610, loss = 0.67 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 21:24:13.178985: step 87620, loss = 0.59 (251.1 examples/sec; 0.510 sec/batch)
2016-02-03 21:24:17.851075: step 87630, loss = 0.64 (262.0 examples/sec; 0.489 sec/batch)
2016-02-03 21:24:22.550083: step 87640, loss = 0.61 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 21:24:27.258483: step 87650, loss = 0.66 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 21:24:31.957960: step 87660, loss = 0.71 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 21:24:36.704448: step 87670, loss = 0.68 (256.9 examples/sec; 0.498 sec/batch)
2016-02-03 21:24:41.333781: step 87680, loss = 0.74 (297.8 examples/sec; 0.430 sec/batch)
2016-02-03 21:24:46.006044: step 87690, loss = 0.67 (298.4 examples/sec; 0.429 sec/batch)
2016-02-03 21:24:50.689734: step 87700, loss = 0.52 (299.7 examples/sec; 0.427 sec/batch)
2016-02-03 21:24:55.926124: step 87710, loss = 0.72 (268.2 examples/sec; 0.477 sec/batch)
2016-02-03 21:25:00.610382: step 87720, loss = 0.68 (293.1 examples/sec; 0.437 sec/batch)
2016-02-03 21:25:05.173894: step 87730, loss = 0.61 (270.5 examples/sec; 0.473 sec/batch)
2016-02-03 21:25:09.859014: step 87740, loss = 0.65 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 21:25:14.445331: step 87750, loss = 0.73 (289.1 examples/sec; 0.443 sec/batch)
2016-02-03 21:25:19.209007: step 87760, loss = 0.75 (265.8 examples/sec; 0.481 sec/batch)
2016-02-03 21:25:23.978072: step 87770, loss = 0.64 (260.6 examples/sec; 0.491 sec/batch)
2016-02-03 21:25:28.703245: step 87780, loss = 0.81 (273.2 examples/sec; 0.469 sec/batch)
2016-02-03 21:25:33.310074: step 87790, loss = 0.77 (290.1 examples/sec; 0.441 sec/batch)
2016-02-03 21:25:38.021607: step 87800, loss = 0.76 (285.1 examples/sec; 0.449 sec/batch)
2016-02-03 21:25:43.254761: step 87810, loss = 0.68 (280.4 examples/sec; 0.456 sec/batch)
2016-02-03 21:25:47.993658: step 87820, loss = 0.68 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 21:25:52.695572: step 87830, loss = 0.63 (264.9 examples/sec; 0.483 sec/batch)
2016-02-03 21:25:57.415856: step 87840, loss = 0.80 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 21:26:02.175841: step 87850, loss = 0.72 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 21:26:06.883054: step 87860, loss = 0.81 (282.9 examples/sec; 0.452 sec/batch)
2016-02-03 21:26:11.654996: step 87870, loss = 0.65 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 21:26:16.380691: step 87880, loss = 0.68 (261.0 examples/sec; 0.490 sec/batch)
2016-02-03 21:26:21.070115: step 87890, loss = 0.74 (279.8 examples/sec; 0.458 sec/batch)
2016-02-03 21:26:25.813540: step 87900, loss = 0.62 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 21:26:30.945563: step 87910, loss = 0.67 (283.4 examples/sec; 0.452 sec/batch)
2016-02-03 21:26:35.684200: step 87920, loss = 0.57 (251.9 examples/sec; 0.508 sec/batch)
2016-02-03 21:26:40.329194: step 87930, loss = 0.82 (286.6 examples/sec; 0.447 sec/batch)
2016-02-03 21:26:45.036921: step 87940, loss = 0.68 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 21:26:49.729944: step 87950, loss = 0.74 (255.7 examples/sec; 0.501 sec/batch)
2016-02-03 21:26:54.425156: step 87960, loss = 0.72 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 21:26:59.112105: step 87970, loss = 0.65 (281.1 examples/sec; 0.455 sec/batch)
2016-02-03 21:27:03.741103: step 87980, loss = 0.80 (257.6 examples/sec; 0.497 sec/batch)
2016-02-03 21:27:08.416380: step 87990, loss = 0.85 (260.1 examples/sec; 0.492 sec/batch)
2016-02-03 21:27:13.129797: step 88000, loss = 0.72 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 21:27:18.258214: step 88010, loss = 0.66 (325.1 examples/sec; 0.394 sec/batch)
2016-02-03 21:27:22.981018: step 88020, loss = 0.67 (298.4 examples/sec; 0.429 sec/batch)
2016-02-03 21:27:27.652473: step 88030, loss = 0.60 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 21:27:32.421302: step 88040, loss = 0.65 (253.6 examples/sec; 0.505 sec/batch)
2016-02-03 21:27:37.129598: step 88050, loss = 0.83 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 21:27:41.797383: step 88060, loss = 0.69 (303.9 examples/sec; 0.421 sec/batch)
2016-02-03 21:27:46.424367: step 88070, loss = 0.73 (290.5 examples/sec; 0.441 sec/batch)
2016-02-03 21:27:51.049580: step 88080, loss = 0.59 (289.2 examples/sec; 0.443 sec/batch)
2016-02-03 21:27:55.777228: step 88090, loss = 0.71 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 21:28:00.494710: step 88100, loss = 0.62 (283.8 examples/sec; 0.451 sec/batch)
2016-02-03 21:28:05.686888: step 88110, loss = 0.59 (284.6 examples/sec; 0.450 sec/batch)
2016-02-03 21:28:10.330603: step 88120, loss = 0.59 (285.1 examples/sec; 0.449 sec/batch)
2016-02-03 21:28:15.003189: step 88130, loss = 0.63 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 21:28:19.707668: step 88140, loss = 0.70 (264.3 examples/sec; 0.484 sec/batch)
2016-02-03 21:28:24.455560: step 88150, loss = 0.78 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 21:28:29.120877: step 88160, loss = 0.79 (275.6 examples/sec; 0.464 sec/batch)
2016-02-03 21:28:33.915860: step 88170, loss = 0.62 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 21:28:38.663951: step 88180, loss = 0.65 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 21:28:43.351185: step 88190, loss = 0.67 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 21:28:48.044228: step 88200, loss = 0.80 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 21:28:53.204534: step 88210, loss = 0.76 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 21:28:57.886541: step 88220, loss = 0.82 (285.2 examples/sec; 0.449 sec/batch)
2016-02-03 21:29:02.548550: step 88230, loss = 0.62 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 21:29:07.287541: step 88240, loss = 0.91 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 21:29:12.019550: step 88250, loss = 0.60 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 21:29:16.760712: step 88260, loss = 0.81 (293.8 examples/sec; 0.436 sec/batch)
2016-02-03 21:29:21.476105: step 88270, loss = 0.77 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 21:29:26.177381: step 88280, loss = 0.83 (287.3 examples/sec; 0.445 sec/batch)
2016-02-03 21:29:30.903558: step 88290, loss = 0.82 (280.4 examples/sec; 0.456 sec/batch)
2016-02-03 21:29:35.682129: step 88300, loss = 0.72 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 21:29:40.914219: step 88310, loss = 0.61 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 21:29:45.697892: step 88320, loss = 0.71 (288.7 examples/sec; 0.443 sec/batch)
2016-02-03 21:29:50.410944: step 88330, loss = 0.81 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 21:29:55.130137: step 88340, loss = 0.72 (254.3 examples/sec; 0.503 sec/batch)
2016-02-03 21:29:59.847341: step 88350, loss = 0.71 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 21:30:04.681953: step 88360, loss = 0.69 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 21:30:09.367549: step 88370, loss = 0.72 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 21:30:14.074878: step 88380, loss = 0.63 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 21:30:18.770642: step 88390, loss = 0.83 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 21:30:23.577855: step 88400, loss = 0.60 (258.4 examples/sec; 0.495 sec/batch)
2016-02-03 21:30:28.864280: step 88410, loss = 0.95 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 21:30:33.588902: step 88420, loss = 0.67 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 21:30:38.367911: step 88430, loss = 0.64 (248.6 examples/sec; 0.515 sec/batch)
2016-02-03 21:30:43.027863: step 88440, loss = 0.73 (257.6 examples/sec; 0.497 sec/batch)
2016-02-03 21:30:47.717352: step 88450, loss = 0.65 (264.7 examples/sec; 0.484 sec/batch)
2016-02-03 21:30:52.474069: step 88460, loss = 0.82 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 21:30:57.167779: step 88470, loss = 0.77 (251.3 examples/sec; 0.509 sec/batch)
2016-02-03 21:31:01.943039: step 88480, loss = 0.74 (259.2 examples/sec; 0.494 sec/batch)
2016-02-03 21:31:06.587188: step 88490, loss = 0.65 (301.8 examples/sec; 0.424 sec/batch)
2016-02-03 21:31:11.252614: step 88500, loss = 0.61 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 21:31:16.530993: step 88510, loss = 0.70 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 21:31:21.285755: step 88520, loss = 0.78 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 21:31:25.793845: step 88530, loss = 0.62 (319.5 examples/sec; 0.401 sec/batch)
2016-02-03 21:31:30.299629: step 88540, loss = 0.79 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 21:31:34.699733: step 88550, loss = 0.59 (289.1 examples/sec; 0.443 sec/batch)
2016-02-03 21:31:39.314136: step 88560, loss = 0.65 (262.9 examples/sec; 0.487 sec/batch)
2016-02-03 21:31:43.846308: step 88570, loss = 0.62 (281.4 examples/sec; 0.455 sec/batch)
2016-02-03 21:31:48.402587: step 88580, loss = 0.68 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 21:31:53.052360: step 88590, loss = 0.66 (296.5 examples/sec; 0.432 sec/batch)
2016-02-03 21:31:57.744242: step 88600, loss = 0.83 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 21:32:03.004784: step 88610, loss = 0.72 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 21:32:07.793062: step 88620, loss = 0.61 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 21:32:12.545684: step 88630, loss = 0.83 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 21:32:17.270044: step 88640, loss = 0.67 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 21:32:22.009528: step 88650, loss = 0.59 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 21:32:26.740266: step 88660, loss = 0.63 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 21:32:31.458596: step 88670, loss = 0.86 (257.0 examples/sec; 0.498 sec/batch)
2016-02-03 21:32:36.204721: step 88680, loss = 0.82 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 21:32:40.853127: step 88690, loss = 0.63 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 21:32:45.515145: step 88700, loss = 0.66 (268.6 examples/sec; 0.477 sec/batch)
2016-02-03 21:32:50.707904: step 88710, loss = 0.72 (293.3 examples/sec; 0.436 sec/batch)
2016-02-03 21:32:55.378639: step 88720, loss = 0.73 (261.1 examples/sec; 0.490 sec/batch)
2016-02-03 21:33:00.001081: step 88730, loss = 0.73 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 21:33:04.748165: step 88740, loss = 0.61 (250.2 examples/sec; 0.512 sec/batch)
2016-02-03 21:33:09.548937: step 88750, loss = 0.69 (252.7 examples/sec; 0.507 sec/batch)
2016-02-03 21:33:14.262629: step 88760, loss = 0.76 (274.4 examples/sec; 0.467 sec/batch)
2016-02-03 21:33:19.015304: step 88770, loss = 0.73 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 21:33:23.657864: step 88780, loss = 0.66 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 21:33:28.432131: step 88790, loss = 0.89 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 21:33:33.123032: step 88800, loss = 0.75 (283.0 examples/sec; 0.452 sec/batch)
2016-02-03 21:33:38.264473: step 88810, loss = 0.83 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 21:33:42.937056: step 88820, loss = 0.79 (295.1 examples/sec; 0.434 sec/batch)
2016-02-03 21:33:47.694916: step 88830, loss = 0.62 (260.6 examples/sec; 0.491 sec/batch)
2016-02-03 21:33:52.357703: step 88840, loss = 0.71 (268.0 examples/sec; 0.478 sec/batch)
2016-02-03 21:33:57.125746: step 88850, loss = 0.88 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 21:34:01.953453: step 88860, loss = 0.64 (238.2 examples/sec; 0.537 sec/batch)
2016-02-03 21:34:06.642473: step 88870, loss = 0.77 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 21:34:11.424147: step 88880, loss = 0.56 (262.9 examples/sec; 0.487 sec/batch)
2016-02-03 21:34:16.084401: step 88890, loss = 0.79 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 21:34:20.797757: step 88900, loss = 0.82 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 21:34:25.997602: step 88910, loss = 0.64 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 21:34:30.750556: step 88920, loss = 0.84 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 21:34:35.504050: step 88930, loss = 0.79 (272.7 examples/sec; 0.469 sec/batch)
2016-02-03 21:34:40.225881: step 88940, loss = 0.62 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 21:34:44.909465: step 88950, loss = 0.62 (281.4 examples/sec; 0.455 sec/batch)
2016-02-03 21:34:49.594506: step 88960, loss = 0.75 (284.6 examples/sec; 0.450 sec/batch)
2016-02-03 21:34:54.295432: step 88970, loss = 0.79 (269.3 examples/sec; 0.475 sec/batch)
2016-02-03 21:34:58.937314: step 88980, loss = 0.67 (296.6 examples/sec; 0.431 sec/batch)
2016-02-03 21:35:03.551198: step 88990, loss = 0.70 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 21:35:08.202069: step 89000, loss = 0.75 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 21:35:13.318496: step 89010, loss = 0.69 (270.5 examples/sec; 0.473 sec/batch)
2016-02-03 21:35:17.997225: step 89020, loss = 0.65 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 21:35:22.744875: step 89030, loss = 0.79 (260.8 examples/sec; 0.491 sec/batch)
2016-02-03 21:35:27.505995: step 89040, loss = 0.72 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 21:35:32.291007: step 89050, loss = 0.61 (253.8 examples/sec; 0.504 sec/batch)
2016-02-03 21:35:36.972826: step 89060, loss = 0.65 (283.9 examples/sec; 0.451 sec/batch)
2016-02-03 21:35:41.699749: step 89070, loss = 0.71 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 21:35:46.381044: step 89080, loss = 0.81 (263.2 examples/sec; 0.486 sec/batch)
2016-02-03 21:35:51.105490: step 89090, loss = 0.68 (255.2 examples/sec; 0.502 sec/batch)
2016-02-03 21:35:55.833616: step 89100, loss = 0.75 (253.0 examples/sec; 0.506 sec/batch)
2016-02-03 21:36:01.053566: step 89110, loss = 0.78 (296.5 examples/sec; 0.432 sec/batch)
2016-02-03 21:36:05.769419: step 89120, loss = 0.79 (286.5 examples/sec; 0.447 sec/batch)
2016-02-03 21:36:10.397793: step 89130, loss = 0.72 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 21:36:15.093224: step 89140, loss = 0.70 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 21:36:19.730150: step 89150, loss = 0.76 (301.3 examples/sec; 0.425 sec/batch)
2016-02-03 21:36:24.517378: step 89160, loss = 0.84 (260.5 examples/sec; 0.491 sec/batch)
2016-02-03 21:36:29.204368: step 89170, loss = 0.80 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 21:36:33.923146: step 89180, loss = 0.70 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 21:36:38.664129: step 89190, loss = 0.72 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 21:36:43.395737: step 89200, loss = 0.65 (256.0 examples/sec; 0.500 sec/batch)
2016-02-03 21:36:48.685116: step 89210, loss = 0.81 (279.7 examples/sec; 0.458 sec/batch)
2016-02-03 21:36:53.373169: step 89220, loss = 0.80 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 21:36:58.078046: step 89230, loss = 0.57 (264.7 examples/sec; 0.484 sec/batch)
2016-02-03 21:37:02.746858: step 89240, loss = 0.57 (291.3 examples/sec; 0.439 sec/batch)
2016-02-03 21:37:07.408921: step 89250, loss = 0.70 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 21:37:12.107619: step 89260, loss = 0.64 (268.6 examples/sec; 0.477 sec/batch)
2016-02-03 21:37:16.884152: step 89270, loss = 0.75 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 21:37:21.646413: step 89280, loss = 0.66 (246.1 examples/sec; 0.520 sec/batch)
2016-02-03 21:37:26.310294: step 89290, loss = 0.63 (296.4 examples/sec; 0.432 sec/batch)
2016-02-03 21:37:30.961983: step 89300, loss = 0.80 (305.3 examples/sec; 0.419 sec/batch)
2016-02-03 21:37:36.136226: step 89310, loss = 0.71 (283.8 examples/sec; 0.451 sec/batch)
2016-02-03 21:37:40.823005: step 89320, loss = 0.67 (262.1 examples/sec; 0.488 sec/batch)
2016-02-03 21:37:45.520220: step 89330, loss = 0.81 (293.1 examples/sec; 0.437 sec/batch)
2016-02-03 21:37:50.159381: step 89340, loss = 0.78 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 21:37:54.805817: step 89350, loss = 0.78 (298.2 examples/sec; 0.429 sec/batch)
2016-02-03 21:37:59.448000: step 89360, loss = 0.71 (280.1 examples/sec; 0.457 sec/batch)
2016-02-03 21:38:04.234328: step 89370, loss = 0.68 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 21:38:08.963978: step 89380, loss = 0.60 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 21:38:13.693587: step 89390, loss = 0.72 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 21:38:18.460899: step 89400, loss = 0.66 (259.7 examples/sec; 0.493 sec/batch)
2016-02-03 21:38:23.608676: step 89410, loss = 0.82 (252.1 examples/sec; 0.508 sec/batch)
2016-02-03 21:38:28.232531: step 89420, loss = 0.72 (270.9 examples/sec; 0.472 sec/batch)
2016-02-03 21:38:32.909150: step 89430, loss = 0.67 (289.7 examples/sec; 0.442 sec/batch)
2016-02-03 21:38:37.627800: step 89440, loss = 0.87 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 21:38:42.328774: step 89450, loss = 0.73 (288.3 examples/sec; 0.444 sec/batch)
2016-02-03 21:38:47.076228: step 89460, loss = 0.68 (247.2 examples/sec; 0.518 sec/batch)
2016-02-03 21:38:51.729483: step 89470, loss = 0.80 (286.1 examples/sec; 0.447 sec/batch)
2016-02-03 21:38:56.395675: step 89480, loss = 0.67 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 21:39:01.061275: step 89490, loss = 0.65 (281.5 examples/sec; 0.455 sec/batch)
2016-02-03 21:39:05.788041: step 89500, loss = 0.83 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 21:39:11.009379: step 89510, loss = 0.79 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 21:39:15.845455: step 89520, loss = 0.83 (276.7 examples/sec; 0.463 sec/batch)
2016-02-03 21:39:20.595200: step 89530, loss = 0.69 (256.7 examples/sec; 0.499 sec/batch)
2016-02-03 21:39:25.213259: step 89540, loss = 0.71 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 21:39:29.940021: step 89550, loss = 0.62 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 21:39:34.654239: step 89560, loss = 0.63 (283.8 examples/sec; 0.451 sec/batch)
2016-02-03 21:39:39.422043: step 89570, loss = 0.70 (257.4 examples/sec; 0.497 sec/batch)
2016-02-03 21:39:44.155924: step 89580, loss = 0.85 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 21:39:48.777317: step 89590, loss = 0.84 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 21:39:53.416634: step 89600, loss = 0.74 (284.9 examples/sec; 0.449 sec/batch)
2016-02-03 21:39:58.577795: step 89610, loss = 0.67 (295.8 examples/sec; 0.433 sec/batch)
2016-02-03 21:40:03.298820: step 89620, loss = 0.77 (257.2 examples/sec; 0.498 sec/batch)
2016-02-03 21:40:08.008499: step 89630, loss = 0.75 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 21:40:12.710758: step 89640, loss = 0.65 (252.5 examples/sec; 0.507 sec/batch)
2016-02-03 21:40:17.410177: step 89650, loss = 0.67 (297.3 examples/sec; 0.430 sec/batch)
2016-02-03 21:40:22.236199: step 89660, loss = 0.77 (252.1 examples/sec; 0.508 sec/batch)
2016-02-03 21:40:26.874746: step 89670, loss = 0.67 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 21:40:31.550249: step 89680, loss = 0.74 (283.7 examples/sec; 0.451 sec/batch)
2016-02-03 21:40:36.228181: step 89690, loss = 0.79 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 21:40:41.010890: step 89700, loss = 0.71 (253.7 examples/sec; 0.504 sec/batch)
2016-02-03 21:40:46.152449: step 89710, loss = 0.63 (305.2 examples/sec; 0.419 sec/batch)
2016-02-03 21:40:50.889710: step 89720, loss = 0.67 (283.8 examples/sec; 0.451 sec/batch)
2016-02-03 21:40:55.551378: step 89730, loss = 0.86 (288.9 examples/sec; 0.443 sec/batch)
2016-02-03 21:41:00.232913: step 89740, loss = 0.68 (292.7 examples/sec; 0.437 sec/batch)
2016-02-03 21:41:04.928435: step 89750, loss = 0.71 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 21:41:09.689016: step 89760, loss = 0.66 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 21:41:14.377533: step 89770, loss = 0.93 (293.2 examples/sec; 0.436 sec/batch)
2016-02-03 21:41:19.131114: step 89780, loss = 0.69 (246.3 examples/sec; 0.520 sec/batch)
2016-02-03 21:41:23.854874: step 89790, loss = 1.09 (257.1 examples/sec; 0.498 sec/batch)
2016-02-03 21:41:28.486087: step 89800, loss = 0.62 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 21:41:33.753736: step 89810, loss = 0.74 (267.3 examples/sec; 0.479 sec/batch)
2016-02-03 21:41:38.426173: step 89820, loss = 0.69 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 21:41:43.199341: step 89830, loss = 0.80 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 21:41:47.875466: step 89840, loss = 0.66 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 21:41:52.588696: step 89850, loss = 0.80 (264.9 examples/sec; 0.483 sec/batch)
2016-02-03 21:41:57.341001: step 89860, loss = 0.77 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 21:42:02.007091: step 89870, loss = 0.75 (289.7 examples/sec; 0.442 sec/batch)
2016-02-03 21:42:06.794916: step 89880, loss = 0.59 (295.3 examples/sec; 0.434 sec/batch)
2016-02-03 21:42:11.505950: step 89890, loss = 0.65 (286.6 examples/sec; 0.447 sec/batch)
2016-02-03 21:42:16.231603: step 89900, loss = 0.74 (312.1 examples/sec; 0.410 sec/batch)
2016-02-03 21:42:21.513665: step 89910, loss = 0.78 (275.7 examples/sec; 0.464 sec/batch)
2016-02-03 21:42:26.287416: step 89920, loss = 0.59 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 21:42:30.922859: step 89930, loss = 0.66 (288.2 examples/sec; 0.444 sec/batch)
2016-02-03 21:42:35.720529: step 89940, loss = 0.62 (292.2 examples/sec; 0.438 sec/batch)
2016-02-03 21:42:40.443447: step 89950, loss = 0.62 (296.5 examples/sec; 0.432 sec/batch)
2016-02-03 21:42:45.169538: step 89960, loss = 0.66 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 21:42:49.874507: step 89970, loss = 0.69 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 21:42:54.648660: step 89980, loss = 0.68 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 21:42:59.330276: step 89990, loss = 0.74 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 21:43:04.005585: step 90000, loss = 0.64 (288.0 examples/sec; 0.444 sec/batch)
2016-02-03 21:43:09.243369: step 90010, loss = 0.75 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 21:43:13.950460: step 90020, loss = 0.67 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 21:43:18.605759: step 90030, loss = 0.72 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 21:43:23.342031: step 90040, loss = 0.72 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 21:43:28.034916: step 90050, loss = 0.66 (287.6 examples/sec; 0.445 sec/batch)
2016-02-03 21:43:32.743089: step 90060, loss = 0.80 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 21:43:37.413115: step 90070, loss = 0.76 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 21:43:42.099312: step 90080, loss = 0.66 (286.5 examples/sec; 0.447 sec/batch)
2016-02-03 21:43:46.789653: step 90090, loss = 0.65 (258.6 examples/sec; 0.495 sec/batch)
2016-02-03 21:43:51.503480: step 90100, loss = 0.71 (268.0 examples/sec; 0.478 sec/batch)
2016-02-03 21:43:56.772497: step 90110, loss = 0.80 (258.7 examples/sec; 0.495 sec/batch)
2016-02-03 21:44:01.459409: step 90120, loss = 0.58 (260.6 examples/sec; 0.491 sec/batch)
2016-02-03 21:44:06.182894: step 90130, loss = 0.70 (280.9 examples/sec; 0.456 sec/batch)
2016-02-03 21:44:10.835753: step 90140, loss = 0.67 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 21:44:15.427985: step 90150, loss = 0.76 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 21:44:20.204268: step 90160, loss = 0.74 (299.8 examples/sec; 0.427 sec/batch)
2016-02-03 21:44:24.904790: step 90170, loss = 0.67 (254.6 examples/sec; 0.503 sec/batch)
2016-02-03 21:44:29.650644: step 90180, loss = 0.72 (259.1 examples/sec; 0.494 sec/batch)
2016-02-03 21:44:34.391486: step 90190, loss = 0.86 (258.9 examples/sec; 0.494 sec/batch)
2016-02-03 21:44:39.111936: step 90200, loss = 0.76 (281.7 examples/sec; 0.454 sec/batch)
2016-02-03 21:44:44.388747: step 90210, loss = 0.80 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 21:44:49.127315: step 90220, loss = 0.56 (250.8 examples/sec; 0.510 sec/batch)
2016-02-03 21:44:53.811046: step 90230, loss = 0.61 (266.9 examples/sec; 0.480 sec/batch)
2016-02-03 21:44:58.438795: step 90240, loss = 0.77 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 21:45:03.089094: step 90250, loss = 0.76 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 21:45:07.770532: step 90260, loss = 0.78 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 21:45:12.469745: step 90270, loss = 0.72 (301.5 examples/sec; 0.425 sec/batch)
2016-02-03 21:45:17.059590: step 90280, loss = 0.57 (285.2 examples/sec; 0.449 sec/batch)
2016-02-03 21:45:21.774749: step 90290, loss = 0.76 (254.6 examples/sec; 0.503 sec/batch)
2016-02-03 21:45:26.460824: step 90300, loss = 0.78 (293.6 examples/sec; 0.436 sec/batch)
2016-02-03 21:45:31.611212: step 90310, loss = 0.62 (287.2 examples/sec; 0.446 sec/batch)
2016-02-03 21:45:36.294824: step 90320, loss = 0.58 (265.3 examples/sec; 0.483 sec/batch)
2016-02-03 21:45:40.910678: step 90330, loss = 0.70 (282.8 examples/sec; 0.453 sec/batch)
2016-02-03 21:45:45.650266: step 90340, loss = 0.62 (285.4 examples/sec; 0.449 sec/batch)
2016-02-03 21:45:50.283262: step 90350, loss = 0.70 (299.8 examples/sec; 0.427 sec/batch)
2016-02-03 21:45:54.992034: step 90360, loss = 0.70 (264.2 examples/sec; 0.484 sec/batch)
2016-02-03 21:45:59.795383: step 90370, loss = 0.67 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 21:46:04.413357: step 90380, loss = 0.62 (293.8 examples/sec; 0.436 sec/batch)
2016-02-03 21:46:09.071945: step 90390, loss = 0.72 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 21:46:13.729703: step 90400, loss = 0.73 (292.5 examples/sec; 0.438 sec/batch)
2016-02-03 21:46:18.959642: step 90410, loss = 0.73 (264.2 examples/sec; 0.485 sec/batch)
2016-02-03 21:46:23.618942: step 90420, loss = 0.89 (280.5 examples/sec; 0.456 sec/batch)
2016-02-03 21:46:28.260860: step 90430, loss = 0.71 (270.9 examples/sec; 0.472 sec/batch)
2016-02-03 21:46:32.992750: step 90440, loss = 0.68 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 21:46:37.732906: step 90450, loss = 0.71 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 21:46:42.385631: step 90460, loss = 0.85 (292.9 examples/sec; 0.437 sec/batch)
2016-02-03 21:46:47.016975: step 90470, loss = 0.69 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 21:46:51.689794: step 90480, loss = 0.73 (260.5 examples/sec; 0.491 sec/batch)
2016-02-03 21:46:56.403886: step 90490, loss = 0.60 (249.6 examples/sec; 0.513 sec/batch)
2016-02-03 21:47:01.116592: step 90500, loss = 0.80 (267.2 examples/sec; 0.479 sec/batch)
2016-02-03 21:47:06.316608: step 90510, loss = 0.84 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 21:47:11.039906: step 90520, loss = 0.67 (288.7 examples/sec; 0.443 sec/batch)
2016-02-03 21:47:15.724371: step 90530, loss = 0.68 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 21:47:20.359730: step 90540, loss = 0.69 (300.8 examples/sec; 0.425 sec/batch)
2016-02-03 21:47:25.052111: step 90550, loss = 0.62 (277.9 examples/sec; 0.461 sec/batch)
2016-02-03 21:47:29.682498: step 90560, loss = 0.76 (268.0 examples/sec; 0.478 sec/batch)
2016-02-03 21:47:34.455754: step 90570, loss = 0.73 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 21:47:39.144452: step 90580, loss = 0.69 (291.2 examples/sec; 0.440 sec/batch)
2016-02-03 21:47:43.888812: step 90590, loss = 0.79 (280.1 examples/sec; 0.457 sec/batch)
2016-02-03 21:47:48.683700: step 90600, loss = 0.66 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 21:47:53.965945: step 90610, loss = 0.88 (246.8 examples/sec; 0.519 sec/batch)
2016-02-03 21:47:58.571163: step 90620, loss = 1.01 (293.0 examples/sec; 0.437 sec/batch)
2016-02-03 21:48:03.279357: step 90630, loss = 0.67 (285.0 examples/sec; 0.449 sec/batch)
2016-02-03 21:48:08.007930: step 90640, loss = 0.67 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 21:48:12.719754: step 90650, loss = 0.77 (260.4 examples/sec; 0.492 sec/batch)
2016-02-03 21:48:17.430141: step 90660, loss = 0.62 (259.9 examples/sec; 0.492 sec/batch)
2016-02-03 21:48:22.058292: step 90670, loss = 0.90 (287.8 examples/sec; 0.445 sec/batch)
2016-02-03 21:48:26.752134: step 90680, loss = 0.88 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 21:48:31.521322: step 90690, loss = 0.70 (264.2 examples/sec; 0.484 sec/batch)
2016-02-03 21:48:36.271143: step 90700, loss = 0.81 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 21:48:41.473563: step 90710, loss = 0.67 (265.3 examples/sec; 0.483 sec/batch)
2016-02-03 21:48:46.152500: step 90720, loss = 0.58 (260.7 examples/sec; 0.491 sec/batch)
2016-02-03 21:48:50.811837: step 90730, loss = 0.79 (253.1 examples/sec; 0.506 sec/batch)
2016-02-03 21:48:55.487205: step 90740, loss = 0.69 (292.6 examples/sec; 0.437 sec/batch)
2016-02-03 21:49:00.224758: step 90750, loss = 0.62 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 21:49:04.957328: step 90760, loss = 0.72 (251.2 examples/sec; 0.510 sec/batch)
2016-02-03 21:49:09.620191: step 90770, loss = 0.55 (289.7 examples/sec; 0.442 sec/batch)
2016-02-03 21:49:14.356910: step 90780, loss = 0.66 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 21:49:18.995134: step 90790, loss = 0.69 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 21:49:23.613705: step 90800, loss = 0.71 (287.0 examples/sec; 0.446 sec/batch)
2016-02-03 21:49:28.822084: step 90810, loss = 0.63 (286.0 examples/sec; 0.448 sec/batch)
2016-02-03 21:49:33.507169: step 90820, loss = 0.67 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 21:49:38.134206: step 90830, loss = 0.75 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 21:49:42.888698: step 90840, loss = 0.61 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 21:49:47.546855: step 90850, loss = 0.75 (303.4 examples/sec; 0.422 sec/batch)
2016-02-03 21:49:52.334030: step 90860, loss = 0.73 (255.3 examples/sec; 0.501 sec/batch)
2016-02-03 21:49:57.024367: step 90870, loss = 0.71 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 21:50:01.706496: step 90880, loss = 0.64 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 21:50:06.337013: step 90890, loss = 0.81 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 21:50:10.963637: step 90900, loss = 0.66 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 21:50:16.117508: step 90910, loss = 0.75 (255.2 examples/sec; 0.502 sec/batch)
2016-02-03 21:50:20.764138: step 90920, loss = 0.65 (269.8 examples/sec; 0.475 sec/batch)
2016-02-03 21:50:25.462147: step 90930, loss = 0.67 (283.4 examples/sec; 0.452 sec/batch)
2016-02-03 21:50:30.078175: step 90940, loss = 0.58 (282.8 examples/sec; 0.453 sec/batch)
2016-02-03 21:50:34.732121: step 90950, loss = 0.80 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 21:50:39.285149: step 90960, loss = 0.65 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 21:50:43.839116: step 90970, loss = 0.67 (294.1 examples/sec; 0.435 sec/batch)
2016-02-03 21:50:48.533163: step 90980, loss = 0.80 (288.2 examples/sec; 0.444 sec/batch)
2016-02-03 21:50:53.267144: step 90990, loss = 0.76 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 21:50:57.935622: step 91000, loss = 0.72 (255.8 examples/sec; 0.500 sec/batch)
2016-02-03 21:51:03.154616: step 91010, loss = 0.74 (253.4 examples/sec; 0.505 sec/batch)
2016-02-03 21:51:07.805676: step 91020, loss = 0.57 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 21:51:12.551033: step 91030, loss = 0.68 (258.0 examples/sec; 0.496 sec/batch)
2016-02-03 21:51:17.208055: step 91040, loss = 0.79 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 21:51:21.858426: step 91050, loss = 0.84 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 21:51:26.442279: step 91060, loss = 0.74 (317.1 examples/sec; 0.404 sec/batch)
2016-02-03 21:51:31.127047: step 91070, loss = 0.80 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 21:51:35.911386: step 91080, loss = 0.75 (262.0 examples/sec; 0.489 sec/batch)
2016-02-03 21:51:40.660533: step 91090, loss = 0.67 (260.9 examples/sec; 0.491 sec/batch)
2016-02-03 21:51:45.327573: step 91100, loss = 0.64 (255.9 examples/sec; 0.500 sec/batch)
2016-02-03 21:51:50.550167: step 91110, loss = 0.87 (284.1 examples/sec; 0.451 sec/batch)
2016-02-03 21:51:55.161787: step 91120, loss = 0.94 (290.6 examples/sec; 0.441 sec/batch)
2016-02-03 21:51:59.881075: step 91130, loss = 0.71 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 21:52:04.576548: step 91140, loss = 0.75 (257.1 examples/sec; 0.498 sec/batch)
2016-02-03 21:52:09.269917: step 91150, loss = 0.77 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 21:52:13.950454: step 91160, loss = 0.75 (285.2 examples/sec; 0.449 sec/batch)
2016-02-03 21:52:18.684949: step 91170, loss = 0.78 (258.8 examples/sec; 0.495 sec/batch)
2016-02-03 21:52:23.344708: step 91180, loss = 0.81 (284.7 examples/sec; 0.450 sec/batch)
2016-02-03 21:52:28.017450: step 91190, loss = 0.71 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 21:52:32.707788: step 91200, loss = 0.74 (286.7 examples/sec; 0.446 sec/batch)
2016-02-03 21:52:37.956983: step 91210, loss = 0.72 (278.6 examples/sec; 0.460 sec/batch)
2016-02-03 21:52:42.669551: step 91220, loss = 0.78 (292.6 examples/sec; 0.437 sec/batch)
2016-02-03 21:52:47.375830: step 91230, loss = 0.77 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 21:52:52.106136: step 91240, loss = 0.65 (264.3 examples/sec; 0.484 sec/batch)
2016-02-03 21:52:56.813536: step 91250, loss = 0.73 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 21:53:01.509465: step 91260, loss = 0.80 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 21:53:06.231734: step 91270, loss = 0.79 (257.1 examples/sec; 0.498 sec/batch)
2016-02-03 21:53:11.009681: step 91280, loss = 0.70 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 21:53:15.785553: step 91290, loss = 0.72 (255.1 examples/sec; 0.502 sec/batch)
2016-02-03 21:53:20.502451: step 91300, loss = 0.76 (266.3 examples/sec; 0.481 sec/batch)
2016-02-03 21:53:25.740030: step 91310, loss = 0.68 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 21:53:30.436612: step 91320, loss = 0.61 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 21:53:35.068600: step 91330, loss = 0.81 (257.7 examples/sec; 0.497 sec/batch)
2016-02-03 21:53:39.759948: step 91340, loss = 0.84 (275.0 examples/sec; 0.465 sec/batch)
2016-02-03 21:53:44.499509: step 91350, loss = 0.72 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 21:53:49.211703: step 91360, loss = 0.80 (255.5 examples/sec; 0.501 sec/batch)
2016-02-03 21:53:53.854037: step 91370, loss = 0.78 (283.8 examples/sec; 0.451 sec/batch)
2016-02-03 21:53:58.543480: step 91380, loss = 0.67 (261.9 examples/sec; 0.489 sec/batch)
2016-02-03 21:54:03.324835: step 91390, loss = 0.75 (264.1 examples/sec; 0.485 sec/batch)
2016-02-03 21:54:08.022104: step 91400, loss = 0.71 (260.7 examples/sec; 0.491 sec/batch)
2016-02-03 21:54:13.248118: step 91410, loss = 0.74 (281.2 examples/sec; 0.455 sec/batch)
2016-02-03 21:54:17.872294: step 91420, loss = 0.68 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 21:54:22.541939: step 91430, loss = 0.69 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 21:54:27.187270: step 91440, loss = 0.66 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 21:54:31.933924: step 91450, loss = 0.72 (257.3 examples/sec; 0.497 sec/batch)
2016-02-03 21:54:36.618738: step 91460, loss = 0.78 (289.3 examples/sec; 0.442 sec/batch)
2016-02-03 21:54:41.287376: step 91470, loss = 0.69 (272.7 examples/sec; 0.469 sec/batch)
2016-02-03 21:54:45.939858: step 91480, loss = 0.65 (258.3 examples/sec; 0.496 sec/batch)
2016-02-03 21:54:50.623060: step 91490, loss = 0.73 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 21:54:55.265856: step 91500, loss = 0.76 (291.7 examples/sec; 0.439 sec/batch)
2016-02-03 21:55:00.473661: step 91510, loss = 0.62 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 21:55:05.164106: step 91520, loss = 0.57 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 21:55:09.882147: step 91530, loss = 0.88 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 21:55:14.614932: step 91540, loss = 0.61 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 21:55:19.357495: step 91550, loss = 0.64 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 21:55:24.103177: step 91560, loss = 0.59 (263.2 examples/sec; 0.486 sec/batch)
2016-02-03 21:55:28.894978: step 91570, loss = 0.77 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 21:55:33.557388: step 91580, loss = 0.65 (270.5 examples/sec; 0.473 sec/batch)
2016-02-03 21:55:38.248290: step 91590, loss = 0.63 (255.8 examples/sec; 0.500 sec/batch)
2016-02-03 21:55:42.994078: step 91600, loss = 0.79 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 21:55:48.198097: step 91610, loss = 0.65 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 21:55:52.870916: step 91620, loss = 0.60 (274.4 examples/sec; 0.467 sec/batch)
2016-02-03 21:55:57.583252: step 91630, loss = 0.74 (289.8 examples/sec; 0.442 sec/batch)
2016-02-03 21:56:02.272422: step 91640, loss = 0.65 (287.8 examples/sec; 0.445 sec/batch)
2016-02-03 21:56:07.090930: step 91650, loss = 0.88 (284.2 examples/sec; 0.450 sec/batch)
2016-02-03 21:56:11.899123: step 91660, loss = 0.72 (261.3 examples/sec; 0.490 sec/batch)
2016-02-03 21:56:16.570265: step 91670, loss = 0.72 (277.9 examples/sec; 0.461 sec/batch)
2016-02-03 21:56:21.232301: step 91680, loss = 0.67 (290.1 examples/sec; 0.441 sec/batch)
2016-02-03 21:56:25.956196: step 91690, loss = 0.74 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 21:56:30.622613: step 91700, loss = 0.66 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 21:56:35.757049: step 91710, loss = 0.92 (287.4 examples/sec; 0.445 sec/batch)
2016-02-03 21:56:40.559382: step 91720, loss = 0.65 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 21:56:45.357381: step 91730, loss = 0.75 (253.4 examples/sec; 0.505 sec/batch)
2016-02-03 21:56:50.047576: step 91740, loss = 0.70 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 21:56:54.734463: step 91750, loss = 0.61 (255.6 examples/sec; 0.501 sec/batch)
2016-02-03 21:56:59.445554: step 91760, loss = 0.57 (262.1 examples/sec; 0.488 sec/batch)
2016-02-03 21:57:04.186227: step 91770, loss = 0.84 (250.5 examples/sec; 0.511 sec/batch)
2016-02-03 21:57:08.828540: step 91780, loss = 0.73 (293.5 examples/sec; 0.436 sec/batch)
2016-02-03 21:57:13.622063: step 91790, loss = 0.78 (253.2 examples/sec; 0.506 sec/batch)
2016-02-03 21:57:18.367408: step 91800, loss = 0.84 (251.0 examples/sec; 0.510 sec/batch)
2016-02-03 21:57:23.604121: step 91810, loss = 0.70 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 21:57:28.344269: step 91820, loss = 0.65 (255.0 examples/sec; 0.502 sec/batch)
2016-02-03 21:57:33.002983: step 91830, loss = 0.55 (288.8 examples/sec; 0.443 sec/batch)
2016-02-03 21:57:37.733547: step 91840, loss = 0.88 (259.5 examples/sec; 0.493 sec/batch)
2016-02-03 21:57:42.380080: step 91850, loss = 0.64 (289.6 examples/sec; 0.442 sec/batch)
2016-02-03 21:57:47.109296: step 91860, loss = 0.57 (266.9 examples/sec; 0.480 sec/batch)
2016-02-03 21:57:51.807232: step 91870, loss = 0.72 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 21:57:56.598515: step 91880, loss = 0.72 (250.6 examples/sec; 0.511 sec/batch)
2016-02-03 21:58:01.326935: step 91890, loss = 0.69 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 21:58:06.111747: step 91900, loss = 0.65 (258.6 examples/sec; 0.495 sec/batch)
2016-02-03 21:58:11.364024: step 91910, loss = 0.58 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 21:58:15.982923: step 91920, loss = 0.58 (258.5 examples/sec; 0.495 sec/batch)
2016-02-03 21:58:20.723548: step 91930, loss = 0.58 (258.6 examples/sec; 0.495 sec/batch)
2016-02-03 21:58:25.387506: step 91940, loss = 0.64 (280.9 examples/sec; 0.456 sec/batch)
2016-02-03 21:58:30.077658: step 91950, loss = 0.85 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 21:58:34.762351: step 91960, loss = 0.79 (269.3 examples/sec; 0.475 sec/batch)
2016-02-03 21:58:39.465812: step 91970, loss = 0.68 (252.5 examples/sec; 0.507 sec/batch)
2016-02-03 21:58:44.149948: step 91980, loss = 0.67 (281.1 examples/sec; 0.455 sec/batch)
2016-02-03 21:58:48.850798: step 91990, loss = 0.66 (256.3 examples/sec; 0.499 sec/batch)
2016-02-03 21:58:53.522497: step 92000, loss = 0.78 (282.9 examples/sec; 0.452 sec/batch)
2016-02-03 21:58:58.781288: step 92010, loss = 0.64 (260.8 examples/sec; 0.491 sec/batch)
2016-02-03 21:59:03.396596: step 92020, loss = 0.71 (284.0 examples/sec; 0.451 sec/batch)
2016-02-03 21:59:08.140399: step 92030, loss = 0.80 (264.2 examples/sec; 0.484 sec/batch)
2016-02-03 21:59:12.888476: step 92040, loss = 0.58 (260.0 examples/sec; 0.492 sec/batch)
2016-02-03 21:59:17.643475: step 92050, loss = 0.56 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 21:59:22.346461: step 92060, loss = 0.58 (304.5 examples/sec; 0.420 sec/batch)
2016-02-03 21:59:27.054998: step 92070, loss = 0.63 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 21:59:31.744169: step 92080, loss = 0.59 (279.8 examples/sec; 0.457 sec/batch)
2016-02-03 21:59:36.380304: step 92090, loss = 0.60 (290.8 examples/sec; 0.440 sec/batch)
2016-02-03 21:59:41.059204: step 92100, loss = 0.61 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 21:59:46.290649: step 92110, loss = 0.73 (299.6 examples/sec; 0.427 sec/batch)
2016-02-03 21:59:51.061710: step 92120, loss = 0.78 (283.0 examples/sec; 0.452 sec/batch)
2016-02-03 21:59:55.856451: step 92130, loss = 0.63 (261.3 examples/sec; 0.490 sec/batch)
2016-02-03 22:00:00.511812: step 92140, loss = 0.79 (298.0 examples/sec; 0.430 sec/batch)
2016-02-03 22:00:05.145161: step 92150, loss = 0.86 (290.2 examples/sec; 0.441 sec/batch)
2016-02-03 22:00:09.763360: step 92160, loss = 0.69 (307.2 examples/sec; 0.417 sec/batch)
2016-02-03 22:00:14.375449: step 92170, loss = 0.79 (295.1 examples/sec; 0.434 sec/batch)
2016-02-03 22:00:19.065213: step 92180, loss = 0.71 (263.1 examples/sec; 0.487 sec/batch)
2016-02-03 22:00:23.795723: step 92190, loss = 0.79 (272.0 examples/sec; 0.471 sec/batch)
2016-02-03 22:00:28.443117: step 92200, loss = 0.59 (256.6 examples/sec; 0.499 sec/batch)
2016-02-03 22:00:33.624127: step 92210, loss = 0.65 (257.6 examples/sec; 0.497 sec/batch)
2016-02-03 22:00:38.269558: step 92220, loss = 0.74 (290.8 examples/sec; 0.440 sec/batch)
2016-02-03 22:00:42.984905: step 92230, loss = 0.63 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 22:00:47.731098: step 92240, loss = 0.57 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 22:00:52.464193: step 92250, loss = 0.57 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 22:00:57.153166: step 92260, loss = 0.79 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 22:01:01.897631: step 92270, loss = 0.59 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 22:01:06.619916: step 92280, loss = 0.74 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 22:01:11.252421: step 92290, loss = 0.70 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 22:01:15.929183: step 92300, loss = 0.79 (277.2 examples/sec; 0.462 sec/batch)
2016-02-03 22:01:21.231145: step 92310, loss = 0.81 (284.2 examples/sec; 0.450 sec/batch)
2016-02-03 22:01:25.905964: step 92320, loss = 0.81 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 22:01:30.626411: step 92330, loss = 0.57 (284.6 examples/sec; 0.450 sec/batch)
2016-02-03 22:01:35.348982: step 92340, loss = 0.57 (280.5 examples/sec; 0.456 sec/batch)
2016-02-03 22:01:40.080423: step 92350, loss = 0.91 (254.8 examples/sec; 0.502 sec/batch)
2016-02-03 22:01:44.791183: step 92360, loss = 0.92 (280.5 examples/sec; 0.456 sec/batch)
2016-02-03 22:01:49.455952: step 92370, loss = 0.61 (284.4 examples/sec; 0.450 sec/batch)
2016-02-03 22:01:54.170438: step 92380, loss = 0.68 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 22:01:58.949487: step 92390, loss = 0.53 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 22:02:03.599582: step 92400, loss = 0.60 (254.8 examples/sec; 0.502 sec/batch)
2016-02-03 22:02:08.875258: step 92410, loss = 0.66 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 22:02:13.611219: step 92420, loss = 0.65 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 22:02:18.319154: step 92430, loss = 0.76 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 22:02:22.994747: step 92440, loss = 0.65 (289.1 examples/sec; 0.443 sec/batch)
2016-02-03 22:02:27.804792: step 92450, loss = 0.69 (256.7 examples/sec; 0.499 sec/batch)
2016-02-03 22:02:32.489269: step 92460, loss = 0.65 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 22:02:37.161960: step 92470, loss = 0.83 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 22:02:41.874994: step 92480, loss = 0.79 (277.8 examples/sec; 0.461 sec/batch)
2016-02-03 22:02:46.529846: step 92490, loss = 0.69 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 22:02:51.197161: step 92500, loss = 0.65 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 22:02:56.451148: step 92510, loss = 0.55 (294.8 examples/sec; 0.434 sec/batch)
2016-02-03 22:03:01.160081: step 92520, loss = 0.64 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 22:03:05.882689: step 92530, loss = 0.75 (253.7 examples/sec; 0.505 sec/batch)
2016-02-03 22:03:10.527839: step 92540, loss = 0.70 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 22:03:15.201118: step 92550, loss = 0.74 (262.0 examples/sec; 0.488 sec/batch)
2016-02-03 22:03:19.652868: step 92560, loss = 0.80 (288.5 examples/sec; 0.444 sec/batch)
2016-02-03 22:03:24.364899: step 92570, loss = 0.61 (286.5 examples/sec; 0.447 sec/batch)
2016-02-03 22:03:29.022244: step 92580, loss = 0.71 (289.2 examples/sec; 0.443 sec/batch)
2016-02-03 22:03:33.566060: step 92590, loss = 0.93 (283.7 examples/sec; 0.451 sec/batch)
2016-02-03 22:03:38.225505: step 92600, loss = 0.77 (266.9 examples/sec; 0.479 sec/batch)
2016-02-03 22:03:43.429888: step 92610, loss = 0.60 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 22:03:48.120230: step 92620, loss = 0.75 (283.5 examples/sec; 0.451 sec/batch)
2016-02-03 22:03:52.866523: step 92630, loss = 0.66 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 22:03:57.426117: step 92640, loss = 0.75 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 22:04:02.135443: step 92650, loss = 0.61 (264.3 examples/sec; 0.484 sec/batch)
2016-02-03 22:04:06.846787: step 92660, loss = 0.76 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 22:04:11.527483: step 92670, loss = 0.67 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 22:04:16.231803: step 92680, loss = 0.75 (258.7 examples/sec; 0.495 sec/batch)
2016-02-03 22:04:20.885385: step 92690, loss = 0.69 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 22:04:25.576408: step 92700, loss = 0.66 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 22:04:30.751502: step 92710, loss = 0.78 (285.6 examples/sec; 0.448 sec/batch)
2016-02-03 22:04:35.501560: step 92720, loss = 0.63 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 22:04:40.213908: step 92730, loss = 0.68 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 22:04:44.898323: step 92740, loss = 0.76 (261.3 examples/sec; 0.490 sec/batch)
2016-02-03 22:04:49.609812: step 92750, loss = 0.66 (277.6 examples/sec; 0.461 sec/batch)
2016-02-03 22:04:54.265685: step 92760, loss = 0.68 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 22:04:58.976898: step 92770, loss = 0.69 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 22:05:03.653970: step 92780, loss = 0.73 (286.2 examples/sec; 0.447 sec/batch)
2016-02-03 22:05:08.413255: step 92790, loss = 0.82 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 22:05:13.103025: step 92800, loss = 0.65 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 22:05:18.272373: step 92810, loss = 0.70 (286.7 examples/sec; 0.446 sec/batch)
2016-02-03 22:05:23.023404: step 92820, loss = 0.80 (265.3 examples/sec; 0.482 sec/batch)
2016-02-03 22:05:27.646424: step 92830, loss = 0.69 (289.7 examples/sec; 0.442 sec/batch)
2016-02-03 22:05:32.370429: step 92840, loss = 0.76 (286.5 examples/sec; 0.447 sec/batch)
2016-02-03 22:05:37.070113: step 92850, loss = 0.68 (290.3 examples/sec; 0.441 sec/batch)
2016-02-03 22:05:41.805271: step 92860, loss = 0.71 (292.5 examples/sec; 0.438 sec/batch)
2016-02-03 22:05:46.446282: step 92870, loss = 0.69 (293.5 examples/sec; 0.436 sec/batch)
2016-02-03 22:05:51.208619: step 92880, loss = 0.61 (265.8 examples/sec; 0.482 sec/batch)
2016-02-03 22:05:55.879159: step 92890, loss = 0.63 (256.3 examples/sec; 0.499 sec/batch)
2016-02-03 22:06:00.557049: step 92900, loss = 0.65 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 22:06:05.846357: step 92910, loss = 0.69 (257.5 examples/sec; 0.497 sec/batch)
2016-02-03 22:06:10.500009: step 92920, loss = 0.62 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 22:06:15.188110: step 92930, loss = 0.79 (283.7 examples/sec; 0.451 sec/batch)
2016-02-03 22:06:19.881876: step 92940, loss = 0.68 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 22:06:24.431051: step 92950, loss = 0.64 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 22:06:29.166956: step 92960, loss = 0.69 (243.8 examples/sec; 0.525 sec/batch)
2016-02-03 22:06:33.915415: step 92970, loss = 0.77 (260.4 examples/sec; 0.492 sec/batch)
2016-02-03 22:06:38.577905: step 92980, loss = 0.68 (286.1 examples/sec; 0.447 sec/batch)
2016-02-03 22:06:43.385341: step 92990, loss = 0.87 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 22:06:48.116799: step 93000, loss = 0.75 (248.7 examples/sec; 0.515 sec/batch)
2016-02-03 22:06:53.386559: step 93010, loss = 0.57 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 22:06:58.089956: step 93020, loss = 0.74 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 22:07:02.795368: step 93030, loss = 0.60 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 22:07:07.516695: step 93040, loss = 0.76 (288.9 examples/sec; 0.443 sec/batch)
2016-02-03 22:07:12.319718: step 93050, loss = 0.72 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 22:07:17.064528: step 93060, loss = 0.61 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 22:07:21.815139: step 93070, loss = 0.80 (252.3 examples/sec; 0.507 sec/batch)
2016-02-03 22:07:26.498720: step 93080, loss = 0.66 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 22:07:31.214738: step 93090, loss = 0.70 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 22:07:35.851244: step 93100, loss = 0.53 (290.6 examples/sec; 0.441 sec/batch)
2016-02-03 22:07:41.016238: step 93110, loss = 0.72 (296.5 examples/sec; 0.432 sec/batch)
2016-02-03 22:07:45.675928: step 93120, loss = 0.69 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 22:07:50.350080: step 93130, loss = 0.61 (264.5 examples/sec; 0.484 sec/batch)
2016-02-03 22:07:55.064458: step 93140, loss = 0.86 (251.2 examples/sec; 0.510 sec/batch)
2016-02-03 22:07:59.782970: step 93150, loss = 0.73 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 22:08:04.473355: step 93160, loss = 0.60 (281.7 examples/sec; 0.454 sec/batch)
2016-02-03 22:08:09.113628: step 93170, loss = 0.67 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 22:08:13.739182: step 93180, loss = 0.76 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 22:08:18.454784: step 93190, loss = 0.63 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 22:08:23.138519: step 93200, loss = 0.75 (292.7 examples/sec; 0.437 sec/batch)
2016-02-03 22:08:28.354192: step 93210, loss = 0.64 (290.9 examples/sec; 0.440 sec/batch)
2016-02-03 22:08:33.091487: step 93220, loss = 0.75 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 22:08:37.819897: step 93230, loss = 0.60 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 22:08:42.577219: step 93240, loss = 0.62 (297.1 examples/sec; 0.431 sec/batch)
2016-02-03 22:08:47.193558: step 93250, loss = 0.58 (252.8 examples/sec; 0.506 sec/batch)
2016-02-03 22:08:51.898464: step 93260, loss = 0.66 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 22:08:56.630140: step 93270, loss = 0.68 (266.3 examples/sec; 0.481 sec/batch)
2016-02-03 22:09:01.361212: step 93280, loss = 0.76 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 22:09:06.089970: step 93290, loss = 0.59 (280.4 examples/sec; 0.456 sec/batch)
2016-02-03 22:09:10.785927: step 93300, loss = 0.80 (284.2 examples/sec; 0.450 sec/batch)
2016-02-03 22:09:16.033241: step 93310, loss = 0.68 (258.5 examples/sec; 0.495 sec/batch)
2016-02-03 22:09:20.738303: step 93320, loss = 0.55 (243.9 examples/sec; 0.525 sec/batch)
2016-02-03 22:09:25.405179: step 93330, loss = 0.73 (296.5 examples/sec; 0.432 sec/batch)
2016-02-03 22:09:30.096761: step 93340, loss = 0.69 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 22:09:34.776528: step 93350, loss = 0.74 (309.8 examples/sec; 0.413 sec/batch)
2016-02-03 22:09:39.511507: step 93360, loss = 0.65 (288.4 examples/sec; 0.444 sec/batch)
2016-02-03 22:09:44.267797: step 93370, loss = 0.63 (237.1 examples/sec; 0.540 sec/batch)
2016-02-03 22:09:48.959007: step 93380, loss = 0.66 (258.9 examples/sec; 0.494 sec/batch)
2016-02-03 22:09:53.743469: step 93390, loss = 0.87 (264.5 examples/sec; 0.484 sec/batch)
2016-02-03 22:09:58.391008: step 93400, loss = 0.83 (292.5 examples/sec; 0.438 sec/batch)
2016-02-03 22:10:03.568888: step 93410, loss = 0.71 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 22:10:08.301857: step 93420, loss = 0.87 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 22:10:13.057869: step 93430, loss = 0.63 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 22:10:17.745690: step 93440, loss = 0.64 (256.6 examples/sec; 0.499 sec/batch)
2016-02-03 22:10:22.422448: step 93450, loss = 0.61 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 22:10:27.126914: step 93460, loss = 0.75 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 22:10:31.736714: step 93470, loss = 0.57 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 22:10:36.420889: step 93480, loss = 0.62 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 22:10:41.135720: step 93490, loss = 0.73 (261.0 examples/sec; 0.490 sec/batch)
2016-02-03 22:10:45.915317: step 93500, loss = 0.79 (259.3 examples/sec; 0.494 sec/batch)
2016-02-03 22:10:51.121049: step 93510, loss = 0.74 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 22:10:55.792629: step 93520, loss = 0.67 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 22:11:00.498675: step 93530, loss = 0.82 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 22:11:05.168150: step 93540, loss = 0.67 (291.8 examples/sec; 0.439 sec/batch)
2016-02-03 22:11:09.910265: step 93550, loss = 0.67 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 22:11:14.572953: step 93560, loss = 0.73 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 22:11:19.227204: step 93570, loss = 0.69 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 22:11:23.991099: step 93580, loss = 0.79 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 22:11:28.722151: step 93590, loss = 0.80 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 22:11:33.505199: step 93600, loss = 0.76 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 22:11:38.733714: step 93610, loss = 0.82 (306.9 examples/sec; 0.417 sec/batch)
2016-02-03 22:11:43.444869: step 93620, loss = 0.67 (287.2 examples/sec; 0.446 sec/batch)
2016-02-03 22:11:48.150445: step 93630, loss = 0.85 (283.9 examples/sec; 0.451 sec/batch)
2016-02-03 22:11:52.887079: step 93640, loss = 0.81 (252.8 examples/sec; 0.506 sec/batch)
2016-02-03 22:11:57.578663: step 93650, loss = 0.61 (254.0 examples/sec; 0.504 sec/batch)
2016-02-03 22:12:02.325380: step 93660, loss = 0.56 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 22:12:07.057732: step 93670, loss = 0.77 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 22:12:11.853638: step 93680, loss = 0.71 (253.2 examples/sec; 0.506 sec/batch)
2016-02-03 22:12:16.604026: step 93690, loss = 0.59 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 22:12:21.327485: step 93700, loss = 0.73 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 22:12:26.584750: step 93710, loss = 0.65 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 22:12:31.334257: step 93720, loss = 0.68 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 22:12:36.004984: step 93730, loss = 0.68 (293.8 examples/sec; 0.436 sec/batch)
2016-02-03 22:12:40.722949: step 93740, loss = 0.75 (258.7 examples/sec; 0.495 sec/batch)
2016-02-03 22:12:45.454528: step 93750, loss = 0.72 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 22:12:50.158696: step 93760, loss = 0.72 (238.5 examples/sec; 0.537 sec/batch)
2016-02-03 22:12:54.788900: step 93770, loss = 0.74 (259.2 examples/sec; 0.494 sec/batch)
2016-02-03 22:12:59.514974: step 93780, loss = 0.74 (277.9 examples/sec; 0.461 sec/batch)
2016-02-03 22:13:04.248528: step 93790, loss = 0.65 (249.2 examples/sec; 0.514 sec/batch)
2016-02-03 22:13:08.967854: step 93800, loss = 0.73 (258.8 examples/sec; 0.495 sec/batch)
2016-02-03 22:13:14.050473: step 93810, loss = 0.81 (287.2 examples/sec; 0.446 sec/batch)
2016-02-03 22:13:18.726254: step 93820, loss = 0.74 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 22:13:23.307729: step 93830, loss = 0.64 (293.7 examples/sec; 0.436 sec/batch)
2016-02-03 22:13:27.913339: step 93840, loss = 0.70 (256.0 examples/sec; 0.500 sec/batch)
2016-02-03 22:13:32.464312: step 93850, loss = 0.71 (303.2 examples/sec; 0.422 sec/batch)
2016-02-03 22:13:37.176105: step 93860, loss = 0.69 (302.2 examples/sec; 0.424 sec/batch)
2016-02-03 22:13:41.969680: step 93870, loss = 0.67 (260.0 examples/sec; 0.492 sec/batch)
2016-02-03 22:13:46.601496: step 93880, loss = 0.81 (283.8 examples/sec; 0.451 sec/batch)
2016-02-03 22:13:51.282938: step 93890, loss = 0.65 (300.0 examples/sec; 0.427 sec/batch)
2016-02-03 22:13:55.963807: step 93900, loss = 0.75 (259.8 examples/sec; 0.493 sec/batch)
2016-02-03 22:14:01.185516: step 93910, loss = 0.56 (262.6 examples/sec; 0.487 sec/batch)
2016-02-03 22:14:05.908757: step 93920, loss = 0.64 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 22:14:10.624908: step 93930, loss = 0.84 (254.5 examples/sec; 0.503 sec/batch)
2016-02-03 22:14:15.426278: step 93940, loss = 0.70 (260.8 examples/sec; 0.491 sec/batch)
2016-02-03 22:14:20.140661: step 93950, loss = 0.70 (268.6 examples/sec; 0.476 sec/batch)
2016-02-03 22:14:24.779303: step 93960, loss = 0.72 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 22:14:29.517754: step 93970, loss = 0.59 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 22:14:34.297208: step 93980, loss = 0.65 (260.5 examples/sec; 0.491 sec/batch)
2016-02-03 22:14:39.030931: step 93990, loss = 0.80 (267.5 examples/sec; 0.479 sec/batch)
2016-02-03 22:14:43.767180: step 94000, loss = 0.72 (246.7 examples/sec; 0.519 sec/batch)
2016-02-03 22:14:49.003819: step 94010, loss = 0.79 (291.3 examples/sec; 0.439 sec/batch)
2016-02-03 22:14:53.700561: step 94020, loss = 0.85 (265.6 examples/sec; 0.482 sec/batch)
2016-02-03 22:14:58.454379: step 94030, loss = 0.76 (261.4 examples/sec; 0.490 sec/batch)
2016-02-03 22:15:03.079282: step 94040, loss = 0.92 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 22:15:07.795457: step 94050, loss = 0.74 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 22:15:12.532814: step 94060, loss = 0.72 (262.4 examples/sec; 0.488 sec/batch)
2016-02-03 22:15:17.248740: step 94070, loss = 0.66 (280.5 examples/sec; 0.456 sec/batch)
2016-02-03 22:15:22.007076: step 94080, loss = 0.95 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 22:15:26.678179: step 94090, loss = 0.78 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 22:15:31.354689: step 94100, loss = 0.61 (247.5 examples/sec; 0.517 sec/batch)
2016-02-03 22:15:36.625016: step 94110, loss = 0.75 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 22:15:41.230437: step 94120, loss = 0.71 (299.0 examples/sec; 0.428 sec/batch)
2016-02-03 22:15:45.860811: step 94130, loss = 0.68 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 22:15:50.453493: step 94140, loss = 0.88 (257.3 examples/sec; 0.497 sec/batch)
2016-02-03 22:15:55.055169: step 94150, loss = 0.79 (260.0 examples/sec; 0.492 sec/batch)
2016-02-03 22:15:59.652456: step 94160, loss = 0.64 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 22:16:04.382176: step 94170, loss = 0.56 (266.9 examples/sec; 0.480 sec/batch)
2016-02-03 22:16:09.151412: step 94180, loss = 0.81 (271.5 examples/sec; 0.472 sec/batch)
2016-02-03 22:16:13.776974: step 94190, loss = 0.69 (285.0 examples/sec; 0.449 sec/batch)
2016-02-03 22:16:18.425671: step 94200, loss = 0.68 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 22:16:23.529778: step 94210, loss = 0.65 (291.4 examples/sec; 0.439 sec/batch)
2016-02-03 22:16:28.181897: step 94220, loss = 0.80 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 22:16:32.859813: step 94230, loss = 0.63 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 22:16:37.576273: step 94240, loss = 0.68 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 22:16:42.242061: step 94250, loss = 0.79 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 22:16:46.928379: step 94260, loss = 0.83 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 22:16:51.692159: step 94270, loss = 0.58 (254.7 examples/sec; 0.503 sec/batch)
2016-02-03 22:16:56.468281: step 94280, loss = 0.72 (251.6 examples/sec; 0.509 sec/batch)
2016-02-03 22:17:01.123724: step 94290, loss = 0.62 (295.3 examples/sec; 0.433 sec/batch)
2016-02-03 22:17:05.862032: step 94300, loss = 0.75 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 22:17:11.136571: step 94310, loss = 0.66 (255.4 examples/sec; 0.501 sec/batch)
2016-02-03 22:17:15.946510: step 94320, loss = 0.70 (256.8 examples/sec; 0.498 sec/batch)
2016-02-03 22:17:20.657927: step 94330, loss = 0.79 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 22:17:25.290304: step 94340, loss = 0.59 (298.6 examples/sec; 0.429 sec/batch)
2016-02-03 22:17:30.041784: step 94350, loss = 0.68 (290.6 examples/sec; 0.440 sec/batch)
2016-02-03 22:17:34.726583: step 94360, loss = 0.75 (260.1 examples/sec; 0.492 sec/batch)
2016-02-03 22:17:39.397019: step 94370, loss = 0.74 (265.0 examples/sec; 0.483 sec/batch)
2016-02-03 22:17:44.045085: step 94380, loss = 0.87 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 22:17:48.735642: step 94390, loss = 0.66 (263.6 examples/sec; 0.486 sec/batch)
2016-02-03 22:17:53.467101: step 94400, loss = 0.69 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 22:17:58.720790: step 94410, loss = 0.74 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 22:18:03.337848: step 94420, loss = 0.68 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 22:18:08.063942: step 94430, loss = 0.72 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 22:18:12.735955: step 94440, loss = 0.80 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 22:18:17.443784: step 94450, loss = 0.76 (260.9 examples/sec; 0.491 sec/batch)
2016-02-03 22:18:22.135926: step 94460, loss = 0.73 (275.7 examples/sec; 0.464 sec/batch)
2016-02-03 22:18:26.869527: step 94470, loss = 0.69 (287.0 examples/sec; 0.446 sec/batch)
2016-02-03 22:18:31.571048: step 94480, loss = 0.60 (260.5 examples/sec; 0.491 sec/batch)
2016-02-03 22:18:36.266710: step 94490, loss = 0.74 (288.5 examples/sec; 0.444 sec/batch)
2016-02-03 22:18:40.974180: step 94500, loss = 0.82 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 22:18:46.140434: step 94510, loss = 0.66 (291.0 examples/sec; 0.440 sec/batch)
2016-02-03 22:18:50.904118: step 94520, loss = 0.74 (255.7 examples/sec; 0.501 sec/batch)
2016-02-03 22:18:55.498624: step 94530, loss = 0.77 (304.9 examples/sec; 0.420 sec/batch)
2016-02-03 22:19:00.200140: step 94540, loss = 0.64 (295.5 examples/sec; 0.433 sec/batch)
2016-02-03 22:19:04.980792: step 94550, loss = 0.63 (253.5 examples/sec; 0.505 sec/batch)
2016-02-03 22:19:09.686244: step 94560, loss = 0.64 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 22:19:14.340647: step 94570, loss = 0.66 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 22:19:19.064691: step 94580, loss = 0.65 (249.3 examples/sec; 0.513 sec/batch)
2016-02-03 22:19:23.761163: step 94590, loss = 0.86 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 22:19:28.447463: step 94600, loss = 0.68 (288.0 examples/sec; 0.444 sec/batch)
2016-02-03 22:19:33.660786: step 94610, loss = 0.71 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 22:19:38.377209: step 94620, loss = 0.76 (269.4 examples/sec; 0.475 sec/batch)
2016-02-03 22:19:43.059682: step 94630, loss = 0.76 (254.3 examples/sec; 0.503 sec/batch)
2016-02-03 22:19:47.799385: step 94640, loss = 0.62 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 22:19:52.639159: step 94650, loss = 0.74 (256.6 examples/sec; 0.499 sec/batch)
2016-02-03 22:19:57.400403: step 94660, loss = 0.74 (233.5 examples/sec; 0.548 sec/batch)
2016-02-03 22:20:02.099256: step 94670, loss = 0.69 (247.5 examples/sec; 0.517 sec/batch)
2016-02-03 22:20:06.697250: step 94680, loss = 0.82 (300.1 examples/sec; 0.426 sec/batch)
2016-02-03 22:20:11.372202: step 94690, loss = 0.57 (293.6 examples/sec; 0.436 sec/batch)
2016-02-03 22:20:16.073975: step 94700, loss = 0.58 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 22:20:21.350171: step 94710, loss = 0.62 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 22:20:26.088504: step 94720, loss = 0.65 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 22:20:30.835557: step 94730, loss = 0.72 (276.4 examples/sec; 0.463 sec/batch)
2016-02-03 22:20:35.579095: step 94740, loss = 0.66 (264.9 examples/sec; 0.483 sec/batch)
2016-02-03 22:20:40.219519: step 94750, loss = 0.72 (285.3 examples/sec; 0.449 sec/batch)
2016-02-03 22:20:44.944984: step 94760, loss = 0.58 (289.6 examples/sec; 0.442 sec/batch)
2016-02-03 22:20:49.603144: step 94770, loss = 0.83 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 22:20:54.190581: step 94780, loss = 0.68 (289.0 examples/sec; 0.443 sec/batch)
2016-02-03 22:20:58.926640: step 94790, loss = 0.77 (263.6 examples/sec; 0.486 sec/batch)
2016-02-03 22:21:03.601389: step 94800, loss = 0.67 (281.1 examples/sec; 0.455 sec/batch)
2016-02-03 22:21:08.795745: step 94810, loss = 0.82 (261.3 examples/sec; 0.490 sec/batch)
2016-02-03 22:21:13.340371: step 94820, loss = 0.63 (263.0 examples/sec; 0.487 sec/batch)
2016-02-03 22:21:17.890223: step 94830, loss = 0.73 (280.0 examples/sec; 0.457 sec/batch)
2016-02-03 22:21:22.406797: step 94840, loss = 0.66 (316.0 examples/sec; 0.405 sec/batch)
2016-02-03 22:21:27.163021: step 94850, loss = 0.51 (254.8 examples/sec; 0.502 sec/batch)
2016-02-03 22:21:31.867879: step 94860, loss = 0.86 (261.3 examples/sec; 0.490 sec/batch)
2016-02-03 22:21:36.379808: step 94870, loss = 0.64 (286.4 examples/sec; 0.447 sec/batch)
2016-02-03 22:21:41.008182: step 94880, loss = 0.72 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 22:21:45.713951: step 94890, loss = 0.70 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 22:21:50.336761: step 94900, loss = 0.57 (284.7 examples/sec; 0.450 sec/batch)
2016-02-03 22:21:55.565513: step 94910, loss = 0.75 (259.0 examples/sec; 0.494 sec/batch)
2016-02-03 22:22:00.298620: step 94920, loss = 0.70 (255.9 examples/sec; 0.500 sec/batch)
2016-02-03 22:22:04.940273: step 94930, loss = 0.55 (283.3 examples/sec; 0.452 sec/batch)
2016-02-03 22:22:09.647816: step 94940, loss = 0.75 (258.8 examples/sec; 0.495 sec/batch)
2016-02-03 22:22:14.290672: step 94950, loss = 0.73 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 22:22:18.944712: step 94960, loss = 0.76 (253.2 examples/sec; 0.505 sec/batch)
2016-02-03 22:22:23.612865: step 94970, loss = 0.78 (281.2 examples/sec; 0.455 sec/batch)
2016-02-03 22:22:28.206159: step 94980, loss = 0.61 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 22:22:32.928351: step 94990, loss = 0.67 (270.5 examples/sec; 0.473 sec/batch)
2016-02-03 22:22:37.472576: step 95000, loss = 0.66 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 22:22:42.589750: step 95010, loss = 0.89 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 22:22:47.352254: step 95020, loss = 0.76 (260.3 examples/sec; 0.492 sec/batch)
2016-02-03 22:22:51.956644: step 95030, loss = 0.67 (283.0 examples/sec; 0.452 sec/batch)
2016-02-03 22:22:56.695484: step 95040, loss = 0.75 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 22:23:01.300732: step 95050, loss = 0.68 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 22:23:06.011734: step 95060, loss = 0.70 (285.3 examples/sec; 0.449 sec/batch)
2016-02-03 22:23:10.657467: step 95070, loss = 0.68 (281.7 examples/sec; 0.454 sec/batch)
2016-02-03 22:23:15.392381: step 95080, loss = 0.66 (265.0 examples/sec; 0.483 sec/batch)
2016-02-03 22:23:20.065290: step 95090, loss = 0.75 (284.2 examples/sec; 0.450 sec/batch)
2016-02-03 22:23:24.752270: step 95100, loss = 0.68 (301.5 examples/sec; 0.425 sec/batch)
2016-02-03 22:23:29.803075: step 95110, loss = 0.70 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 22:23:34.492427: step 95120, loss = 0.75 (276.4 examples/sec; 0.463 sec/batch)
2016-02-03 22:23:39.025830: step 95130, loss = 0.81 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 22:23:43.661511: step 95140, loss = 0.63 (279.2 examples/sec; 0.458 sec/batch)
2016-02-03 22:23:48.293192: step 95150, loss = 0.65 (264.1 examples/sec; 0.485 sec/batch)
2016-02-03 22:23:53.007087: step 95160, loss = 0.75 (263.1 examples/sec; 0.486 sec/batch)
2016-02-03 22:23:57.553245: step 95170, loss = 0.71 (284.9 examples/sec; 0.449 sec/batch)
2016-02-03 22:24:02.219179: step 95180, loss = 0.70 (256.7 examples/sec; 0.499 sec/batch)
2016-02-03 22:24:06.875817: step 95190, loss = 0.67 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 22:24:11.609888: step 95200, loss = 0.67 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 22:24:16.940691: step 95210, loss = 0.68 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 22:24:21.589881: step 95220, loss = 0.73 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 22:24:26.250035: step 95230, loss = 0.67 (268.6 examples/sec; 0.477 sec/batch)
2016-02-03 22:24:30.938371: step 95240, loss = 0.70 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 22:24:35.724751: step 95250, loss = 0.80 (259.7 examples/sec; 0.493 sec/batch)
2016-02-03 22:24:40.338672: step 95260, loss = 0.72 (291.0 examples/sec; 0.440 sec/batch)
2016-02-03 22:24:45.036841: step 95270, loss = 0.73 (281.0 examples/sec; 0.455 sec/batch)
2016-02-03 22:24:49.781672: step 95280, loss = 0.72 (262.6 examples/sec; 0.487 sec/batch)
2016-02-03 22:24:54.399998: step 95290, loss = 0.56 (277.7 examples/sec; 0.461 sec/batch)
2016-02-03 22:24:59.128601: step 95300, loss = 0.54 (281.1 examples/sec; 0.455 sec/batch)
2016-02-03 22:25:04.364551: step 95310, loss = 0.73 (265.2 examples/sec; 0.483 sec/batch)
2016-02-03 22:25:09.128607: step 95320, loss = 0.70 (294.8 examples/sec; 0.434 sec/batch)
2016-02-03 22:25:13.813849: step 95330, loss = 0.83 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 22:25:18.613707: step 95340, loss = 0.74 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 22:25:23.270902: step 95350, loss = 0.72 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 22:25:27.814526: step 95360, loss = 0.62 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 22:25:32.290372: step 95370, loss = 0.72 (296.2 examples/sec; 0.432 sec/batch)
2016-02-03 22:25:36.905583: step 95380, loss = 0.75 (286.8 examples/sec; 0.446 sec/batch)
2016-02-03 22:25:41.520889: step 95390, loss = 0.66 (241.1 examples/sec; 0.531 sec/batch)
2016-02-03 22:25:46.042022: step 95400, loss = 0.81 (293.7 examples/sec; 0.436 sec/batch)
2016-02-03 22:25:51.080122: step 95410, loss = 0.48 (254.0 examples/sec; 0.504 sec/batch)
2016-02-03 22:25:55.523882: step 95420, loss = 0.75 (285.5 examples/sec; 0.448 sec/batch)
2016-02-03 22:26:00.223509: step 95430, loss = 0.69 (280.5 examples/sec; 0.456 sec/batch)
2016-02-03 22:26:04.806478: step 95440, loss = 0.64 (287.0 examples/sec; 0.446 sec/batch)
2016-02-03 22:26:09.415323: step 95450, loss = 0.64 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 22:26:14.129638: step 95460, loss = 0.84 (259.9 examples/sec; 0.493 sec/batch)
2016-02-03 22:26:18.555110: step 95470, loss = 0.71 (337.5 examples/sec; 0.379 sec/batch)
2016-02-03 22:26:23.213343: step 95480, loss = 0.75 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 22:26:27.852330: step 95490, loss = 0.71 (276.6 examples/sec; 0.463 sec/batch)
2016-02-03 22:26:32.524022: step 95500, loss = 0.75 (259.8 examples/sec; 0.493 sec/batch)
2016-02-03 22:26:37.737002: step 95510, loss = 0.80 (262.1 examples/sec; 0.488 sec/batch)
2016-02-03 22:26:42.424094: step 95520, loss = 0.69 (284.5 examples/sec; 0.450 sec/batch)
2016-02-03 22:26:47.127779: step 95530, loss = 0.67 (284.0 examples/sec; 0.451 sec/batch)
2016-02-03 22:26:51.920150: step 95540, loss = 0.67 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 22:26:56.548818: step 95550, loss = 0.80 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 22:27:01.227111: step 95560, loss = 0.70 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 22:27:06.064784: step 95570, loss = 0.62 (245.0 examples/sec; 0.523 sec/batch)
2016-02-03 22:27:10.726491: step 95580, loss = 0.69 (276.7 examples/sec; 0.463 sec/batch)
2016-02-03 22:27:15.379658: step 95590, loss = 0.68 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 22:27:20.068938: step 95600, loss = 0.78 (254.7 examples/sec; 0.503 sec/batch)
2016-02-03 22:27:25.251416: step 95610, loss = 0.80 (264.7 examples/sec; 0.484 sec/batch)
2016-02-03 22:27:29.954990: step 95620, loss = 0.59 (295.0 examples/sec; 0.434 sec/batch)
2016-02-03 22:27:34.641990: step 95630, loss = 0.69 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 22:27:39.388502: step 95640, loss = 0.63 (255.0 examples/sec; 0.502 sec/batch)
2016-02-03 22:27:44.127317: step 95650, loss = 0.57 (245.2 examples/sec; 0.522 sec/batch)
2016-02-03 22:27:48.790328: step 95660, loss = 0.82 (277.8 examples/sec; 0.461 sec/batch)
2016-02-03 22:27:53.516903: step 95670, loss = 0.79 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 22:27:58.269104: step 95680, loss = 0.64 (265.8 examples/sec; 0.482 sec/batch)
2016-02-03 22:28:03.067343: step 95690, loss = 0.76 (262.2 examples/sec; 0.488 sec/batch)
2016-02-03 22:28:07.777786: step 95700, loss = 0.63 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 22:28:12.980911: step 95710, loss = 0.66 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 22:28:17.712195: step 95720, loss = 0.63 (259.3 examples/sec; 0.494 sec/batch)
2016-02-03 22:28:22.417691: step 95730, loss = 0.57 (273.8 examples/sec; 0.468 sec/batch)
2016-02-03 22:28:27.174575: step 95740, loss = 0.77 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 22:28:31.912381: step 95750, loss = 0.67 (295.6 examples/sec; 0.433 sec/batch)
2016-02-03 22:28:36.724348: step 95760, loss = 0.83 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 22:28:41.457665: step 95770, loss = 0.64 (255.8 examples/sec; 0.500 sec/batch)
2016-02-03 22:28:46.065126: step 95780, loss = 0.73 (301.6 examples/sec; 0.424 sec/batch)
2016-02-03 22:28:50.755519: step 95790, loss = 0.76 (286.1 examples/sec; 0.447 sec/batch)
2016-02-03 22:28:55.457142: step 95800, loss = 0.71 (276.4 examples/sec; 0.463 sec/batch)
2016-02-03 22:29:00.692679: step 95810, loss = 0.70 (287.9 examples/sec; 0.445 sec/batch)
2016-02-03 22:29:05.420324: step 95820, loss = 0.62 (258.0 examples/sec; 0.496 sec/batch)
2016-02-03 22:29:10.145231: step 95830, loss = 0.77 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 22:29:14.970556: step 95840, loss = 0.75 (256.6 examples/sec; 0.499 sec/batch)
2016-02-03 22:29:19.743982: step 95850, loss = 0.64 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 22:29:24.470660: step 95860, loss = 0.72 (270.7 examples/sec; 0.473 sec/batch)
2016-02-03 22:29:29.251385: step 95870, loss = 0.64 (276.2 examples/sec; 0.464 sec/batch)
2016-02-03 22:29:33.968250: step 95880, loss = 0.65 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 22:29:38.716420: step 95890, loss = 0.59 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 22:29:43.512619: step 95900, loss = 0.71 (256.2 examples/sec; 0.500 sec/batch)
2016-02-03 22:29:48.831485: step 95910, loss = 0.68 (262.1 examples/sec; 0.488 sec/batch)
2016-02-03 22:29:53.529689: step 95920, loss = 0.85 (285.0 examples/sec; 0.449 sec/batch)
2016-02-03 22:29:58.312930: step 95930, loss = 0.76 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 22:30:03.038446: step 95940, loss = 0.65 (263.9 examples/sec; 0.485 sec/batch)
2016-02-03 22:30:07.753540: step 95950, loss = 0.74 (291.6 examples/sec; 0.439 sec/batch)
2016-02-03 22:30:12.583468: step 95960, loss = 0.68 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 22:30:17.322787: step 95970, loss = 0.68 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 22:30:22.066525: step 95980, loss = 0.72 (288.9 examples/sec; 0.443 sec/batch)
2016-02-03 22:30:26.831941: step 95990, loss = 0.82 (253.6 examples/sec; 0.505 sec/batch)
2016-02-03 22:30:31.474800: step 96000, loss = 0.68 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 22:30:36.683573: step 96010, loss = 0.60 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 22:30:41.331152: step 96020, loss = 0.62 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 22:30:46.022386: step 96030, loss = 0.63 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 22:30:50.696627: step 96040, loss = 0.65 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 22:30:55.292519: step 96050, loss = 0.74 (277.9 examples/sec; 0.461 sec/batch)
2016-02-03 22:31:00.066664: step 96060, loss = 0.59 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 22:31:04.796033: step 96070, loss = 0.57 (251.6 examples/sec; 0.509 sec/batch)
2016-02-03 22:31:09.533848: step 96080, loss = 0.88 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 22:31:14.197065: step 96090, loss = 0.61 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 22:31:18.946292: step 96100, loss = 0.71 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 22:31:24.253096: step 96110, loss = 0.87 (275.4 examples/sec; 0.465 sec/batch)
2016-02-03 22:31:28.969037: step 96120, loss = 0.80 (258.3 examples/sec; 0.496 sec/batch)
2016-02-03 22:31:33.616050: step 96130, loss = 0.71 (304.5 examples/sec; 0.420 sec/batch)
2016-02-03 22:31:38.320933: step 96140, loss = 0.60 (289.3 examples/sec; 0.442 sec/batch)
2016-02-03 22:31:43.050943: step 96150, loss = 0.79 (252.5 examples/sec; 0.507 sec/batch)
2016-02-03 22:31:47.690665: step 96160, loss = 0.78 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 22:31:52.316101: step 96170, loss = 0.73 (281.2 examples/sec; 0.455 sec/batch)
2016-02-03 22:31:56.975021: step 96180, loss = 0.60 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 22:32:01.753712: step 96190, loss = 0.70 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 22:32:06.401233: step 96200, loss = 0.65 (283.1 examples/sec; 0.452 sec/batch)
2016-02-03 22:32:11.583293: step 96210, loss = 0.62 (287.8 examples/sec; 0.445 sec/batch)
2016-02-03 22:32:16.330513: step 96220, loss = 0.59 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 22:32:21.061683: step 96230, loss = 0.62 (293.6 examples/sec; 0.436 sec/batch)
2016-02-03 22:32:25.795066: step 96240, loss = 0.65 (299.0 examples/sec; 0.428 sec/batch)
2016-02-03 22:32:30.534229: step 96250, loss = 0.64 (249.2 examples/sec; 0.514 sec/batch)
2016-02-03 22:32:35.165838: step 96260, loss = 0.74 (276.7 examples/sec; 0.463 sec/batch)
2016-02-03 22:32:39.890351: step 96270, loss = 0.77 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 22:32:44.541083: step 96280, loss = 0.77 (287.4 examples/sec; 0.445 sec/batch)
2016-02-03 22:32:49.265400: step 96290, loss = 0.73 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 22:32:54.057385: step 96300, loss = 0.77 (260.9 examples/sec; 0.491 sec/batch)
2016-02-03 22:32:59.280682: step 96310, loss = 0.74 (288.1 examples/sec; 0.444 sec/batch)
2016-02-03 22:33:03.942478: step 96320, loss = 0.82 (294.0 examples/sec; 0.435 sec/batch)
2016-02-03 22:33:08.664183: step 96330, loss = 0.68 (251.7 examples/sec; 0.509 sec/batch)
2016-02-03 22:33:13.294683: step 96340, loss = 0.62 (252.0 examples/sec; 0.508 sec/batch)
2016-02-03 22:33:17.984817: step 96350, loss = 0.53 (259.8 examples/sec; 0.493 sec/batch)
2016-02-03 22:33:22.749823: step 96360, loss = 0.87 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 22:33:27.527096: step 96370, loss = 0.60 (276.3 examples/sec; 0.463 sec/batch)
2016-02-03 22:33:32.278099: step 96380, loss = 0.79 (258.7 examples/sec; 0.495 sec/batch)
2016-02-03 22:33:37.054103: step 96390, loss = 0.75 (262.9 examples/sec; 0.487 sec/batch)
2016-02-03 22:33:41.769792: step 96400, loss = 0.75 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 22:33:46.952758: step 96410, loss = 0.68 (254.3 examples/sec; 0.503 sec/batch)
2016-02-03 22:33:51.738651: step 96420, loss = 0.66 (245.2 examples/sec; 0.522 sec/batch)
2016-02-03 22:33:56.494186: step 96430, loss = 0.67 (258.6 examples/sec; 0.495 sec/batch)
2016-02-03 22:34:01.316848: step 96440, loss = 0.68 (252.8 examples/sec; 0.506 sec/batch)
2016-02-03 22:34:06.013835: step 96450, loss = 0.71 (283.1 examples/sec; 0.452 sec/batch)
2016-02-03 22:34:10.644176: step 96460, loss = 0.69 (302.5 examples/sec; 0.423 sec/batch)
2016-02-03 22:34:15.339549: step 96470, loss = 0.75 (301.1 examples/sec; 0.425 sec/batch)
2016-02-03 22:34:20.045134: step 96480, loss = 0.86 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 22:34:24.639456: step 96490, loss = 0.65 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 22:34:29.256020: step 96500, loss = 0.61 (259.2 examples/sec; 0.494 sec/batch)
2016-02-03 22:34:34.472775: step 96510, loss = 0.67 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 22:34:39.138030: step 96520, loss = 0.68 (255.3 examples/sec; 0.501 sec/batch)
2016-02-03 22:34:43.860071: step 96530, loss = 0.69 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 22:34:48.572156: step 96540, loss = 0.67 (266.3 examples/sec; 0.481 sec/batch)
2016-02-03 22:34:53.308843: step 96550, loss = 0.63 (291.5 examples/sec; 0.439 sec/batch)
2016-02-03 22:34:58.009109: step 96560, loss = 0.75 (288.2 examples/sec; 0.444 sec/batch)
2016-02-03 22:35:02.801318: step 96570, loss = 0.69 (258.4 examples/sec; 0.495 sec/batch)
2016-02-03 22:35:07.518228: step 96580, loss = 0.63 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 22:35:12.274716: step 96590, loss = 0.74 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 22:35:17.057984: step 96600, loss = 0.75 (250.2 examples/sec; 0.512 sec/batch)
2016-02-03 22:35:22.237131: step 96610, loss = 0.63 (261.3 examples/sec; 0.490 sec/batch)
2016-02-03 22:35:26.908792: step 96620, loss = 0.80 (291.8 examples/sec; 0.439 sec/batch)
2016-02-03 22:35:31.654568: step 96630, loss = 0.69 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 22:35:36.387717: step 96640, loss = 0.51 (301.4 examples/sec; 0.425 sec/batch)
2016-02-03 22:35:41.109592: step 96650, loss = 0.80 (250.7 examples/sec; 0.511 sec/batch)
2016-02-03 22:35:45.883071: step 96660, loss = 0.70 (271.5 examples/sec; 0.472 sec/batch)
2016-02-03 22:35:50.659703: step 96670, loss = 0.79 (289.2 examples/sec; 0.443 sec/batch)
2016-02-03 22:35:55.395764: step 96680, loss = 0.82 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 22:36:00.076826: step 96690, loss = 0.61 (261.5 examples/sec; 0.489 sec/batch)
2016-02-03 22:36:04.863556: step 96700, loss = 0.79 (252.2 examples/sec; 0.507 sec/batch)
2016-02-03 22:36:10.144997: step 96710, loss = 0.67 (259.7 examples/sec; 0.493 sec/batch)
2016-02-03 22:36:14.862769: step 96720, loss = 0.68 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 22:36:19.627501: step 96730, loss = 0.63 (257.8 examples/sec; 0.497 sec/batch)
2016-02-03 22:36:24.348712: step 96740, loss = 0.61 (261.3 examples/sec; 0.490 sec/batch)
2016-02-03 22:36:29.055943: step 96750, loss = 0.81 (261.0 examples/sec; 0.490 sec/batch)
2016-02-03 22:36:33.795045: step 96760, loss = 0.64 (280.9 examples/sec; 0.456 sec/batch)
2016-02-03 22:36:38.472872: step 96770, loss = 0.79 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 22:36:43.243345: step 96780, loss = 0.79 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 22:36:48.070772: step 96790, loss = 0.53 (257.3 examples/sec; 0.497 sec/batch)
2016-02-03 22:36:52.802622: step 96800, loss = 0.84 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 22:36:58.061028: step 96810, loss = 0.79 (257.1 examples/sec; 0.498 sec/batch)
2016-02-03 22:37:02.809211: step 96820, loss = 0.67 (247.4 examples/sec; 0.517 sec/batch)
2016-02-03 22:37:07.547268: step 96830, loss = 0.53 (287.5 examples/sec; 0.445 sec/batch)
2016-02-03 22:37:12.274581: step 96840, loss = 0.58 (260.8 examples/sec; 0.491 sec/batch)
2016-02-03 22:37:16.959352: step 96850, loss = 0.77 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 22:37:21.697547: step 96860, loss = 0.53 (262.6 examples/sec; 0.487 sec/batch)
2016-02-03 22:37:26.422110: step 96870, loss = 0.63 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 22:37:31.191045: step 96880, loss = 0.88 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 22:37:35.949692: step 96890, loss = 0.68 (248.8 examples/sec; 0.514 sec/batch)
2016-02-03 22:37:40.682918: step 96900, loss = 0.72 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 22:37:46.005532: step 96910, loss = 0.74 (252.5 examples/sec; 0.507 sec/batch)
2016-02-03 22:37:50.708160: step 96920, loss = 0.68 (263.0 examples/sec; 0.487 sec/batch)
2016-02-03 22:37:55.410039: step 96930, loss = 0.75 (272.6 examples/sec; 0.470 sec/batch)
2016-02-03 22:38:00.132224: step 96940, loss = 0.69 (265.8 examples/sec; 0.482 sec/batch)
2016-02-03 22:38:04.851291: step 96950, loss = 0.63 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 22:38:09.499994: step 96960, loss = 0.76 (288.0 examples/sec; 0.444 sec/batch)
2016-02-03 22:38:14.211240: step 96970, loss = 0.63 (282.4 examples/sec; 0.453 sec/batch)
2016-02-03 22:38:18.985120: step 96980, loss = 0.82 (287.2 examples/sec; 0.446 sec/batch)
2016-02-03 22:38:23.686889: step 96990, loss = 0.74 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 22:38:28.402923: step 97000, loss = 0.76 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 22:38:33.604573: step 97010, loss = 0.61 (289.0 examples/sec; 0.443 sec/batch)
2016-02-03 22:38:38.310270: step 97020, loss = 0.76 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 22:38:43.040734: step 97030, loss = 0.73 (298.8 examples/sec; 0.428 sec/batch)
2016-02-03 22:38:47.739959: step 97040, loss = 0.71 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 22:38:52.466153: step 97050, loss = 0.55 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 22:38:57.213837: step 97060, loss = 0.77 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 22:39:01.955774: step 97070, loss = 0.70 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 22:39:06.687115: step 97080, loss = 0.67 (252.7 examples/sec; 0.507 sec/batch)
2016-02-03 22:39:11.491246: step 97090, loss = 0.76 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 22:39:16.190447: step 97100, loss = 0.73 (291.6 examples/sec; 0.439 sec/batch)
2016-02-03 22:39:21.421085: step 97110, loss = 0.65 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 22:39:26.266942: step 97120, loss = 0.81 (253.6 examples/sec; 0.505 sec/batch)
2016-02-03 22:39:30.974292: step 97130, loss = 0.83 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 22:39:35.705848: step 97140, loss = 0.66 (256.2 examples/sec; 0.500 sec/batch)
2016-02-03 22:39:40.386884: step 97150, loss = 0.60 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 22:39:45.124720: step 97160, loss = 0.65 (258.7 examples/sec; 0.495 sec/batch)
2016-02-03 22:39:49.872337: step 97170, loss = 0.53 (275.6 examples/sec; 0.464 sec/batch)
2016-02-03 22:39:54.692681: step 97180, loss = 0.79 (262.6 examples/sec; 0.487 sec/batch)
2016-02-03 22:39:59.378165: step 97190, loss = 0.85 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 22:40:04.133396: step 97200, loss = 0.74 (273.8 examples/sec; 0.468 sec/batch)
2016-02-03 22:40:09.432357: step 97210, loss = 0.59 (259.0 examples/sec; 0.494 sec/batch)
2016-02-03 22:40:14.206304: step 97220, loss = 0.71 (256.8 examples/sec; 0.498 sec/batch)
2016-02-03 22:40:19.018716: step 97230, loss = 0.58 (253.4 examples/sec; 0.505 sec/batch)
2016-02-03 22:40:23.709941: step 97240, loss = 0.67 (265.2 examples/sec; 0.483 sec/batch)
2016-02-03 22:40:28.506157: step 97250, loss = 0.72 (255.0 examples/sec; 0.502 sec/batch)
2016-02-03 22:40:33.236779: step 97260, loss = 0.73 (263.0 examples/sec; 0.487 sec/batch)
2016-02-03 22:40:37.974768: step 97270, loss = 0.68 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 22:40:42.686473: step 97280, loss = 0.70 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 22:40:47.400441: step 97290, loss = 0.71 (287.4 examples/sec; 0.445 sec/batch)
2016-02-03 22:40:52.090777: step 97300, loss = 0.62 (257.4 examples/sec; 0.497 sec/batch)
2016-02-03 22:40:57.383094: step 97310, loss = 0.88 (244.3 examples/sec; 0.524 sec/batch)
2016-02-03 22:41:02.091894: step 97320, loss = 0.68 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 22:41:06.771781: step 97330, loss = 0.71 (285.8 examples/sec; 0.448 sec/batch)
2016-02-03 22:41:11.509632: step 97340, loss = 0.66 (263.2 examples/sec; 0.486 sec/batch)
2016-02-03 22:41:16.230491: step 97350, loss = 0.65 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 22:41:20.957089: step 97360, loss = 0.71 (283.5 examples/sec; 0.452 sec/batch)
2016-02-03 22:41:25.740516: step 97370, loss = 0.68 (280.5 examples/sec; 0.456 sec/batch)
2016-02-03 22:41:30.487721: step 97380, loss = 0.75 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 22:41:35.211663: step 97390, loss = 0.85 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 22:41:39.930258: step 97400, loss = 0.75 (300.5 examples/sec; 0.426 sec/batch)
2016-02-03 22:41:45.250631: step 97410, loss = 0.59 (250.0 examples/sec; 0.512 sec/batch)
2016-02-03 22:41:50.008092: step 97420, loss = 0.66 (260.1 examples/sec; 0.492 sec/batch)
2016-02-03 22:41:54.673818: step 97430, loss = 0.75 (310.7 examples/sec; 0.412 sec/batch)
2016-02-03 22:41:59.418641: step 97440, loss = 0.74 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 22:42:04.087353: step 97450, loss = 0.78 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 22:42:08.737633: step 97460, loss = 0.69 (285.8 examples/sec; 0.448 sec/batch)
2016-02-03 22:42:13.452329: step 97470, loss = 0.70 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 22:42:18.186409: step 97480, loss = 0.68 (251.3 examples/sec; 0.509 sec/batch)
2016-02-03 22:42:22.836834: step 97490, loss = 0.75 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 22:42:27.521626: step 97500, loss = 0.68 (275.0 examples/sec; 0.465 sec/batch)
2016-02-03 22:42:32.827083: step 97510, loss = 0.64 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 22:42:37.519152: step 97520, loss = 0.69 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 22:42:42.267925: step 97530, loss = 0.66 (286.8 examples/sec; 0.446 sec/batch)
2016-02-03 22:42:47.027951: step 97540, loss = 0.64 (257.6 examples/sec; 0.497 sec/batch)
2016-02-03 22:42:51.762209: step 97550, loss = 0.77 (253.1 examples/sec; 0.506 sec/batch)
2016-02-03 22:42:56.459489: step 97560, loss = 0.69 (281.4 examples/sec; 0.455 sec/batch)
2016-02-03 22:43:01.222820: step 97570, loss = 0.68 (271.1 examples/sec; 0.472 sec/batch)
2016-02-03 22:43:05.937519: step 97580, loss = 0.67 (287.8 examples/sec; 0.445 sec/batch)
2016-02-03 22:43:10.675686: step 97590, loss = 0.68 (253.1 examples/sec; 0.506 sec/batch)
2016-02-03 22:43:15.408744: step 97600, loss = 0.78 (259.2 examples/sec; 0.494 sec/batch)
2016-02-03 22:43:20.716258: step 97610, loss = 0.60 (253.5 examples/sec; 0.505 sec/batch)
2016-02-03 22:43:25.491431: step 97620, loss = 0.77 (266.3 examples/sec; 0.481 sec/batch)
2016-02-03 22:43:30.306164: step 97630, loss = 0.75 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 22:43:35.012852: step 97640, loss = 0.73 (288.6 examples/sec; 0.444 sec/batch)
2016-02-03 22:43:39.831537: step 97650, loss = 0.65 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 22:43:44.583125: step 97660, loss = 0.80 (258.4 examples/sec; 0.495 sec/batch)
2016-02-03 22:43:49.312489: step 97670, loss = 0.85 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 22:43:53.994486: step 97680, loss = 0.66 (284.2 examples/sec; 0.450 sec/batch)
2016-02-03 22:43:58.729728: step 97690, loss = 0.64 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 22:44:03.355939: step 97700, loss = 0.83 (292.1 examples/sec; 0.438 sec/batch)
2016-02-03 22:44:08.534363: step 97710, loss = 0.76 (305.9 examples/sec; 0.418 sec/batch)
2016-02-03 22:44:13.285705: step 97720, loss = 0.64 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 22:44:17.978205: step 97730, loss = 0.65 (279.6 examples/sec; 0.458 sec/batch)
2016-02-03 22:44:22.708807: step 97740, loss = 0.79 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 22:44:27.435607: step 97750, loss = 0.70 (265.4 examples/sec; 0.482 sec/batch)
2016-02-03 22:44:32.125778: step 97760, loss = 0.66 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 22:44:36.865589: step 97770, loss = 0.67 (245.3 examples/sec; 0.522 sec/batch)
2016-02-03 22:44:41.556814: step 97780, loss = 0.74 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 22:44:46.334135: step 97790, loss = 0.67 (303.2 examples/sec; 0.422 sec/batch)
2016-02-03 22:44:51.064289: step 97800, loss = 0.66 (286.5 examples/sec; 0.447 sec/batch)
2016-02-03 22:44:56.277335: step 97810, loss = 0.61 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 22:45:00.970028: step 97820, loss = 0.81 (260.4 examples/sec; 0.492 sec/batch)
2016-02-03 22:45:05.577026: step 97830, loss = 0.90 (289.9 examples/sec; 0.441 sec/batch)
2016-02-03 22:45:10.296907: step 97840, loss = 0.71 (278.5 examples/sec; 0.460 sec/batch)
2016-02-03 22:45:15.020718: step 97850, loss = 0.76 (257.9 examples/sec; 0.496 sec/batch)
2016-02-03 22:45:19.745552: step 97860, loss = 0.71 (253.0 examples/sec; 0.506 sec/batch)
2016-02-03 22:45:24.466379: step 97870, loss = 0.58 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 22:45:29.251623: step 97880, loss = 0.82 (255.5 examples/sec; 0.501 sec/batch)
2016-02-03 22:45:33.969453: step 97890, loss = 0.57 (258.3 examples/sec; 0.496 sec/batch)
2016-02-03 22:45:38.642491: step 97900, loss = 0.81 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 22:45:43.943750: step 97910, loss = 0.72 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 22:45:48.693939: step 97920, loss = 0.74 (282.5 examples/sec; 0.453 sec/batch)
2016-02-03 22:45:53.460671: step 97930, loss = 0.62 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 22:45:58.133470: step 97940, loss = 0.63 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 22:46:02.838269: step 97950, loss = 0.79 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 22:46:07.545065: step 97960, loss = 0.72 (247.7 examples/sec; 0.517 sec/batch)
2016-02-03 22:46:12.250624: step 97970, loss = 0.63 (273.3 examples/sec; 0.468 sec/batch)
2016-02-03 22:46:17.001061: step 97980, loss = 0.63 (260.9 examples/sec; 0.491 sec/batch)
2016-02-03 22:46:21.697206: step 97990, loss = 0.69 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 22:46:26.367364: step 98000, loss = 0.58 (333.5 examples/sec; 0.384 sec/batch)
2016-02-03 22:46:31.634881: step 98010, loss = 0.59 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 22:46:36.304737: step 98020, loss = 0.62 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 22:46:41.071689: step 98030, loss = 0.77 (256.2 examples/sec; 0.500 sec/batch)
2016-02-03 22:46:45.865382: step 98040, loss = 0.77 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 22:46:50.591802: step 98050, loss = 0.60 (258.7 examples/sec; 0.495 sec/batch)
2016-02-03 22:46:55.341067: step 98060, loss = 0.75 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 22:46:59.976134: step 98070, loss = 0.79 (286.1 examples/sec; 0.447 sec/batch)
2016-02-03 22:47:04.725901: step 98080, loss = 0.73 (265.2 examples/sec; 0.483 sec/batch)
2016-02-03 22:47:09.543753: step 98090, loss = 0.69 (254.7 examples/sec; 0.503 sec/batch)
2016-02-03 22:47:14.310071: step 98100, loss = 0.64 (259.8 examples/sec; 0.493 sec/batch)
2016-02-03 22:47:19.602214: step 98110, loss = 0.67 (244.9 examples/sec; 0.523 sec/batch)
2016-02-03 22:47:24.216376: step 98120, loss = 0.78 (297.8 examples/sec; 0.430 sec/batch)
2016-02-03 22:47:28.953351: step 98130, loss = 0.67 (273.2 examples/sec; 0.469 sec/batch)
2016-02-03 22:47:33.583636: step 98140, loss = 0.73 (273.2 examples/sec; 0.469 sec/batch)
2016-02-03 22:47:38.349132: step 98150, loss = 0.71 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 22:47:43.086860: step 98160, loss = 0.61 (250.0 examples/sec; 0.512 sec/batch)
2016-02-03 22:47:47.716282: step 98170, loss = 0.76 (288.2 examples/sec; 0.444 sec/batch)
2016-02-03 22:47:52.318784: step 98180, loss = 0.65 (268.2 examples/sec; 0.477 sec/batch)
2016-02-03 22:47:57.004856: step 98190, loss = 0.64 (297.0 examples/sec; 0.431 sec/batch)
2016-02-03 22:48:01.726548: step 98200, loss = 0.56 (259.7 examples/sec; 0.493 sec/batch)
2016-02-03 22:48:06.942734: step 98210, loss = 0.78 (280.3 examples/sec; 0.457 sec/batch)
2016-02-03 22:48:11.680126: step 98220, loss = 0.75 (278.8 examples/sec; 0.459 sec/batch)
2016-02-03 22:48:16.501988: step 98230, loss = 0.71 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 22:48:21.211398: step 98240, loss = 0.73 (261.9 examples/sec; 0.489 sec/batch)
2016-02-03 22:48:25.846481: step 98250, loss = 0.58 (308.4 examples/sec; 0.415 sec/batch)
2016-02-03 22:48:30.548630: step 98260, loss = 0.90 (275.7 examples/sec; 0.464 sec/batch)
2016-02-03 22:48:35.231115: step 98270, loss = 0.67 (268.6 examples/sec; 0.477 sec/batch)
2016-02-03 22:48:39.940609: step 98280, loss = 0.75 (301.8 examples/sec; 0.424 sec/batch)
2016-02-03 22:48:44.485819: step 98290, loss = 0.76 (276.6 examples/sec; 0.463 sec/batch)
2016-02-03 22:48:49.249570: step 98300, loss = 0.55 (260.0 examples/sec; 0.492 sec/batch)
2016-02-03 22:48:54.428958: step 98310, loss = 0.78 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 22:48:59.173419: step 98320, loss = 0.67 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 22:49:03.824493: step 98330, loss = 0.80 (289.9 examples/sec; 0.442 sec/batch)
2016-02-03 22:49:08.583578: step 98340, loss = 0.58 (279.6 examples/sec; 0.458 sec/batch)
2016-02-03 22:49:13.361921: step 98350, loss = 0.61 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 22:49:17.985545: step 98360, loss = 0.72 (287.8 examples/sec; 0.445 sec/batch)
2016-02-03 22:49:22.732030: step 98370, loss = 0.65 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 22:49:27.437488: step 98380, loss = 0.73 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 22:49:32.134382: step 98390, loss = 0.62 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 22:49:36.819770: step 98400, loss = 0.63 (259.4 examples/sec; 0.493 sec/batch)
2016-02-03 22:49:42.038932: step 98410, loss = 0.68 (295.5 examples/sec; 0.433 sec/batch)
2016-02-03 22:49:46.724879: step 98420, loss = 0.71 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 22:49:51.432939: step 98430, loss = 0.65 (287.2 examples/sec; 0.446 sec/batch)
2016-02-03 22:49:56.109210: step 98440, loss = 0.70 (254.0 examples/sec; 0.504 sec/batch)
2016-02-03 22:50:00.869956: step 98450, loss = 0.85 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 22:50:05.483190: step 98460, loss = 0.73 (289.1 examples/sec; 0.443 sec/batch)
2016-02-03 22:50:10.221074: step 98470, loss = 0.65 (275.6 examples/sec; 0.464 sec/batch)
2016-02-03 22:50:14.908788: step 98480, loss = 0.77 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 22:50:19.641658: step 98490, loss = 0.57 (257.8 examples/sec; 0.497 sec/batch)
2016-02-03 22:50:24.303966: step 98500, loss = 0.65 (290.9 examples/sec; 0.440 sec/batch)
2016-02-03 22:50:29.592422: step 98510, loss = 0.70 (260.4 examples/sec; 0.492 sec/batch)
2016-02-03 22:50:34.283812: step 98520, loss = 0.62 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 22:50:38.967212: step 98530, loss = 0.67 (252.6 examples/sec; 0.507 sec/batch)
2016-02-03 22:50:43.590532: step 98540, loss = 0.66 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 22:50:48.328905: step 98550, loss = 0.74 (299.6 examples/sec; 0.427 sec/batch)
2016-02-03 22:50:52.965033: step 98560, loss = 0.72 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 22:50:57.605653: step 98570, loss = 0.72 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 22:51:02.305788: step 98580, loss = 0.84 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 22:51:07.042991: step 98590, loss = 0.82 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 22:51:11.804021: step 98600, loss = 0.57 (281.0 examples/sec; 0.455 sec/batch)
2016-02-03 22:51:17.106581: step 98610, loss = 0.62 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 22:51:21.751332: step 98620, loss = 0.66 (280.5 examples/sec; 0.456 sec/batch)
2016-02-03 22:51:26.557664: step 98630, loss = 0.73 (272.5 examples/sec; 0.470 sec/batch)
2016-02-03 22:51:31.342351: step 98640, loss = 0.74 (263.7 examples/sec; 0.485 sec/batch)
2016-02-03 22:51:36.100525: step 98650, loss = 0.56 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 22:51:40.843126: step 98660, loss = 0.74 (251.4 examples/sec; 0.509 sec/batch)
2016-02-03 22:51:45.594709: step 98670, loss = 0.69 (243.0 examples/sec; 0.527 sec/batch)
2016-02-03 22:51:50.154181: step 98680, loss = 0.66 (264.3 examples/sec; 0.484 sec/batch)
2016-02-03 22:51:54.902641: step 98690, loss = 0.70 (268.6 examples/sec; 0.476 sec/batch)
2016-02-03 22:51:59.611828: step 98700, loss = 0.60 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 22:52:04.853915: step 98710, loss = 0.72 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 22:52:09.470143: step 98720, loss = 0.65 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 22:52:14.210760: step 98730, loss = 0.69 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 22:52:18.890117: step 98740, loss = 0.63 (289.2 examples/sec; 0.443 sec/batch)
2016-02-03 22:52:23.547430: step 98750, loss = 0.70 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 22:52:28.330110: step 98760, loss = 0.63 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 22:52:33.038385: step 98770, loss = 0.69 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 22:52:37.803824: step 98780, loss = 0.79 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 22:52:42.413307: step 98790, loss = 0.66 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 22:52:47.090688: step 98800, loss = 0.75 (241.7 examples/sec; 0.530 sec/batch)
2016-02-03 22:52:52.256499: step 98810, loss = 0.63 (266.6 examples/sec; 0.480 sec/batch)
2016-02-03 22:52:56.962665: step 98820, loss = 0.72 (259.4 examples/sec; 0.494 sec/batch)
2016-02-03 22:53:01.635983: step 98830, loss = 0.74 (253.0 examples/sec; 0.506 sec/batch)
2016-02-03 22:53:06.380717: step 98840, loss = 0.86 (260.9 examples/sec; 0.491 sec/batch)
2016-02-03 22:53:11.014814: step 98850, loss = 0.82 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 22:53:15.755779: step 98860, loss = 0.70 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 22:53:20.512013: step 98870, loss = 0.62 (255.8 examples/sec; 0.500 sec/batch)
2016-02-03 22:53:25.212540: step 98880, loss = 0.65 (284.1 examples/sec; 0.451 sec/batch)
2016-02-03 22:53:30.030283: step 98890, loss = 0.68 (254.7 examples/sec; 0.503 sec/batch)
2016-02-03 22:53:34.674734: step 98900, loss = 0.74 (300.6 examples/sec; 0.426 sec/batch)
2016-02-03 22:53:39.944489: step 98910, loss = 0.62 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 22:53:44.688150: step 98920, loss = 0.69 (283.1 examples/sec; 0.452 sec/batch)
2016-02-03 22:53:49.430587: step 98930, loss = 0.68 (250.4 examples/sec; 0.511 sec/batch)
2016-02-03 22:53:54.106969: step 98940, loss = 0.54 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 22:53:58.796158: step 98950, loss = 0.61 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 22:54:03.504542: step 98960, loss = 0.88 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 22:54:08.205219: step 98970, loss = 0.74 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 22:54:12.951626: step 98980, loss = 0.88 (280.1 examples/sec; 0.457 sec/batch)
2016-02-03 22:54:17.752135: step 98990, loss = 0.78 (255.1 examples/sec; 0.502 sec/batch)
2016-02-03 22:54:22.472838: step 99000, loss = 0.71 (266.4 examples/sec; 0.480 sec/batch)
2016-02-03 22:54:27.745586: step 99010, loss = 0.83 (255.0 examples/sec; 0.502 sec/batch)
2016-02-03 22:54:32.486247: step 99020, loss = 0.80 (285.7 examples/sec; 0.448 sec/batch)
2016-02-03 22:54:37.247644: step 99030, loss = 0.79 (248.7 examples/sec; 0.515 sec/batch)
2016-02-03 22:54:42.010108: step 99040, loss = 0.64 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 22:54:46.612766: step 99050, loss = 0.64 (283.4 examples/sec; 0.452 sec/batch)
2016-02-03 22:54:51.332348: step 99060, loss = 0.68 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 22:54:56.084710: step 99070, loss = 0.71 (243.7 examples/sec; 0.525 sec/batch)
2016-02-03 22:55:00.709282: step 99080, loss = 0.72 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 22:55:05.436576: step 99090, loss = 0.61 (244.6 examples/sec; 0.523 sec/batch)
2016-02-03 22:55:10.116518: step 99100, loss = 0.72 (267.3 examples/sec; 0.479 sec/batch)
2016-02-03 22:55:15.328991: step 99110, loss = 0.63 (264.7 examples/sec; 0.484 sec/batch)
2016-02-03 22:55:19.977875: step 99120, loss = 0.73 (272.6 examples/sec; 0.469 sec/batch)
2016-02-03 22:55:24.692917: step 99130, loss = 0.90 (256.9 examples/sec; 0.498 sec/batch)
2016-02-03 22:55:29.337858: step 99140, loss = 0.69 (257.6 examples/sec; 0.497 sec/batch)
2016-02-03 22:55:34.052083: step 99150, loss = 0.71 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 22:55:38.737472: step 99160, loss = 0.81 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 22:55:43.437302: step 99170, loss = 0.56 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 22:55:48.036715: step 99180, loss = 0.56 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 22:55:52.779610: step 99190, loss = 0.65 (262.6 examples/sec; 0.487 sec/batch)
2016-02-03 22:55:57.382262: step 99200, loss = 0.76 (288.1 examples/sec; 0.444 sec/batch)
2016-02-03 22:56:02.546926: step 99210, loss = 0.70 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 22:56:07.272354: step 99220, loss = 0.83 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 22:56:12.031336: step 99230, loss = 0.78 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 22:56:16.788369: step 99240, loss = 0.67 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 22:56:21.429308: step 99250, loss = 0.81 (292.9 examples/sec; 0.437 sec/batch)
2016-02-03 22:56:26.178793: step 99260, loss = 0.68 (272.6 examples/sec; 0.470 sec/batch)
2016-02-03 22:56:30.892126: step 99270, loss = 0.63 (282.9 examples/sec; 0.452 sec/batch)
2016-02-03 22:56:35.614414: step 99280, loss = 0.75 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 22:56:40.363139: step 99290, loss = 0.66 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 22:56:45.092937: step 99300, loss = 0.66 (251.3 examples/sec; 0.509 sec/batch)
2016-02-03 22:56:50.290194: step 99310, loss = 0.90 (249.4 examples/sec; 0.513 sec/batch)
2016-02-03 22:56:54.985148: step 99320, loss = 0.75 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 22:56:59.636393: step 99330, loss = 0.61 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 22:57:04.400774: step 99340, loss = 0.80 (280.6 examples/sec; 0.456 sec/batch)
2016-02-03 22:57:09.064516: step 99350, loss = 0.64 (291.1 examples/sec; 0.440 sec/batch)
2016-02-03 22:57:13.807881: step 99360, loss = 0.74 (261.5 examples/sec; 0.489 sec/batch)
2016-02-03 22:57:18.472625: step 99370, loss = 0.74 (288.7 examples/sec; 0.443 sec/batch)
2016-02-03 22:57:23.187889: step 99380, loss = 0.58 (255.8 examples/sec; 0.500 sec/batch)
2016-02-03 22:57:27.907936: step 99390, loss = 0.61 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 22:57:32.581371: step 99400, loss = 0.65 (290.8 examples/sec; 0.440 sec/batch)
2016-02-03 22:57:37.697365: step 99410, loss = 0.77 (302.7 examples/sec; 0.423 sec/batch)
2016-02-03 22:57:42.444193: step 99420, loss = 0.74 (259.8 examples/sec; 0.493 sec/batch)
2016-02-03 22:57:47.033952: step 99430, loss = 0.81 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 22:57:51.696129: step 99440, loss = 0.71 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 22:57:56.397494: step 99450, loss = 0.68 (257.0 examples/sec; 0.498 sec/batch)
2016-02-03 22:58:01.034573: step 99460, loss = 0.80 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 22:58:05.704479: step 99470, loss = 0.73 (299.9 examples/sec; 0.427 sec/batch)
2016-02-03 22:58:10.421411: step 99480, loss = 0.76 (268.0 examples/sec; 0.478 sec/batch)
2016-02-03 22:58:15.075267: step 99490, loss = 0.71 (297.3 examples/sec; 0.430 sec/batch)
2016-02-03 22:58:19.750863: step 99500, loss = 0.73 (263.1 examples/sec; 0.487 sec/batch)
2016-02-03 22:58:24.960254: step 99510, loss = 0.57 (285.2 examples/sec; 0.449 sec/batch)
2016-02-03 22:58:29.650973: step 99520, loss = 0.69 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 22:58:34.301924: step 99530, loss = 0.63 (255.4 examples/sec; 0.501 sec/batch)
2016-02-03 22:58:38.920382: step 99540, loss = 0.75 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 22:58:43.585745: step 99550, loss = 0.79 (239.4 examples/sec; 0.535 sec/batch)
2016-02-03 22:58:48.197981: step 99560, loss = 0.91 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 22:58:52.915055: step 99570, loss = 0.78 (270.9 examples/sec; 0.472 sec/batch)
2016-02-03 22:58:57.578172: step 99580, loss = 0.68 (290.5 examples/sec; 0.441 sec/batch)
2016-02-03 22:59:02.256208: step 99590, loss = 0.70 (279.0 examples/sec; 0.459 sec/batch)
2016-02-03 22:59:06.869908: step 99600, loss = 0.66 (273.2 examples/sec; 0.469 sec/batch)
2016-02-03 22:59:12.030268: step 99610, loss = 0.70 (298.5 examples/sec; 0.429 sec/batch)
2016-02-03 22:59:16.565057: step 99620, loss = 0.79 (274.5 examples/sec; 0.466 sec/batch)
2016-02-03 22:59:21.161340: step 99630, loss = 0.62 (284.2 examples/sec; 0.450 sec/batch)
2016-02-03 22:59:25.765018: step 99640, loss = 0.75 (280.6 examples/sec; 0.456 sec/batch)
2016-02-03 22:59:30.482895: step 99650, loss = 0.62 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 22:59:35.241064: step 99660, loss = 0.68 (285.8 examples/sec; 0.448 sec/batch)
2016-02-03 22:59:39.931690: step 99670, loss = 0.69 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 22:59:44.601507: step 99680, loss = 0.72 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 22:59:49.286305: step 99690, loss = 0.69 (279.7 examples/sec; 0.458 sec/batch)
2016-02-03 22:59:53.923623: step 99700, loss = 0.72 (275.6 examples/sec; 0.464 sec/batch)
2016-02-03 22:59:59.122200: step 99710, loss = 0.76 (291.3 examples/sec; 0.439 sec/batch)
2016-02-03 23:00:03.739458: step 99720, loss = 0.86 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 23:00:08.386592: step 99730, loss = 0.68 (259.3 examples/sec; 0.494 sec/batch)
2016-02-03 23:00:13.073740: step 99740, loss = 0.59 (285.2 examples/sec; 0.449 sec/batch)
2016-02-03 23:00:17.675757: step 99750, loss = 0.67 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 23:00:22.389959: step 99760, loss = 0.77 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 23:00:27.041237: step 99770, loss = 0.76 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 23:00:31.765095: step 99780, loss = 0.66 (254.0 examples/sec; 0.504 sec/batch)
2016-02-03 23:00:36.516228: step 99790, loss = 0.72 (281.6 examples/sec; 0.455 sec/batch)
2016-02-03 23:00:41.160035: step 99800, loss = 0.70 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 23:00:46.330764: step 99810, loss = 0.68 (286.0 examples/sec; 0.448 sec/batch)
2016-02-03 23:00:51.036591: step 99820, loss = 0.72 (259.4 examples/sec; 0.493 sec/batch)
2016-02-03 23:00:55.678767: step 99830, loss = 0.65 (296.4 examples/sec; 0.432 sec/batch)
2016-02-03 23:01:00.419975: step 99840, loss = 0.71 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 23:01:05.154252: step 99850, loss = 0.61 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 23:01:09.804056: step 99860, loss = 0.56 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 23:01:14.505450: step 99870, loss = 0.66 (269.6 examples/sec; 0.475 sec/batch)
2016-02-03 23:01:19.191058: step 99880, loss = 0.86 (265.3 examples/sec; 0.482 sec/batch)
2016-02-03 23:01:24.010941: step 99890, loss = 0.79 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 23:01:28.726496: step 99900, loss = 0.65 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 23:01:33.911841: step 99910, loss = 0.62 (257.2 examples/sec; 0.498 sec/batch)
2016-02-03 23:01:38.592777: step 99920, loss = 0.60 (286.1 examples/sec; 0.447 sec/batch)
2016-02-03 23:01:43.291356: step 99930, loss = 0.78 (268.4 examples/sec; 0.477 sec/batch)
2016-02-03 23:01:47.933112: step 99940, loss = 0.68 (265.0 examples/sec; 0.483 sec/batch)
2016-02-03 23:01:52.603957: step 99950, loss = 0.63 (281.7 examples/sec; 0.454 sec/batch)
2016-02-03 23:01:57.256612: step 99960, loss = 0.74 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 23:02:01.964240: step 99970, loss = 0.70 (249.0 examples/sec; 0.514 sec/batch)
2016-02-03 23:02:06.581233: step 99980, loss = 0.75 (302.8 examples/sec; 0.423 sec/batch)
2016-02-03 23:02:11.358059: step 99990, loss = 0.62 (277.8 examples/sec; 0.461 sec/batch)
2016-02-03 23:02:16.022534: step 100000, loss = 0.68 (296.8 examples/sec; 0.431 sec/batch)
2016-02-03 23:02:21.127616: step 100010, loss = 0.58 (263.5 examples/sec; 0.486 sec/batch)
2016-02-03 23:02:25.848749: step 100020, loss = 0.69 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 23:02:30.561263: step 100030, loss = 0.74 (301.7 examples/sec; 0.424 sec/batch)
2016-02-03 23:02:35.266205: step 100040, loss = 0.76 (292.0 examples/sec; 0.438 sec/batch)
2016-02-03 23:02:39.939640: step 100050, loss = 0.62 (285.9 examples/sec; 0.448 sec/batch)
2016-02-03 23:02:44.615543: step 100060, loss = 0.85 (258.1 examples/sec; 0.496 sec/batch)
2016-02-03 23:02:49.292726: step 100070, loss = 0.80 (288.9 examples/sec; 0.443 sec/batch)
2016-02-03 23:02:54.038200: step 100080, loss = 0.69 (245.6 examples/sec; 0.521 sec/batch)
2016-02-03 23:02:58.654687: step 100090, loss = 0.82 (284.7 examples/sec; 0.450 sec/batch)
2016-02-03 23:03:03.445784: step 100100, loss = 0.73 (262.2 examples/sec; 0.488 sec/batch)
2016-02-03 23:03:08.644310: step 100110, loss = 0.62 (264.2 examples/sec; 0.485 sec/batch)
2016-02-03 23:03:13.289397: step 100120, loss = 0.77 (296.7 examples/sec; 0.431 sec/batch)
2016-02-03 23:03:17.867018: step 100130, loss = 0.59 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 23:03:22.387264: step 100140, loss = 0.57 (293.1 examples/sec; 0.437 sec/batch)
2016-02-03 23:03:27.083800: step 100150, loss = 0.60 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 23:03:31.812082: step 100160, loss = 0.81 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 23:03:36.577397: step 100170, loss = 0.67 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 23:03:41.289827: step 100180, loss = 0.82 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 23:03:46.049537: step 100190, loss = 0.56 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 23:03:50.800344: step 100200, loss = 0.78 (291.9 examples/sec; 0.438 sec/batch)
2016-02-03 23:03:55.952787: step 100210, loss = 0.69 (276.3 examples/sec; 0.463 sec/batch)
2016-02-03 23:04:00.605873: step 100220, loss = 0.59 (275.0 examples/sec; 0.465 sec/batch)
2016-02-03 23:04:05.333838: step 100230, loss = 0.55 (268.6 examples/sec; 0.476 sec/batch)
2016-02-03 23:04:10.046758: step 100240, loss = 0.68 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 23:04:14.678959: step 100250, loss = 0.62 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 23:04:19.385951: step 100260, loss = 0.56 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 23:04:24.122024: step 100270, loss = 0.71 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 23:04:28.793994: step 100280, loss = 0.61 (276.3 examples/sec; 0.463 sec/batch)
2016-02-03 23:04:33.459356: step 100290, loss = 0.67 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 23:04:38.131599: step 100300, loss = 0.60 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 23:04:43.238186: step 100310, loss = 0.78 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 23:04:47.953614: step 100320, loss = 0.69 (260.6 examples/sec; 0.491 sec/batch)
2016-02-03 23:04:52.634737: step 100330, loss = 0.69 (260.1 examples/sec; 0.492 sec/batch)
2016-02-03 23:04:57.332751: step 100340, loss = 0.65 (247.0 examples/sec; 0.518 sec/batch)
2016-02-03 23:05:02.002918: step 100350, loss = 0.82 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 23:05:06.716899: step 100360, loss = 0.84 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 23:05:11.437464: step 100370, loss = 0.84 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 23:05:16.149919: step 100380, loss = 0.82 (268.8 examples/sec; 0.476 sec/batch)
2016-02-03 23:05:20.823316: step 100390, loss = 0.66 (250.4 examples/sec; 0.511 sec/batch)
2016-02-03 23:05:25.551622: step 100400, loss = 0.63 (280.3 examples/sec; 0.457 sec/batch)
2016-02-03 23:05:30.774388: step 100410, loss = 0.67 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 23:05:35.430196: step 100420, loss = 0.67 (289.2 examples/sec; 0.443 sec/batch)
2016-02-03 23:05:40.210106: step 100430, loss = 0.67 (256.3 examples/sec; 0.499 sec/batch)
2016-02-03 23:05:44.904394: step 100440, loss = 0.65 (281.1 examples/sec; 0.455 sec/batch)
2016-02-03 23:05:49.574630: step 100450, loss = 0.73 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 23:05:54.297575: step 100460, loss = 0.59 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 23:05:58.989903: step 100470, loss = 0.73 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 23:06:03.674644: step 100480, loss = 0.73 (304.3 examples/sec; 0.421 sec/batch)
2016-02-03 23:06:08.345029: step 100490, loss = 0.64 (257.5 examples/sec; 0.497 sec/batch)
2016-02-03 23:06:13.023524: step 100500, loss = 0.67 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 23:06:18.189714: step 100510, loss = 0.79 (299.9 examples/sec; 0.427 sec/batch)
2016-02-03 23:06:22.876219: step 100520, loss = 0.73 (287.0 examples/sec; 0.446 sec/batch)
2016-02-03 23:06:27.682446: step 100530, loss = 0.66 (236.9 examples/sec; 0.540 sec/batch)
2016-02-03 23:06:32.379024: step 100540, loss = 0.77 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 23:06:37.081632: step 100550, loss = 0.74 (269.3 examples/sec; 0.475 sec/batch)
2016-02-03 23:06:41.751688: step 100560, loss = 0.70 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 23:06:46.478885: step 100570, loss = 0.78 (266.9 examples/sec; 0.480 sec/batch)
2016-02-03 23:06:51.116254: step 100580, loss = 0.77 (255.6 examples/sec; 0.501 sec/batch)
2016-02-03 23:06:55.756890: step 100590, loss = 0.66 (258.1 examples/sec; 0.496 sec/batch)
2016-02-03 23:07:00.432592: step 100600, loss = 0.70 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 23:07:05.565158: step 100610, loss = 0.66 (262.0 examples/sec; 0.489 sec/batch)
2016-02-03 23:07:10.244031: step 100620, loss = 0.62 (264.4 examples/sec; 0.484 sec/batch)
2016-02-03 23:07:14.942064: step 100630, loss = 0.65 (269.3 examples/sec; 0.475 sec/batch)
2016-02-03 23:07:19.681863: step 100640, loss = 0.73 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 23:07:24.353834: step 100650, loss = 0.80 (274.4 examples/sec; 0.466 sec/batch)
2016-02-03 23:07:29.032970: step 100660, loss = 0.63 (293.9 examples/sec; 0.436 sec/batch)
2016-02-03 23:07:33.747893: step 100670, loss = 0.69 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 23:07:38.511373: step 100680, loss = 0.81 (295.0 examples/sec; 0.434 sec/batch)
2016-02-03 23:07:43.306998: step 100690, loss = 0.69 (253.7 examples/sec; 0.505 sec/batch)
2016-02-03 23:07:48.041152: step 100700, loss = 0.75 (246.5 examples/sec; 0.519 sec/batch)
2016-02-03 23:07:53.324217: step 100710, loss = 0.80 (286.0 examples/sec; 0.448 sec/batch)
2016-02-03 23:07:58.016947: step 100720, loss = 0.74 (269.3 examples/sec; 0.475 sec/batch)
2016-02-03 23:08:02.723003: step 100730, loss = 0.66 (277.2 examples/sec; 0.462 sec/batch)
2016-02-03 23:08:07.457311: step 100740, loss = 0.59 (258.2 examples/sec; 0.496 sec/batch)
2016-02-03 23:08:12.151558: step 100750, loss = 0.66 (307.7 examples/sec; 0.416 sec/batch)
2016-02-03 23:08:16.910693: step 100760, loss = 0.73 (256.5 examples/sec; 0.499 sec/batch)
2016-02-03 23:08:21.615406: step 100770, loss = 0.79 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 23:08:26.288034: step 100780, loss = 0.62 (272.8 examples/sec; 0.469 sec/batch)
2016-02-03 23:08:31.006803: step 100790, loss = 0.82 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 23:08:35.662664: step 100800, loss = 0.74 (262.1 examples/sec; 0.488 sec/batch)
2016-02-03 23:08:40.765646: step 100810, loss = 0.61 (279.6 examples/sec; 0.458 sec/batch)
2016-02-03 23:08:45.376718: step 100820, loss = 0.65 (286.9 examples/sec; 0.446 sec/batch)
2016-02-03 23:08:50.094650: step 100830, loss = 0.64 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 23:08:54.810222: step 100840, loss = 0.64 (276.3 examples/sec; 0.463 sec/batch)
2016-02-03 23:08:59.534093: step 100850, loss = 0.61 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 23:09:04.189389: step 100860, loss = 0.71 (252.6 examples/sec; 0.507 sec/batch)
2016-02-03 23:09:08.948504: step 100870, loss = 0.79 (264.9 examples/sec; 0.483 sec/batch)
2016-02-03 23:09:13.590564: step 100880, loss = 0.61 (298.3 examples/sec; 0.429 sec/batch)
2016-02-03 23:09:18.301456: step 100890, loss = 0.58 (307.0 examples/sec; 0.417 sec/batch)
2016-02-03 23:09:23.030457: step 100900, loss = 0.61 (273.9 examples/sec; 0.467 sec/batch)
2016-02-03 23:09:28.261091: step 100910, loss = 0.76 (268.9 examples/sec; 0.476 sec/batch)
2016-02-03 23:09:32.903126: step 100920, loss = 0.82 (279.8 examples/sec; 0.457 sec/batch)
2016-02-03 23:09:37.597306: step 100930, loss = 0.64 (265.2 examples/sec; 0.483 sec/batch)
2016-02-03 23:09:42.303456: step 100940, loss = 0.66 (267.2 examples/sec; 0.479 sec/batch)
2016-02-03 23:09:46.914459: step 100950, loss = 0.65 (286.7 examples/sec; 0.447 sec/batch)
2016-02-03 23:09:51.622413: step 100960, loss = 0.68 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 23:09:56.336823: step 100970, loss = 0.65 (270.9 examples/sec; 0.473 sec/batch)
2016-02-03 23:10:00.907471: step 100980, loss = 0.85 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 23:10:05.566191: step 100990, loss = 0.60 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 23:10:10.234502: step 101000, loss = 0.71 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 23:10:15.467072: step 101010, loss = 0.68 (278.5 examples/sec; 0.460 sec/batch)
2016-02-03 23:10:20.153048: step 101020, loss = 0.56 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 23:10:24.778150: step 101030, loss = 0.57 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 23:10:29.409226: step 101040, loss = 0.72 (257.6 examples/sec; 0.497 sec/batch)
2016-02-03 23:10:34.056028: step 101050, loss = 0.61 (300.5 examples/sec; 0.426 sec/batch)
2016-02-03 23:10:38.753697: step 101060, loss = 0.77 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 23:10:43.473039: step 101070, loss = 0.79 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 23:10:48.170726: step 101080, loss = 0.70 (281.2 examples/sec; 0.455 sec/batch)
2016-02-03 23:10:52.899986: step 101090, loss = 0.56 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 23:10:57.560071: step 101100, loss = 0.66 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 23:11:02.725024: step 101110, loss = 0.70 (287.2 examples/sec; 0.446 sec/batch)
2016-02-03 23:11:07.424335: step 101120, loss = 0.75 (266.4 examples/sec; 0.480 sec/batch)
2016-02-03 23:11:12.101292: step 101130, loss = 0.82 (278.9 examples/sec; 0.459 sec/batch)
2016-02-03 23:11:16.810409: step 101140, loss = 0.75 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 23:11:21.476883: step 101150, loss = 0.54 (288.6 examples/sec; 0.444 sec/batch)
2016-02-03 23:11:26.184303: step 101160, loss = 0.52 (256.8 examples/sec; 0.498 sec/batch)
2016-02-03 23:11:30.864124: step 101170, loss = 0.92 (254.3 examples/sec; 0.503 sec/batch)
2016-02-03 23:11:35.602657: step 101180, loss = 0.63 (256.6 examples/sec; 0.499 sec/batch)
2016-02-03 23:11:40.293529: step 101190, loss = 0.74 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 23:11:45.082090: step 101200, loss = 0.73 (257.7 examples/sec; 0.497 sec/batch)
2016-02-03 23:11:50.350343: step 101210, loss = 0.78 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 23:11:54.996765: step 101220, loss = 0.64 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 23:11:59.676713: step 101230, loss = 0.67 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 23:12:04.335473: step 101240, loss = 0.58 (295.4 examples/sec; 0.433 sec/batch)
2016-02-03 23:12:09.116706: step 101250, loss = 0.71 (273.2 examples/sec; 0.469 sec/batch)
2016-02-03 23:12:13.850408: step 101260, loss = 0.71 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 23:12:18.534207: step 101270, loss = 0.80 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 23:12:23.172117: step 101280, loss = 0.63 (259.1 examples/sec; 0.494 sec/batch)
2016-02-03 23:12:27.921621: step 101290, loss = 0.71 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 23:12:32.664670: step 101300, loss = 0.76 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 23:12:37.793484: step 101310, loss = 0.66 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 23:12:42.495455: step 101320, loss = 0.87 (260.7 examples/sec; 0.491 sec/batch)
2016-02-03 23:12:47.198083: step 101330, loss = 0.86 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 23:12:51.954337: step 101340, loss = 0.71 (297.2 examples/sec; 0.431 sec/batch)
2016-02-03 23:12:56.647813: step 101350, loss = 0.82 (280.4 examples/sec; 0.457 sec/batch)
2016-02-03 23:13:01.329059: step 101360, loss = 0.55 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 23:13:06.094674: step 101370, loss = 0.67 (281.5 examples/sec; 0.455 sec/batch)
2016-02-03 23:13:10.801719: step 101380, loss = 0.58 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 23:13:15.553552: step 101390, loss = 0.78 (251.5 examples/sec; 0.509 sec/batch)
2016-02-03 23:13:20.349146: step 101400, loss = 0.81 (289.7 examples/sec; 0.442 sec/batch)
2016-02-03 23:13:25.637639: step 101410, loss = 0.66 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 23:13:30.274453: step 101420, loss = 0.72 (290.5 examples/sec; 0.441 sec/batch)
2016-02-03 23:13:34.994695: step 101430, loss = 0.59 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 23:13:39.616518: step 101440, loss = 0.70 (282.9 examples/sec; 0.452 sec/batch)
2016-02-03 23:13:44.347210: step 101450, loss = 0.84 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 23:13:49.039248: step 101460, loss = 0.87 (308.6 examples/sec; 0.415 sec/batch)
2016-02-03 23:13:53.801113: step 101470, loss = 0.60 (255.3 examples/sec; 0.501 sec/batch)
2016-02-03 23:13:58.445762: step 101480, loss = 0.74 (287.5 examples/sec; 0.445 sec/batch)
2016-02-03 23:14:03.170760: step 101490, loss = 0.87 (249.4 examples/sec; 0.513 sec/batch)
2016-02-03 23:14:07.792242: step 101500, loss = 0.55 (282.6 examples/sec; 0.453 sec/batch)
2016-02-03 23:14:12.997533: step 101510, loss = 0.72 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 23:14:17.683556: step 101520, loss = 0.66 (263.9 examples/sec; 0.485 sec/batch)
2016-02-03 23:14:22.394010: step 101530, loss = 0.65 (268.5 examples/sec; 0.477 sec/batch)
2016-02-03 23:14:27.080549: step 101540, loss = 0.79 (279.8 examples/sec; 0.457 sec/batch)
2016-02-03 23:14:31.805566: step 101550, loss = 0.63 (272.6 examples/sec; 0.470 sec/batch)
2016-02-03 23:14:36.522758: step 101560, loss = 0.67 (277.6 examples/sec; 0.461 sec/batch)
2016-02-03 23:14:41.245320: step 101570, loss = 0.70 (283.6 examples/sec; 0.451 sec/batch)
2016-02-03 23:14:45.941159: step 101580, loss = 0.65 (292.7 examples/sec; 0.437 sec/batch)
2016-02-03 23:14:50.653419: step 101590, loss = 0.65 (260.9 examples/sec; 0.491 sec/batch)
2016-02-03 23:14:55.281159: step 101600, loss = 0.78 (269.3 examples/sec; 0.475 sec/batch)
2016-02-03 23:15:00.530285: step 101610, loss = 0.65 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 23:15:05.238161: step 101620, loss = 0.66 (250.7 examples/sec; 0.510 sec/batch)
2016-02-03 23:15:09.903293: step 101630, loss = 0.75 (273.8 examples/sec; 0.467 sec/batch)
2016-02-03 23:15:14.615622: step 101640, loss = 0.78 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 23:15:19.405680: step 101650, loss = 0.73 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 23:15:24.207166: step 101660, loss = 0.72 (262.2 examples/sec; 0.488 sec/batch)
2016-02-03 23:15:28.934256: step 101670, loss = 0.87 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 23:15:33.670597: step 101680, loss = 0.71 (277.7 examples/sec; 0.461 sec/batch)
2016-02-03 23:15:38.407238: step 101690, loss = 0.73 (287.4 examples/sec; 0.445 sec/batch)
2016-02-03 23:15:43.035344: step 101700, loss = 0.66 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 23:15:48.289694: step 101710, loss = 0.71 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 23:15:53.021145: step 101720, loss = 0.78 (263.1 examples/sec; 0.486 sec/batch)
2016-02-03 23:15:57.723817: step 101730, loss = 0.69 (269.9 examples/sec; 0.474 sec/batch)
2016-02-03 23:16:02.334601: step 101740, loss = 0.78 (299.0 examples/sec; 0.428 sec/batch)
2016-02-03 23:16:07.060237: step 101750, loss = 0.72 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 23:16:11.611257: step 101760, loss = 0.77 (284.7 examples/sec; 0.450 sec/batch)
2016-02-03 23:16:16.325700: step 101770, loss = 0.79 (256.6 examples/sec; 0.499 sec/batch)
2016-02-03 23:16:20.970002: step 101780, loss = 0.60 (293.2 examples/sec; 0.437 sec/batch)
2016-02-03 23:16:25.733324: step 101790, loss = 0.75 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 23:16:30.474374: step 101800, loss = 0.78 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 23:16:35.634447: step 101810, loss = 0.81 (256.4 examples/sec; 0.499 sec/batch)
2016-02-03 23:16:40.367603: step 101820, loss = 0.72 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 23:16:45.029318: step 101830, loss = 0.77 (269.0 examples/sec; 0.476 sec/batch)
2016-02-03 23:16:49.781499: step 101840, loss = 0.68 (264.9 examples/sec; 0.483 sec/batch)
2016-02-03 23:16:54.482505: step 101850, loss = 0.68 (290.5 examples/sec; 0.441 sec/batch)
2016-02-03 23:16:59.184473: step 101860, loss = 0.79 (295.3 examples/sec; 0.433 sec/batch)
2016-02-03 23:17:03.915335: step 101870, loss = 0.58 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 23:17:08.579579: step 101880, loss = 0.57 (260.0 examples/sec; 0.492 sec/batch)
2016-02-03 23:17:13.279177: step 101890, loss = 0.78 (269.3 examples/sec; 0.475 sec/batch)
2016-02-03 23:17:18.014404: step 101900, loss = 0.63 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 23:17:23.254114: step 101910, loss = 0.83 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 23:17:27.948944: step 101920, loss = 0.75 (284.0 examples/sec; 0.451 sec/batch)
2016-02-03 23:17:32.682206: step 101930, loss = 0.74 (266.8 examples/sec; 0.480 sec/batch)
2016-02-03 23:17:37.405725: step 101940, loss = 0.71 (259.4 examples/sec; 0.494 sec/batch)
2016-02-03 23:17:42.132426: step 101950, loss = 0.85 (313.6 examples/sec; 0.408 sec/batch)
2016-02-03 23:17:46.800106: step 101960, loss = 0.60 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 23:17:51.492692: step 101970, loss = 0.74 (286.9 examples/sec; 0.446 sec/batch)
2016-02-03 23:17:56.261888: step 101980, loss = 0.71 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 23:18:01.016602: step 101990, loss = 0.64 (254.9 examples/sec; 0.502 sec/batch)
2016-02-03 23:18:05.779366: step 102000, loss = 0.81 (266.3 examples/sec; 0.481 sec/batch)
2016-02-03 23:18:10.951201: step 102010, loss = 0.66 (284.4 examples/sec; 0.450 sec/batch)
2016-02-03 23:18:15.689576: step 102020, loss = 0.67 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 23:18:20.369490: step 102030, loss = 0.64 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 23:18:25.056153: step 102040, loss = 0.64 (289.5 examples/sec; 0.442 sec/batch)
2016-02-03 23:18:29.896053: step 102050, loss = 0.74 (271.2 examples/sec; 0.472 sec/batch)
2016-02-03 23:18:34.560496: step 102060, loss = 0.66 (259.0 examples/sec; 0.494 sec/batch)
2016-02-03 23:18:39.309475: step 102070, loss = 0.68 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 23:18:44.013037: step 102080, loss = 0.76 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 23:18:48.775691: step 102090, loss = 0.74 (253.6 examples/sec; 0.505 sec/batch)
2016-02-03 23:18:53.415596: step 102100, loss = 0.66 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 23:18:58.605804: step 102110, loss = 0.59 (289.1 examples/sec; 0.443 sec/batch)
2016-02-03 23:19:03.295688: step 102120, loss = 0.69 (307.0 examples/sec; 0.417 sec/batch)
2016-02-03 23:19:08.054333: step 102130, loss = 0.76 (250.5 examples/sec; 0.511 sec/batch)
2016-02-03 23:19:12.720869: step 102140, loss = 0.66 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 23:19:17.470014: step 102150, loss = 0.72 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 23:19:22.166999: step 102160, loss = 0.77 (276.8 examples/sec; 0.462 sec/batch)
2016-02-03 23:19:26.849404: step 102170, loss = 0.70 (290.1 examples/sec; 0.441 sec/batch)
2016-02-03 23:19:31.594815: step 102180, loss = 0.53 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 23:19:36.311625: step 102190, loss = 0.62 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 23:19:41.053319: step 102200, loss = 0.84 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 23:19:46.224656: step 102210, loss = 0.71 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 23:19:50.979234: step 102220, loss = 0.71 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 23:19:55.752060: step 102230, loss = 0.58 (257.0 examples/sec; 0.498 sec/batch)
2016-02-03 23:20:00.493183: step 102240, loss = 0.67 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 23:20:05.250730: step 102250, loss = 0.71 (272.1 examples/sec; 0.470 sec/batch)
2016-02-03 23:20:09.904607: step 102260, loss = 0.68 (283.7 examples/sec; 0.451 sec/batch)
2016-02-03 23:20:14.637467: step 102270, loss = 0.66 (273.8 examples/sec; 0.468 sec/batch)
2016-02-03 23:20:19.397919: step 102280, loss = 0.76 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 23:20:24.106650: step 102290, loss = 0.78 (254.0 examples/sec; 0.504 sec/batch)
2016-02-03 23:20:28.829151: step 102300, loss = 0.83 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 23:20:34.066343: step 102310, loss = 0.64 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 23:20:38.786267: step 102320, loss = 0.68 (291.6 examples/sec; 0.439 sec/batch)
2016-02-03 23:20:43.521494: step 102330, loss = 0.80 (285.4 examples/sec; 0.449 sec/batch)
2016-02-03 23:20:48.257795: step 102340, loss = 0.68 (250.3 examples/sec; 0.511 sec/batch)
2016-02-03 23:20:52.953825: step 102350, loss = 0.77 (291.5 examples/sec; 0.439 sec/batch)
2016-02-03 23:20:57.723489: step 102360, loss = 0.70 (288.7 examples/sec; 0.443 sec/batch)
2016-02-03 23:21:02.450463: step 102370, loss = 0.59 (260.5 examples/sec; 0.491 sec/batch)
2016-02-03 23:21:07.188226: step 102380, loss = 0.70 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 23:21:11.891172: step 102390, loss = 0.72 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 23:21:16.666398: step 102400, loss = 0.78 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 23:21:21.859506: step 102410, loss = 0.88 (256.3 examples/sec; 0.499 sec/batch)
2016-02-03 23:21:26.616927: step 102420, loss = 0.66 (261.8 examples/sec; 0.489 sec/batch)
2016-02-03 23:21:31.341633: step 102430, loss = 0.78 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 23:21:35.964607: step 102440, loss = 0.75 (286.2 examples/sec; 0.447 sec/batch)
2016-02-03 23:21:40.690210: step 102450, loss = 0.73 (259.3 examples/sec; 0.494 sec/batch)
2016-02-03 23:21:45.455534: step 102460, loss = 0.73 (259.4 examples/sec; 0.493 sec/batch)
2016-02-03 23:21:50.205397: step 102470, loss = 0.81 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 23:21:54.930097: step 102480, loss = 0.71 (259.8 examples/sec; 0.493 sec/batch)
2016-02-03 23:21:59.655459: step 102490, loss = 0.58 (287.6 examples/sec; 0.445 sec/batch)
2016-02-03 23:22:04.353728: step 102500, loss = 0.68 (280.6 examples/sec; 0.456 sec/batch)
2016-02-03 23:22:09.595912: step 102510, loss = 0.57 (256.6 examples/sec; 0.499 sec/batch)
2016-02-03 23:22:14.322283: step 102520, loss = 0.66 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 23:22:19.058633: step 102530, loss = 0.75 (256.0 examples/sec; 0.500 sec/batch)
2016-02-03 23:22:23.776282: step 102540, loss = 0.58 (286.6 examples/sec; 0.447 sec/batch)
2016-02-03 23:22:28.497131: step 102550, loss = 0.71 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 23:22:33.182747: step 102560, loss = 0.67 (274.0 examples/sec; 0.467 sec/batch)
2016-02-03 23:22:37.885317: step 102570, loss = 0.60 (262.1 examples/sec; 0.488 sec/batch)
2016-02-03 23:22:42.560203: step 102580, loss = 0.76 (302.1 examples/sec; 0.424 sec/batch)
2016-02-03 23:22:47.264780: step 102590, loss = 0.56 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 23:22:51.966368: step 102600, loss = 0.67 (248.8 examples/sec; 0.514 sec/batch)
2016-02-03 23:22:57.203683: step 102610, loss = 0.70 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 23:23:01.904679: step 102620, loss = 0.70 (287.0 examples/sec; 0.446 sec/batch)
2016-02-03 23:23:06.540904: step 102630, loss = 0.75 (286.4 examples/sec; 0.447 sec/batch)
2016-02-03 23:23:11.188883: step 102640, loss = 0.71 (261.1 examples/sec; 0.490 sec/batch)
2016-02-03 23:23:15.859407: step 102650, loss = 0.70 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 23:23:20.639459: step 102660, loss = 0.71 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 23:23:25.364653: step 102670, loss = 0.87 (263.1 examples/sec; 0.487 sec/batch)
2016-02-03 23:23:30.105193: step 102680, loss = 0.65 (281.2 examples/sec; 0.455 sec/batch)
2016-02-03 23:23:34.793293: step 102690, loss = 0.69 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 23:23:39.466813: step 102700, loss = 0.78 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 23:23:44.579560: step 102710, loss = 0.70 (285.0 examples/sec; 0.449 sec/batch)
2016-02-03 23:23:49.313490: step 102720, loss = 0.67 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 23:23:54.112162: step 102730, loss = 0.57 (250.6 examples/sec; 0.511 sec/batch)
2016-02-03 23:23:58.852541: step 102740, loss = 0.72 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 23:24:03.572273: step 102750, loss = 0.74 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 23:24:08.248164: step 102760, loss = 0.70 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 23:24:12.975630: step 102770, loss = 0.82 (243.2 examples/sec; 0.526 sec/batch)
2016-02-03 23:24:17.664268: step 102780, loss = 0.67 (282.4 examples/sec; 0.453 sec/batch)
2016-02-03 23:24:22.401050: step 102790, loss = 0.73 (281.2 examples/sec; 0.455 sec/batch)
2016-02-03 23:24:27.149223: step 102800, loss = 0.80 (271.5 examples/sec; 0.472 sec/batch)
2016-02-03 23:24:32.455182: step 102810, loss = 0.70 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 23:24:37.137152: step 102820, loss = 0.72 (245.2 examples/sec; 0.522 sec/batch)
2016-02-03 23:24:41.782113: step 102830, loss = 0.63 (287.1 examples/sec; 0.446 sec/batch)
2016-02-03 23:24:46.407884: step 102840, loss = 0.77 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 23:24:51.094716: step 102850, loss = 0.69 (274.3 examples/sec; 0.467 sec/batch)
2016-02-03 23:24:55.800155: step 102860, loss = 0.74 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 23:25:00.509046: step 102870, loss = 0.73 (288.7 examples/sec; 0.443 sec/batch)
2016-02-03 23:25:05.245572: step 102880, loss = 0.75 (254.4 examples/sec; 0.503 sec/batch)
2016-02-03 23:25:09.918366: step 102890, loss = 0.75 (277.1 examples/sec; 0.462 sec/batch)
2016-02-03 23:25:14.576907: step 102900, loss = 0.70 (267.7 examples/sec; 0.478 sec/batch)
2016-02-03 23:25:19.713187: step 102910, loss = 0.86 (293.7 examples/sec; 0.436 sec/batch)
2016-02-03 23:25:24.347650: step 102920, loss = 0.72 (282.2 examples/sec; 0.454 sec/batch)
2016-02-03 23:25:28.872813: step 102930, loss = 0.70 (289.1 examples/sec; 0.443 sec/batch)
2016-02-03 23:25:33.641633: step 102940, loss = 0.66 (267.2 examples/sec; 0.479 sec/batch)
2016-02-03 23:25:38.292074: step 102950, loss = 0.76 (296.4 examples/sec; 0.432 sec/batch)
2016-02-03 23:25:42.983511: step 102960, loss = 0.59 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 23:25:47.719700: step 102970, loss = 0.72 (278.0 examples/sec; 0.460 sec/batch)
2016-02-03 23:25:52.292328: step 102980, loss = 0.74 (300.4 examples/sec; 0.426 sec/batch)
2016-02-03 23:25:56.987933: step 102990, loss = 0.62 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 23:26:01.728747: step 103000, loss = 0.81 (267.0 examples/sec; 0.479 sec/batch)
2016-02-03 23:26:06.810479: step 103010, loss = 0.73 (292.2 examples/sec; 0.438 sec/batch)
2016-02-03 23:26:11.501367: step 103020, loss = 0.52 (280.5 examples/sec; 0.456 sec/batch)
2016-02-03 23:26:16.146149: step 103030, loss = 0.72 (257.0 examples/sec; 0.498 sec/batch)
2016-02-03 23:26:20.767725: step 103040, loss = 0.73 (283.9 examples/sec; 0.451 sec/batch)
2016-02-03 23:26:25.422227: step 103050, loss = 0.75 (289.3 examples/sec; 0.443 sec/batch)
2016-02-03 23:26:29.975112: step 103060, loss = 0.84 (286.0 examples/sec; 0.448 sec/batch)
2016-02-03 23:26:34.667189: step 103070, loss = 0.67 (283.9 examples/sec; 0.451 sec/batch)
2016-02-03 23:26:39.241841: step 103080, loss = 0.66 (284.6 examples/sec; 0.450 sec/batch)
2016-02-03 23:26:43.872458: step 103090, loss = 0.66 (287.4 examples/sec; 0.445 sec/batch)
2016-02-03 23:26:48.639605: step 103100, loss = 0.75 (243.0 examples/sec; 0.527 sec/batch)
2016-02-03 23:26:53.817553: step 103110, loss = 0.83 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 23:26:58.396340: step 103120, loss = 0.78 (298.2 examples/sec; 0.429 sec/batch)
2016-02-03 23:27:02.999332: step 103130, loss = 0.71 (271.8 examples/sec; 0.471 sec/batch)
2016-02-03 23:27:07.629054: step 103140, loss = 0.74 (297.8 examples/sec; 0.430 sec/batch)
2016-02-03 23:27:12.229528: step 103150, loss = 0.65 (288.0 examples/sec; 0.444 sec/batch)
2016-02-03 23:27:16.963713: step 103160, loss = 0.88 (285.1 examples/sec; 0.449 sec/batch)
2016-02-03 23:27:21.622861: step 103170, loss = 0.90 (293.4 examples/sec; 0.436 sec/batch)
2016-02-03 23:27:26.271328: step 103180, loss = 0.74 (285.8 examples/sec; 0.448 sec/batch)
2016-02-03 23:27:30.903974: step 103190, loss = 0.74 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 23:27:35.440582: step 103200, loss = 0.77 (306.6 examples/sec; 0.418 sec/batch)
2016-02-03 23:27:40.563858: step 103210, loss = 0.59 (297.3 examples/sec; 0.431 sec/batch)
2016-02-03 23:27:45.267013: step 103220, loss = 0.61 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 23:27:49.904850: step 103230, loss = 0.93 (267.2 examples/sec; 0.479 sec/batch)
2016-02-03 23:27:54.621218: step 103240, loss = 0.81 (267.2 examples/sec; 0.479 sec/batch)
2016-02-03 23:27:59.265804: step 103250, loss = 0.60 (290.3 examples/sec; 0.441 sec/batch)
2016-02-03 23:28:03.985152: step 103260, loss = 0.68 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 23:28:08.557729: step 103270, loss = 0.74 (296.4 examples/sec; 0.432 sec/batch)
2016-02-03 23:28:13.152477: step 103280, loss = 0.78 (285.5 examples/sec; 0.448 sec/batch)
2016-02-03 23:28:17.853056: step 103290, loss = 0.59 (274.8 examples/sec; 0.466 sec/batch)
2016-02-03 23:28:22.553725: step 103300, loss = 0.94 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 23:28:27.685162: step 103310, loss = 0.72 (298.9 examples/sec; 0.428 sec/batch)
2016-02-03 23:28:32.372506: step 103320, loss = 0.56 (278.4 examples/sec; 0.460 sec/batch)
2016-02-03 23:28:37.042902: step 103330, loss = 0.68 (284.9 examples/sec; 0.449 sec/batch)
2016-02-03 23:28:41.661107: step 103340, loss = 0.61 (267.1 examples/sec; 0.479 sec/batch)
2016-02-03 23:28:46.325606: step 103350, loss = 0.60 (291.2 examples/sec; 0.440 sec/batch)
2016-02-03 23:28:50.999503: step 103360, loss = 0.52 (268.1 examples/sec; 0.478 sec/batch)
2016-02-03 23:28:55.815087: step 103370, loss = 0.77 (253.4 examples/sec; 0.505 sec/batch)
2016-02-03 23:29:00.563138: step 103380, loss = 0.65 (247.1 examples/sec; 0.518 sec/batch)
2016-02-03 23:29:05.302765: step 103390, loss = 0.64 (252.4 examples/sec; 0.507 sec/batch)
2016-02-03 23:29:10.127746: step 103400, loss = 0.66 (249.6 examples/sec; 0.513 sec/batch)
2016-02-03 23:29:15.295388: step 103410, loss = 0.66 (251.3 examples/sec; 0.509 sec/batch)
2016-02-03 23:29:20.035923: step 103420, loss = 0.73 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 23:29:24.798402: step 103430, loss = 0.60 (251.9 examples/sec; 0.508 sec/batch)
2016-02-03 23:29:29.458167: step 103440, loss = 0.82 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 23:29:34.130425: step 103450, loss = 0.59 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 23:29:38.780988: step 103460, loss = 0.86 (252.7 examples/sec; 0.507 sec/batch)
2016-02-03 23:29:43.382700: step 103470, loss = 0.77 (289.7 examples/sec; 0.442 sec/batch)
2016-02-03 23:29:48.202491: step 103480, loss = 0.65 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 23:29:52.884132: step 103490, loss = 0.67 (283.1 examples/sec; 0.452 sec/batch)
2016-02-03 23:29:57.662441: step 103500, loss = 0.81 (283.8 examples/sec; 0.451 sec/batch)
2016-02-03 23:30:02.912005: step 103510, loss = 0.71 (286.4 examples/sec; 0.447 sec/batch)
2016-02-03 23:30:07.564082: step 103520, loss = 0.72 (283.8 examples/sec; 0.451 sec/batch)
2016-02-03 23:30:12.284077: step 103530, loss = 0.64 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 23:30:17.040115: step 103540, loss = 0.78 (286.3 examples/sec; 0.447 sec/batch)
2016-02-03 23:30:21.699868: step 103550, loss = 0.70 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 23:30:26.344725: step 103560, loss = 0.61 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 23:30:31.103420: step 103570, loss = 0.76 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 23:30:35.824716: step 103580, loss = 0.74 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 23:30:40.576958: step 103590, loss = 0.72 (296.5 examples/sec; 0.432 sec/batch)
2016-02-03 23:30:45.305907: step 103600, loss = 0.75 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 23:30:50.575291: step 103610, loss = 0.66 (262.5 examples/sec; 0.488 sec/batch)
2016-02-03 23:30:55.187039: step 103620, loss = 0.64 (279.3 examples/sec; 0.458 sec/batch)
2016-02-03 23:30:59.873112: step 103630, loss = 0.77 (290.6 examples/sec; 0.440 sec/batch)
2016-02-03 23:31:04.624619: step 103640, loss = 0.64 (263.3 examples/sec; 0.486 sec/batch)
2016-02-03 23:31:09.309687: step 103650, loss = 0.75 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 23:31:14.020391: step 103660, loss = 0.82 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 23:31:18.681835: step 103670, loss = 0.68 (299.8 examples/sec; 0.427 sec/batch)
2016-02-03 23:31:23.431569: step 103680, loss = 0.82 (243.2 examples/sec; 0.526 sec/batch)
2016-02-03 23:31:28.144014: step 103690, loss = 0.55 (248.0 examples/sec; 0.516 sec/batch)
2016-02-03 23:31:32.837331: step 103700, loss = 0.73 (279.8 examples/sec; 0.457 sec/batch)
2016-02-03 23:31:38.087630: step 103710, loss = 0.59 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 23:31:42.840623: step 103720, loss = 0.68 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 23:31:47.392647: step 103730, loss = 0.70 (287.6 examples/sec; 0.445 sec/batch)
2016-02-03 23:31:52.077090: step 103740, loss = 0.74 (262.2 examples/sec; 0.488 sec/batch)
2016-02-03 23:31:56.765667: step 103750, loss = 0.92 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 23:32:01.390478: step 103760, loss = 0.69 (265.2 examples/sec; 0.483 sec/batch)
2016-02-03 23:32:06.114712: step 103770, loss = 0.66 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 23:32:10.718003: step 103780, loss = 0.59 (285.6 examples/sec; 0.448 sec/batch)
2016-02-03 23:32:15.374064: step 103790, loss = 0.68 (254.1 examples/sec; 0.504 sec/batch)
2016-02-03 23:32:20.107451: step 103800, loss = 0.72 (269.2 examples/sec; 0.475 sec/batch)
2016-02-03 23:32:25.357158: step 103810, loss = 0.67 (286.3 examples/sec; 0.447 sec/batch)
2016-02-03 23:32:30.069224: step 103820, loss = 0.69 (291.6 examples/sec; 0.439 sec/batch)
2016-02-03 23:32:34.811562: step 103830, loss = 0.71 (261.1 examples/sec; 0.490 sec/batch)
2016-02-03 23:32:39.527065: step 103840, loss = 0.83 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 23:32:44.235719: step 103850, loss = 0.75 (276.6 examples/sec; 0.463 sec/batch)
2016-02-03 23:32:48.919583: step 103860, loss = 0.72 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 23:32:53.608996: step 103870, loss = 0.70 (279.1 examples/sec; 0.459 sec/batch)
2016-02-03 23:32:58.295877: step 103880, loss = 0.78 (274.2 examples/sec; 0.467 sec/batch)
2016-02-03 23:33:03.016296: step 103890, loss = 0.85 (273.1 examples/sec; 0.469 sec/batch)
2016-02-03 23:33:07.736192: step 103900, loss = 0.77 (264.1 examples/sec; 0.485 sec/batch)
2016-02-03 23:33:12.953717: step 103910, loss = 0.63 (310.2 examples/sec; 0.413 sec/batch)
2016-02-03 23:33:17.666806: step 103920, loss = 0.61 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 23:33:22.335867: step 103930, loss = 0.78 (254.8 examples/sec; 0.502 sec/batch)
2016-02-03 23:33:27.025122: step 103940, loss = 0.71 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 23:33:31.623150: step 103950, loss = 0.75 (280.3 examples/sec; 0.457 sec/batch)
2016-02-03 23:33:36.399494: step 103960, loss = 0.56 (270.6 examples/sec; 0.473 sec/batch)
2016-02-03 23:33:41.050814: step 103970, loss = 0.81 (275.6 examples/sec; 0.465 sec/batch)
2016-02-03 23:33:45.800124: step 103980, loss = 0.65 (261.9 examples/sec; 0.489 sec/batch)
2016-02-03 23:33:50.372648: step 103990, loss = 0.72 (285.6 examples/sec; 0.448 sec/batch)
2016-02-03 23:33:55.100215: step 104000, loss = 0.59 (259.8 examples/sec; 0.493 sec/batch)
2016-02-03 23:34:00.344066: step 104010, loss = 0.78 (278.1 examples/sec; 0.460 sec/batch)
2016-02-03 23:34:05.132668: step 104020, loss = 0.66 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 23:34:09.839751: step 104030, loss = 0.62 (264.2 examples/sec; 0.485 sec/batch)
2016-02-03 23:34:14.589220: step 104040, loss = 0.58 (295.8 examples/sec; 0.433 sec/batch)
2016-02-03 23:34:19.290402: step 104050, loss = 0.59 (282.6 examples/sec; 0.453 sec/batch)
2016-02-03 23:34:23.961646: step 104060, loss = 0.72 (305.4 examples/sec; 0.419 sec/batch)
2016-02-03 23:34:28.703138: step 104070, loss = 0.57 (264.2 examples/sec; 0.485 sec/batch)
2016-02-03 23:34:33.475094: step 104080, loss = 0.69 (293.1 examples/sec; 0.437 sec/batch)
2016-02-03 23:34:38.199708: step 104090, loss = 0.80 (290.7 examples/sec; 0.440 sec/batch)
2016-02-03 23:34:42.967002: step 104100, loss = 0.67 (265.0 examples/sec; 0.483 sec/batch)
2016-02-03 23:34:48.234403: step 104110, loss = 0.71 (280.1 examples/sec; 0.457 sec/batch)
2016-02-03 23:34:52.922007: step 104120, loss = 0.64 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 23:34:57.629740: step 104130, loss = 0.70 (295.3 examples/sec; 0.434 sec/batch)
2016-02-03 23:35:02.372697: step 104140, loss = 0.66 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 23:35:07.061266: step 104150, loss = 0.78 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 23:35:11.758282: step 104160, loss = 0.67 (289.1 examples/sec; 0.443 sec/batch)
2016-02-03 23:35:16.492013: step 104170, loss = 0.74 (251.2 examples/sec; 0.510 sec/batch)
2016-02-03 23:35:21.200616: step 104180, loss = 0.63 (304.7 examples/sec; 0.420 sec/batch)
2016-02-03 23:35:25.925519: step 104190, loss = 0.73 (258.2 examples/sec; 0.496 sec/batch)
2016-02-03 23:35:30.641654: step 104200, loss = 0.62 (304.4 examples/sec; 0.420 sec/batch)
2016-02-03 23:35:35.888948: step 104210, loss = 0.71 (243.0 examples/sec; 0.527 sec/batch)
2016-02-03 23:35:40.588200: step 104220, loss = 0.85 (278.0 examples/sec; 0.460 sec/batch)
2016-02-03 23:35:45.296415: step 104230, loss = 0.63 (290.6 examples/sec; 0.440 sec/batch)
2016-02-03 23:35:50.079039: step 104240, loss = 0.70 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 23:35:54.817830: step 104250, loss = 0.85 (268.3 examples/sec; 0.477 sec/batch)
2016-02-03 23:35:59.526349: step 104260, loss = 0.71 (246.5 examples/sec; 0.519 sec/batch)
2016-02-03 23:36:04.167655: step 104270, loss = 0.72 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 23:36:08.837365: step 104280, loss = 0.70 (293.1 examples/sec; 0.437 sec/batch)
2016-02-03 23:36:13.538795: step 104290, loss = 0.67 (246.8 examples/sec; 0.519 sec/batch)
2016-02-03 23:36:18.206248: step 104300, loss = 0.65 (285.4 examples/sec; 0.448 sec/batch)
2016-02-03 23:36:23.481432: step 104310, loss = 0.72 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 23:36:28.265177: step 104320, loss = 0.82 (270.4 examples/sec; 0.473 sec/batch)
2016-02-03 23:36:32.986472: step 104330, loss = 0.64 (263.4 examples/sec; 0.486 sec/batch)
2016-02-03 23:36:37.692730: step 104340, loss = 0.49 (260.0 examples/sec; 0.492 sec/batch)
2016-02-03 23:36:42.360602: step 104350, loss = 1.00 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 23:36:47.027633: step 104360, loss = 0.69 (282.0 examples/sec; 0.454 sec/batch)
2016-02-03 23:36:51.746438: step 104370, loss = 0.66 (285.4 examples/sec; 0.448 sec/batch)
2016-02-03 23:36:56.478993: step 104380, loss = 0.59 (258.2 examples/sec; 0.496 sec/batch)
2016-02-03 23:37:01.194683: step 104390, loss = 0.68 (281.1 examples/sec; 0.455 sec/batch)
2016-02-03 23:37:05.933373: step 104400, loss = 0.80 (272.2 examples/sec; 0.470 sec/batch)
2016-02-03 23:37:11.168896: step 104410, loss = 0.67 (256.9 examples/sec; 0.498 sec/batch)
2016-02-03 23:37:15.834227: step 104420, loss = 0.72 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 23:37:20.625179: step 104430, loss = 0.68 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 23:37:25.382564: step 104440, loss = 0.65 (260.8 examples/sec; 0.491 sec/batch)
2016-02-03 23:37:30.096060: step 104450, loss = 0.55 (279.9 examples/sec; 0.457 sec/batch)
2016-02-03 23:37:34.837113: step 104460, loss = 0.91 (263.2 examples/sec; 0.486 sec/batch)
2016-02-03 23:37:39.568997: step 104470, loss = 0.66 (264.1 examples/sec; 0.485 sec/batch)
2016-02-03 23:37:44.337970: step 104480, loss = 0.65 (256.1 examples/sec; 0.500 sec/batch)
2016-02-03 23:37:49.132471: step 104490, loss = 0.69 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 23:37:53.894012: step 104500, loss = 0.75 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 23:37:58.993225: step 104510, loss = 0.72 (261.0 examples/sec; 0.490 sec/batch)
2016-02-03 23:38:03.677143: step 104520, loss = 0.57 (281.5 examples/sec; 0.455 sec/batch)
2016-02-03 23:38:08.281408: step 104530, loss = 0.83 (309.0 examples/sec; 0.414 sec/batch)
2016-02-03 23:38:12.948267: step 104540, loss = 0.67 (305.2 examples/sec; 0.419 sec/batch)
2016-02-03 23:38:17.666668: step 104550, loss = 0.66 (261.9 examples/sec; 0.489 sec/batch)
2016-02-03 23:38:22.363040: step 104560, loss = 0.65 (289.9 examples/sec; 0.442 sec/batch)
2016-02-03 23:38:27.101158: step 104570, loss = 0.49 (292.1 examples/sec; 0.438 sec/batch)
2016-02-03 23:38:31.753960: step 104580, loss = 0.69 (269.2 examples/sec; 0.475 sec/batch)
2016-02-03 23:38:36.384570: step 104590, loss = 0.63 (310.2 examples/sec; 0.413 sec/batch)
2016-02-03 23:38:40.932226: step 104600, loss = 0.78 (288.5 examples/sec; 0.444 sec/batch)
2016-02-03 23:38:46.238416: step 104610, loss = 0.57 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 23:38:50.839569: step 104620, loss = 0.65 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 23:38:55.513068: step 104630, loss = 0.70 (293.4 examples/sec; 0.436 sec/batch)
2016-02-03 23:39:00.128270: step 104640, loss = 0.80 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 23:39:04.752326: step 104650, loss = 0.65 (292.5 examples/sec; 0.438 sec/batch)
2016-02-03 23:39:09.380240: step 104660, loss = 0.79 (273.6 examples/sec; 0.468 sec/batch)
2016-02-03 23:39:13.996191: step 104670, loss = 0.71 (249.7 examples/sec; 0.513 sec/batch)
2016-02-03 23:39:18.533025: step 104680, loss = 0.79 (283.0 examples/sec; 0.452 sec/batch)
2016-02-03 23:39:23.208432: step 104690, loss = 0.71 (265.1 examples/sec; 0.483 sec/batch)
2016-02-03 23:39:27.708750: step 104700, loss = 0.61 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 23:39:32.867068: step 104710, loss = 0.67 (272.7 examples/sec; 0.469 sec/batch)
2016-02-03 23:39:37.502324: step 104720, loss = 0.83 (279.2 examples/sec; 0.458 sec/batch)
2016-02-03 23:39:42.120500: step 104730, loss = 0.80 (292.9 examples/sec; 0.437 sec/batch)
2016-02-03 23:39:46.859787: step 104740, loss = 0.75 (261.3 examples/sec; 0.490 sec/batch)
2016-02-03 23:39:51.695515: step 104750, loss = 0.65 (249.8 examples/sec; 0.512 sec/batch)
2016-02-03 23:39:56.360226: step 104760, loss = 0.60 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 23:40:01.059236: step 104770, loss = 0.59 (291.0 examples/sec; 0.440 sec/batch)
2016-02-03 23:40:05.693281: step 104780, loss = 0.66 (276.5 examples/sec; 0.463 sec/batch)
2016-02-03 23:40:10.424327: step 104790, loss = 0.74 (263.9 examples/sec; 0.485 sec/batch)
2016-02-03 23:40:15.127468: step 104800, loss = 0.68 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 23:40:20.330713: step 104810, loss = 0.85 (297.6 examples/sec; 0.430 sec/batch)
2016-02-03 23:40:25.051876: step 104820, loss = 0.77 (261.2 examples/sec; 0.490 sec/batch)
2016-02-03 23:40:29.622518: step 104830, loss = 0.71 (296.2 examples/sec; 0.432 sec/batch)
2016-02-03 23:40:34.297540: step 104840, loss = 0.59 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 23:40:38.955075: step 104850, loss = 0.74 (274.6 examples/sec; 0.466 sec/batch)
2016-02-03 23:40:43.669176: step 104860, loss = 0.64 (256.2 examples/sec; 0.500 sec/batch)
2016-02-03 23:40:48.301135: step 104870, loss = 0.66 (285.9 examples/sec; 0.448 sec/batch)
2016-02-03 23:40:52.986497: step 104880, loss = 0.62 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 23:40:57.634529: step 104890, loss = 0.74 (280.3 examples/sec; 0.457 sec/batch)
2016-02-03 23:41:02.362947: step 104900, loss = 0.60 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 23:41:07.514096: step 104910, loss = 0.67 (281.3 examples/sec; 0.455 sec/batch)
2016-02-03 23:41:12.163651: step 104920, loss = 0.59 (298.8 examples/sec; 0.428 sec/batch)
2016-02-03 23:41:16.870117: step 104930, loss = 0.69 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 23:41:21.532546: step 104940, loss = 0.75 (284.1 examples/sec; 0.451 sec/batch)
2016-02-03 23:41:26.177260: step 104950, loss = 0.77 (260.2 examples/sec; 0.492 sec/batch)
2016-02-03 23:41:30.853637: step 104960, loss = 0.69 (297.7 examples/sec; 0.430 sec/batch)
2016-02-03 23:41:35.530850: step 104970, loss = 0.69 (271.9 examples/sec; 0.471 sec/batch)
2016-02-03 23:41:40.258415: step 104980, loss = 0.72 (287.3 examples/sec; 0.446 sec/batch)
2016-02-03 23:41:44.832361: step 104990, loss = 0.77 (287.4 examples/sec; 0.445 sec/batch)
2016-02-03 23:41:49.576557: step 105000, loss = 0.71 (261.4 examples/sec; 0.490 sec/batch)
2016-02-03 23:41:54.676347: step 105010, loss = 0.82 (269.5 examples/sec; 0.475 sec/batch)
2016-02-03 23:41:59.439556: step 105020, loss = 0.70 (263.1 examples/sec; 0.487 sec/batch)
2016-02-03 23:42:04.183404: step 105030, loss = 0.67 (258.6 examples/sec; 0.495 sec/batch)
2016-02-03 23:42:08.867800: step 105040, loss = 0.70 (315.8 examples/sec; 0.405 sec/batch)
2016-02-03 23:42:13.597473: step 105050, loss = 0.72 (260.4 examples/sec; 0.492 sec/batch)
2016-02-03 23:42:18.258439: step 105060, loss = 0.73 (297.8 examples/sec; 0.430 sec/batch)
2016-02-03 23:42:23.000370: step 105070, loss = 0.65 (281.5 examples/sec; 0.455 sec/batch)
2016-02-03 23:42:27.571905: step 105080, loss = 0.62 (275.2 examples/sec; 0.465 sec/batch)
2016-02-03 23:42:32.285065: step 105090, loss = 0.70 (270.0 examples/sec; 0.474 sec/batch)
2016-02-03 23:42:36.965302: step 105100, loss = 0.82 (295.4 examples/sec; 0.433 sec/batch)
2016-02-03 23:42:42.157571: step 105110, loss = 0.59 (275.0 examples/sec; 0.465 sec/batch)
2016-02-03 23:42:46.764405: step 105120, loss = 0.71 (287.9 examples/sec; 0.445 sec/batch)
2016-02-03 23:42:51.376130: step 105130, loss = 0.63 (278.7 examples/sec; 0.459 sec/batch)
2016-02-03 23:42:56.025589: step 105140, loss = 0.80 (255.6 examples/sec; 0.501 sec/batch)
2016-02-03 23:43:00.725932: step 105150, loss = 0.66 (259.9 examples/sec; 0.493 sec/batch)
2016-02-03 23:43:05.373175: step 105160, loss = 0.70 (282.5 examples/sec; 0.453 sec/batch)
2016-02-03 23:43:10.098690: step 105170, loss = 0.70 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 23:43:14.792246: step 105180, loss = 0.67 (264.0 examples/sec; 0.485 sec/batch)
2016-02-03 23:43:19.487545: step 105190, loss = 0.69 (281.7 examples/sec; 0.454 sec/batch)
2016-02-03 23:43:24.134578: step 105200, loss = 0.64 (267.5 examples/sec; 0.479 sec/batch)
2016-02-03 23:43:29.347825: step 105210, loss = 0.64 (268.6 examples/sec; 0.477 sec/batch)
2016-02-03 23:43:34.084563: step 105220, loss = 0.51 (255.4 examples/sec; 0.501 sec/batch)
2016-02-03 23:43:38.767183: step 105230, loss = 0.76 (272.6 examples/sec; 0.470 sec/batch)
2016-02-03 23:43:43.510365: step 105240, loss = 0.79 (268.6 examples/sec; 0.476 sec/batch)
2016-02-03 23:43:48.246113: step 105250, loss = 0.63 (276.2 examples/sec; 0.463 sec/batch)
2016-02-03 23:43:52.941935: step 105260, loss = 0.78 (270.8 examples/sec; 0.473 sec/batch)
2016-02-03 23:43:57.522636: step 105270, loss = 0.66 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 23:44:02.196316: step 105280, loss = 0.61 (257.2 examples/sec; 0.498 sec/batch)
2016-02-03 23:44:06.894262: step 105290, loss = 0.74 (260.4 examples/sec; 0.492 sec/batch)
2016-02-03 23:44:11.537330: step 105300, loss = 0.52 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 23:44:16.660246: step 105310, loss = 0.74 (259.1 examples/sec; 0.494 sec/batch)
2016-02-03 23:44:21.273584: step 105320, loss = 0.76 (270.9 examples/sec; 0.472 sec/batch)
2016-02-03 23:44:25.931901: step 105330, loss = 0.54 (267.6 examples/sec; 0.478 sec/batch)
2016-02-03 23:44:30.564400: step 105340, loss = 0.76 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 23:44:35.212310: step 105350, loss = 0.67 (285.3 examples/sec; 0.449 sec/batch)
2016-02-03 23:44:39.962080: step 105360, loss = 0.71 (276.9 examples/sec; 0.462 sec/batch)
2016-02-03 23:44:44.504174: step 105370, loss = 0.73 (267.3 examples/sec; 0.479 sec/batch)
2016-02-03 23:44:49.177893: step 105380, loss = 0.60 (268.7 examples/sec; 0.476 sec/batch)
2016-02-03 23:44:53.727283: step 105390, loss = 0.77 (280.9 examples/sec; 0.456 sec/batch)
2016-02-03 23:44:58.281759: step 105400, loss = 0.67 (283.9 examples/sec; 0.451 sec/batch)
2016-02-03 23:45:03.307328: step 105410, loss = 0.87 (310.0 examples/sec; 0.413 sec/batch)
2016-02-03 23:45:07.906236: step 105420, loss = 0.56 (264.1 examples/sec; 0.485 sec/batch)
2016-02-03 23:45:12.604881: step 105430, loss = 0.58 (276.9 examples/sec; 0.462 sec/batch)
2016-02-03 23:45:17.273272: step 105440, loss = 0.84 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 23:45:21.909122: step 105450, loss = 0.46 (268.1 examples/sec; 0.477 sec/batch)
2016-02-03 23:45:26.472179: step 105460, loss = 0.54 (295.4 examples/sec; 0.433 sec/batch)
2016-02-03 23:45:31.193255: step 105470, loss = 0.63 (269.8 examples/sec; 0.474 sec/batch)
2016-02-03 23:45:35.857733: step 105480, loss = 0.70 (282.8 examples/sec; 0.453 sec/batch)
2016-02-03 23:45:40.497543: step 105490, loss = 0.59 (265.9 examples/sec; 0.481 sec/batch)
2016-02-03 23:45:45.140023: step 105500, loss = 0.64 (283.0 examples/sec; 0.452 sec/batch)
2016-02-03 23:45:50.350528: step 105510, loss = 0.60 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 23:45:55.144587: step 105520, loss = 0.66 (249.2 examples/sec; 0.514 sec/batch)
2016-02-03 23:45:59.802957: step 105530, loss = 0.81 (250.3 examples/sec; 0.511 sec/batch)
2016-02-03 23:46:04.471006: step 105540, loss = 0.86 (283.2 examples/sec; 0.452 sec/batch)
2016-02-03 23:46:09.144654: step 105550, loss = 0.60 (271.5 examples/sec; 0.471 sec/batch)
2016-02-03 23:46:13.848530: step 105560, loss = 0.57 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 23:46:18.503363: step 105570, loss = 0.62 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 23:46:23.218880: step 105580, loss = 0.74 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 23:46:27.888018: step 105590, loss = 0.80 (264.8 examples/sec; 0.483 sec/batch)
2016-02-03 23:46:32.578376: step 105600, loss = 0.69 (282.7 examples/sec; 0.453 sec/batch)
2016-02-03 23:46:37.798627: step 105610, loss = 0.61 (269.3 examples/sec; 0.475 sec/batch)
2016-02-03 23:46:42.481276: step 105620, loss = 0.68 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 23:46:47.269961: step 105630, loss = 0.74 (267.8 examples/sec; 0.478 sec/batch)
2016-02-03 23:46:52.044906: step 105640, loss = 0.73 (249.5 examples/sec; 0.513 sec/batch)
2016-02-03 23:46:56.802557: step 105650, loss = 0.72 (266.2 examples/sec; 0.481 sec/batch)
2016-02-03 23:47:01.456998: step 105660, loss = 0.70 (251.7 examples/sec; 0.509 sec/batch)
2016-02-03 23:47:06.155163: step 105670, loss = 0.61 (290.1 examples/sec; 0.441 sec/batch)
2016-02-03 23:47:10.896451: step 105680, loss = 0.72 (258.8 examples/sec; 0.495 sec/batch)
2016-02-03 23:47:15.643273: step 105690, loss = 0.63 (240.2 examples/sec; 0.533 sec/batch)
2016-02-03 23:47:20.373658: step 105700, loss = 0.71 (281.5 examples/sec; 0.455 sec/batch)
2016-02-03 23:47:25.538373: step 105710, loss = 0.71 (273.8 examples/sec; 0.468 sec/batch)
2016-02-03 23:47:30.243999: step 105720, loss = 0.78 (276.1 examples/sec; 0.464 sec/batch)
2016-02-03 23:47:34.985846: step 105730, loss = 0.59 (254.6 examples/sec; 0.503 sec/batch)
2016-02-03 23:47:39.658276: step 105740, loss = 0.72 (275.1 examples/sec; 0.465 sec/batch)
2016-02-03 23:47:44.425640: step 105750, loss = 0.65 (277.5 examples/sec; 0.461 sec/batch)
2016-02-03 23:47:49.096813: step 105760, loss = 0.70 (287.6 examples/sec; 0.445 sec/batch)
2016-02-03 23:47:53.797458: step 105770, loss = 0.64 (257.4 examples/sec; 0.497 sec/batch)
2016-02-03 23:47:58.520803: step 105780, loss = 0.73 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 23:48:03.200917: step 105790, loss = 0.68 (285.0 examples/sec; 0.449 sec/batch)
2016-02-03 23:48:07.960325: step 105800, loss = 0.74 (287.4 examples/sec; 0.445 sec/batch)
2016-02-03 23:48:13.208449: step 105810, loss = 0.60 (248.4 examples/sec; 0.515 sec/batch)
2016-02-03 23:48:17.824436: step 105820, loss = 0.75 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 23:48:22.572581: step 105830, loss = 0.63 (249.6 examples/sec; 0.513 sec/batch)
2016-02-03 23:48:27.247486: step 105840, loss = 0.73 (277.3 examples/sec; 0.462 sec/batch)
2016-02-03 23:48:31.966507: step 105850, loss = 0.57 (280.3 examples/sec; 0.457 sec/batch)
2016-02-03 23:48:36.657115: step 105860, loss = 0.71 (267.2 examples/sec; 0.479 sec/batch)
2016-02-03 23:48:41.358898: step 105870, loss = 0.74 (271.6 examples/sec; 0.471 sec/batch)
2016-02-03 23:48:46.114203: step 105880, loss = 0.74 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 23:48:50.775083: step 105890, loss = 0.61 (282.1 examples/sec; 0.454 sec/batch)
2016-02-03 23:48:55.476799: step 105900, loss = 0.73 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 23:49:00.692021: step 105910, loss = 0.84 (254.9 examples/sec; 0.502 sec/batch)
2016-02-03 23:49:05.376577: step 105920, loss = 0.77 (282.6 examples/sec; 0.453 sec/batch)
2016-02-03 23:49:10.041935: step 105930, loss = 0.72 (275.8 examples/sec; 0.464 sec/batch)
2016-02-03 23:49:14.752623: step 105940, loss = 0.75 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 23:49:19.466733: step 105950, loss = 0.70 (263.9 examples/sec; 0.485 sec/batch)
2016-02-03 23:49:24.209626: step 105960, loss = 0.66 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 23:49:28.940530: step 105970, loss = 0.62 (267.0 examples/sec; 0.479 sec/batch)
2016-02-03 23:49:33.625707: step 105980, loss = 0.76 (263.8 examples/sec; 0.485 sec/batch)
2016-02-03 23:49:38.341463: step 105990, loss = 0.81 (282.3 examples/sec; 0.453 sec/batch)
2016-02-03 23:49:43.023551: step 106000, loss = 0.59 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 23:49:48.238780: step 106010, loss = 0.71 (262.3 examples/sec; 0.488 sec/batch)
2016-02-03 23:49:52.930441: step 106020, loss = 0.73 (261.6 examples/sec; 0.489 sec/batch)
2016-02-03 23:49:57.653294: step 106030, loss = 0.77 (261.0 examples/sec; 0.490 sec/batch)
2016-02-03 23:50:02.354462: step 106040, loss = 0.71 (298.3 examples/sec; 0.429 sec/batch)
2016-02-03 23:50:07.028598: step 106050, loss = 0.72 (295.1 examples/sec; 0.434 sec/batch)
2016-02-03 23:50:11.695200: step 106060, loss = 0.55 (277.4 examples/sec; 0.461 sec/batch)
2016-02-03 23:50:16.387117: step 106070, loss = 0.76 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 23:50:21.036484: step 106080, loss = 0.64 (276.0 examples/sec; 0.464 sec/batch)
2016-02-03 23:50:25.743064: step 106090, loss = 0.75 (271.7 examples/sec; 0.471 sec/batch)
2016-02-03 23:50:30.416127: step 106100, loss = 0.66 (302.9 examples/sec; 0.423 sec/batch)
2016-02-03 23:50:35.631639: step 106110, loss = 0.59 (303.9 examples/sec; 0.421 sec/batch)
2016-02-03 23:50:40.251680: step 106120, loss = 0.66 (288.1 examples/sec; 0.444 sec/batch)
2016-02-03 23:50:44.976404: step 106130, loss = 0.72 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 23:50:49.637749: step 106140, loss = 0.60 (279.5 examples/sec; 0.458 sec/batch)
2016-02-03 23:50:54.297549: step 106150, loss = 0.72 (287.9 examples/sec; 0.445 sec/batch)
2016-02-03 23:50:59.030708: step 106160, loss = 0.88 (288.7 examples/sec; 0.443 sec/batch)
2016-02-03 23:51:03.806387: step 106170, loss = 0.58 (281.1 examples/sec; 0.455 sec/batch)
2016-02-03 23:51:08.520841: step 106180, loss = 0.63 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 23:51:13.238228: step 106190, loss = 0.69 (246.2 examples/sec; 0.520 sec/batch)
2016-02-03 23:51:17.943335: step 106200, loss = 0.63 (267.4 examples/sec; 0.479 sec/batch)
2016-02-03 23:51:23.105359: step 106210, loss = 0.63 (266.7 examples/sec; 0.480 sec/batch)
2016-02-03 23:51:27.784564: step 106220, loss = 0.54 (287.9 examples/sec; 0.445 sec/batch)
2016-02-03 23:51:32.502765: step 106230, loss = 0.73 (253.1 examples/sec; 0.506 sec/batch)
2016-02-03 23:51:37.143637: step 106240, loss = 0.69 (256.8 examples/sec; 0.498 sec/batch)
2016-02-03 23:51:41.864208: step 106250, loss = 0.61 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 23:51:46.539244: step 106260, loss = 0.79 (284.7 examples/sec; 0.450 sec/batch)
2016-02-03 23:51:51.241399: step 106270, loss = 0.61 (266.0 examples/sec; 0.481 sec/batch)
2016-02-03 23:51:55.893065: step 106280, loss = 0.81 (280.2 examples/sec; 0.457 sec/batch)
2016-02-03 23:52:00.620910: step 106290, loss = 0.75 (249.4 examples/sec; 0.513 sec/batch)
2016-02-03 23:52:05.347350: step 106300, loss = 0.71 (244.7 examples/sec; 0.523 sec/batch)
2016-02-03 23:52:10.561614: step 106310, loss = 0.73 (266.1 examples/sec; 0.481 sec/batch)
2016-02-03 23:52:15.213239: step 106320, loss = 0.69 (283.6 examples/sec; 0.451 sec/batch)
2016-02-03 23:52:19.919387: step 106330, loss = 0.61 (259.6 examples/sec; 0.493 sec/batch)
2016-02-03 23:52:24.565698: step 106340, loss = 0.71 (259.0 examples/sec; 0.494 sec/batch)
2016-02-03 23:52:29.306143: step 106350, loss = 0.69 (273.4 examples/sec; 0.468 sec/batch)
2016-02-03 23:52:34.041824: step 106360, loss = 0.61 (270.2 examples/sec; 0.474 sec/batch)
2016-02-03 23:52:38.711401: step 106370, loss = 0.65 (296.4 examples/sec; 0.432 sec/batch)
2016-02-03 23:52:43.392177: step 106380, loss = 0.60 (272.9 examples/sec; 0.469 sec/batch)
2016-02-03 23:52:48.108088: step 106390, loss = 0.67 (253.1 examples/sec; 0.506 sec/batch)
2016-02-03 23:52:52.859387: step 106400, loss = 0.73 (264.6 examples/sec; 0.484 sec/batch)
2016-02-03 23:52:58.095547: step 106410, loss = 0.71 (278.2 examples/sec; 0.460 sec/batch)
2016-02-03 23:53:02.773276: step 106420, loss = 0.72 (284.3 examples/sec; 0.450 sec/batch)
2016-02-03 23:53:07.444770: step 106430, loss = 0.59 (292.4 examples/sec; 0.438 sec/batch)
2016-02-03 23:53:12.077330: step 106440, loss = 0.69 (274.1 examples/sec; 0.467 sec/batch)
2016-02-03 23:53:16.799798: step 106450, loss = 0.71 (265.5 examples/sec; 0.482 sec/batch)
2016-02-03 23:53:21.469742: step 106460, loss = 0.65 (287.5 examples/sec; 0.445 sec/batch)
2016-02-03 23:53:26.191291: step 106470, loss = 0.64 (275.7 examples/sec; 0.464 sec/batch)
2016-02-03 23:53:30.922335: step 106480, loss = 0.72 (264.7 examples/sec; 0.484 sec/batch)
2016-02-03 23:53:35.643025: step 106490, loss = 0.61 (280.6 examples/sec; 0.456 sec/batch)
2016-02-03 23:53:40.393962: step 106500, loss = 0.60 (249.6 examples/sec; 0.513 sec/batch)
2016-02-03 23:53:45.608963: step 106510, loss = 0.65 (254.5 examples/sec; 0.503 sec/batch)
2016-02-03 23:53:50.280098: step 106520, loss = 0.60 (276.4 examples/sec; 0.463 sec/batch)
2016-02-03 23:53:55.019595: step 106530, loss = 0.75 (285.5 examples/sec; 0.448 sec/batch)
2016-02-03 23:53:59.852670: step 106540, loss = 0.77 (236.0 examples/sec; 0.542 sec/batch)
2016-02-03 23:54:04.591006: step 106550, loss = 0.87 (277.6 examples/sec; 0.461 sec/batch)
2016-02-03 23:54:09.296529: step 106560, loss = 0.80 (267.9 examples/sec; 0.478 sec/batch)
2016-02-03 23:54:14.001205: step 106570, loss = 0.75 (254.8 examples/sec; 0.502 sec/batch)
2016-02-03 23:54:18.713916: step 106580, loss = 0.77 (265.2 examples/sec; 0.483 sec/batch)
2016-02-03 23:54:23.424068: step 106590, loss = 0.70 (256.4 examples/sec; 0.499 sec/batch)
2016-02-03 23:54:28.136417: step 106600, loss = 0.56 (269.2 examples/sec; 0.475 sec/batch)
2016-02-03 23:54:33.425917: step 106610, loss = 0.74 (242.0 examples/sec; 0.529 sec/batch)
2016-02-03 23:54:38.069333: step 106620, loss = 0.55 (273.0 examples/sec; 0.469 sec/batch)
2016-02-03 23:54:42.748227: step 106630, loss = 0.66 (284.6 examples/sec; 0.450 sec/batch)
2016-02-03 23:54:47.377041: step 106640, loss = 0.78 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 23:54:52.081165: step 106650, loss = 0.72 (275.9 examples/sec; 0.464 sec/batch)
2016-02-03 23:54:56.818732: step 106660, loss = 0.65 (255.0 examples/sec; 0.502 sec/batch)
2016-02-03 23:55:01.524895: step 106670, loss = 0.65 (252.3 examples/sec; 0.507 sec/batch)
2016-02-03 23:55:06.291059: step 106680, loss = 0.69 (287.7 examples/sec; 0.445 sec/batch)
2016-02-03 23:55:11.034577: step 106690, loss = 0.64 (281.8 examples/sec; 0.454 sec/batch)
2016-02-03 23:55:15.837376: step 106700, loss = 0.58 (274.9 examples/sec; 0.466 sec/batch)
2016-02-03 23:55:21.109023: step 106710, loss = 0.74 (278.6 examples/sec; 0.459 sec/batch)
2016-02-03 23:55:25.827298: step 106720, loss = 0.57 (274.7 examples/sec; 0.466 sec/batch)
2016-02-03 23:55:30.608996: step 106730, loss = 0.68 (252.0 examples/sec; 0.508 sec/batch)
2016-02-03 23:55:35.380165: step 106740, loss = 0.66 (280.9 examples/sec; 0.456 sec/batch)
2016-02-03 23:55:40.044664: step 106750, loss = 0.63 (296.7 examples/sec; 0.431 sec/batch)
2016-02-03 23:55:44.719243: step 106760, loss = 0.64 (266.5 examples/sec; 0.480 sec/batch)
2016-02-03 23:55:49.464802: step 106770, loss = 0.82 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 23:55:54.176029: step 106780, loss = 0.70 (251.4 examples/sec; 0.509 sec/batch)
2016-02-03 23:55:58.859403: step 106790, loss = 0.85 (276.6 examples/sec; 0.463 sec/batch)
2016-02-03 23:56:03.519039: step 106800, loss = 0.89 (287.5 examples/sec; 0.445 sec/batch)
2016-02-03 23:56:08.780581: step 106810, loss = 0.64 (280.7 examples/sec; 0.456 sec/batch)
2016-02-03 23:56:13.546809: step 106820, loss = 0.72 (257.3 examples/sec; 0.498 sec/batch)
2016-02-03 23:56:18.274484: step 106830, loss = 1.04 (273.7 examples/sec; 0.468 sec/batch)
2016-02-03 23:56:22.964457: step 106840, loss = 0.73 (260.1 examples/sec; 0.492 sec/batch)
2016-02-03 23:56:27.702499: step 106850, loss = 0.70 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 23:56:32.387792: step 106860, loss = 0.68 (282.8 examples/sec; 0.453 sec/batch)
2016-02-03 23:56:37.181761: step 106870, loss = 0.64 (249.5 examples/sec; 0.513 sec/batch)
2016-02-03 23:56:41.867289: step 106880, loss = 0.79 (262.7 examples/sec; 0.487 sec/batch)
2016-02-03 23:56:46.528357: step 106890, loss = 0.59 (281.9 examples/sec; 0.454 sec/batch)
2016-02-03 23:56:51.166673: step 106900, loss = 0.75 (289.4 examples/sec; 0.442 sec/batch)
2016-02-03 23:56:56.382703: step 106910, loss = 0.74 (278.3 examples/sec; 0.460 sec/batch)
2016-02-03 23:57:01.163423: step 106920, loss = 0.65 (277.0 examples/sec; 0.462 sec/batch)
2016-02-03 23:57:05.802603: step 106930, loss = 0.67 (271.0 examples/sec; 0.472 sec/batch)
2016-02-03 23:57:10.570277: step 106940, loss = 0.63 (287.7 examples/sec; 0.445 sec/batch)
2016-02-03 23:57:15.272367: step 106950, loss = 0.62 (275.0 examples/sec; 0.465 sec/batch)
2016-02-03 23:57:20.059613: step 106960, loss = 0.77 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 23:57:24.764362: step 106970, loss = 0.56 (280.8 examples/sec; 0.456 sec/batch)
2016-02-03 23:57:29.539246: step 106980, loss = 0.66 (262.8 examples/sec; 0.487 sec/batch)
2016-02-03 23:57:34.212326: step 106990, loss = 0.70 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 23:57:38.934020: step 107000, loss = 0.66 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 23:57:44.130825: step 107010, loss = 0.76 (275.3 examples/sec; 0.465 sec/batch)
2016-02-03 23:57:48.813964: step 107020, loss = 0.70 (291.6 examples/sec; 0.439 sec/batch)
2016-02-03 23:57:53.592455: step 107030, loss = 0.68 (247.1 examples/sec; 0.518 sec/batch)
2016-02-03 23:57:58.274000: step 107040, loss = 0.80 (285.4 examples/sec; 0.448 sec/batch)
2016-02-03 23:58:03.008672: step 107050, loss = 0.79 (271.4 examples/sec; 0.472 sec/batch)
2016-02-03 23:58:07.725567: step 107060, loss = 0.61 (260.8 examples/sec; 0.491 sec/batch)
2016-02-03 23:58:12.401372: step 107070, loss = 0.81 (293.0 examples/sec; 0.437 sec/batch)
2016-02-03 23:58:17.096031: step 107080, loss = 0.75 (259.8 examples/sec; 0.493 sec/batch)
2016-02-03 23:58:21.760768: step 107090, loss = 0.73 (270.1 examples/sec; 0.474 sec/batch)
2016-02-03 23:58:26.539474: step 107100, loss = 0.68 (257.4 examples/sec; 0.497 sec/batch)
2016-02-03 23:58:31.753595: step 107110, loss = 0.85 (272.3 examples/sec; 0.470 sec/batch)
2016-02-03 23:58:36.484467: step 107120, loss = 0.73 (272.6 examples/sec; 0.469 sec/batch)
2016-02-03 23:58:41.156703: step 107130, loss = 0.62 (281.1 examples/sec; 0.455 sec/batch)
2016-02-03 23:58:45.780545: step 107140, loss = 0.56 (273.5 examples/sec; 0.468 sec/batch)
2016-02-03 23:58:50.496766: step 107150, loss = 0.63 (275.5 examples/sec; 0.465 sec/batch)
2016-02-03 23:58:55.215711: step 107160, loss = 0.69 (284.2 examples/sec; 0.450 sec/batch)
2016-02-03 23:58:59.979652: step 107170, loss = 0.48 (269.1 examples/sec; 0.476 sec/batch)
2016-02-03 23:59:04.734354: step 107180, loss = 0.68 (270.3 examples/sec; 0.474 sec/batch)
2016-02-03 23:59:09.514349: step 107190, loss = 0.74 (272.4 examples/sec; 0.470 sec/batch)
2016-02-03 23:59:14.246254: step 107200, loss = 0.72 (269.7 examples/sec; 0.475 sec/batch)
2016-02-03 23:59:19.491681: step 107210, loss = 0.73 (270.3 examples/sec; 0.473 sec/batch)
2016-02-03 23:59:24.267805: step 107220, loss = 0.72 (262.0 examples/sec; 0.488 sec/batch)
2016-02-03 23:59:28.956352: step 107230, loss = 0.66 (265.7 examples/sec; 0.482 sec/batch)
2016-02-03 23:59:33.782138: step 107240, loss = 0.65 (264.7 examples/sec; 0.484 sec/batch)
2016-02-03 23:59:38.505050: step 107250, loss = 0.78 (271.3 examples/sec; 0.472 sec/batch)
2016-02-03 23:59:43.267714: step 107260, loss = 0.74 (287.7 examples/sec; 0.445 sec/batch)
2016-02-03 23:59:47.996396: step 107270, loss = 0.63 (295.6 examples/sec; 0.433 sec/batch)
2016-02-03 23:59:52.743114: step 107280, loss = 0.84 (288.1 examples/sec; 0.444 sec/batch)
2016-02-03 23:59:57.531690: step 107290, loss = 0.93 (254.7 examples/sec; 0.503 sec/batch)
2016-02-04 00:00:02.199324: step 107300, loss = 0.53 (281.3 examples/sec; 0.455 sec/batch)
2016-02-04 00:00:07.447643: step 107310, loss = 0.67 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 00:00:12.241999: step 107320, loss = 0.56 (277.6 examples/sec; 0.461 sec/batch)
2016-02-04 00:00:16.978085: step 107330, loss = 0.78 (256.7 examples/sec; 0.499 sec/batch)
2016-02-04 00:00:21.645428: step 107340, loss = 0.69 (272.4 examples/sec; 0.470 sec/batch)
2016-02-04 00:00:26.419141: step 107350, loss = 0.57 (278.2 examples/sec; 0.460 sec/batch)
2016-02-04 00:00:31.116078: step 107360, loss = 0.73 (253.6 examples/sec; 0.505 sec/batch)
2016-02-04 00:00:35.721394: step 107370, loss = 0.81 (266.9 examples/sec; 0.480 sec/batch)
2016-02-04 00:00:40.468417: step 107380, loss = 0.51 (278.3 examples/sec; 0.460 sec/batch)
2016-02-04 00:00:45.170393: step 107390, loss = 0.77 (267.1 examples/sec; 0.479 sec/batch)
2016-02-04 00:00:49.921808: step 107400, loss = 0.67 (259.3 examples/sec; 0.494 sec/batch)
2016-02-04 00:00:55.128782: step 107410, loss = 0.57 (244.2 examples/sec; 0.524 sec/batch)
2016-02-04 00:00:59.852013: step 107420, loss = 0.67 (280.3 examples/sec; 0.457 sec/batch)
2016-02-04 00:01:04.635288: step 107430, loss = 0.68 (255.1 examples/sec; 0.502 sec/batch)
2016-02-04 00:01:09.345656: step 107440, loss = 0.71 (286.6 examples/sec; 0.447 sec/batch)
2016-02-04 00:01:13.981480: step 107450, loss = 0.71 (297.5 examples/sec; 0.430 sec/batch)
2016-02-04 00:01:18.623056: step 107460, loss = 0.56 (281.9 examples/sec; 0.454 sec/batch)
2016-02-04 00:01:23.432289: step 107470, loss = 0.62 (245.9 examples/sec; 0.521 sec/batch)
2016-02-04 00:01:28.141044: step 107480, loss = 0.62 (263.6 examples/sec; 0.486 sec/batch)
2016-02-04 00:01:32.828915: step 107490, loss = 0.78 (272.7 examples/sec; 0.469 sec/batch)
2016-02-04 00:01:37.506707: step 107500, loss = 0.76 (301.1 examples/sec; 0.425 sec/batch)
2016-02-04 00:01:42.670230: step 107510, loss = 0.58 (280.0 examples/sec; 0.457 sec/batch)
2016-02-04 00:01:47.424931: step 107520, loss = 0.80 (260.0 examples/sec; 0.492 sec/batch)
2016-02-04 00:01:52.152391: step 107530, loss = 0.74 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 00:01:56.794779: step 107540, loss = 0.66 (283.6 examples/sec; 0.451 sec/batch)
2016-02-04 00:02:01.479845: step 107550, loss = 0.62 (262.6 examples/sec; 0.488 sec/batch)
2016-02-04 00:02:06.130907: step 107560, loss = 0.71 (295.9 examples/sec; 0.433 sec/batch)
2016-02-04 00:02:10.878272: step 107570, loss = 0.66 (256.7 examples/sec; 0.499 sec/batch)
2016-02-04 00:02:15.660461: step 107580, loss = 0.80 (271.5 examples/sec; 0.472 sec/batch)
2016-02-04 00:02:20.280091: step 107590, loss = 0.60 (283.0 examples/sec; 0.452 sec/batch)
2016-02-04 00:02:24.971207: step 107600, loss = 0.78 (284.0 examples/sec; 0.451 sec/batch)
2016-02-04 00:02:30.178268: step 107610, loss = 0.76 (263.6 examples/sec; 0.486 sec/batch)
2016-02-04 00:02:34.866910: step 107620, loss = 0.99 (281.8 examples/sec; 0.454 sec/batch)
2016-02-04 00:02:39.603339: step 107630, loss = 0.71 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 00:02:44.290991: step 107640, loss = 0.68 (262.7 examples/sec; 0.487 sec/batch)
2016-02-04 00:02:48.990746: step 107650, loss = 0.78 (267.7 examples/sec; 0.478 sec/batch)
2016-02-04 00:02:53.669828: step 107660, loss = 0.58 (279.8 examples/sec; 0.457 sec/batch)
2016-02-04 00:02:58.323116: step 107670, loss = 0.61 (273.9 examples/sec; 0.467 sec/batch)
2016-02-04 00:03:03.031663: step 107680, loss = 0.68 (279.7 examples/sec; 0.458 sec/batch)
2016-02-04 00:03:07.691957: step 107690, loss = 0.66 (253.9 examples/sec; 0.504 sec/batch)
2016-02-04 00:03:12.323496: step 107700, loss = 0.68 (288.2 examples/sec; 0.444 sec/batch)
2016-02-04 00:03:17.613889: step 107710, loss = 0.68 (263.8 examples/sec; 0.485 sec/batch)
2016-02-04 00:03:22.324464: step 107720, loss = 0.70 (274.8 examples/sec; 0.466 sec/batch)
2016-02-04 00:03:27.064782: step 107730, loss = 0.66 (281.1 examples/sec; 0.455 sec/batch)
2016-02-04 00:03:31.829320: step 107740, loss = 0.77 (286.9 examples/sec; 0.446 sec/batch)
2016-02-04 00:03:36.556996: step 107750, loss = 0.65 (275.4 examples/sec; 0.465 sec/batch)
2016-02-04 00:03:41.279649: step 107760, loss = 0.72 (254.3 examples/sec; 0.503 sec/batch)
2016-02-04 00:03:45.972180: step 107770, loss = 0.64 (277.3 examples/sec; 0.462 sec/batch)
2016-02-04 00:03:50.695472: step 107780, loss = 0.68 (262.2 examples/sec; 0.488 sec/batch)
2016-02-04 00:03:55.313296: step 107790, loss = 0.74 (297.8 examples/sec; 0.430 sec/batch)
2016-02-04 00:03:59.953056: step 107800, loss = 0.66 (288.9 examples/sec; 0.443 sec/batch)
2016-02-04 00:04:05.164525: step 107810, loss = 0.68 (268.3 examples/sec; 0.477 sec/batch)
2016-02-04 00:04:09.858463: step 107820, loss = 0.64 (270.8 examples/sec; 0.473 sec/batch)
2016-02-04 00:04:14.520196: step 107830, loss = 0.68 (306.9 examples/sec; 0.417 sec/batch)
2016-02-04 00:04:19.245165: step 107840, loss = 0.66 (265.1 examples/sec; 0.483 sec/batch)
2016-02-04 00:04:23.951405: step 107850, loss = 0.73 (266.9 examples/sec; 0.480 sec/batch)
2016-02-04 00:04:28.678374: step 107860, loss = 0.52 (261.1 examples/sec; 0.490 sec/batch)
2016-02-04 00:04:33.322815: step 107870, loss = 0.63 (267.2 examples/sec; 0.479 sec/batch)
2016-02-04 00:04:37.920169: step 107880, loss = 0.70 (259.2 examples/sec; 0.494 sec/batch)
2016-02-04 00:04:42.486094: step 107890, loss = 0.72 (296.3 examples/sec; 0.432 sec/batch)
2016-02-04 00:04:47.160015: step 107900, loss = 0.78 (295.4 examples/sec; 0.433 sec/batch)
2016-02-04 00:04:52.504752: step 107910, loss = 0.59 (250.5 examples/sec; 0.511 sec/batch)
2016-02-04 00:04:57.194278: step 107920, loss = 0.68 (270.4 examples/sec; 0.473 sec/batch)
2016-02-04 00:05:01.910575: step 107930, loss = 0.75 (269.4 examples/sec; 0.475 sec/batch)
2016-02-04 00:05:06.606441: step 107940, loss = 0.66 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 00:05:11.334325: step 107950, loss = 0.70 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 00:05:16.059832: step 107960, loss = 0.76 (282.2 examples/sec; 0.454 sec/batch)
2016-02-04 00:05:20.771162: step 107970, loss = 0.73 (264.0 examples/sec; 0.485 sec/batch)
2016-02-04 00:05:25.405626: step 107980, loss = 0.67 (273.1 examples/sec; 0.469 sec/batch)
2016-02-04 00:05:30.253839: step 107990, loss = 0.58 (261.9 examples/sec; 0.489 sec/batch)
2016-02-04 00:05:34.891065: step 108000, loss = 0.56 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 00:05:40.151325: step 108010, loss = 0.85 (279.4 examples/sec; 0.458 sec/batch)
2016-02-04 00:05:44.831350: step 108020, loss = 0.84 (273.6 examples/sec; 0.468 sec/batch)
2016-02-04 00:05:49.567357: step 108030, loss = 0.67 (269.7 examples/sec; 0.475 sec/batch)
2016-02-04 00:05:54.226350: step 108040, loss = 0.70 (264.1 examples/sec; 0.485 sec/batch)
2016-02-04 00:05:58.872282: step 108050, loss = 0.65 (297.2 examples/sec; 0.431 sec/batch)
2016-02-04 00:06:03.569488: step 108060, loss = 0.62 (291.2 examples/sec; 0.440 sec/batch)
2016-02-04 00:06:08.239163: step 108070, loss = 0.67 (271.2 examples/sec; 0.472 sec/batch)
2016-02-04 00:06:12.926847: step 108080, loss = 0.76 (289.4 examples/sec; 0.442 sec/batch)
2016-02-04 00:06:17.598729: step 108090, loss = 0.70 (267.5 examples/sec; 0.478 sec/batch)
2016-02-04 00:06:22.262154: step 108100, loss = 0.71 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 00:06:27.513339: step 108110, loss = 0.58 (271.0 examples/sec; 0.472 sec/batch)
2016-02-04 00:06:32.234811: step 108120, loss = 0.77 (288.8 examples/sec; 0.443 sec/batch)
2016-02-04 00:06:36.944808: step 108130, loss = 0.75 (262.8 examples/sec; 0.487 sec/batch)
2016-02-04 00:06:41.531367: step 108140, loss = 0.67 (305.0 examples/sec; 0.420 sec/batch)
2016-02-04 00:06:46.325393: step 108150, loss = 0.62 (263.8 examples/sec; 0.485 sec/batch)
2016-02-04 00:06:50.949243: step 108160, loss = 0.60 (283.3 examples/sec; 0.452 sec/batch)
2016-02-04 00:06:55.599996: step 108170, loss = 0.68 (299.5 examples/sec; 0.427 sec/batch)
2016-02-04 00:07:00.340955: step 108180, loss = 0.59 (265.1 examples/sec; 0.483 sec/batch)
2016-02-04 00:07:05.044152: step 108190, loss = 0.63 (267.1 examples/sec; 0.479 sec/batch)
2016-02-04 00:07:09.773067: step 108200, loss = 0.66 (243.8 examples/sec; 0.525 sec/batch)
2016-02-04 00:07:14.963283: step 108210, loss = 0.70 (277.5 examples/sec; 0.461 sec/batch)
2016-02-04 00:07:19.562888: step 108220, loss = 0.64 (268.9 examples/sec; 0.476 sec/batch)
2016-02-04 00:07:24.283083: step 108230, loss = 0.78 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 00:07:28.981217: step 108240, loss = 0.58 (266.8 examples/sec; 0.480 sec/batch)
2016-02-04 00:07:33.689487: step 108250, loss = 0.70 (268.6 examples/sec; 0.477 sec/batch)
2016-02-04 00:07:38.383903: step 108260, loss = 0.71 (294.4 examples/sec; 0.435 sec/batch)
2016-02-04 00:07:43.134453: step 108270, loss = 0.90 (261.1 examples/sec; 0.490 sec/batch)
2016-02-04 00:07:47.854573: step 108280, loss = 0.64 (280.5 examples/sec; 0.456 sec/batch)
2016-02-04 00:07:52.489062: step 108290, loss = 0.71 (296.1 examples/sec; 0.432 sec/batch)
2016-02-04 00:07:57.213608: step 108300, loss = 0.70 (273.9 examples/sec; 0.467 sec/batch)
2016-02-04 00:08:02.513621: step 108310, loss = 0.75 (270.2 examples/sec; 0.474 sec/batch)
2016-02-04 00:08:07.166452: step 108320, loss = 0.79 (272.8 examples/sec; 0.469 sec/batch)
2016-02-04 00:08:11.856392: step 108330, loss = 0.62 (280.0 examples/sec; 0.457 sec/batch)
2016-02-04 00:08:16.622141: step 108340, loss = 0.66 (256.0 examples/sec; 0.500 sec/batch)
2016-02-04 00:08:21.267988: step 108350, loss = 0.70 (306.8 examples/sec; 0.417 sec/batch)
2016-02-04 00:08:26.003467: step 108360, loss = 0.65 (255.0 examples/sec; 0.502 sec/batch)
2016-02-04 00:08:30.749187: step 108370, loss = 0.62 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 00:08:35.443841: step 108380, loss = 0.74 (271.0 examples/sec; 0.472 sec/batch)
2016-02-04 00:08:40.131148: step 108390, loss = 0.67 (254.9 examples/sec; 0.502 sec/batch)
2016-02-04 00:08:44.783225: step 108400, loss = 0.72 (319.4 examples/sec; 0.401 sec/batch)
2016-02-04 00:08:50.015286: step 108410, loss = 0.78 (281.5 examples/sec; 0.455 sec/batch)
2016-02-04 00:08:54.748632: step 108420, loss = 0.86 (276.1 examples/sec; 0.464 sec/batch)
2016-02-04 00:08:59.532736: step 108430, loss = 0.66 (281.0 examples/sec; 0.456 sec/batch)
2016-02-04 00:09:04.211118: step 108440, loss = 0.83 (278.6 examples/sec; 0.459 sec/batch)
2016-02-04 00:09:08.815643: step 108450, loss = 0.64 (283.6 examples/sec; 0.451 sec/batch)
2016-02-04 00:09:13.561858: step 108460, loss = 0.65 (281.5 examples/sec; 0.455 sec/batch)
2016-02-04 00:09:18.313547: step 108470, loss = 0.77 (263.9 examples/sec; 0.485 sec/batch)
2016-02-04 00:09:22.949106: step 108480, loss = 0.69 (288.5 examples/sec; 0.444 sec/batch)
2016-02-04 00:09:27.622784: step 108490, loss = 0.84 (273.4 examples/sec; 0.468 sec/batch)
2016-02-04 00:09:32.252553: step 108500, loss = 0.63 (262.1 examples/sec; 0.488 sec/batch)
2016-02-04 00:09:37.427024: step 108510, loss = 0.61 (260.5 examples/sec; 0.491 sec/batch)
2016-02-04 00:09:42.113121: step 108520, loss = 0.70 (249.6 examples/sec; 0.513 sec/batch)
2016-02-04 00:09:46.799028: step 108530, loss = 0.65 (289.1 examples/sec; 0.443 sec/batch)
2016-02-04 00:09:51.607626: step 108540, loss = 0.62 (284.9 examples/sec; 0.449 sec/batch)
2016-02-04 00:09:56.249756: step 108550, loss = 0.73 (264.6 examples/sec; 0.484 sec/batch)
2016-02-04 00:10:00.925998: step 108560, loss = 0.75 (273.2 examples/sec; 0.468 sec/batch)
2016-02-04 00:10:05.571601: step 108570, loss = 0.67 (279.9 examples/sec; 0.457 sec/batch)
2016-02-04 00:10:10.276348: step 108580, loss = 0.73 (272.4 examples/sec; 0.470 sec/batch)
2016-02-04 00:10:15.074324: step 108590, loss = 0.70 (284.7 examples/sec; 0.450 sec/batch)
2016-02-04 00:10:19.724999: step 108600, loss = 0.73 (262.9 examples/sec; 0.487 sec/batch)
2016-02-04 00:10:24.992433: step 108610, loss = 0.70 (282.8 examples/sec; 0.453 sec/batch)
2016-02-04 00:10:29.755800: step 108620, loss = 0.66 (240.0 examples/sec; 0.533 sec/batch)
2016-02-04 00:10:34.460703: step 108630, loss = 0.69 (268.3 examples/sec; 0.477 sec/batch)
2016-02-04 00:10:39.221059: step 108640, loss = 0.61 (287.1 examples/sec; 0.446 sec/batch)
2016-02-04 00:10:44.024338: step 108650, loss = 0.66 (252.2 examples/sec; 0.508 sec/batch)
2016-02-04 00:10:48.798126: step 108660, loss = 0.74 (236.5 examples/sec; 0.541 sec/batch)
2016-02-04 00:10:53.431529: step 108670, loss = 0.63 (274.6 examples/sec; 0.466 sec/batch)
2016-02-04 00:10:58.139617: step 108680, loss = 0.53 (288.7 examples/sec; 0.443 sec/batch)
2016-02-04 00:11:02.857200: step 108690, loss = 0.72 (281.8 examples/sec; 0.454 sec/batch)
2016-02-04 00:11:07.536426: step 108700, loss = 0.65 (271.3 examples/sec; 0.472 sec/batch)
2016-02-04 00:11:12.737911: step 108710, loss = 0.69 (287.6 examples/sec; 0.445 sec/batch)
2016-02-04 00:11:17.386607: step 108720, loss = 0.78 (254.8 examples/sec; 0.502 sec/batch)
2016-02-04 00:11:22.109179: step 108730, loss = 0.69 (280.4 examples/sec; 0.457 sec/batch)
2016-02-04 00:11:26.871007: step 108740, loss = 0.72 (258.5 examples/sec; 0.495 sec/batch)
2016-02-04 00:11:31.626481: step 108750, loss = 0.90 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 00:11:36.326134: step 108760, loss = 0.75 (265.8 examples/sec; 0.481 sec/batch)
2016-02-04 00:11:41.129075: step 108770, loss = 0.91 (248.6 examples/sec; 0.515 sec/batch)
2016-02-04 00:11:45.910563: step 108780, loss = 0.61 (259.0 examples/sec; 0.494 sec/batch)
2016-02-04 00:11:50.675874: step 108790, loss = 0.59 (258.2 examples/sec; 0.496 sec/batch)
2016-02-04 00:11:55.344904: step 108800, loss = 0.64 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 00:12:00.512146: step 108810, loss = 0.61 (283.6 examples/sec; 0.451 sec/batch)
2016-02-04 00:12:05.240541: step 108820, loss = 0.70 (277.4 examples/sec; 0.461 sec/batch)
2016-02-04 00:12:09.919441: step 108830, loss = 0.69 (258.8 examples/sec; 0.495 sec/batch)
2016-02-04 00:12:14.559289: step 108840, loss = 0.61 (274.8 examples/sec; 0.466 sec/batch)
2016-02-04 00:12:19.330485: step 108850, loss = 0.63 (254.0 examples/sec; 0.504 sec/batch)
2016-02-04 00:12:23.944882: step 108860, loss = 0.66 (274.4 examples/sec; 0.467 sec/batch)
2016-02-04 00:12:28.650184: step 108870, loss = 0.70 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 00:12:33.322659: step 108880, loss = 0.82 (256.6 examples/sec; 0.499 sec/batch)
2016-02-04 00:12:37.988606: step 108890, loss = 0.63 (276.6 examples/sec; 0.463 sec/batch)
2016-02-04 00:12:42.655554: step 108900, loss = 0.60 (273.4 examples/sec; 0.468 sec/batch)
2016-02-04 00:12:47.843431: step 108910, loss = 0.62 (291.7 examples/sec; 0.439 sec/batch)
2016-02-04 00:12:52.500454: step 108920, loss = 0.73 (267.8 examples/sec; 0.478 sec/batch)
2016-02-04 00:12:57.216938: step 108930, loss = 0.67 (249.6 examples/sec; 0.513 sec/batch)
2016-02-04 00:13:01.884108: step 108940, loss = 0.71 (267.9 examples/sec; 0.478 sec/batch)
2016-02-04 00:13:06.584732: step 108950, loss = 0.68 (279.3 examples/sec; 0.458 sec/batch)
2016-02-04 00:13:11.258353: step 108960, loss = 0.72 (289.1 examples/sec; 0.443 sec/batch)
2016-02-04 00:13:15.943246: step 108970, loss = 0.82 (281.9 examples/sec; 0.454 sec/batch)
2016-02-04 00:13:20.702145: step 108980, loss = 0.68 (254.2 examples/sec; 0.504 sec/batch)
2016-02-04 00:13:25.426703: step 108990, loss = 0.62 (256.8 examples/sec; 0.498 sec/batch)
2016-02-04 00:13:30.081182: step 109000, loss = 0.70 (260.8 examples/sec; 0.491 sec/batch)
2016-02-04 00:13:35.271744: step 109010, loss = 0.57 (267.3 examples/sec; 0.479 sec/batch)
2016-02-04 00:13:39.981745: step 109020, loss = 0.79 (283.9 examples/sec; 0.451 sec/batch)
2016-02-04 00:13:44.748305: step 109030, loss = 0.65 (251.3 examples/sec; 0.509 sec/batch)
2016-02-04 00:13:49.393662: step 109040, loss = 0.67 (264.9 examples/sec; 0.483 sec/batch)
2016-02-04 00:13:54.094992: step 109050, loss = 0.78 (287.2 examples/sec; 0.446 sec/batch)
2016-02-04 00:13:58.806155: step 109060, loss = 0.74 (282.7 examples/sec; 0.453 sec/batch)
2016-02-04 00:14:03.599239: step 109070, loss = 0.75 (264.3 examples/sec; 0.484 sec/batch)
2016-02-04 00:14:08.333457: step 109080, loss = 0.68 (270.3 examples/sec; 0.473 sec/batch)
2016-02-04 00:14:13.006751: step 109090, loss = 0.66 (294.3 examples/sec; 0.435 sec/batch)
2016-02-04 00:14:17.779182: step 109100, loss = 0.66 (272.9 examples/sec; 0.469 sec/batch)
2016-02-04 00:14:22.930481: step 109110, loss = 0.71 (286.3 examples/sec; 0.447 sec/batch)
2016-02-04 00:14:27.598012: step 109120, loss = 0.72 (286.4 examples/sec; 0.447 sec/batch)
2016-02-04 00:14:32.266560: step 109130, loss = 0.64 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 00:14:36.896187: step 109140, loss = 0.87 (270.3 examples/sec; 0.474 sec/batch)
2016-02-04 00:14:41.481610: step 109150, loss = 0.71 (290.2 examples/sec; 0.441 sec/batch)
2016-02-04 00:14:46.211201: step 109160, loss = 0.68 (279.3 examples/sec; 0.458 sec/batch)
2016-02-04 00:14:50.987783: step 109170, loss = 0.54 (253.7 examples/sec; 0.505 sec/batch)
2016-02-04 00:14:55.633006: step 109180, loss = 0.74 (265.4 examples/sec; 0.482 sec/batch)
2016-02-04 00:15:00.354093: step 109190, loss = 0.71 (259.2 examples/sec; 0.494 sec/batch)
2016-02-04 00:15:05.086072: step 109200, loss = 0.72 (279.3 examples/sec; 0.458 sec/batch)
2016-02-04 00:15:10.313636: step 109210, loss = 0.67 (278.6 examples/sec; 0.459 sec/batch)
2016-02-04 00:15:15.001998: step 109220, loss = 0.65 (312.0 examples/sec; 0.410 sec/batch)
2016-02-04 00:15:19.654524: step 109230, loss = 0.78 (274.8 examples/sec; 0.466 sec/batch)
2016-02-04 00:15:24.226369: step 109240, loss = 0.73 (284.0 examples/sec; 0.451 sec/batch)
2016-02-04 00:15:28.828796: step 109250, loss = 0.75 (258.7 examples/sec; 0.495 sec/batch)
2016-02-04 00:15:33.583002: step 109260, loss = 0.68 (253.0 examples/sec; 0.506 sec/batch)
2016-02-04 00:15:38.244813: step 109270, loss = 0.66 (263.5 examples/sec; 0.486 sec/batch)
2016-02-04 00:15:42.997519: step 109280, loss = 0.65 (272.3 examples/sec; 0.470 sec/batch)
2016-02-04 00:15:47.712955: step 109290, loss = 0.66 (266.2 examples/sec; 0.481 sec/batch)
2016-02-04 00:15:52.455681: step 109300, loss = 0.64 (276.1 examples/sec; 0.464 sec/batch)
2016-02-04 00:15:57.717431: step 109310, loss = 0.64 (253.0 examples/sec; 0.506 sec/batch)
2016-02-04 00:16:02.428711: step 109320, loss = 0.80 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 00:16:07.087951: step 109330, loss = 0.71 (272.7 examples/sec; 0.469 sec/batch)
2016-02-04 00:16:11.784166: step 109340, loss = 0.54 (265.2 examples/sec; 0.483 sec/batch)
2016-02-04 00:16:16.479772: step 109350, loss = 0.67 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 00:16:21.233041: step 109360, loss = 0.57 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 00:16:25.912071: step 109370, loss = 0.75 (265.5 examples/sec; 0.482 sec/batch)
2016-02-04 00:16:30.562068: step 109380, loss = 0.81 (286.1 examples/sec; 0.447 sec/batch)
2016-02-04 00:16:35.300053: step 109390, loss = 0.63 (277.0 examples/sec; 0.462 sec/batch)
2016-02-04 00:16:39.987140: step 109400, loss = 0.86 (260.1 examples/sec; 0.492 sec/batch)
2016-02-04 00:16:45.151653: step 109410, loss = 0.65 (255.9 examples/sec; 0.500 sec/batch)
2016-02-04 00:16:49.808446: step 109420, loss = 0.63 (266.0 examples/sec; 0.481 sec/batch)
2016-02-04 00:16:54.431547: step 109430, loss = 0.73 (280.7 examples/sec; 0.456 sec/batch)
2016-02-04 00:16:59.169017: step 109440, loss = 0.54 (290.4 examples/sec; 0.441 sec/batch)
2016-02-04 00:17:03.849547: step 109450, loss = 0.65 (300.6 examples/sec; 0.426 sec/batch)
2016-02-04 00:17:08.526645: step 109460, loss = 0.73 (271.1 examples/sec; 0.472 sec/batch)
2016-02-04 00:17:13.257198: step 109470, loss = 0.61 (253.3 examples/sec; 0.505 sec/batch)
2016-02-04 00:17:17.976406: step 109480, loss = 0.63 (252.9 examples/sec; 0.506 sec/batch)
2016-02-04 00:17:22.707779: step 109490, loss = 0.76 (264.7 examples/sec; 0.484 sec/batch)
2016-02-04 00:17:27.363482: step 109500, loss = 0.46 (278.9 examples/sec; 0.459 sec/batch)
2016-02-04 00:17:32.587699: step 109510, loss = 0.75 (287.1 examples/sec; 0.446 sec/batch)
2016-02-04 00:17:37.326677: step 109520, loss = 0.85 (273.4 examples/sec; 0.468 sec/batch)
2016-02-04 00:17:41.959170: step 109530, loss = 0.59 (306.0 examples/sec; 0.418 sec/batch)
2016-02-04 00:17:46.662509: step 109540, loss = 0.58 (287.3 examples/sec; 0.446 sec/batch)
2016-02-04 00:17:51.335948: step 109550, loss = 0.77 (290.4 examples/sec; 0.441 sec/batch)
2016-02-04 00:17:56.061822: step 109560, loss = 0.71 (254.2 examples/sec; 0.504 sec/batch)
2016-02-04 00:18:00.759174: step 109570, loss = 0.81 (266.2 examples/sec; 0.481 sec/batch)
2016-02-04 00:18:05.507400: step 109580, loss = 0.65 (269.3 examples/sec; 0.475 sec/batch)
2016-02-04 00:18:10.234554: step 109590, loss = 0.77 (267.0 examples/sec; 0.479 sec/batch)
2016-02-04 00:18:14.986552: step 109600, loss = 0.79 (277.5 examples/sec; 0.461 sec/batch)
2016-02-04 00:18:20.214750: step 109610, loss = 0.87 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 00:18:24.872883: step 109620, loss = 0.62 (302.4 examples/sec; 0.423 sec/batch)
2016-02-04 00:18:29.601574: step 109630, loss = 0.66 (287.2 examples/sec; 0.446 sec/batch)
2016-02-04 00:18:34.386874: step 109640, loss = 0.74 (279.4 examples/sec; 0.458 sec/batch)
2016-02-04 00:18:39.102975: step 109650, loss = 0.60 (291.5 examples/sec; 0.439 sec/batch)
2016-02-04 00:18:43.903099: step 109660, loss = 0.61 (254.5 examples/sec; 0.503 sec/batch)
2016-02-04 00:18:48.577288: step 109670, loss = 0.76 (282.9 examples/sec; 0.453 sec/batch)
2016-02-04 00:18:53.324492: step 109680, loss = 0.66 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 00:18:58.026402: step 109690, loss = 0.79 (285.7 examples/sec; 0.448 sec/batch)
2016-02-04 00:19:02.673186: step 109700, loss = 0.71 (272.9 examples/sec; 0.469 sec/batch)
2016-02-04 00:19:07.843817: step 109710, loss = 0.72 (281.9 examples/sec; 0.454 sec/batch)
2016-02-04 00:19:12.633606: step 109720, loss = 0.68 (267.9 examples/sec; 0.478 sec/batch)
2016-02-04 00:19:17.272327: step 109730, loss = 0.73 (292.5 examples/sec; 0.438 sec/batch)
2016-02-04 00:19:21.902949: step 109740, loss = 0.76 (280.3 examples/sec; 0.457 sec/batch)
2016-02-04 00:19:26.613249: step 109750, loss = 0.60 (282.2 examples/sec; 0.454 sec/batch)
2016-02-04 00:19:31.317409: step 109760, loss = 0.57 (254.4 examples/sec; 0.503 sec/batch)
2016-02-04 00:19:35.986978: step 109770, loss = 0.71 (272.2 examples/sec; 0.470 sec/batch)
2016-02-04 00:19:40.639053: step 109780, loss = 0.67 (283.2 examples/sec; 0.452 sec/batch)
2016-02-04 00:19:45.306769: step 109790, loss = 0.84 (291.8 examples/sec; 0.439 sec/batch)
2016-02-04 00:19:50.034170: step 109800, loss = 0.60 (249.0 examples/sec; 0.514 sec/batch)
2016-02-04 00:19:55.191035: step 109810, loss = 0.56 (275.5 examples/sec; 0.465 sec/batch)
2016-02-04 00:19:59.864648: step 109820, loss = 0.62 (267.1 examples/sec; 0.479 sec/batch)
2016-02-04 00:20:04.646564: step 109830, loss = 0.69 (277.0 examples/sec; 0.462 sec/batch)
2016-02-04 00:20:09.349120: step 109840, loss = 0.65 (290.0 examples/sec; 0.441 sec/batch)
2016-02-04 00:20:14.042663: step 109850, loss = 0.62 (272.1 examples/sec; 0.470 sec/batch)
2016-02-04 00:20:18.759420: step 109860, loss = 0.81 (292.4 examples/sec; 0.438 sec/batch)
2016-02-04 00:20:23.497109: step 109870, loss = 0.61 (255.7 examples/sec; 0.501 sec/batch)
2016-02-04 00:20:28.261913: step 109880, loss = 0.64 (265.4 examples/sec; 0.482 sec/batch)
2016-02-04 00:20:32.939020: step 109890, loss = 0.83 (280.5 examples/sec; 0.456 sec/batch)
2016-02-04 00:20:37.698149: step 109900, loss = 0.82 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 00:20:42.924171: step 109910, loss = 0.63 (258.7 examples/sec; 0.495 sec/batch)
2016-02-04 00:20:47.608696: step 109920, loss = 0.70 (276.6 examples/sec; 0.463 sec/batch)
2016-02-04 00:20:52.341862: step 109930, loss = 0.64 (266.9 examples/sec; 0.480 sec/batch)
2016-02-04 00:20:57.112650: step 109940, loss = 0.74 (244.5 examples/sec; 0.524 sec/batch)
2016-02-04 00:21:01.777375: step 109950, loss = 0.65 (258.0 examples/sec; 0.496 sec/batch)
2016-02-04 00:21:06.531196: step 109960, loss = 0.73 (273.7 examples/sec; 0.468 sec/batch)
2016-02-04 00:21:11.223694: step 109970, loss = 0.63 (255.8 examples/sec; 0.500 sec/batch)
2016-02-04 00:21:15.912966: step 109980, loss = 0.58 (273.8 examples/sec; 0.467 sec/batch)
2016-02-04 00:21:20.595429: step 109990, loss = 0.65 (267.5 examples/sec; 0.479 sec/batch)
2016-02-04 00:21:25.233003: step 110000, loss = 0.64 (265.0 examples/sec; 0.483 sec/batch)
2016-02-04 00:21:30.437122: step 110010, loss = 0.58 (264.4 examples/sec; 0.484 sec/batch)
2016-02-04 00:21:35.142639: step 110020, loss = 0.61 (275.8 examples/sec; 0.464 sec/batch)
2016-02-04 00:21:39.807282: step 110030, loss = 0.73 (267.8 examples/sec; 0.478 sec/batch)
2016-02-04 00:21:44.533471: step 110040, loss = 0.70 (252.3 examples/sec; 0.507 sec/batch)
2016-02-04 00:21:49.236201: step 110050, loss = 0.62 (256.1 examples/sec; 0.500 sec/batch)
2016-02-04 00:21:54.012118: step 110060, loss = 0.58 (270.2 examples/sec; 0.474 sec/batch)
2016-02-04 00:21:58.722614: step 110070, loss = 0.55 (281.7 examples/sec; 0.454 sec/batch)
2016-02-04 00:22:03.455998: step 110080, loss = 0.67 (262.8 examples/sec; 0.487 sec/batch)
2016-02-04 00:22:08.192785: step 110090, loss = 0.78 (265.1 examples/sec; 0.483 sec/batch)
2016-02-04 00:22:12.902424: step 110100, loss = 0.68 (274.6 examples/sec; 0.466 sec/batch)
2016-02-04 00:22:18.159266: step 110110, loss = 0.62 (257.1 examples/sec; 0.498 sec/batch)
2016-02-04 00:22:22.909131: step 110120, loss = 0.66 (261.9 examples/sec; 0.489 sec/batch)
2016-02-04 00:22:27.630714: step 110130, loss = 0.65 (285.8 examples/sec; 0.448 sec/batch)
2016-02-04 00:22:32.443673: step 110140, loss = 0.66 (258.2 examples/sec; 0.496 sec/batch)
2016-02-04 00:22:37.169755: step 110150, loss = 0.78 (276.7 examples/sec; 0.463 sec/batch)
2016-02-04 00:22:41.809121: step 110160, loss = 0.67 (291.4 examples/sec; 0.439 sec/batch)
2016-02-04 00:22:46.525678: step 110170, loss = 0.67 (256.6 examples/sec; 0.499 sec/batch)
2016-02-04 00:22:51.243437: step 110180, loss = 0.67 (259.8 examples/sec; 0.493 sec/batch)
2016-02-04 00:22:55.902477: step 110190, loss = 0.55 (269.0 examples/sec; 0.476 sec/batch)
2016-02-04 00:23:00.569300: step 110200, loss = 0.69 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 00:23:05.830701: step 110210, loss = 0.80 (255.3 examples/sec; 0.501 sec/batch)
2016-02-04 00:23:10.429836: step 110220, loss = 0.62 (271.4 examples/sec; 0.472 sec/batch)
2016-02-04 00:23:15.114645: step 110230, loss = 0.65 (265.5 examples/sec; 0.482 sec/batch)
2016-02-04 00:23:19.793173: step 110240, loss = 0.60 (271.0 examples/sec; 0.472 sec/batch)
2016-02-04 00:23:24.556263: step 110250, loss = 0.70 (251.2 examples/sec; 0.510 sec/batch)
2016-02-04 00:23:29.266180: step 110260, loss = 0.79 (289.5 examples/sec; 0.442 sec/batch)
2016-02-04 00:23:33.996501: step 110270, loss = 0.79 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 00:23:38.759418: step 110280, loss = 0.80 (273.8 examples/sec; 0.467 sec/batch)
2016-02-04 00:23:43.454706: step 110290, loss = 0.65 (310.4 examples/sec; 0.412 sec/batch)
2016-02-04 00:23:48.156711: step 110300, loss = 0.62 (306.5 examples/sec; 0.418 sec/batch)
2016-02-04 00:23:53.349560: step 110310, loss = 0.70 (276.0 examples/sec; 0.464 sec/batch)
2016-02-04 00:23:58.042269: step 110320, loss = 0.70 (249.5 examples/sec; 0.513 sec/batch)
2016-02-04 00:24:02.774272: step 110330, loss = 0.65 (278.1 examples/sec; 0.460 sec/batch)
2016-02-04 00:24:07.443285: step 110340, loss = 0.84 (287.3 examples/sec; 0.446 sec/batch)
2016-02-04 00:24:12.272843: step 110350, loss = 0.68 (274.1 examples/sec; 0.467 sec/batch)
2016-02-04 00:24:16.981586: step 110360, loss = 0.71 (273.5 examples/sec; 0.468 sec/batch)
2016-02-04 00:24:21.666122: step 110370, loss = 0.67 (269.0 examples/sec; 0.476 sec/batch)
2016-02-04 00:24:26.340245: step 110380, loss = 0.70 (267.1 examples/sec; 0.479 sec/batch)
2016-02-04 00:24:30.998087: step 110390, loss = 0.82 (259.3 examples/sec; 0.494 sec/batch)
2016-02-04 00:24:35.705892: step 110400, loss = 0.74 (278.1 examples/sec; 0.460 sec/batch)
2016-02-04 00:24:40.920621: step 110410, loss = 0.54 (278.6 examples/sec; 0.459 sec/batch)
2016-02-04 00:24:45.567135: step 110420, loss = 0.75 (267.7 examples/sec; 0.478 sec/batch)
2016-02-04 00:24:50.280582: step 110430, loss = 0.51 (306.6 examples/sec; 0.418 sec/batch)
2016-02-04 00:24:54.927880: step 110440, loss = 0.71 (284.8 examples/sec; 0.449 sec/batch)
2016-02-04 00:24:59.605483: step 110450, loss = 0.72 (272.6 examples/sec; 0.470 sec/batch)
2016-02-04 00:25:04.265704: step 110460, loss = 0.66 (288.7 examples/sec; 0.443 sec/batch)
2016-02-04 00:25:08.964732: step 110470, loss = 0.73 (292.4 examples/sec; 0.438 sec/batch)
2016-02-04 00:25:13.569077: step 110480, loss = 0.65 (299.7 examples/sec; 0.427 sec/batch)
2016-02-04 00:25:18.350498: step 110490, loss = 0.65 (256.6 examples/sec; 0.499 sec/batch)
2016-02-04 00:25:23.044946: step 110500, loss = 0.67 (274.6 examples/sec; 0.466 sec/batch)
2016-02-04 00:25:28.292048: step 110510, loss = 0.65 (260.0 examples/sec; 0.492 sec/batch)
2016-02-04 00:25:32.960737: step 110520, loss = 0.62 (269.8 examples/sec; 0.474 sec/batch)
2016-02-04 00:25:37.607570: step 110530, loss = 0.51 (287.1 examples/sec; 0.446 sec/batch)
2016-02-04 00:25:42.277394: step 110540, loss = 0.72 (263.8 examples/sec; 0.485 sec/batch)
2016-02-04 00:25:46.954849: step 110550, loss = 0.80 (273.6 examples/sec; 0.468 sec/batch)
2016-02-04 00:25:51.697038: step 110560, loss = 0.62 (274.0 examples/sec; 0.467 sec/batch)
2016-02-04 00:25:56.477496: step 110570, loss = 0.59 (241.7 examples/sec; 0.530 sec/batch)
2016-02-04 00:26:01.204084: step 110580, loss = 0.79 (275.2 examples/sec; 0.465 sec/batch)
2016-02-04 00:26:05.969083: step 110590, loss = 0.71 (272.0 examples/sec; 0.471 sec/batch)
2016-02-04 00:26:10.649267: step 110600, loss = 0.65 (289.4 examples/sec; 0.442 sec/batch)
2016-02-04 00:26:15.918719: step 110610, loss = 0.65 (277.4 examples/sec; 0.461 sec/batch)
2016-02-04 00:26:20.577513: step 110620, loss = 0.60 (290.9 examples/sec; 0.440 sec/batch)
2016-02-04 00:26:25.307796: step 110630, loss = 0.58 (248.9 examples/sec; 0.514 sec/batch)
2016-02-04 00:26:30.060465: step 110640, loss = 0.64 (251.0 examples/sec; 0.510 sec/batch)
2016-02-04 00:26:34.729899: step 110650, loss = 0.72 (295.8 examples/sec; 0.433 sec/batch)
2016-02-04 00:26:39.393891: step 110660, loss = 0.68 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 00:26:44.112069: step 110670, loss = 0.61 (264.3 examples/sec; 0.484 sec/batch)
2016-02-04 00:26:48.786857: step 110680, loss = 0.68 (272.5 examples/sec; 0.470 sec/batch)
2016-02-04 00:26:53.535669: step 110690, loss = 0.76 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 00:26:58.269047: step 110700, loss = 0.80 (267.6 examples/sec; 0.478 sec/batch)
2016-02-04 00:27:03.436928: step 110710, loss = 0.77 (288.5 examples/sec; 0.444 sec/batch)
2016-02-04 00:27:08.197695: step 110720, loss = 0.51 (249.7 examples/sec; 0.513 sec/batch)
2016-02-04 00:27:12.864063: step 110730, loss = 0.67 (294.5 examples/sec; 0.435 sec/batch)
2016-02-04 00:27:17.564555: step 110740, loss = 0.63 (266.9 examples/sec; 0.480 sec/batch)
2016-02-04 00:27:22.241579: step 110750, loss = 0.61 (265.1 examples/sec; 0.483 sec/batch)
2016-02-04 00:27:26.942899: step 110760, loss = 0.74 (291.5 examples/sec; 0.439 sec/batch)
2016-02-04 00:27:31.634317: step 110770, loss = 0.64 (276.4 examples/sec; 0.463 sec/batch)
2016-02-04 00:27:36.283185: step 110780, loss = 0.70 (271.0 examples/sec; 0.472 sec/batch)
2016-02-04 00:27:40.970600: step 110790, loss = 0.74 (287.3 examples/sec; 0.445 sec/batch)
2016-02-04 00:27:45.630273: step 110800, loss = 0.66 (270.2 examples/sec; 0.474 sec/batch)
2016-02-04 00:27:50.925748: step 110810, loss = 0.62 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 00:27:55.716297: step 110820, loss = 0.72 (246.0 examples/sec; 0.520 sec/batch)
2016-02-04 00:28:00.425856: step 110830, loss = 0.76 (256.6 examples/sec; 0.499 sec/batch)
2016-02-04 00:28:05.170636: step 110840, loss = 0.65 (250.3 examples/sec; 0.511 sec/batch)
2016-02-04 00:28:09.918478: step 110850, loss = 0.81 (258.2 examples/sec; 0.496 sec/batch)
2016-02-04 00:28:14.547199: step 110860, loss = 0.68 (249.9 examples/sec; 0.512 sec/batch)
2016-02-04 00:28:19.286972: step 110870, loss = 0.70 (268.7 examples/sec; 0.476 sec/batch)
2016-02-04 00:28:24.006962: step 110880, loss = 0.70 (262.6 examples/sec; 0.487 sec/batch)
2016-02-04 00:28:28.702439: step 110890, loss = 0.72 (282.9 examples/sec; 0.452 sec/batch)
2016-02-04 00:28:33.414694: step 110900, loss = 0.77 (303.5 examples/sec; 0.422 sec/batch)
2016-02-04 00:28:38.685798: step 110910, loss = 0.63 (262.9 examples/sec; 0.487 sec/batch)
2016-02-04 00:28:43.494553: step 110920, loss = 0.74 (260.1 examples/sec; 0.492 sec/batch)
2016-02-04 00:28:48.209937: step 110930, loss = 0.66 (279.3 examples/sec; 0.458 sec/batch)
2016-02-04 00:28:52.988954: step 110940, loss = 0.56 (254.3 examples/sec; 0.503 sec/batch)
2016-02-04 00:28:57.695585: step 110950, loss = 0.80 (263.9 examples/sec; 0.485 sec/batch)
2016-02-04 00:29:02.489298: step 110960, loss = 0.67 (281.1 examples/sec; 0.455 sec/batch)
2016-02-04 00:29:07.188361: step 110970, loss = 0.85 (299.9 examples/sec; 0.427 sec/batch)
2016-02-04 00:29:11.903810: step 110980, loss = 0.75 (271.8 examples/sec; 0.471 sec/batch)
2016-02-04 00:29:16.585154: step 110990, loss = 0.74 (273.9 examples/sec; 0.467 sec/batch)
2016-02-04 00:29:21.391547: step 111000, loss = 0.63 (258.6 examples/sec; 0.495 sec/batch)
2016-02-04 00:29:26.551831: step 111010, loss = 0.64 (280.7 examples/sec; 0.456 sec/batch)
2016-02-04 00:29:31.301484: step 111020, loss = 0.62 (250.4 examples/sec; 0.511 sec/batch)
2016-02-04 00:29:35.957924: step 111030, loss = 0.53 (275.8 examples/sec; 0.464 sec/batch)
2016-02-04 00:29:40.581589: step 111040, loss = 0.72 (268.4 examples/sec; 0.477 sec/batch)
2016-02-04 00:29:45.282410: step 111050, loss = 0.60 (260.4 examples/sec; 0.492 sec/batch)
2016-02-04 00:29:50.011989: step 111060, loss = 0.74 (281.1 examples/sec; 0.455 sec/batch)
2016-02-04 00:29:54.707597: step 111070, loss = 0.66 (273.9 examples/sec; 0.467 sec/batch)
2016-02-04 00:29:59.509979: step 111080, loss = 0.71 (246.7 examples/sec; 0.519 sec/batch)
2016-02-04 00:30:04.218102: step 111090, loss = 0.70 (268.9 examples/sec; 0.476 sec/batch)
2016-02-04 00:30:08.938780: step 111100, loss = 0.66 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 00:30:14.160498: step 111110, loss = 0.66 (274.7 examples/sec; 0.466 sec/batch)
2016-02-04 00:30:18.881124: step 111120, loss = 0.74 (291.2 examples/sec; 0.440 sec/batch)
2016-02-04 00:30:23.611353: step 111130, loss = 0.63 (278.5 examples/sec; 0.460 sec/batch)
2016-02-04 00:30:28.371265: step 111140, loss = 0.84 (269.2 examples/sec; 0.475 sec/batch)
2016-02-04 00:30:33.081286: step 111150, loss = 0.81 (248.5 examples/sec; 0.515 sec/batch)
2016-02-04 00:30:37.749997: step 111160, loss = 0.78 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 00:30:42.530849: step 111170, loss = 0.73 (258.4 examples/sec; 0.495 sec/batch)
2016-02-04 00:30:47.244366: step 111180, loss = 0.65 (276.4 examples/sec; 0.463 sec/batch)
2016-02-04 00:30:52.015768: step 111190, loss = 0.71 (264.2 examples/sec; 0.484 sec/batch)
2016-02-04 00:30:56.664940: step 111200, loss = 0.67 (277.8 examples/sec; 0.461 sec/batch)
2016-02-04 00:31:01.914276: step 111210, loss = 0.63 (275.7 examples/sec; 0.464 sec/batch)
2016-02-04 00:31:06.637185: step 111220, loss = 0.70 (284.0 examples/sec; 0.451 sec/batch)
2016-02-04 00:31:11.338928: step 111230, loss = 0.72 (265.0 examples/sec; 0.483 sec/batch)
2016-02-04 00:31:16.043425: step 111240, loss = 0.70 (260.0 examples/sec; 0.492 sec/batch)
2016-02-04 00:31:20.800941: step 111250, loss = 0.73 (258.8 examples/sec; 0.495 sec/batch)
2016-02-04 00:31:25.479170: step 111260, loss = 0.64 (263.5 examples/sec; 0.486 sec/batch)
2016-02-04 00:31:30.262420: step 111270, loss = 0.75 (274.4 examples/sec; 0.466 sec/batch)
2016-02-04 00:31:34.992021: step 111280, loss = 0.64 (268.4 examples/sec; 0.477 sec/batch)
2016-02-04 00:31:39.641717: step 111290, loss = 0.69 (286.7 examples/sec; 0.447 sec/batch)
2016-02-04 00:31:44.362544: step 111300, loss = 0.76 (278.0 examples/sec; 0.460 sec/batch)
2016-02-04 00:31:49.467053: step 111310, loss = 0.69 (296.6 examples/sec; 0.432 sec/batch)
2016-02-04 00:31:54.196962: step 111320, loss = 0.72 (299.1 examples/sec; 0.428 sec/batch)
2016-02-04 00:31:58.915609: step 111330, loss = 0.60 (275.0 examples/sec; 0.465 sec/batch)
2016-02-04 00:32:03.656239: step 111340, loss = 0.66 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 00:32:08.385986: step 111350, loss = 0.74 (289.9 examples/sec; 0.442 sec/batch)
2016-02-04 00:32:13.118824: step 111360, loss = 0.60 (253.8 examples/sec; 0.504 sec/batch)
2016-02-04 00:32:17.823776: step 111370, loss = 0.74 (267.5 examples/sec; 0.479 sec/batch)
2016-02-04 00:32:22.482121: step 111380, loss = 0.65 (272.8 examples/sec; 0.469 sec/batch)
2016-02-04 00:32:27.247628: step 111390, loss = 0.55 (272.0 examples/sec; 0.471 sec/batch)
2016-02-04 00:32:31.989408: step 111400, loss = 0.89 (247.5 examples/sec; 0.517 sec/batch)
2016-02-04 00:32:37.202726: step 111410, loss = 0.75 (270.9 examples/sec; 0.473 sec/batch)
2016-02-04 00:32:41.930096: step 111420, loss = 0.63 (264.8 examples/sec; 0.483 sec/batch)
2016-02-04 00:32:46.685005: step 111430, loss = 0.64 (275.8 examples/sec; 0.464 sec/batch)
2016-02-04 00:32:51.352872: step 111440, loss = 0.50 (259.2 examples/sec; 0.494 sec/batch)
2016-02-04 00:32:56.108631: step 111450, loss = 0.63 (264.3 examples/sec; 0.484 sec/batch)
2016-02-04 00:33:00.791319: step 111460, loss = 0.71 (256.6 examples/sec; 0.499 sec/batch)
2016-02-04 00:33:05.494039: step 111470, loss = 0.67 (261.6 examples/sec; 0.489 sec/batch)
2016-02-04 00:33:10.172079: step 111480, loss = 0.64 (275.8 examples/sec; 0.464 sec/batch)
2016-02-04 00:33:14.941347: step 111490, loss = 0.70 (260.6 examples/sec; 0.491 sec/batch)
2016-02-04 00:33:19.610318: step 111500, loss = 0.71 (278.8 examples/sec; 0.459 sec/batch)
2016-02-04 00:33:24.906272: step 111510, loss = 0.64 (247.3 examples/sec; 0.518 sec/batch)
2016-02-04 00:33:29.564994: step 111520, loss = 0.71 (282.9 examples/sec; 0.452 sec/batch)
2016-02-04 00:33:34.261704: step 111530, loss = 0.71 (283.3 examples/sec; 0.452 sec/batch)
2016-02-04 00:33:38.972708: step 111540, loss = 0.55 (259.1 examples/sec; 0.494 sec/batch)
2016-02-04 00:33:43.677050: step 111550, loss = 0.60 (264.0 examples/sec; 0.485 sec/batch)
2016-02-04 00:33:48.334952: step 111560, loss = 0.68 (290.8 examples/sec; 0.440 sec/batch)
2016-02-04 00:33:53.075649: step 111570, loss = 0.69 (273.1 examples/sec; 0.469 sec/batch)
2016-02-04 00:33:57.723398: step 111580, loss = 0.61 (272.7 examples/sec; 0.469 sec/batch)
2016-02-04 00:34:02.481393: step 111590, loss = 0.57 (276.2 examples/sec; 0.463 sec/batch)
2016-02-04 00:34:07.183598: step 111600, loss = 0.65 (269.1 examples/sec; 0.476 sec/batch)
2016-02-04 00:34:12.392997: step 111610, loss = 0.65 (278.3 examples/sec; 0.460 sec/batch)
2016-02-04 00:34:16.989714: step 111620, loss = 0.59 (252.5 examples/sec; 0.507 sec/batch)
2016-02-04 00:34:21.739707: step 111630, loss = 0.94 (251.1 examples/sec; 0.510 sec/batch)
2016-02-04 00:34:26.353399: step 111640, loss = 0.60 (297.2 examples/sec; 0.431 sec/batch)
2016-02-04 00:34:31.073049: step 111650, loss = 0.65 (272.4 examples/sec; 0.470 sec/batch)
2016-02-04 00:34:35.681501: step 111660, loss = 0.62 (279.0 examples/sec; 0.459 sec/batch)
2016-02-04 00:34:40.405502: step 111670, loss = 0.67 (284.9 examples/sec; 0.449 sec/batch)
2016-02-04 00:34:45.135645: step 111680, loss = 0.81 (268.1 examples/sec; 0.477 sec/batch)
2016-02-04 00:34:49.804980: step 111690, loss = 0.66 (294.7 examples/sec; 0.434 sec/batch)
2016-02-04 00:34:54.473376: step 111700, loss = 0.81 (266.9 examples/sec; 0.480 sec/batch)
2016-02-04 00:34:59.696261: step 111710, loss = 0.66 (256.2 examples/sec; 0.500 sec/batch)
2016-02-04 00:35:04.305084: step 111720, loss = 0.72 (285.8 examples/sec; 0.448 sec/batch)
2016-02-04 00:35:09.020855: step 111730, loss = 0.76 (262.9 examples/sec; 0.487 sec/batch)
2016-02-04 00:35:13.738469: step 111740, loss = 0.67 (286.8 examples/sec; 0.446 sec/batch)
2016-02-04 00:35:18.418909: step 111750, loss = 0.67 (284.9 examples/sec; 0.449 sec/batch)
2016-02-04 00:35:23.050703: step 111760, loss = 0.59 (295.9 examples/sec; 0.433 sec/batch)
2016-02-04 00:35:27.707142: step 111770, loss = 0.61 (254.0 examples/sec; 0.504 sec/batch)
2016-02-04 00:35:32.434645: step 111780, loss = 0.57 (263.8 examples/sec; 0.485 sec/batch)
2016-02-04 00:35:37.195133: step 111790, loss = 0.60 (282.3 examples/sec; 0.453 sec/batch)
2016-02-04 00:35:41.956401: step 111800, loss = 0.69 (258.0 examples/sec; 0.496 sec/batch)
2016-02-04 00:35:47.186029: step 111810, loss = 0.75 (275.8 examples/sec; 0.464 sec/batch)
2016-02-04 00:35:51.961285: step 111820, loss = 0.68 (255.3 examples/sec; 0.501 sec/batch)
2016-02-04 00:35:56.629899: step 111830, loss = 0.61 (250.1 examples/sec; 0.512 sec/batch)
2016-02-04 00:36:01.307103: step 111840, loss = 0.67 (298.7 examples/sec; 0.429 sec/batch)
2016-02-04 00:36:06.070718: step 111850, loss = 0.74 (264.4 examples/sec; 0.484 sec/batch)
2016-02-04 00:36:10.752641: step 111860, loss = 0.71 (277.1 examples/sec; 0.462 sec/batch)
2016-02-04 00:36:15.430039: step 111870, loss = 0.65 (257.0 examples/sec; 0.498 sec/batch)
2016-02-04 00:36:20.157003: step 111880, loss = 0.77 (256.6 examples/sec; 0.499 sec/batch)
2016-02-04 00:36:24.782232: step 111890, loss = 0.75 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 00:36:29.464491: step 111900, loss = 0.78 (250.1 examples/sec; 0.512 sec/batch)
2016-02-04 00:36:34.641257: step 111910, loss = 0.55 (300.5 examples/sec; 0.426 sec/batch)
2016-02-04 00:36:39.319525: step 111920, loss = 0.73 (263.6 examples/sec; 0.486 sec/batch)
2016-02-04 00:36:43.944581: step 111930, loss = 0.71 (277.6 examples/sec; 0.461 sec/batch)
2016-02-04 00:36:48.582018: step 111940, loss = 0.70 (278.5 examples/sec; 0.460 sec/batch)
2016-02-04 00:36:53.207500: step 111950, loss = 0.60 (268.1 examples/sec; 0.477 sec/batch)
2016-02-04 00:36:57.976598: step 111960, loss = 0.81 (295.3 examples/sec; 0.434 sec/batch)
2016-02-04 00:37:02.683991: step 111970, loss = 0.58 (277.5 examples/sec; 0.461 sec/batch)
2016-02-04 00:37:07.412216: step 111980, loss = 0.69 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 00:37:12.090600: step 111990, loss = 0.59 (268.6 examples/sec; 0.476 sec/batch)
2016-02-04 00:37:16.758588: step 112000, loss = 0.91 (277.3 examples/sec; 0.462 sec/batch)
2016-02-04 00:37:22.001241: step 112010, loss = 0.66 (278.7 examples/sec; 0.459 sec/batch)
2016-02-04 00:37:26.639164: step 112020, loss = 0.59 (285.1 examples/sec; 0.449 sec/batch)
2016-02-04 00:37:31.196223: step 112030, loss = 0.74 (306.6 examples/sec; 0.417 sec/batch)
2016-02-04 00:37:35.979799: step 112040, loss = 0.60 (255.7 examples/sec; 0.501 sec/batch)
2016-02-04 00:37:40.720633: step 112050, loss = 0.68 (272.0 examples/sec; 0.471 sec/batch)
2016-02-04 00:37:45.374580: step 112060, loss = 0.67 (282.7 examples/sec; 0.453 sec/batch)
2016-02-04 00:37:49.984820: step 112070, loss = 0.73 (282.7 examples/sec; 0.453 sec/batch)
2016-02-04 00:37:54.732589: step 112080, loss = 0.69 (274.5 examples/sec; 0.466 sec/batch)
2016-02-04 00:37:59.368485: step 112090, loss = 0.75 (256.9 examples/sec; 0.498 sec/batch)
2016-02-04 00:38:04.034974: step 112100, loss = 0.75 (268.0 examples/sec; 0.478 sec/batch)
2016-02-04 00:38:09.265090: step 112110, loss = 0.64 (271.6 examples/sec; 0.471 sec/batch)
2016-02-04 00:38:13.894478: step 112120, loss = 0.67 (273.3 examples/sec; 0.468 sec/batch)
2016-02-04 00:38:18.551511: step 112130, loss = 0.66 (309.6 examples/sec; 0.413 sec/batch)
2016-02-04 00:38:23.218890: step 112140, loss = 0.56 (298.4 examples/sec; 0.429 sec/batch)
2016-02-04 00:38:27.869395: step 112150, loss = 0.79 (277.8 examples/sec; 0.461 sec/batch)
2016-02-04 00:38:32.586658: step 112160, loss = 0.58 (258.1 examples/sec; 0.496 sec/batch)
2016-02-04 00:38:37.203688: step 112170, loss = 0.82 (268.2 examples/sec; 0.477 sec/batch)
2016-02-04 00:38:41.907168: step 112180, loss = 0.67 (260.1 examples/sec; 0.492 sec/batch)
2016-02-04 00:38:46.572383: step 112190, loss = 0.64 (288.3 examples/sec; 0.444 sec/batch)
2016-02-04 00:38:51.247479: step 112200, loss = 0.66 (279.0 examples/sec; 0.459 sec/batch)
2016-02-04 00:38:56.471683: step 112210, loss = 0.64 (306.6 examples/sec; 0.418 sec/batch)
2016-02-04 00:39:01.153930: step 112220, loss = 0.58 (255.8 examples/sec; 0.500 sec/batch)
2016-02-04 00:39:05.891385: step 112230, loss = 0.61 (272.7 examples/sec; 0.469 sec/batch)
2016-02-04 00:39:10.616694: step 112240, loss = 0.64 (277.8 examples/sec; 0.461 sec/batch)
2016-02-04 00:39:15.352965: step 112250, loss = 0.62 (276.6 examples/sec; 0.463 sec/batch)
2016-02-04 00:39:20.061599: step 112260, loss = 0.65 (252.8 examples/sec; 0.506 sec/batch)
2016-02-04 00:39:24.709534: step 112270, loss = 0.75 (271.0 examples/sec; 0.472 sec/batch)
2016-02-04 00:39:29.336753: step 112280, loss = 0.65 (270.3 examples/sec; 0.474 sec/batch)
2016-02-04 00:39:34.052186: step 112290, loss = 0.63 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 00:39:38.743863: step 112300, loss = 0.76 (255.3 examples/sec; 0.501 sec/batch)
2016-02-04 00:39:43.924640: step 112310, loss = 0.70 (274.0 examples/sec; 0.467 sec/batch)
2016-02-04 00:39:48.568361: step 112320, loss = 0.70 (285.5 examples/sec; 0.448 sec/batch)
2016-02-04 00:39:53.183483: step 112330, loss = 0.71 (295.6 examples/sec; 0.433 sec/batch)
2016-02-04 00:39:57.765259: step 112340, loss = 0.63 (287.8 examples/sec; 0.445 sec/batch)
2016-02-04 00:40:02.443597: step 112350, loss = 0.61 (253.5 examples/sec; 0.505 sec/batch)
2016-02-04 00:40:07.086229: step 112360, loss = 0.73 (283.6 examples/sec; 0.451 sec/batch)
2016-02-04 00:40:11.785234: step 112370, loss = 0.64 (283.2 examples/sec; 0.452 sec/batch)
2016-02-04 00:40:16.537853: step 112380, loss = 0.72 (259.3 examples/sec; 0.494 sec/batch)
2016-02-04 00:40:21.226863: step 112390, loss = 0.66 (277.6 examples/sec; 0.461 sec/batch)
2016-02-04 00:40:25.982924: step 112400, loss = 0.58 (267.7 examples/sec; 0.478 sec/batch)
2016-02-04 00:40:31.232970: step 112410, loss = 0.60 (273.5 examples/sec; 0.468 sec/batch)
2016-02-04 00:40:35.950856: step 112420, loss = 0.73 (275.6 examples/sec; 0.465 sec/batch)
2016-02-04 00:40:40.626143: step 112430, loss = 0.76 (270.1 examples/sec; 0.474 sec/batch)
2016-02-04 00:40:45.304269: step 112440, loss = 0.73 (269.6 examples/sec; 0.475 sec/batch)
2016-02-04 00:40:49.964313: step 112450, loss = 0.73 (269.3 examples/sec; 0.475 sec/batch)
2016-02-04 00:40:54.701903: step 112460, loss = 0.75 (256.0 examples/sec; 0.500 sec/batch)
2016-02-04 00:40:59.469101: step 112470, loss = 0.63 (254.5 examples/sec; 0.503 sec/batch)
2016-02-04 00:41:04.208683: step 112480, loss = 0.72 (255.0 examples/sec; 0.502 sec/batch)
2016-02-04 00:41:08.871657: step 112490, loss = 0.58 (250.4 examples/sec; 0.511 sec/batch)
2016-02-04 00:41:13.602292: step 112500, loss = 0.75 (274.4 examples/sec; 0.466 sec/batch)
2016-02-04 00:41:18.863239: step 112510, loss = 0.76 (276.8 examples/sec; 0.462 sec/batch)
2016-02-04 00:41:23.597189: step 112520, loss = 0.68 (257.1 examples/sec; 0.498 sec/batch)
2016-02-04 00:41:28.357797: step 112530, loss = 0.63 (265.8 examples/sec; 0.481 sec/batch)
2016-02-04 00:41:33.025003: step 112540, loss = 0.54 (273.8 examples/sec; 0.468 sec/batch)
2016-02-04 00:41:37.743343: step 112550, loss = 0.59 (263.6 examples/sec; 0.486 sec/batch)
2016-02-04 00:41:42.412201: step 112560, loss = 0.69 (281.5 examples/sec; 0.455 sec/batch)
2016-02-04 00:41:47.101243: step 112570, loss = 0.65 (252.9 examples/sec; 0.506 sec/batch)
2016-02-04 00:41:51.757452: step 112580, loss = 0.79 (271.5 examples/sec; 0.471 sec/batch)
2016-02-04 00:41:56.496497: step 112590, loss = 0.63 (238.5 examples/sec; 0.537 sec/batch)
2016-02-04 00:42:01.120009: step 112600, loss = 0.59 (306.8 examples/sec; 0.417 sec/batch)
2016-02-04 00:42:06.352950: step 112610, loss = 0.73 (260.4 examples/sec; 0.492 sec/batch)
2016-02-04 00:42:11.109478: step 112620, loss = 0.66 (287.1 examples/sec; 0.446 sec/batch)
2016-02-04 00:42:15.835621: step 112630, loss = 0.69 (283.3 examples/sec; 0.452 sec/batch)
2016-02-04 00:42:20.573352: step 112640, loss = 0.68 (238.1 examples/sec; 0.538 sec/batch)
2016-02-04 00:42:25.259220: step 112650, loss = 0.56 (294.7 examples/sec; 0.434 sec/batch)
2016-02-04 00:42:29.928543: step 112660, loss = 0.64 (262.2 examples/sec; 0.488 sec/batch)
2016-02-04 00:42:34.646053: step 112670, loss = 0.60 (255.8 examples/sec; 0.500 sec/batch)
2016-02-04 00:42:39.238745: step 112680, loss = 0.71 (267.5 examples/sec; 0.478 sec/batch)
2016-02-04 00:42:43.953223: step 112690, loss = 0.65 (301.1 examples/sec; 0.425 sec/batch)
2016-02-04 00:42:48.641585: step 112700, loss = 0.54 (266.5 examples/sec; 0.480 sec/batch)
2016-02-04 00:42:53.862448: step 112710, loss = 0.63 (278.9 examples/sec; 0.459 sec/batch)
2016-02-04 00:42:58.582679: step 112720, loss = 0.81 (277.8 examples/sec; 0.461 sec/batch)
2016-02-04 00:43:03.259022: step 112730, loss = 0.72 (258.4 examples/sec; 0.495 sec/batch)
2016-02-04 00:43:08.018232: step 112740, loss = 0.84 (240.2 examples/sec; 0.533 sec/batch)
2016-02-04 00:43:12.689957: step 112750, loss = 0.68 (272.0 examples/sec; 0.471 sec/batch)
2016-02-04 00:43:17.350148: step 112760, loss = 0.67 (285.3 examples/sec; 0.449 sec/batch)
2016-02-04 00:43:21.985339: step 112770, loss = 0.72 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 00:43:26.639045: step 112780, loss = 0.63 (293.0 examples/sec; 0.437 sec/batch)
2016-02-04 00:43:31.384273: step 112790, loss = 0.61 (265.1 examples/sec; 0.483 sec/batch)
2016-02-04 00:43:36.075051: step 112800, loss = 0.71 (281.6 examples/sec; 0.455 sec/batch)
2016-02-04 00:43:41.322155: step 112810, loss = 0.79 (303.5 examples/sec; 0.422 sec/batch)
2016-02-04 00:43:46.037947: step 112820, loss = 0.61 (289.4 examples/sec; 0.442 sec/batch)
2016-02-04 00:43:50.742168: step 112830, loss = 0.70 (285.7 examples/sec; 0.448 sec/batch)
2016-02-04 00:43:55.439418: step 112840, loss = 0.60 (276.4 examples/sec; 0.463 sec/batch)
2016-02-04 00:44:00.096236: step 112850, loss = 0.82 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 00:44:04.814552: step 112860, loss = 0.63 (262.3 examples/sec; 0.488 sec/batch)
2016-02-04 00:44:09.544978: step 112870, loss = 0.57 (246.7 examples/sec; 0.519 sec/batch)
2016-02-04 00:44:14.148804: step 112880, loss = 0.67 (281.2 examples/sec; 0.455 sec/batch)
2016-02-04 00:44:18.824642: step 112890, loss = 0.70 (279.5 examples/sec; 0.458 sec/batch)
2016-02-04 00:44:23.509638: step 112900, loss = 0.68 (281.5 examples/sec; 0.455 sec/batch)
2016-02-04 00:44:28.711540: step 112910, loss = 0.70 (278.4 examples/sec; 0.460 sec/batch)
2016-02-04 00:44:33.409243: step 112920, loss = 0.61 (269.0 examples/sec; 0.476 sec/batch)
2016-02-04 00:44:38.097999: step 112930, loss = 0.58 (299.3 examples/sec; 0.428 sec/batch)
2016-02-04 00:44:42.773925: step 112940, loss = 0.63 (277.9 examples/sec; 0.461 sec/batch)
2016-02-04 00:44:47.489242: step 112950, loss = 0.69 (270.5 examples/sec; 0.473 sec/batch)
2016-02-04 00:44:52.188013: step 112960, loss = 0.65 (262.3 examples/sec; 0.488 sec/batch)
2016-02-04 00:44:56.880496: step 112970, loss = 0.65 (278.7 examples/sec; 0.459 sec/batch)
2016-02-04 00:45:01.644488: step 112980, loss = 0.71 (255.0 examples/sec; 0.502 sec/batch)
2016-02-04 00:45:06.362249: step 112990, loss = 0.77 (268.9 examples/sec; 0.476 sec/batch)
2016-02-04 00:45:11.015291: step 113000, loss = 0.71 (261.8 examples/sec; 0.489 sec/batch)
2016-02-04 00:45:16.319357: step 113010, loss = 0.73 (263.8 examples/sec; 0.485 sec/batch)
2016-02-04 00:45:21.036974: step 113020, loss = 0.76 (250.3 examples/sec; 0.511 sec/batch)
2016-02-04 00:45:25.782624: step 113030, loss = 0.56 (267.3 examples/sec; 0.479 sec/batch)
2016-02-04 00:45:30.564175: step 113040, loss = 0.52 (253.3 examples/sec; 0.505 sec/batch)
2016-02-04 00:45:35.250235: step 113050, loss = 0.71 (261.3 examples/sec; 0.490 sec/batch)
2016-02-04 00:45:39.874093: step 113060, loss = 0.64 (272.4 examples/sec; 0.470 sec/batch)
2016-02-04 00:45:44.517306: step 113070, loss = 0.58 (300.3 examples/sec; 0.426 sec/batch)
2016-02-04 00:45:49.172427: step 113080, loss = 0.68 (264.4 examples/sec; 0.484 sec/batch)
2016-02-04 00:45:53.851429: step 113090, loss = 0.70 (291.0 examples/sec; 0.440 sec/batch)
2016-02-04 00:45:58.616236: step 113100, loss = 0.70 (266.8 examples/sec; 0.480 sec/batch)
2016-02-04 00:46:03.770640: step 113110, loss = 0.83 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 00:46:08.410236: step 113120, loss = 0.74 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 00:46:13.074763: step 113130, loss = 0.73 (259.9 examples/sec; 0.492 sec/batch)
2016-02-04 00:46:17.736570: step 113140, loss = 0.73 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 00:46:22.524711: step 113150, loss = 0.65 (263.4 examples/sec; 0.486 sec/batch)
2016-02-04 00:46:27.190672: step 113160, loss = 0.65 (300.2 examples/sec; 0.426 sec/batch)
2016-02-04 00:46:31.881539: step 113170, loss = 0.62 (270.7 examples/sec; 0.473 sec/batch)
2016-02-04 00:46:36.568551: step 113180, loss = 0.72 (262.7 examples/sec; 0.487 sec/batch)
2016-02-04 00:46:41.360528: step 113190, loss = 0.64 (260.6 examples/sec; 0.491 sec/batch)
2016-02-04 00:46:46.039149: step 113200, loss = 0.77 (294.0 examples/sec; 0.435 sec/batch)
2016-02-04 00:46:51.255675: step 113210, loss = 0.68 (291.4 examples/sec; 0.439 sec/batch)
2016-02-04 00:46:55.960661: step 113220, loss = 0.66 (291.8 examples/sec; 0.439 sec/batch)
2016-02-04 00:47:00.697495: step 113230, loss = 0.80 (259.6 examples/sec; 0.493 sec/batch)
2016-02-04 00:47:05.412191: step 113240, loss = 0.74 (255.0 examples/sec; 0.502 sec/batch)
2016-02-04 00:47:10.154350: step 113250, loss = 0.53 (269.1 examples/sec; 0.476 sec/batch)
2016-02-04 00:47:14.802972: step 113260, loss = 0.62 (256.7 examples/sec; 0.499 sec/batch)
2016-02-04 00:47:19.438826: step 113270, loss = 0.76 (266.8 examples/sec; 0.480 sec/batch)
2016-02-04 00:47:24.130121: step 113280, loss = 0.77 (288.0 examples/sec; 0.445 sec/batch)
2016-02-04 00:47:28.878744: step 113290, loss = 0.89 (267.0 examples/sec; 0.479 sec/batch)
2016-02-04 00:47:33.578448: step 113300, loss = 0.56 (273.2 examples/sec; 0.469 sec/batch)
2016-02-04 00:47:38.766106: step 113310, loss = 0.53 (298.3 examples/sec; 0.429 sec/batch)
2016-02-04 00:47:43.485216: step 113320, loss = 0.62 (296.8 examples/sec; 0.431 sec/batch)
2016-02-04 00:47:48.107675: step 113330, loss = 0.59 (255.3 examples/sec; 0.501 sec/batch)
2016-02-04 00:47:52.699532: step 113340, loss = 0.64 (287.1 examples/sec; 0.446 sec/batch)
2016-02-04 00:47:57.422819: step 113350, loss = 0.71 (258.5 examples/sec; 0.495 sec/batch)
2016-02-04 00:48:02.161762: step 113360, loss = 0.64 (256.6 examples/sec; 0.499 sec/batch)
2016-02-04 00:48:06.832031: step 113370, loss = 0.76 (297.0 examples/sec; 0.431 sec/batch)
2016-02-04 00:48:11.529619: step 113380, loss = 0.68 (276.1 examples/sec; 0.464 sec/batch)
2016-02-04 00:48:16.193033: step 113390, loss = 0.54 (269.5 examples/sec; 0.475 sec/batch)
2016-02-04 00:48:20.946964: step 113400, loss = 0.66 (255.0 examples/sec; 0.502 sec/batch)
2016-02-04 00:48:26.196850: step 113410, loss = 0.75 (271.1 examples/sec; 0.472 sec/batch)
2016-02-04 00:48:30.819825: step 113420, loss = 0.83 (261.5 examples/sec; 0.490 sec/batch)
2016-02-04 00:48:35.502374: step 113430, loss = 0.67 (294.7 examples/sec; 0.434 sec/batch)
2016-02-04 00:48:40.187912: step 113440, loss = 0.73 (277.6 examples/sec; 0.461 sec/batch)
2016-02-04 00:48:44.856889: step 113450, loss = 0.72 (292.8 examples/sec; 0.437 sec/batch)
2016-02-04 00:48:49.598260: step 113460, loss = 0.52 (245.6 examples/sec; 0.521 sec/batch)
2016-02-04 00:48:54.267996: step 113470, loss = 0.69 (267.2 examples/sec; 0.479 sec/batch)
2016-02-04 00:48:58.925197: step 113480, loss = 0.60 (288.5 examples/sec; 0.444 sec/batch)
2016-02-04 00:49:03.583028: step 113490, loss = 0.76 (286.3 examples/sec; 0.447 sec/batch)
2016-02-04 00:49:08.349095: step 113500, loss = 0.66 (291.0 examples/sec; 0.440 sec/batch)
2016-02-04 00:49:13.546583: step 113510, loss = 0.65 (288.1 examples/sec; 0.444 sec/batch)
2016-02-04 00:49:18.251676: step 113520, loss = 0.63 (284.6 examples/sec; 0.450 sec/batch)
2016-02-04 00:49:23.016621: step 113530, loss = 0.66 (262.6 examples/sec; 0.487 sec/batch)
2016-02-04 00:49:27.806736: step 113540, loss = 0.78 (263.4 examples/sec; 0.486 sec/batch)
2016-02-04 00:49:32.477301: step 113550, loss = 0.58 (257.7 examples/sec; 0.497 sec/batch)
2016-02-04 00:49:37.195093: step 113560, loss = 0.73 (292.4 examples/sec; 0.438 sec/batch)
2016-02-04 00:49:41.956611: step 113570, loss = 0.82 (273.8 examples/sec; 0.468 sec/batch)
2016-02-04 00:49:46.631980: step 113580, loss = 0.62 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 00:49:51.339995: step 113590, loss = 0.60 (264.7 examples/sec; 0.484 sec/batch)
2016-02-04 00:49:56.006136: step 113600, loss = 0.63 (268.8 examples/sec; 0.476 sec/batch)
2016-02-04 00:50:01.248419: step 113610, loss = 0.75 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 00:50:05.949665: step 113620, loss = 0.89 (299.5 examples/sec; 0.427 sec/batch)
2016-02-04 00:50:10.670298: step 113630, loss = 0.71 (253.1 examples/sec; 0.506 sec/batch)
2016-02-04 00:50:15.374200: step 113640, loss = 0.74 (269.4 examples/sec; 0.475 sec/batch)
2016-02-04 00:50:20.037110: step 113650, loss = 0.64 (290.2 examples/sec; 0.441 sec/batch)
2016-02-04 00:50:24.754944: step 113660, loss = 0.72 (269.4 examples/sec; 0.475 sec/batch)
2016-02-04 00:50:29.439696: step 113670, loss = 0.67 (253.8 examples/sec; 0.504 sec/batch)
2016-02-04 00:50:34.110955: step 113680, loss = 0.72 (262.7 examples/sec; 0.487 sec/batch)
2016-02-04 00:50:38.818613: step 113690, loss = 0.81 (275.8 examples/sec; 0.464 sec/batch)
2016-02-04 00:50:43.571346: step 113700, loss = 0.71 (265.5 examples/sec; 0.482 sec/batch)
2016-02-04 00:50:48.782599: step 113710, loss = 0.63 (273.8 examples/sec; 0.467 sec/batch)
2016-02-04 00:50:53.539189: step 113720, loss = 0.57 (249.2 examples/sec; 0.514 sec/batch)
2016-02-04 00:50:58.225739: step 113730, loss = 0.67 (262.5 examples/sec; 0.488 sec/batch)
2016-02-04 00:51:02.984767: step 113740, loss = 0.85 (260.0 examples/sec; 0.492 sec/batch)
2016-02-04 00:51:07.652118: step 113750, loss = 0.72 (278.2 examples/sec; 0.460 sec/batch)
2016-02-04 00:51:12.464931: step 113760, loss = 0.65 (300.2 examples/sec; 0.426 sec/batch)
2016-02-04 00:51:17.190680: step 113770, loss = 0.68 (241.3 examples/sec; 0.531 sec/batch)
2016-02-04 00:51:21.904105: step 113780, loss = 0.71 (250.2 examples/sec; 0.512 sec/batch)
2016-02-04 00:51:26.636774: step 113790, loss = 0.58 (266.0 examples/sec; 0.481 sec/batch)
2016-02-04 00:51:31.338300: step 113800, loss = 0.65 (279.8 examples/sec; 0.458 sec/batch)
2016-02-04 00:51:36.614251: step 113810, loss = 0.74 (256.8 examples/sec; 0.498 sec/batch)
2016-02-04 00:51:41.316254: step 113820, loss = 0.67 (281.1 examples/sec; 0.455 sec/batch)
2016-02-04 00:51:46.037476: step 113830, loss = 0.54 (277.5 examples/sec; 0.461 sec/batch)
2016-02-04 00:51:50.741968: step 113840, loss = 0.66 (267.7 examples/sec; 0.478 sec/batch)
2016-02-04 00:51:55.382256: step 113850, loss = 0.57 (264.6 examples/sec; 0.484 sec/batch)
2016-02-04 00:52:00.065689: step 113860, loss = 0.59 (270.0 examples/sec; 0.474 sec/batch)
2016-02-04 00:52:04.766106: step 113870, loss = 0.60 (262.3 examples/sec; 0.488 sec/batch)
2016-02-04 00:52:09.494215: step 113880, loss = 0.68 (260.4 examples/sec; 0.492 sec/batch)
2016-02-04 00:52:14.151080: step 113890, loss = 0.75 (284.0 examples/sec; 0.451 sec/batch)
2016-02-04 00:52:18.821771: step 113900, loss = 0.57 (265.7 examples/sec; 0.482 sec/batch)
2016-02-04 00:52:24.041804: step 113910, loss = 0.62 (272.8 examples/sec; 0.469 sec/batch)
2016-02-04 00:52:28.794643: step 113920, loss = 0.78 (263.5 examples/sec; 0.486 sec/batch)
2016-02-04 00:52:33.506692: step 113930, loss = 0.69 (280.4 examples/sec; 0.456 sec/batch)
2016-02-04 00:52:38.229651: step 113940, loss = 0.71 (262.4 examples/sec; 0.488 sec/batch)
2016-02-04 00:52:42.947636: step 113950, loss = 0.67 (252.6 examples/sec; 0.507 sec/batch)
2016-02-04 00:52:47.575939: step 113960, loss = 0.79 (272.2 examples/sec; 0.470 sec/batch)
2016-02-04 00:52:52.325888: step 113970, loss = 0.90 (280.6 examples/sec; 0.456 sec/batch)
2016-02-04 00:52:56.953763: step 113980, loss = 0.64 (267.6 examples/sec; 0.478 sec/batch)
2016-02-04 00:53:01.682468: step 113990, loss = 0.85 (263.9 examples/sec; 0.485 sec/batch)
2016-02-04 00:53:06.359899: step 114000, loss = 0.69 (283.2 examples/sec; 0.452 sec/batch)
2016-02-04 00:53:11.555132: step 114010, loss = 0.48 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 00:53:16.265447: step 114020, loss = 0.61 (259.7 examples/sec; 0.493 sec/batch)
2016-02-04 00:53:21.046322: step 114030, loss = 0.85 (268.6 examples/sec; 0.477 sec/batch)
2016-02-04 00:53:25.703909: step 114040, loss = 0.66 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 00:53:30.340818: step 114050, loss = 0.66 (280.2 examples/sec; 0.457 sec/batch)
2016-02-04 00:53:35.122544: step 114060, loss = 0.51 (235.8 examples/sec; 0.543 sec/batch)
2016-02-04 00:53:39.737340: step 114070, loss = 0.66 (276.6 examples/sec; 0.463 sec/batch)
2016-02-04 00:53:44.411507: step 114080, loss = 0.70 (278.1 examples/sec; 0.460 sec/batch)
2016-02-04 00:53:49.094749: step 114090, loss = 0.57 (275.3 examples/sec; 0.465 sec/batch)
2016-02-04 00:53:53.826969: step 114100, loss = 0.70 (280.3 examples/sec; 0.457 sec/batch)
2016-02-04 00:53:59.081112: step 114110, loss = 0.67 (269.4 examples/sec; 0.475 sec/batch)
2016-02-04 00:54:03.767207: step 114120, loss = 0.66 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 00:54:08.454867: step 114130, loss = 0.58 (277.0 examples/sec; 0.462 sec/batch)
2016-02-04 00:54:13.185391: step 114140, loss = 0.60 (263.7 examples/sec; 0.485 sec/batch)
2016-02-04 00:54:17.903542: step 114150, loss = 0.62 (261.9 examples/sec; 0.489 sec/batch)
2016-02-04 00:54:22.663202: step 114160, loss = 0.71 (278.3 examples/sec; 0.460 sec/batch)
2016-02-04 00:54:27.307349: step 114170, loss = 0.70 (266.1 examples/sec; 0.481 sec/batch)
2016-02-04 00:54:32.055675: step 114180, loss = 0.90 (264.8 examples/sec; 0.483 sec/batch)
2016-02-04 00:54:36.746543: step 114190, loss = 0.72 (275.4 examples/sec; 0.465 sec/batch)
2016-02-04 00:54:41.430530: step 114200, loss = 0.61 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 00:54:46.663510: step 114210, loss = 0.78 (287.0 examples/sec; 0.446 sec/batch)
2016-02-04 00:54:51.386641: step 114220, loss = 0.68 (274.1 examples/sec; 0.467 sec/batch)
2016-02-04 00:54:56.082163: step 114230, loss = 0.63 (293.3 examples/sec; 0.436 sec/batch)
2016-02-04 00:55:00.697306: step 114240, loss = 0.74 (268.2 examples/sec; 0.477 sec/batch)
2016-02-04 00:55:05.363824: step 114250, loss = 0.62 (265.3 examples/sec; 0.482 sec/batch)
2016-02-04 00:55:10.072925: step 114260, loss = 0.74 (276.1 examples/sec; 0.464 sec/batch)
2016-02-04 00:55:14.830146: step 114270, loss = 0.57 (284.8 examples/sec; 0.449 sec/batch)
2016-02-04 00:55:19.462561: step 114280, loss = 0.68 (293.6 examples/sec; 0.436 sec/batch)
2016-02-04 00:55:24.114449: step 114290, loss = 0.94 (285.7 examples/sec; 0.448 sec/batch)
2016-02-04 00:55:28.808259: step 114300, loss = 0.73 (282.4 examples/sec; 0.453 sec/batch)
2016-02-04 00:55:34.025456: step 114310, loss = 0.71 (279.6 examples/sec; 0.458 sec/batch)
2016-02-04 00:55:38.703964: step 114320, loss = 0.73 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 00:55:43.414977: step 114330, loss = 0.60 (260.7 examples/sec; 0.491 sec/batch)
2016-02-04 00:55:48.109264: step 114340, loss = 0.68 (265.8 examples/sec; 0.482 sec/batch)
2016-02-04 00:55:52.764528: step 114350, loss = 0.77 (283.5 examples/sec; 0.452 sec/batch)
2016-02-04 00:55:57.415729: step 114360, loss = 0.57 (258.7 examples/sec; 0.495 sec/batch)
2016-02-04 00:56:02.122078: step 114370, loss = 0.69 (278.5 examples/sec; 0.460 sec/batch)
2016-02-04 00:56:06.863956: step 114380, loss = 0.65 (280.7 examples/sec; 0.456 sec/batch)
2016-02-04 00:56:11.574644: step 114390, loss = 0.79 (259.0 examples/sec; 0.494 sec/batch)
2016-02-04 00:56:16.238196: step 114400, loss = 0.62 (267.7 examples/sec; 0.478 sec/batch)
2016-02-04 00:56:21.376194: step 114410, loss = 0.60 (263.9 examples/sec; 0.485 sec/batch)
2016-02-04 00:56:26.160030: step 114420, loss = 0.74 (274.8 examples/sec; 0.466 sec/batch)
2016-02-04 00:56:30.827500: step 114430, loss = 0.70 (269.1 examples/sec; 0.476 sec/batch)
2016-02-04 00:56:35.569517: step 114440, loss = 0.82 (273.2 examples/sec; 0.468 sec/batch)
2016-02-04 00:56:40.255735: step 114450, loss = 0.59 (262.4 examples/sec; 0.488 sec/batch)
2016-02-04 00:56:44.957676: step 114460, loss = 0.66 (314.4 examples/sec; 0.407 sec/batch)
2016-02-04 00:56:49.634635: step 114470, loss = 0.62 (282.5 examples/sec; 0.453 sec/batch)
2016-02-04 00:56:54.349799: step 114480, loss = 0.56 (248.6 examples/sec; 0.515 sec/batch)
2016-02-04 00:56:59.002145: step 114490, loss = 0.67 (266.9 examples/sec; 0.480 sec/batch)
2016-02-04 00:57:03.638045: step 114500, loss = 0.68 (267.5 examples/sec; 0.479 sec/batch)
2016-02-04 00:57:08.840086: step 114510, loss = 0.66 (269.2 examples/sec; 0.475 sec/batch)
2016-02-04 00:57:13.525198: step 114520, loss = 0.58 (259.7 examples/sec; 0.493 sec/batch)
2016-02-04 00:57:18.221104: step 114530, loss = 0.89 (269.3 examples/sec; 0.475 sec/batch)
2016-02-04 00:57:22.857980: step 114540, loss = 0.61 (279.2 examples/sec; 0.459 sec/batch)
2016-02-04 00:57:27.605658: step 114550, loss = 0.75 (261.3 examples/sec; 0.490 sec/batch)
2016-02-04 00:57:32.311960: step 114560, loss = 0.70 (263.4 examples/sec; 0.486 sec/batch)
2016-02-04 00:57:37.001665: step 114570, loss = 0.66 (260.1 examples/sec; 0.492 sec/batch)
2016-02-04 00:57:41.730258: step 114580, loss = 0.68 (266.5 examples/sec; 0.480 sec/batch)
2016-02-04 00:57:46.364156: step 114590, loss = 0.78 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 00:57:51.067540: step 114600, loss = 0.74 (259.7 examples/sec; 0.493 sec/batch)
2016-02-04 00:57:56.399049: step 114610, loss = 0.66 (262.5 examples/sec; 0.488 sec/batch)
2016-02-04 00:58:01.215817: step 114620, loss = 0.63 (244.5 examples/sec; 0.523 sec/batch)
2016-02-04 00:58:05.928627: step 114630, loss = 0.60 (278.9 examples/sec; 0.459 sec/batch)
2016-02-04 00:58:10.637119: step 114640, loss = 0.70 (271.5 examples/sec; 0.472 sec/batch)
2016-02-04 00:58:15.296806: step 114650, loss = 0.66 (277.3 examples/sec; 0.462 sec/batch)
2016-02-04 00:58:20.058265: step 114660, loss = 0.76 (248.7 examples/sec; 0.515 sec/batch)
2016-02-04 00:58:24.721397: step 114670, loss = 0.83 (273.5 examples/sec; 0.468 sec/batch)
2016-02-04 00:58:29.508582: step 114680, loss = 0.67 (256.4 examples/sec; 0.499 sec/batch)
2016-02-04 00:58:34.197244: step 114690, loss = 0.75 (249.0 examples/sec; 0.514 sec/batch)
2016-02-04 00:58:38.855314: step 114700, loss = 0.78 (284.3 examples/sec; 0.450 sec/batch)
2016-02-04 00:58:44.140545: step 114710, loss = 0.82 (261.1 examples/sec; 0.490 sec/batch)
2016-02-04 00:58:48.759183: step 114720, loss = 0.62 (273.3 examples/sec; 0.468 sec/batch)
2016-02-04 00:58:53.446050: step 114730, loss = 0.76 (271.0 examples/sec; 0.472 sec/batch)
2016-02-04 00:58:58.161118: step 114740, loss = 0.61 (255.2 examples/sec; 0.502 sec/batch)
2016-02-04 00:59:02.877448: step 114750, loss = 0.72 (261.2 examples/sec; 0.490 sec/batch)
2016-02-04 00:59:07.501928: step 114760, loss = 0.75 (295.6 examples/sec; 0.433 sec/batch)
2016-02-04 00:59:12.103535: step 114770, loss = 0.67 (286.7 examples/sec; 0.446 sec/batch)
2016-02-04 00:59:16.855343: step 114780, loss = 0.71 (264.2 examples/sec; 0.484 sec/batch)
2016-02-04 00:59:21.592283: step 114790, loss = 0.81 (285.2 examples/sec; 0.449 sec/batch)
2016-02-04 00:59:26.291606: step 114800, loss = 0.77 (289.7 examples/sec; 0.442 sec/batch)
2016-02-04 00:59:31.526906: step 114810, loss = 0.71 (270.4 examples/sec; 0.473 sec/batch)
2016-02-04 00:59:36.276281: step 114820, loss = 0.60 (269.4 examples/sec; 0.475 sec/batch)
2016-02-04 00:59:40.941885: step 114830, loss = 0.58 (269.8 examples/sec; 0.474 sec/batch)
2016-02-04 00:59:45.677534: step 114840, loss = 0.72 (268.3 examples/sec; 0.477 sec/batch)
2016-02-04 00:59:50.398161: step 114850, loss = 0.82 (279.0 examples/sec; 0.459 sec/batch)
2016-02-04 00:59:55.126144: step 114860, loss = 0.66 (270.7 examples/sec; 0.473 sec/batch)
2016-02-04 00:59:59.783845: step 114870, loss = 0.72 (273.6 examples/sec; 0.468 sec/batch)
2016-02-04 01:00:04.433701: step 114880, loss = 0.79 (284.8 examples/sec; 0.449 sec/batch)
2016-02-04 01:00:09.073213: step 114890, loss = 0.60 (307.4 examples/sec; 0.416 sec/batch)
2016-02-04 01:00:13.810904: step 114900, loss = 0.81 (277.4 examples/sec; 0.461 sec/batch)
2016-02-04 01:00:18.978698: step 114910, loss = 0.64 (280.8 examples/sec; 0.456 sec/batch)
2016-02-04 01:00:23.712397: step 114920, loss = 0.80 (258.0 examples/sec; 0.496 sec/batch)
2016-02-04 01:00:28.421280: step 114930, loss = 0.70 (266.0 examples/sec; 0.481 sec/batch)
2016-02-04 01:00:33.137445: step 114940, loss = 0.68 (274.1 examples/sec; 0.467 sec/batch)
2016-02-04 01:00:37.820798: step 114950, loss = 0.80 (296.1 examples/sec; 0.432 sec/batch)
2016-02-04 01:00:42.585631: step 114960, loss = 0.77 (251.0 examples/sec; 0.510 sec/batch)
2016-02-04 01:00:47.327350: step 114970, loss = 0.66 (247.2 examples/sec; 0.518 sec/batch)
2016-02-04 01:00:52.064955: step 114980, loss = 0.67 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 01:00:56.753940: step 114990, loss = 0.55 (278.2 examples/sec; 0.460 sec/batch)
2016-02-04 01:01:01.528106: step 115000, loss = 0.66 (253.2 examples/sec; 0.506 sec/batch)
2016-02-04 01:01:06.721585: step 115010, loss = 0.62 (289.9 examples/sec; 0.442 sec/batch)
2016-02-04 01:01:11.390610: step 115020, loss = 0.82 (270.9 examples/sec; 0.472 sec/batch)
2016-02-04 01:01:16.084429: step 115030, loss = 0.85 (272.6 examples/sec; 0.470 sec/batch)
2016-02-04 01:01:20.747502: step 115040, loss = 0.69 (277.4 examples/sec; 0.461 sec/batch)
2016-02-04 01:01:25.495033: step 115050, loss = 0.73 (275.4 examples/sec; 0.465 sec/batch)
2016-02-04 01:01:30.110386: step 115060, loss = 0.64 (266.8 examples/sec; 0.480 sec/batch)
2016-02-04 01:01:34.799874: step 115070, loss = 0.75 (279.0 examples/sec; 0.459 sec/batch)
2016-02-04 01:01:39.557067: step 115080, loss = 0.60 (271.1 examples/sec; 0.472 sec/batch)
2016-02-04 01:01:44.277823: step 115090, loss = 0.72 (280.6 examples/sec; 0.456 sec/batch)
2016-02-04 01:01:48.924527: step 115100, loss = 0.63 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 01:01:54.064561: step 115110, loss = 0.93 (274.9 examples/sec; 0.466 sec/batch)
2016-02-04 01:01:58.746228: step 115120, loss = 0.75 (320.9 examples/sec; 0.399 sec/batch)
2016-02-04 01:02:03.427239: step 115130, loss = 0.75 (264.7 examples/sec; 0.484 sec/batch)
2016-02-04 01:02:08.096324: step 115140, loss = 0.60 (258.4 examples/sec; 0.495 sec/batch)
2016-02-04 01:02:12.700708: step 115150, loss = 0.66 (278.8 examples/sec; 0.459 sec/batch)
2016-02-04 01:02:17.382760: step 115160, loss = 0.75 (259.2 examples/sec; 0.494 sec/batch)
2016-02-04 01:02:22.054080: step 115170, loss = 0.66 (269.7 examples/sec; 0.475 sec/batch)
2016-02-04 01:02:26.691715: step 115180, loss = 0.63 (276.8 examples/sec; 0.462 sec/batch)
2016-02-04 01:02:31.345291: step 115190, loss = 0.67 (268.2 examples/sec; 0.477 sec/batch)
2016-02-04 01:02:36.119582: step 115200, loss = 0.66 (285.2 examples/sec; 0.449 sec/batch)
2016-02-04 01:02:41.341444: step 115210, loss = 0.68 (289.6 examples/sec; 0.442 sec/batch)
2016-02-04 01:02:45.981711: step 115220, loss = 0.77 (275.8 examples/sec; 0.464 sec/batch)
2016-02-04 01:02:50.579704: step 115230, loss = 0.75 (282.0 examples/sec; 0.454 sec/batch)
2016-02-04 01:02:55.246433: step 115240, loss = 0.68 (267.0 examples/sec; 0.479 sec/batch)
2016-02-04 01:02:59.907070: step 115250, loss = 0.69 (287.2 examples/sec; 0.446 sec/batch)
2016-02-04 01:03:04.629790: step 115260, loss = 0.61 (288.5 examples/sec; 0.444 sec/batch)
2016-02-04 01:03:09.426528: step 115270, loss = 0.79 (267.3 examples/sec; 0.479 sec/batch)
2016-02-04 01:03:14.146239: step 115280, loss = 0.72 (258.7 examples/sec; 0.495 sec/batch)
2016-02-04 01:03:18.723685: step 115290, loss = 0.78 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 01:03:23.383650: step 115300, loss = 0.55 (241.2 examples/sec; 0.531 sec/batch)
2016-02-04 01:03:28.564878: step 115310, loss = 0.60 (257.2 examples/sec; 0.498 sec/batch)
2016-02-04 01:03:33.238478: step 115320, loss = 0.74 (274.2 examples/sec; 0.467 sec/batch)
2016-02-04 01:03:37.972462: step 115330, loss = 0.81 (268.3 examples/sec; 0.477 sec/batch)
2016-02-04 01:03:42.657776: step 115340, loss = 0.69 (279.3 examples/sec; 0.458 sec/batch)
2016-02-04 01:03:47.375427: step 115350, loss = 0.58 (282.9 examples/sec; 0.452 sec/batch)
2016-02-04 01:03:52.125348: step 115360, loss = 0.70 (272.9 examples/sec; 0.469 sec/batch)
2016-02-04 01:03:56.751243: step 115370, loss = 0.73 (273.7 examples/sec; 0.468 sec/batch)
2016-02-04 01:04:01.391973: step 115380, loss = 0.75 (292.1 examples/sec; 0.438 sec/batch)
2016-02-04 01:04:06.124524: step 115390, loss = 0.63 (296.5 examples/sec; 0.432 sec/batch)
2016-02-04 01:04:10.849993: step 115400, loss = 0.68 (257.6 examples/sec; 0.497 sec/batch)
2016-02-04 01:04:16.044709: step 115410, loss = 0.69 (271.1 examples/sec; 0.472 sec/batch)
2016-02-04 01:04:20.807033: step 115420, loss = 0.72 (277.8 examples/sec; 0.461 sec/batch)
2016-02-04 01:04:25.580739: step 115430, loss = 0.87 (281.5 examples/sec; 0.455 sec/batch)
2016-02-04 01:04:30.295365: step 115440, loss = 0.63 (283.0 examples/sec; 0.452 sec/batch)
2016-02-04 01:04:35.073024: step 115450, loss = 0.76 (286.4 examples/sec; 0.447 sec/batch)
2016-02-04 01:04:39.852245: step 115460, loss = 0.68 (255.6 examples/sec; 0.501 sec/batch)
2016-02-04 01:04:44.548142: step 115470, loss = 0.68 (282.6 examples/sec; 0.453 sec/batch)
2016-02-04 01:04:49.230402: step 115480, loss = 0.63 (276.9 examples/sec; 0.462 sec/batch)
2016-02-04 01:04:53.919684: step 115490, loss = 0.68 (276.1 examples/sec; 0.464 sec/batch)
2016-02-04 01:04:58.542732: step 115500, loss = 0.84 (270.0 examples/sec; 0.474 sec/batch)
2016-02-04 01:05:03.863159: step 115510, loss = 0.72 (263.4 examples/sec; 0.486 sec/batch)
2016-02-04 01:05:08.551913: step 115520, loss = 0.72 (261.8 examples/sec; 0.489 sec/batch)
2016-02-04 01:05:13.251593: step 115530, loss = 0.64 (275.4 examples/sec; 0.465 sec/batch)
2016-02-04 01:05:17.870234: step 115540, loss = 0.74 (274.4 examples/sec; 0.467 sec/batch)
2016-02-04 01:05:22.528999: step 115550, loss = 0.83 (300.8 examples/sec; 0.426 sec/batch)
2016-02-04 01:05:27.203211: step 115560, loss = 0.62 (263.1 examples/sec; 0.486 sec/batch)
2016-02-04 01:05:31.924233: step 115570, loss = 0.71 (261.9 examples/sec; 0.489 sec/batch)
2016-02-04 01:05:36.565161: step 115580, loss = 0.69 (287.7 examples/sec; 0.445 sec/batch)
2016-02-04 01:05:41.291121: step 115590, loss = 0.61 (272.3 examples/sec; 0.470 sec/batch)
2016-02-04 01:05:45.956386: step 115600, loss = 0.78 (281.8 examples/sec; 0.454 sec/batch)
2016-02-04 01:05:51.085060: step 115610, loss = 0.62 (292.4 examples/sec; 0.438 sec/batch)
2016-02-04 01:05:55.804405: step 115620, loss = 0.85 (275.7 examples/sec; 0.464 sec/batch)
2016-02-04 01:06:00.465739: step 115630, loss = 0.70 (278.1 examples/sec; 0.460 sec/batch)
2016-02-04 01:06:05.158002: step 115640, loss = 0.67 (255.5 examples/sec; 0.501 sec/batch)
2016-02-04 01:06:09.856654: step 115650, loss = 0.80 (284.3 examples/sec; 0.450 sec/batch)
2016-02-04 01:06:14.634277: step 115660, loss = 0.64 (291.9 examples/sec; 0.438 sec/batch)
2016-02-04 01:06:19.339275: step 115670, loss = 0.62 (264.4 examples/sec; 0.484 sec/batch)
2016-02-04 01:06:24.118936: step 115680, loss = 0.61 (278.8 examples/sec; 0.459 sec/batch)
2016-02-04 01:06:28.825464: step 115690, loss = 0.73 (251.2 examples/sec; 0.510 sec/batch)
2016-02-04 01:06:33.539881: step 115700, loss = 0.65 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 01:06:38.840271: step 115710, loss = 0.69 (270.5 examples/sec; 0.473 sec/batch)
2016-02-04 01:06:43.580774: step 115720, loss = 0.63 (281.4 examples/sec; 0.455 sec/batch)
2016-02-04 01:06:48.297219: step 115730, loss = 0.82 (294.2 examples/sec; 0.435 sec/batch)
2016-02-04 01:06:53.064027: step 115740, loss = 0.73 (250.7 examples/sec; 0.511 sec/batch)
2016-02-04 01:06:57.722067: step 115750, loss = 0.54 (288.8 examples/sec; 0.443 sec/batch)
2016-02-04 01:07:02.400397: step 115760, loss = 0.76 (277.7 examples/sec; 0.461 sec/batch)
2016-02-04 01:07:07.068731: step 115770, loss = 0.61 (268.3 examples/sec; 0.477 sec/batch)
2016-02-04 01:07:11.731020: step 115780, loss = 0.81 (294.0 examples/sec; 0.435 sec/batch)
2016-02-04 01:07:16.414728: step 115790, loss = 0.78 (279.9 examples/sec; 0.457 sec/batch)
2016-02-04 01:07:21.167399: step 115800, loss = 0.73 (246.6 examples/sec; 0.519 sec/batch)
2016-02-04 01:07:26.356414: step 115810, loss = 0.73 (281.9 examples/sec; 0.454 sec/batch)
2016-02-04 01:07:31.009695: step 115820, loss = 0.61 (276.0 examples/sec; 0.464 sec/batch)
2016-02-04 01:07:35.750108: step 115830, loss = 0.79 (248.6 examples/sec; 0.515 sec/batch)
2016-02-04 01:07:40.425100: step 115840, loss = 0.72 (255.2 examples/sec; 0.501 sec/batch)
2016-02-04 01:07:45.063511: step 115850, loss = 0.70 (300.1 examples/sec; 0.426 sec/batch)
2016-02-04 01:07:49.787682: step 115860, loss = 0.59 (298.5 examples/sec; 0.429 sec/batch)
2016-02-04 01:07:54.533567: step 115870, loss = 0.68 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 01:07:59.217584: step 115880, loss = 0.70 (261.9 examples/sec; 0.489 sec/batch)
2016-02-04 01:08:03.935338: step 115890, loss = 0.91 (284.7 examples/sec; 0.450 sec/batch)
2016-02-04 01:08:08.669701: step 115900, loss = 0.78 (274.0 examples/sec; 0.467 sec/batch)
2016-02-04 01:08:13.895256: step 115910, loss = 0.75 (273.9 examples/sec; 0.467 sec/batch)
2016-02-04 01:08:18.548665: step 115920, loss = 0.52 (298.6 examples/sec; 0.429 sec/batch)
2016-02-04 01:08:23.326983: step 115930, loss = 0.61 (277.6 examples/sec; 0.461 sec/batch)
2016-02-04 01:08:28.011928: step 115940, loss = 0.80 (253.9 examples/sec; 0.504 sec/batch)
2016-02-04 01:08:32.754486: step 115950, loss = 0.67 (265.9 examples/sec; 0.481 sec/batch)
2016-02-04 01:08:37.484140: step 115960, loss = 0.57 (272.6 examples/sec; 0.470 sec/batch)
2016-02-04 01:08:42.090422: step 115970, loss = 0.67 (271.6 examples/sec; 0.471 sec/batch)
2016-02-04 01:08:46.771644: step 115980, loss = 0.72 (281.4 examples/sec; 0.455 sec/batch)
2016-02-04 01:08:51.450568: step 115990, loss = 0.88 (294.4 examples/sec; 0.435 sec/batch)
2016-02-04 01:08:56.215163: step 116000, loss = 0.88 (252.1 examples/sec; 0.508 sec/batch)
2016-02-04 01:09:01.500738: step 116010, loss = 0.61 (296.8 examples/sec; 0.431 sec/batch)
2016-02-04 01:09:06.227359: step 116020, loss = 0.75 (298.6 examples/sec; 0.429 sec/batch)
2016-02-04 01:09:10.954774: step 116030, loss = 0.76 (272.8 examples/sec; 0.469 sec/batch)
2016-02-04 01:09:15.668172: step 116040, loss = 0.80 (284.3 examples/sec; 0.450 sec/batch)
2016-02-04 01:09:20.380997: step 116050, loss = 0.73 (279.2 examples/sec; 0.458 sec/batch)
2016-02-04 01:09:25.075567: step 116060, loss = 0.71 (273.6 examples/sec; 0.468 sec/batch)
2016-02-04 01:09:29.801410: step 116070, loss = 0.83 (260.5 examples/sec; 0.491 sec/batch)
2016-02-04 01:09:34.412852: step 116080, loss = 0.66 (313.5 examples/sec; 0.408 sec/batch)
2016-02-04 01:09:39.070656: step 116090, loss = 0.76 (284.3 examples/sec; 0.450 sec/batch)
2016-02-04 01:09:43.751132: step 116100, loss = 0.69 (277.3 examples/sec; 0.462 sec/batch)
2016-02-04 01:09:49.046355: step 116110, loss = 0.65 (283.7 examples/sec; 0.451 sec/batch)
2016-02-04 01:09:53.749615: step 116120, loss = 0.88 (278.4 examples/sec; 0.460 sec/batch)
2016-02-04 01:09:58.484667: step 116130, loss = 0.76 (254.1 examples/sec; 0.504 sec/batch)
2016-02-04 01:10:03.129696: step 116140, loss = 0.65 (286.5 examples/sec; 0.447 sec/batch)
2016-02-04 01:10:07.894867: step 116150, loss = 0.69 (256.2 examples/sec; 0.500 sec/batch)
2016-02-04 01:10:12.641206: step 116160, loss = 0.71 (250.8 examples/sec; 0.510 sec/batch)
2016-02-04 01:10:17.321409: step 116170, loss = 0.80 (266.0 examples/sec; 0.481 sec/batch)
2016-02-04 01:10:21.977314: step 116180, loss = 0.53 (283.0 examples/sec; 0.452 sec/batch)
2016-02-04 01:10:26.673276: step 116190, loss = 0.78 (275.8 examples/sec; 0.464 sec/batch)
2016-02-04 01:10:31.372045: step 116200, loss = 0.90 (257.0 examples/sec; 0.498 sec/batch)
2016-02-04 01:10:36.585474: step 116210, loss = 0.65 (284.6 examples/sec; 0.450 sec/batch)
2016-02-04 01:10:41.306613: step 116220, loss = 0.73 (253.5 examples/sec; 0.505 sec/batch)
2016-02-04 01:10:45.965103: step 116230, loss = 0.75 (264.6 examples/sec; 0.484 sec/batch)
2016-02-04 01:10:50.696659: step 116240, loss = 0.85 (250.5 examples/sec; 0.511 sec/batch)
2016-02-04 01:10:55.386147: step 116250, loss = 0.68 (273.6 examples/sec; 0.468 sec/batch)
2016-02-04 01:11:00.082315: step 116260, loss = 0.63 (269.4 examples/sec; 0.475 sec/batch)
2016-02-04 01:11:04.865175: step 116270, loss = 0.85 (287.3 examples/sec; 0.446 sec/batch)
2016-02-04 01:11:09.532472: step 116280, loss = 0.68 (281.0 examples/sec; 0.456 sec/batch)
2016-02-04 01:11:14.233709: step 116290, loss = 0.79 (264.4 examples/sec; 0.484 sec/batch)
2016-02-04 01:11:18.903603: step 116300, loss = 0.73 (273.6 examples/sec; 0.468 sec/batch)
2016-02-04 01:11:24.138549: step 116310, loss = 0.76 (266.9 examples/sec; 0.480 sec/batch)
2016-02-04 01:11:28.869962: step 116320, loss = 0.76 (267.1 examples/sec; 0.479 sec/batch)
2016-02-04 01:11:33.669149: step 116330, loss = 0.72 (254.4 examples/sec; 0.503 sec/batch)
2016-02-04 01:11:38.302862: step 116340, loss = 0.58 (274.1 examples/sec; 0.467 sec/batch)
2016-02-04 01:11:43.035566: step 116350, loss = 0.69 (272.7 examples/sec; 0.469 sec/batch)
2016-02-04 01:11:47.824627: step 116360, loss = 0.62 (266.7 examples/sec; 0.480 sec/batch)
2016-02-04 01:11:52.569832: step 116370, loss = 0.81 (270.1 examples/sec; 0.474 sec/batch)
2016-02-04 01:11:57.262390: step 116380, loss = 0.73 (277.4 examples/sec; 0.461 sec/batch)
2016-02-04 01:12:02.075876: step 116390, loss = 0.66 (258.4 examples/sec; 0.495 sec/batch)
2016-02-04 01:12:06.718430: step 116400, loss = 0.65 (299.8 examples/sec; 0.427 sec/batch)
2016-02-04 01:12:11.978561: step 116410, loss = 0.62 (276.8 examples/sec; 0.462 sec/batch)
2016-02-04 01:12:16.721212: step 116420, loss = 0.83 (276.8 examples/sec; 0.462 sec/batch)
2016-02-04 01:12:21.420382: step 116430, loss = 0.68 (266.8 examples/sec; 0.480 sec/batch)
2016-02-04 01:12:26.161896: step 116440, loss = 0.62 (266.2 examples/sec; 0.481 sec/batch)
2016-02-04 01:12:30.878226: step 116450, loss = 0.68 (286.4 examples/sec; 0.447 sec/batch)
2016-02-04 01:12:35.572966: step 116460, loss = 0.48 (277.5 examples/sec; 0.461 sec/batch)
2016-02-04 01:12:40.294598: step 116470, loss = 0.74 (258.0 examples/sec; 0.496 sec/batch)
2016-02-04 01:12:44.984925: step 116480, loss = 0.56 (257.3 examples/sec; 0.497 sec/batch)
2016-02-04 01:12:49.580908: step 116490, loss = 0.71 (287.9 examples/sec; 0.445 sec/batch)
2016-02-04 01:12:54.285596: step 116500, loss = 0.70 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 01:12:59.575978: step 116510, loss = 0.74 (284.8 examples/sec; 0.450 sec/batch)
2016-02-04 01:13:04.294605: step 116520, loss = 0.61 (300.2 examples/sec; 0.426 sec/batch)
2016-02-04 01:13:08.973905: step 116530, loss = 0.65 (272.9 examples/sec; 0.469 sec/batch)
2016-02-04 01:13:13.637135: step 116540, loss = 0.67 (261.7 examples/sec; 0.489 sec/batch)
2016-02-04 01:13:18.294980: step 116550, loss = 0.54 (270.8 examples/sec; 0.473 sec/batch)
2016-02-04 01:13:22.902864: step 116560, loss = 0.65 (289.2 examples/sec; 0.443 sec/batch)
2016-02-04 01:13:27.592243: step 116570, loss = 0.75 (274.6 examples/sec; 0.466 sec/batch)
2016-02-04 01:13:32.191970: step 116580, loss = 0.66 (280.7 examples/sec; 0.456 sec/batch)
2016-02-04 01:13:36.954567: step 116590, loss = 0.92 (269.8 examples/sec; 0.474 sec/batch)
2016-02-04 01:13:41.651117: step 116600, loss = 0.68 (249.2 examples/sec; 0.514 sec/batch)
2016-02-04 01:13:46.707839: step 116610, loss = 0.67 (291.3 examples/sec; 0.439 sec/batch)
2016-02-04 01:13:51.405554: step 116620, loss = 0.73 (273.1 examples/sec; 0.469 sec/batch)
2016-02-04 01:13:56.154135: step 116630, loss = 0.61 (299.0 examples/sec; 0.428 sec/batch)
2016-02-04 01:14:00.901405: step 116640, loss = 0.63 (250.3 examples/sec; 0.511 sec/batch)
2016-02-04 01:14:05.629407: step 116650, loss = 0.62 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 01:14:10.335214: step 116660, loss = 0.67 (282.3 examples/sec; 0.453 sec/batch)
2016-02-04 01:14:14.991602: step 116670, loss = 0.67 (270.9 examples/sec; 0.473 sec/batch)
2016-02-04 01:14:19.702765: step 116680, loss = 0.62 (268.9 examples/sec; 0.476 sec/batch)
2016-02-04 01:14:24.434292: step 116690, loss = 0.67 (276.0 examples/sec; 0.464 sec/batch)
2016-02-04 01:14:29.171274: step 116700, loss = 0.82 (247.3 examples/sec; 0.518 sec/batch)
2016-02-04 01:14:34.450617: step 116710, loss = 0.61 (276.3 examples/sec; 0.463 sec/batch)
2016-02-04 01:14:39.155544: step 116720, loss = 0.65 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 01:14:43.873299: step 116730, loss = 0.68 (263.0 examples/sec; 0.487 sec/batch)
2016-02-04 01:14:48.596243: step 116740, loss = 0.72 (264.7 examples/sec; 0.484 sec/batch)
2016-02-04 01:14:53.285858: step 116750, loss = 0.78 (255.9 examples/sec; 0.500 sec/batch)
2016-02-04 01:14:57.961702: step 116760, loss = 0.76 (272.9 examples/sec; 0.469 sec/batch)
2016-02-04 01:15:02.685179: step 116770, loss = 0.78 (266.9 examples/sec; 0.479 sec/batch)
2016-02-04 01:15:07.383635: step 116780, loss = 0.71 (297.7 examples/sec; 0.430 sec/batch)
2016-02-04 01:15:12.157368: step 116790, loss = 0.85 (275.9 examples/sec; 0.464 sec/batch)
2016-02-04 01:15:16.883301: step 116800, loss = 0.67 (253.2 examples/sec; 0.505 sec/batch)
2016-02-04 01:15:22.136469: step 116810, loss = 0.55 (264.1 examples/sec; 0.485 sec/batch)
2016-02-04 01:15:26.807972: step 116820, loss = 0.68 (280.0 examples/sec; 0.457 sec/batch)
2016-02-04 01:15:31.487540: step 116830, loss = 0.65 (264.3 examples/sec; 0.484 sec/batch)
2016-02-04 01:15:36.160001: step 116840, loss = 0.76 (303.4 examples/sec; 0.422 sec/batch)
2016-02-04 01:15:40.866820: step 116850, loss = 0.65 (277.8 examples/sec; 0.461 sec/batch)
2016-02-04 01:15:45.522632: step 116860, loss = 0.62 (283.2 examples/sec; 0.452 sec/batch)
2016-02-04 01:15:50.135127: step 116870, loss = 0.57 (267.0 examples/sec; 0.479 sec/batch)
2016-02-04 01:15:54.861876: step 116880, loss = 0.78 (266.1 examples/sec; 0.481 sec/batch)
2016-02-04 01:15:59.573371: step 116890, loss = 0.65 (275.3 examples/sec; 0.465 sec/batch)
2016-02-04 01:16:04.365077: step 116900, loss = 0.63 (268.0 examples/sec; 0.478 sec/batch)
2016-02-04 01:16:09.543624: step 116910, loss = 0.70 (264.8 examples/sec; 0.483 sec/batch)
2016-02-04 01:16:14.289425: step 116920, loss = 0.69 (258.9 examples/sec; 0.494 sec/batch)
2016-02-04 01:16:18.935223: step 116930, loss = 0.77 (296.7 examples/sec; 0.431 sec/batch)
2016-02-04 01:16:23.680967: step 116940, loss = 0.87 (268.9 examples/sec; 0.476 sec/batch)
2016-02-04 01:16:28.397287: step 116950, loss = 0.64 (256.3 examples/sec; 0.499 sec/batch)
2016-02-04 01:16:33.051305: step 116960, loss = 0.73 (274.0 examples/sec; 0.467 sec/batch)
2016-02-04 01:16:37.780884: step 116970, loss = 0.71 (246.1 examples/sec; 0.520 sec/batch)
2016-02-04 01:16:42.454803: step 116980, loss = 0.61 (260.1 examples/sec; 0.492 sec/batch)
2016-02-04 01:16:47.071677: step 116990, loss = 0.63 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 01:16:51.780183: step 117000, loss = 0.55 (260.8 examples/sec; 0.491 sec/batch)
2016-02-04 01:16:56.992235: step 117010, loss = 0.72 (286.7 examples/sec; 0.447 sec/batch)
2016-02-04 01:17:01.670543: step 117020, loss = 0.56 (280.8 examples/sec; 0.456 sec/batch)
2016-02-04 01:17:06.417030: step 117030, loss = 0.57 (272.2 examples/sec; 0.470 sec/batch)
2016-02-04 01:17:10.997513: step 117040, loss = 0.59 (268.6 examples/sec; 0.477 sec/batch)
2016-02-04 01:17:15.678047: step 117050, loss = 0.87 (270.8 examples/sec; 0.473 sec/batch)
2016-02-04 01:17:20.337872: step 117060, loss = 0.74 (280.3 examples/sec; 0.457 sec/batch)
2016-02-04 01:17:24.962982: step 117070, loss = 0.74 (293.5 examples/sec; 0.436 sec/batch)
2016-02-04 01:17:29.670562: step 117080, loss = 0.67 (282.2 examples/sec; 0.454 sec/batch)
2016-02-04 01:17:34.377537: step 117090, loss = 0.73 (291.3 examples/sec; 0.439 sec/batch)
2016-02-04 01:17:39.120918: step 117100, loss = 0.77 (267.6 examples/sec; 0.478 sec/batch)
2016-02-04 01:17:44.328155: step 117110, loss = 0.73 (274.0 examples/sec; 0.467 sec/batch)
2016-02-04 01:17:49.041417: step 117120, loss = 0.78 (259.4 examples/sec; 0.494 sec/batch)
2016-02-04 01:17:53.753599: step 117130, loss = 0.66 (246.6 examples/sec; 0.519 sec/batch)
2016-02-04 01:17:58.462425: step 117140, loss = 0.77 (270.9 examples/sec; 0.473 sec/batch)
2016-02-04 01:18:03.136314: step 117150, loss = 0.74 (275.5 examples/sec; 0.465 sec/batch)
2016-02-04 01:18:07.797675: step 117160, loss = 0.71 (272.6 examples/sec; 0.470 sec/batch)
2016-02-04 01:18:12.508707: step 117170, loss = 0.67 (278.4 examples/sec; 0.460 sec/batch)
2016-02-04 01:18:17.275518: step 117180, loss = 0.55 (254.8 examples/sec; 0.502 sec/batch)
2016-02-04 01:18:21.973050: step 117190, loss = 0.58 (257.3 examples/sec; 0.497 sec/batch)
2016-02-04 01:18:26.639526: step 117200, loss = 0.70 (278.4 examples/sec; 0.460 sec/batch)
2016-02-04 01:18:31.759015: step 117210, loss = 0.66 (274.8 examples/sec; 0.466 sec/batch)
2016-02-04 01:18:36.460251: step 117220, loss = 0.71 (256.7 examples/sec; 0.499 sec/batch)
2016-02-04 01:18:41.123542: step 117230, loss = 0.70 (271.6 examples/sec; 0.471 sec/batch)
2016-02-04 01:18:45.818857: step 117240, loss = 0.70 (282.7 examples/sec; 0.453 sec/batch)
2016-02-04 01:18:50.517953: step 117250, loss = 0.64 (272.8 examples/sec; 0.469 sec/batch)
2016-02-04 01:18:55.258949: step 117260, loss = 0.64 (239.5 examples/sec; 0.534 sec/batch)
2016-02-04 01:18:59.988542: step 117270, loss = 0.72 (267.8 examples/sec; 0.478 sec/batch)
2016-02-04 01:19:04.641970: step 117280, loss = 0.67 (268.9 examples/sec; 0.476 sec/batch)
2016-02-04 01:19:09.403274: step 117290, loss = 0.72 (250.4 examples/sec; 0.511 sec/batch)
2016-02-04 01:19:14.075435: step 117300, loss = 0.83 (273.2 examples/sec; 0.469 sec/batch)
2016-02-04 01:19:19.274481: step 117310, loss = 0.70 (273.5 examples/sec; 0.468 sec/batch)
2016-02-04 01:19:24.024966: step 117320, loss = 0.73 (276.6 examples/sec; 0.463 sec/batch)
2016-02-04 01:19:28.710912: step 117330, loss = 0.67 (271.2 examples/sec; 0.472 sec/batch)
2016-02-04 01:19:33.405291: step 117340, loss = 0.75 (276.3 examples/sec; 0.463 sec/batch)
2016-02-04 01:19:38.119122: step 117350, loss = 0.64 (260.8 examples/sec; 0.491 sec/batch)
2016-02-04 01:19:42.803793: step 117360, loss = 0.64 (286.0 examples/sec; 0.448 sec/batch)
2016-02-04 01:19:47.525566: step 117370, loss = 0.63 (265.1 examples/sec; 0.483 sec/batch)
2016-02-04 01:19:52.246964: step 117380, loss = 0.53 (266.5 examples/sec; 0.480 sec/batch)
2016-02-04 01:19:56.925542: step 117390, loss = 0.56 (279.4 examples/sec; 0.458 sec/batch)
2016-02-04 01:20:01.642728: step 117400, loss = 0.58 (275.8 examples/sec; 0.464 sec/batch)
2016-02-04 01:20:06.835278: step 117410, loss = 0.72 (253.9 examples/sec; 0.504 sec/batch)
2016-02-04 01:20:11.490521: step 117420, loss = 0.81 (272.3 examples/sec; 0.470 sec/batch)
2016-02-04 01:20:16.143120: step 117430, loss = 0.57 (277.1 examples/sec; 0.462 sec/batch)
2016-02-04 01:20:20.792289: step 117440, loss = 0.61 (288.2 examples/sec; 0.444 sec/batch)
2016-02-04 01:20:25.515158: step 117450, loss = 0.78 (266.2 examples/sec; 0.481 sec/batch)
2016-02-04 01:20:30.130993: step 117460, loss = 0.64 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 01:20:34.864371: step 117470, loss = 0.62 (290.3 examples/sec; 0.441 sec/batch)
2016-02-04 01:20:39.670713: step 117480, loss = 0.67 (268.5 examples/sec; 0.477 sec/batch)
2016-02-04 01:20:44.428152: step 117490, loss = 0.72 (274.6 examples/sec; 0.466 sec/batch)
2016-02-04 01:20:49.157848: step 117500, loss = 0.62 (265.9 examples/sec; 0.481 sec/batch)
2016-02-04 01:20:54.358327: step 117510, loss = 0.66 (278.4 examples/sec; 0.460 sec/batch)
2016-02-04 01:20:59.038209: step 117520, loss = 0.81 (238.8 examples/sec; 0.536 sec/batch)
2016-02-04 01:21:03.661507: step 117530, loss = 0.67 (281.5 examples/sec; 0.455 sec/batch)
2016-02-04 01:21:08.381240: step 117540, loss = 0.69 (266.5 examples/sec; 0.480 sec/batch)
2016-02-04 01:21:13.088827: step 117550, loss = 0.62 (277.8 examples/sec; 0.461 sec/batch)
2016-02-04 01:21:17.805262: step 117560, loss = 0.68 (259.5 examples/sec; 0.493 sec/batch)
2016-02-04 01:21:22.535617: step 117570, loss = 0.69 (267.6 examples/sec; 0.478 sec/batch)
2016-02-04 01:21:27.269093: step 117580, loss = 0.72 (253.2 examples/sec; 0.506 sec/batch)
2016-02-04 01:21:31.923152: step 117590, loss = 0.65 (292.5 examples/sec; 0.438 sec/batch)
2016-02-04 01:21:36.656922: step 117600, loss = 0.71 (275.3 examples/sec; 0.465 sec/batch)
2016-02-04 01:21:41.792787: step 117610, loss = 0.65 (269.4 examples/sec; 0.475 sec/batch)
2016-02-04 01:21:46.416496: step 117620, loss = 0.54 (281.1 examples/sec; 0.455 sec/batch)
2016-02-04 01:21:50.949120: step 117630, loss = 0.67 (295.2 examples/sec; 0.434 sec/batch)
2016-02-04 01:21:55.627163: step 117640, loss = 0.77 (287.1 examples/sec; 0.446 sec/batch)
2016-02-04 01:22:00.383082: step 117650, loss = 0.78 (264.4 examples/sec; 0.484 sec/batch)
2016-02-04 01:22:04.984490: step 117660, loss = 0.67 (291.5 examples/sec; 0.439 sec/batch)
2016-02-04 01:22:09.729275: step 117670, loss = 0.65 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 01:22:14.382017: step 117680, loss = 0.80 (310.8 examples/sec; 0.412 sec/batch)
2016-02-04 01:22:19.112869: step 117690, loss = 0.73 (252.9 examples/sec; 0.506 sec/batch)
2016-02-04 01:22:23.758865: step 117700, loss = 0.61 (277.7 examples/sec; 0.461 sec/batch)
2016-02-04 01:22:28.960423: step 117710, loss = 0.76 (296.4 examples/sec; 0.432 sec/batch)
2016-02-04 01:22:33.722737: step 117720, loss = 0.68 (255.9 examples/sec; 0.500 sec/batch)
2016-02-04 01:22:38.305395: step 117730, loss = 0.69 (322.4 examples/sec; 0.397 sec/batch)
2016-02-04 01:22:43.007470: step 117740, loss = 0.80 (298.6 examples/sec; 0.429 sec/batch)
2016-02-04 01:22:47.715572: step 117750, loss = 0.79 (279.0 examples/sec; 0.459 sec/batch)
2016-02-04 01:22:52.396219: step 117760, loss = 0.65 (267.2 examples/sec; 0.479 sec/batch)
2016-02-04 01:22:57.123342: step 117770, loss = 0.65 (246.8 examples/sec; 0.519 sec/batch)
2016-02-04 01:23:01.833559: step 117780, loss = 0.83 (263.8 examples/sec; 0.485 sec/batch)
2016-02-04 01:23:06.527624: step 117790, loss = 0.74 (296.7 examples/sec; 0.431 sec/batch)
2016-02-04 01:23:11.208019: step 117800, loss = 0.65 (286.4 examples/sec; 0.447 sec/batch)
2016-02-04 01:23:16.435678: step 117810, loss = 0.87 (278.8 examples/sec; 0.459 sec/batch)
2016-02-04 01:23:21.240617: step 117820, loss = 0.52 (272.1 examples/sec; 0.470 sec/batch)
2016-02-04 01:23:25.910894: step 117830, loss = 0.71 (270.8 examples/sec; 0.473 sec/batch)
2016-02-04 01:23:30.631965: step 117840, loss = 0.69 (267.2 examples/sec; 0.479 sec/batch)
2016-02-04 01:23:35.449783: step 117850, loss = 0.64 (255.6 examples/sec; 0.501 sec/batch)
2016-02-04 01:23:40.117693: step 117860, loss = 0.57 (277.6 examples/sec; 0.461 sec/batch)
2016-02-04 01:23:44.883908: step 117870, loss = 0.67 (278.2 examples/sec; 0.460 sec/batch)
2016-02-04 01:23:49.572533: step 117880, loss = 0.59 (253.3 examples/sec; 0.505 sec/batch)
2016-02-04 01:23:54.326882: step 117890, loss = 0.63 (259.9 examples/sec; 0.492 sec/batch)
2016-02-04 01:23:59.123015: step 117900, loss = 0.61 (255.8 examples/sec; 0.500 sec/batch)
2016-02-04 01:24:04.309411: step 117910, loss = 0.85 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 01:24:09.028505: step 117920, loss = 0.70 (273.5 examples/sec; 0.468 sec/batch)
2016-02-04 01:24:13.704898: step 117930, loss = 0.83 (292.6 examples/sec; 0.437 sec/batch)
2016-02-04 01:24:18.393928: step 117940, loss = 0.67 (278.9 examples/sec; 0.459 sec/batch)
2016-02-04 01:24:23.004071: step 117950, loss = 0.76 (297.6 examples/sec; 0.430 sec/batch)
2016-02-04 01:24:27.726218: step 117960, loss = 0.74 (259.8 examples/sec; 0.493 sec/batch)
2016-02-04 01:24:32.380561: step 117970, loss = 1.01 (267.4 examples/sec; 0.479 sec/batch)
2016-02-04 01:24:37.085973: step 117980, loss = 0.74 (258.7 examples/sec; 0.495 sec/batch)
2016-02-04 01:24:41.789721: step 117990, loss = 0.64 (277.8 examples/sec; 0.461 sec/batch)
2016-02-04 01:24:46.462375: step 118000, loss = 0.67 (287.8 examples/sec; 0.445 sec/batch)
2016-02-04 01:24:51.729815: step 118010, loss = 0.64 (272.3 examples/sec; 0.470 sec/batch)
2016-02-04 01:24:56.468700: step 118020, loss = 0.63 (272.0 examples/sec; 0.471 sec/batch)
2016-02-04 01:25:01.195391: step 118030, loss = 0.64 (273.7 examples/sec; 0.468 sec/batch)
2016-02-04 01:25:05.895025: step 118040, loss = 0.56 (286.0 examples/sec; 0.447 sec/batch)
2016-02-04 01:25:10.577714: step 118050, loss = 0.61 (296.3 examples/sec; 0.432 sec/batch)
2016-02-04 01:25:15.279938: step 118060, loss = 0.59 (258.0 examples/sec; 0.496 sec/batch)
2016-02-04 01:25:19.888557: step 118070, loss = 0.66 (294.2 examples/sec; 0.435 sec/batch)
2016-02-04 01:25:24.650416: step 118080, loss = 0.71 (271.0 examples/sec; 0.472 sec/batch)
2016-02-04 01:25:29.336510: step 118090, loss = 0.76 (259.1 examples/sec; 0.494 sec/batch)
2016-02-04 01:25:34.058344: step 118100, loss = 0.74 (279.6 examples/sec; 0.458 sec/batch)
2016-02-04 01:25:39.336394: step 118110, loss = 0.67 (248.2 examples/sec; 0.516 sec/batch)
2016-02-04 01:25:44.052459: step 118120, loss = 0.70 (262.6 examples/sec; 0.488 sec/batch)
2016-02-04 01:25:48.658076: step 118130, loss = 0.65 (264.9 examples/sec; 0.483 sec/batch)
2016-02-04 01:25:53.295489: step 118140, loss = 0.61 (264.0 examples/sec; 0.485 sec/batch)
2016-02-04 01:25:57.969341: step 118150, loss = 0.64 (276.4 examples/sec; 0.463 sec/batch)
2016-02-04 01:26:02.678343: step 118160, loss = 0.70 (286.0 examples/sec; 0.448 sec/batch)
2016-02-04 01:26:07.394928: step 118170, loss = 0.63 (256.1 examples/sec; 0.500 sec/batch)
2016-02-04 01:26:12.050177: step 118180, loss = 0.87 (256.9 examples/sec; 0.498 sec/batch)
2016-02-04 01:26:16.738134: step 118190, loss = 0.68 (264.2 examples/sec; 0.484 sec/batch)
2016-02-04 01:26:21.385429: step 118200, loss = 0.61 (286.8 examples/sec; 0.446 sec/batch)
2016-02-04 01:26:26.596330: step 118210, loss = 0.78 (283.3 examples/sec; 0.452 sec/batch)
2016-02-04 01:26:31.278418: step 118220, loss = 0.75 (287.3 examples/sec; 0.446 sec/batch)
2016-02-04 01:26:35.894880: step 118230, loss = 0.72 (266.2 examples/sec; 0.481 sec/batch)
2016-02-04 01:26:40.585712: step 118240, loss = 0.59 (287.6 examples/sec; 0.445 sec/batch)
2016-02-04 01:26:45.367567: step 118250, loss = 0.57 (251.1 examples/sec; 0.510 sec/batch)
2016-02-04 01:26:50.095575: step 118260, loss = 0.77 (280.9 examples/sec; 0.456 sec/batch)
2016-02-04 01:26:54.832096: step 118270, loss = 0.60 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 01:26:59.511011: step 118280, loss = 0.69 (265.2 examples/sec; 0.483 sec/batch)
2016-02-04 01:27:04.218236: step 118290, loss = 0.73 (258.1 examples/sec; 0.496 sec/batch)
2016-02-04 01:27:08.814215: step 118300, loss = 0.63 (292.9 examples/sec; 0.437 sec/batch)
2016-02-04 01:27:14.032490: step 118310, loss = 0.76 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 01:27:18.740069: step 118320, loss = 0.67 (279.0 examples/sec; 0.459 sec/batch)
2016-02-04 01:27:23.411034: step 118330, loss = 0.55 (256.3 examples/sec; 0.499 sec/batch)
2016-02-04 01:27:28.015720: step 118340, loss = 0.67 (280.7 examples/sec; 0.456 sec/batch)
2016-02-04 01:27:32.717754: step 118350, loss = 0.63 (260.1 examples/sec; 0.492 sec/batch)
2016-02-04 01:27:37.430511: step 118360, loss = 0.65 (283.2 examples/sec; 0.452 sec/batch)
2016-02-04 01:27:42.137832: step 118370, loss = 0.81 (284.3 examples/sec; 0.450 sec/batch)
2016-02-04 01:27:46.852915: step 118380, loss = 0.69 (263.0 examples/sec; 0.487 sec/batch)
2016-02-04 01:27:51.512287: step 118390, loss = 0.60 (259.2 examples/sec; 0.494 sec/batch)
2016-02-04 01:27:56.222459: step 118400, loss = 0.69 (296.5 examples/sec; 0.432 sec/batch)
2016-02-04 01:28:01.507879: step 118410, loss = 0.68 (279.0 examples/sec; 0.459 sec/batch)
2016-02-04 01:28:06.147759: step 118420, loss = 0.63 (266.1 examples/sec; 0.481 sec/batch)
2016-02-04 01:28:10.821909: step 118430, loss = 0.70 (260.2 examples/sec; 0.492 sec/batch)
2016-02-04 01:28:15.540065: step 118440, loss = 0.61 (282.3 examples/sec; 0.453 sec/batch)
2016-02-04 01:28:20.185773: step 118450, loss = 0.61 (270.2 examples/sec; 0.474 sec/batch)
2016-02-04 01:28:24.916139: step 118460, loss = 0.80 (268.2 examples/sec; 0.477 sec/batch)
2016-02-04 01:28:29.619053: step 118470, loss = 0.57 (264.4 examples/sec; 0.484 sec/batch)
2016-02-04 01:28:34.405332: step 118480, loss = 0.70 (262.5 examples/sec; 0.488 sec/batch)
2016-02-04 01:28:39.073033: step 118490, loss = 0.69 (269.3 examples/sec; 0.475 sec/batch)
2016-02-04 01:28:43.837818: step 118500, loss = 0.77 (254.1 examples/sec; 0.504 sec/batch)
2016-02-04 01:28:48.955786: step 118510, loss = 0.64 (271.8 examples/sec; 0.471 sec/batch)
2016-02-04 01:28:53.609683: step 118520, loss = 0.70 (272.4 examples/sec; 0.470 sec/batch)
2016-02-04 01:28:58.261171: step 118530, loss = 0.76 (284.4 examples/sec; 0.450 sec/batch)
2016-02-04 01:29:02.938969: step 118540, loss = 0.55 (289.3 examples/sec; 0.442 sec/batch)
2016-02-04 01:29:07.559394: step 118550, loss = 0.59 (278.3 examples/sec; 0.460 sec/batch)
2016-02-04 01:29:12.234772: step 118560, loss = 0.64 (271.6 examples/sec; 0.471 sec/batch)
2016-02-04 01:29:16.879733: step 118570, loss = 0.61 (269.6 examples/sec; 0.475 sec/batch)
2016-02-04 01:29:21.626766: step 118580, loss = 0.78 (266.9 examples/sec; 0.480 sec/batch)
2016-02-04 01:29:26.360682: step 118590, loss = 0.65 (267.7 examples/sec; 0.478 sec/batch)
2016-02-04 01:29:31.040607: step 118600, loss = 0.69 (267.5 examples/sec; 0.479 sec/batch)
2016-02-04 01:29:36.332544: step 118610, loss = 0.75 (246.9 examples/sec; 0.518 sec/batch)
2016-02-04 01:29:41.029486: step 118620, loss = 0.69 (274.5 examples/sec; 0.466 sec/batch)
2016-02-04 01:29:45.726798: step 118630, loss = 0.63 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 01:29:50.382565: step 118640, loss = 0.65 (308.0 examples/sec; 0.416 sec/batch)
2016-02-04 01:29:55.078707: step 118650, loss = 0.58 (278.5 examples/sec; 0.460 sec/batch)
2016-02-04 01:29:59.815079: step 118660, loss = 0.63 (269.6 examples/sec; 0.475 sec/batch)
2016-02-04 01:30:04.550520: step 118670, loss = 0.68 (252.3 examples/sec; 0.507 sec/batch)
2016-02-04 01:30:09.255328: step 118680, loss = 0.78 (290.1 examples/sec; 0.441 sec/batch)
2016-02-04 01:30:13.974196: step 118690, loss = 0.52 (275.0 examples/sec; 0.466 sec/batch)
2016-02-04 01:30:18.641048: step 118700, loss = 0.80 (285.7 examples/sec; 0.448 sec/batch)
2016-02-04 01:30:23.908933: step 118710, loss = 0.50 (250.5 examples/sec; 0.511 sec/batch)
2016-02-04 01:30:28.576478: step 118720, loss = 0.64 (273.2 examples/sec; 0.469 sec/batch)
2016-02-04 01:30:33.278015: step 118730, loss = 0.65 (266.4 examples/sec; 0.480 sec/batch)
2016-02-04 01:30:37.961003: step 118740, loss = 0.71 (263.7 examples/sec; 0.485 sec/batch)
2016-02-04 01:30:42.658766: step 118750, loss = 0.70 (285.6 examples/sec; 0.448 sec/batch)
2016-02-04 01:30:47.352764: step 118760, loss = 0.73 (279.6 examples/sec; 0.458 sec/batch)
2016-02-04 01:30:52.156296: step 118770, loss = 0.68 (270.3 examples/sec; 0.473 sec/batch)
2016-02-04 01:30:56.842473: step 118780, loss = 0.70 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 01:31:01.428756: step 118790, loss = 0.74 (298.4 examples/sec; 0.429 sec/batch)
2016-02-04 01:31:06.155538: step 118800, loss = 0.62 (275.7 examples/sec; 0.464 sec/batch)
2016-02-04 01:31:11.349334: step 118810, loss = 0.53 (296.5 examples/sec; 0.432 sec/batch)
2016-02-04 01:31:16.160243: step 118820, loss = 0.60 (254.7 examples/sec; 0.503 sec/batch)
2016-02-04 01:31:20.847314: step 118830, loss = 0.75 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 01:31:25.561132: step 118840, loss = 0.65 (277.4 examples/sec; 0.461 sec/batch)
2016-02-04 01:31:30.342324: step 118850, loss = 0.62 (269.3 examples/sec; 0.475 sec/batch)
2016-02-04 01:31:35.090705: step 118860, loss = 0.70 (261.6 examples/sec; 0.489 sec/batch)
2016-02-04 01:31:39.774754: step 118870, loss = 0.63 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 01:31:44.467067: step 118880, loss = 0.64 (274.3 examples/sec; 0.467 sec/batch)
2016-02-04 01:31:49.096751: step 118890, loss = 0.82 (295.5 examples/sec; 0.433 sec/batch)
2016-02-04 01:31:53.794541: step 118900, loss = 0.56 (281.9 examples/sec; 0.454 sec/batch)
2016-02-04 01:31:58.867722: step 118910, loss = 0.64 (278.9 examples/sec; 0.459 sec/batch)
2016-02-04 01:32:03.486769: step 118920, loss = 0.63 (298.5 examples/sec; 0.429 sec/batch)
2016-02-04 01:32:08.129877: step 118930, loss = 0.65 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 01:32:12.796474: step 118940, loss = 0.75 (283.9 examples/sec; 0.451 sec/batch)
2016-02-04 01:32:17.497474: step 118950, loss = 0.65 (266.7 examples/sec; 0.480 sec/batch)
2016-02-04 01:32:22.235576: step 118960, loss = 0.74 (258.0 examples/sec; 0.496 sec/batch)
2016-02-04 01:32:26.938237: step 118970, loss = 0.69 (288.0 examples/sec; 0.444 sec/batch)
2016-02-04 01:32:31.541613: step 118980, loss = 0.63 (278.5 examples/sec; 0.460 sec/batch)
2016-02-04 01:32:36.321169: step 118990, loss = 0.73 (266.2 examples/sec; 0.481 sec/batch)
2016-02-04 01:32:41.057460: step 119000, loss = 0.68 (256.6 examples/sec; 0.499 sec/batch)
2016-02-04 01:32:46.251910: step 119010, loss = 0.80 (286.1 examples/sec; 0.447 sec/batch)
2016-02-04 01:32:50.986932: step 119020, loss = 0.61 (274.0 examples/sec; 0.467 sec/batch)
2016-02-04 01:32:55.694312: step 119030, loss = 0.88 (285.8 examples/sec; 0.448 sec/batch)
2016-02-04 01:33:00.361045: step 119040, loss = 0.79 (285.8 examples/sec; 0.448 sec/batch)
2016-02-04 01:33:05.054276: step 119050, loss = 0.85 (276.0 examples/sec; 0.464 sec/batch)
2016-02-04 01:33:09.765346: step 119060, loss = 0.71 (272.9 examples/sec; 0.469 sec/batch)
2016-02-04 01:33:14.444526: step 119070, loss = 0.75 (290.2 examples/sec; 0.441 sec/batch)
2016-02-04 01:33:19.135133: step 119080, loss = 0.99 (263.2 examples/sec; 0.486 sec/batch)
2016-02-04 01:33:23.899343: step 119090, loss = 0.62 (285.6 examples/sec; 0.448 sec/batch)
2016-02-04 01:33:28.722907: step 119100, loss = 0.69 (259.6 examples/sec; 0.493 sec/batch)
2016-02-04 01:33:33.997682: step 119110, loss = 0.81 (274.6 examples/sec; 0.466 sec/batch)
2016-02-04 01:33:38.729930: step 119120, loss = 0.75 (278.3 examples/sec; 0.460 sec/batch)
2016-02-04 01:33:43.375639: step 119130, loss = 0.61 (273.8 examples/sec; 0.467 sec/batch)
2016-02-04 01:33:48.134127: step 119140, loss = 0.64 (258.4 examples/sec; 0.495 sec/batch)
2016-02-04 01:33:52.836472: step 119150, loss = 0.60 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 01:33:57.488620: step 119160, loss = 0.74 (284.8 examples/sec; 0.449 sec/batch)
2016-02-04 01:34:02.231708: step 119170, loss = 0.76 (275.4 examples/sec; 0.465 sec/batch)
2016-02-04 01:34:06.994934: step 119180, loss = 0.60 (271.0 examples/sec; 0.472 sec/batch)
2016-02-04 01:34:11.701310: step 119190, loss = 0.64 (293.6 examples/sec; 0.436 sec/batch)
2016-02-04 01:34:16.412393: step 119200, loss = 0.87 (276.7 examples/sec; 0.463 sec/batch)
2016-02-04 01:34:21.745087: step 119210, loss = 0.57 (275.5 examples/sec; 0.465 sec/batch)
2016-02-04 01:34:26.449003: step 119220, loss = 0.70 (300.2 examples/sec; 0.426 sec/batch)
2016-02-04 01:34:31.151671: step 119230, loss = 0.70 (291.3 examples/sec; 0.439 sec/batch)
2016-02-04 01:34:35.896830: step 119240, loss = 0.69 (261.9 examples/sec; 0.489 sec/batch)
2016-02-04 01:34:40.508414: step 119250, loss = 0.75 (305.5 examples/sec; 0.419 sec/batch)
2016-02-04 01:34:45.296691: step 119260, loss = 0.80 (270.9 examples/sec; 0.472 sec/batch)
2016-02-04 01:34:49.981249: step 119270, loss = 0.70 (301.5 examples/sec; 0.424 sec/batch)
2016-02-04 01:34:54.657636: step 119280, loss = 0.68 (300.1 examples/sec; 0.427 sec/batch)
2016-02-04 01:34:59.324801: step 119290, loss = 0.75 (283.9 examples/sec; 0.451 sec/batch)
2016-02-04 01:35:04.096532: step 119300, loss = 0.74 (260.5 examples/sec; 0.491 sec/batch)
2016-02-04 01:35:09.292437: step 119310, loss = 0.66 (278.7 examples/sec; 0.459 sec/batch)
2016-02-04 01:35:13.906356: step 119320, loss = 0.64 (299.4 examples/sec; 0.428 sec/batch)
2016-02-04 01:35:18.630778: step 119330, loss = 0.71 (265.6 examples/sec; 0.482 sec/batch)
2016-02-04 01:35:23.350336: step 119340, loss = 0.66 (261.6 examples/sec; 0.489 sec/batch)
2016-02-04 01:35:28.030110: step 119350, loss = 0.63 (270.2 examples/sec; 0.474 sec/batch)
2016-02-04 01:35:32.746243: step 119360, loss = 0.74 (286.5 examples/sec; 0.447 sec/batch)
2016-02-04 01:35:37.449314: step 119370, loss = 0.77 (280.0 examples/sec; 0.457 sec/batch)
2016-02-04 01:35:42.147614: step 119380, loss = 0.86 (285.9 examples/sec; 0.448 sec/batch)
2016-02-04 01:35:46.858649: step 119390, loss = 0.75 (307.6 examples/sec; 0.416 sec/batch)
2016-02-04 01:35:51.602101: step 119400, loss = 0.70 (276.1 examples/sec; 0.464 sec/batch)
2016-02-04 01:35:56.886114: step 119410, loss = 0.57 (278.7 examples/sec; 0.459 sec/batch)
2016-02-04 01:36:01.543062: step 119420, loss = 0.73 (272.8 examples/sec; 0.469 sec/batch)
2016-02-04 01:36:06.315745: step 119430, loss = 0.75 (244.4 examples/sec; 0.524 sec/batch)
2016-02-04 01:36:10.962877: step 119440, loss = 0.67 (265.2 examples/sec; 0.483 sec/batch)
2016-02-04 01:36:15.592552: step 119450, loss = 0.70 (273.4 examples/sec; 0.468 sec/batch)
2016-02-04 01:36:20.287005: step 119460, loss = 0.61 (264.0 examples/sec; 0.485 sec/batch)
2016-02-04 01:36:24.922654: step 119470, loss = 0.73 (259.9 examples/sec; 0.492 sec/batch)
2016-02-04 01:36:29.563833: step 119480, loss = 0.71 (291.1 examples/sec; 0.440 sec/batch)
2016-02-04 01:36:34.227659: step 119490, loss = 0.70 (286.0 examples/sec; 0.448 sec/batch)
2016-02-04 01:36:38.875815: step 119500, loss = 0.57 (295.8 examples/sec; 0.433 sec/batch)
2016-02-04 01:36:44.129475: step 119510, loss = 0.91 (259.7 examples/sec; 0.493 sec/batch)
2016-02-04 01:36:48.893069: step 119520, loss = 0.82 (277.3 examples/sec; 0.462 sec/batch)
2016-02-04 01:36:53.631871: step 119530, loss = 0.69 (254.1 examples/sec; 0.504 sec/batch)
2016-02-04 01:36:58.334406: step 119540, loss = 0.66 (256.1 examples/sec; 0.500 sec/batch)
2016-02-04 01:37:03.002584: step 119550, loss = 0.70 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 01:37:07.732459: step 119560, loss = 0.70 (263.4 examples/sec; 0.486 sec/batch)
2016-02-04 01:37:12.353876: step 119570, loss = 0.77 (296.3 examples/sec; 0.432 sec/batch)
2016-02-04 01:37:17.066961: step 119580, loss = 0.68 (288.8 examples/sec; 0.443 sec/batch)
2016-02-04 01:37:21.827475: step 119590, loss = 0.70 (238.3 examples/sec; 0.537 sec/batch)
2016-02-04 01:37:26.473710: step 119600, loss = 0.66 (263.4 examples/sec; 0.486 sec/batch)
2016-02-04 01:37:31.710668: step 119610, loss = 0.76 (265.7 examples/sec; 0.482 sec/batch)
2016-02-04 01:37:36.356885: step 119620, loss = 0.76 (271.2 examples/sec; 0.472 sec/batch)
2016-02-04 01:37:40.983994: step 119630, loss = 0.64 (292.0 examples/sec; 0.438 sec/batch)
2016-02-04 01:37:45.718364: step 119640, loss = 0.70 (296.2 examples/sec; 0.432 sec/batch)
2016-02-04 01:37:50.478431: step 119650, loss = 0.58 (282.8 examples/sec; 0.453 sec/batch)
2016-02-04 01:37:55.235456: step 119660, loss = 0.78 (281.8 examples/sec; 0.454 sec/batch)
2016-02-04 01:37:59.978378: step 119670, loss = 0.71 (263.6 examples/sec; 0.485 sec/batch)
2016-02-04 01:38:04.667436: step 119680, loss = 0.65 (282.9 examples/sec; 0.452 sec/batch)
2016-02-04 01:38:09.370627: step 119690, loss = 0.66 (259.0 examples/sec; 0.494 sec/batch)
2016-02-04 01:38:14.059212: step 119700, loss = 0.67 (256.7 examples/sec; 0.499 sec/batch)
2016-02-04 01:38:19.261719: step 119710, loss = 0.63 (262.6 examples/sec; 0.487 sec/batch)
2016-02-04 01:38:23.982952: step 119720, loss = 0.77 (258.6 examples/sec; 0.495 sec/batch)
2016-02-04 01:38:28.708500: step 119730, loss = 0.65 (254.8 examples/sec; 0.502 sec/batch)
2016-02-04 01:38:33.364029: step 119740, loss = 0.72 (271.1 examples/sec; 0.472 sec/batch)
2016-02-04 01:38:37.966421: step 119750, loss = 0.61 (270.8 examples/sec; 0.473 sec/batch)
2016-02-04 01:38:42.657347: step 119760, loss = 0.68 (274.7 examples/sec; 0.466 sec/batch)
2016-02-04 01:38:47.364976: step 119770, loss = 0.64 (245.3 examples/sec; 0.522 sec/batch)
2016-02-04 01:38:51.994491: step 119780, loss = 0.56 (265.1 examples/sec; 0.483 sec/batch)
2016-02-04 01:38:56.683043: step 119790, loss = 0.63 (285.5 examples/sec; 0.448 sec/batch)
2016-02-04 01:39:01.286797: step 119800, loss = 0.72 (293.0 examples/sec; 0.437 sec/batch)
2016-02-04 01:39:06.423864: step 119810, loss = 0.64 (264.7 examples/sec; 0.484 sec/batch)
2016-02-04 01:39:11.119793: step 119820, loss = 0.83 (277.9 examples/sec; 0.461 sec/batch)
2016-02-04 01:39:15.902781: step 119830, loss = 0.78 (274.7 examples/sec; 0.466 sec/batch)
2016-02-04 01:39:20.612886: step 119840, loss = 0.62 (267.3 examples/sec; 0.479 sec/batch)
2016-02-04 01:39:25.361431: step 119850, loss = 0.71 (270.0 examples/sec; 0.474 sec/batch)
2016-02-04 01:39:30.046113: step 119860, loss = 0.75 (257.2 examples/sec; 0.498 sec/batch)
2016-02-04 01:39:34.799252: step 119870, loss = 0.78 (279.6 examples/sec; 0.458 sec/batch)
2016-02-04 01:39:39.414126: step 119880, loss = 0.66 (278.9 examples/sec; 0.459 sec/batch)
2016-02-04 01:39:44.162593: step 119890, loss = 0.63 (250.8 examples/sec; 0.510 sec/batch)
2016-02-04 01:39:48.844308: step 119900, loss = 0.67 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 01:39:54.076996: step 119910, loss = 0.58 (268.1 examples/sec; 0.477 sec/batch)
2016-02-04 01:39:58.810058: step 119920, loss = 0.62 (276.4 examples/sec; 0.463 sec/batch)
2016-02-04 01:40:03.496439: step 119930, loss = 0.67 (282.8 examples/sec; 0.453 sec/batch)
2016-02-04 01:40:08.266047: step 119940, loss = 0.61 (248.6 examples/sec; 0.515 sec/batch)
2016-02-04 01:40:12.954445: step 119950, loss = 0.63 (271.6 examples/sec; 0.471 sec/batch)
2016-02-04 01:40:17.678426: step 119960, loss = 0.65 (242.1 examples/sec; 0.529 sec/batch)
2016-02-04 01:40:22.299453: step 119970, loss = 0.78 (267.4 examples/sec; 0.479 sec/batch)
2016-02-04 01:40:26.964335: step 119980, loss = 0.63 (288.2 examples/sec; 0.444 sec/batch)
2016-02-04 01:40:31.575846: step 119990, loss = 0.68 (272.9 examples/sec; 0.469 sec/batch)
2016-02-04 01:40:36.254083: step 120000, loss = 0.65 (298.1 examples/sec; 0.429 sec/batch)
2016-02-04 01:40:41.448216: step 120010, loss = 0.86 (306.5 examples/sec; 0.418 sec/batch)
2016-02-04 01:40:46.136534: step 120020, loss = 0.76 (254.2 examples/sec; 0.504 sec/batch)
2016-02-04 01:40:50.784820: step 120030, loss = 0.76 (267.3 examples/sec; 0.479 sec/batch)
2016-02-04 01:40:55.513836: step 120040, loss = 0.70 (288.9 examples/sec; 0.443 sec/batch)
2016-02-04 01:41:00.140955: step 120050, loss = 0.71 (295.1 examples/sec; 0.434 sec/batch)
2016-02-04 01:41:04.903971: step 120060, loss = 0.69 (258.7 examples/sec; 0.495 sec/batch)
2016-02-04 01:41:09.618030: step 120070, loss = 0.76 (263.5 examples/sec; 0.486 sec/batch)
2016-02-04 01:41:14.368145: step 120080, loss = 0.56 (271.5 examples/sec; 0.471 sec/batch)
2016-02-04 01:41:19.077914: step 120090, loss = 0.58 (256.8 examples/sec; 0.498 sec/batch)
2016-02-04 01:41:23.787030: step 120100, loss = 0.55 (272.1 examples/sec; 0.470 sec/batch)
2016-02-04 01:41:28.997461: step 120110, loss = 0.67 (267.1 examples/sec; 0.479 sec/batch)
2016-02-04 01:41:33.702985: step 120120, loss = 0.51 (287.3 examples/sec; 0.445 sec/batch)
2016-02-04 01:41:38.361823: step 120130, loss = 0.63 (295.2 examples/sec; 0.434 sec/batch)
2016-02-04 01:41:43.011748: step 120140, loss = 0.53 (261.9 examples/sec; 0.489 sec/batch)
2016-02-04 01:41:47.744812: step 120150, loss = 0.65 (258.1 examples/sec; 0.496 sec/batch)
2016-02-04 01:41:52.490286: step 120160, loss = 0.86 (279.3 examples/sec; 0.458 sec/batch)
2016-02-04 01:41:57.154744: step 120170, loss = 0.67 (278.6 examples/sec; 0.459 sec/batch)
2016-02-04 01:42:01.888871: step 120180, loss = 0.68 (279.4 examples/sec; 0.458 sec/batch)
2016-02-04 01:42:06.568061: step 120190, loss = 0.71 (265.2 examples/sec; 0.483 sec/batch)
2016-02-04 01:42:11.249804: step 120200, loss = 0.63 (279.2 examples/sec; 0.459 sec/batch)
2016-02-04 01:42:16.463387: step 120210, loss = 0.78 (258.9 examples/sec; 0.494 sec/batch)
2016-02-04 01:42:21.180809: step 120220, loss = 0.59 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 01:42:25.968023: step 120230, loss = 0.70 (267.5 examples/sec; 0.478 sec/batch)
2016-02-04 01:42:30.703716: step 120240, loss = 0.74 (266.4 examples/sec; 0.480 sec/batch)
2016-02-04 01:42:35.385453: step 120250, loss = 0.68 (251.6 examples/sec; 0.509 sec/batch)
2016-02-04 01:42:40.127306: step 120260, loss = 0.83 (261.9 examples/sec; 0.489 sec/batch)
2016-02-04 01:42:44.882557: step 120270, loss = 0.67 (260.1 examples/sec; 0.492 sec/batch)
2016-02-04 01:42:49.572922: step 120280, loss = 0.68 (254.0 examples/sec; 0.504 sec/batch)
2016-02-04 01:42:54.228312: step 120290, loss = 0.71 (262.8 examples/sec; 0.487 sec/batch)
2016-02-04 01:42:58.844595: step 120300, loss = 0.69 (276.7 examples/sec; 0.463 sec/batch)
2016-02-04 01:43:04.117615: step 120310, loss = 0.73 (289.4 examples/sec; 0.442 sec/batch)
2016-02-04 01:43:08.862715: step 120320, loss = 0.61 (283.5 examples/sec; 0.452 sec/batch)
2016-02-04 01:43:13.592558: step 120330, loss = 0.62 (275.5 examples/sec; 0.465 sec/batch)
2016-02-04 01:43:18.248225: step 120340, loss = 0.70 (263.5 examples/sec; 0.486 sec/batch)
2016-02-04 01:43:22.995323: step 120350, loss = 0.75 (273.1 examples/sec; 0.469 sec/batch)
2016-02-04 01:43:27.695117: step 120360, loss = 0.60 (257.1 examples/sec; 0.498 sec/batch)
2016-02-04 01:43:32.279795: step 120370, loss = 0.68 (285.3 examples/sec; 0.449 sec/batch)
2016-02-04 01:43:37.080444: step 120380, loss = 0.86 (264.6 examples/sec; 0.484 sec/batch)
2016-02-04 01:43:41.739923: step 120390, loss = 0.73 (282.9 examples/sec; 0.453 sec/batch)
2016-02-04 01:43:46.369165: step 120400, loss = 0.66 (294.6 examples/sec; 0.434 sec/batch)
2016-02-04 01:43:51.596549: step 120410, loss = 0.64 (276.8 examples/sec; 0.462 sec/batch)
2016-02-04 01:43:56.278482: step 120420, loss = 0.68 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 01:44:00.900523: step 120430, loss = 0.55 (275.2 examples/sec; 0.465 sec/batch)
2016-02-04 01:44:05.659036: step 120440, loss = 0.75 (266.0 examples/sec; 0.481 sec/batch)
2016-02-04 01:44:10.332063: step 120450, loss = 0.73 (273.2 examples/sec; 0.468 sec/batch)
2016-02-04 01:44:15.010168: step 120460, loss = 0.76 (288.7 examples/sec; 0.443 sec/batch)
2016-02-04 01:44:19.626884: step 120470, loss = 0.67 (283.5 examples/sec; 0.451 sec/batch)
2016-02-04 01:44:24.282761: step 120480, loss = 0.61 (278.9 examples/sec; 0.459 sec/batch)
2016-02-04 01:44:29.009562: step 120490, loss = 0.68 (285.5 examples/sec; 0.448 sec/batch)
2016-02-04 01:44:33.733056: step 120500, loss = 0.67 (283.4 examples/sec; 0.452 sec/batch)
2016-02-04 01:44:38.990623: step 120510, loss = 0.77 (266.8 examples/sec; 0.480 sec/batch)
2016-02-04 01:44:43.681889: step 120520, loss = 0.69 (275.0 examples/sec; 0.465 sec/batch)
2016-02-04 01:44:48.390485: step 120530, loss = 0.66 (301.5 examples/sec; 0.425 sec/batch)
2016-02-04 01:44:53.110906: step 120540, loss = 0.70 (243.3 examples/sec; 0.526 sec/batch)
2016-02-04 01:44:57.832226: step 120550, loss = 0.59 (261.0 examples/sec; 0.490 sec/batch)
2016-02-04 01:45:02.513225: step 120560, loss = 0.76 (278.7 examples/sec; 0.459 sec/batch)
2016-02-04 01:45:07.208566: step 120570, loss = 0.63 (280.3 examples/sec; 0.457 sec/batch)
2016-02-04 01:45:11.976274: step 120580, loss = 0.74 (259.1 examples/sec; 0.494 sec/batch)
2016-02-04 01:45:16.668823: step 120590, loss = 0.70 (277.3 examples/sec; 0.462 sec/batch)
2016-02-04 01:45:21.422951: step 120600, loss = 0.61 (254.7 examples/sec; 0.503 sec/batch)
2016-02-04 01:45:26.669890: step 120610, loss = 0.65 (253.0 examples/sec; 0.506 sec/batch)
2016-02-04 01:45:31.332284: step 120620, loss = 0.61 (273.4 examples/sec; 0.468 sec/batch)
2016-02-04 01:45:36.080674: step 120630, loss = 0.62 (250.2 examples/sec; 0.512 sec/batch)
2016-02-04 01:45:40.789830: step 120640, loss = 0.76 (300.2 examples/sec; 0.426 sec/batch)
2016-02-04 01:45:45.479417: step 120650, loss = 0.63 (266.0 examples/sec; 0.481 sec/batch)
2016-02-04 01:45:50.060450: step 120660, loss = 0.76 (261.4 examples/sec; 0.490 sec/batch)
2016-02-04 01:45:54.822134: step 120670, loss = 0.85 (261.4 examples/sec; 0.490 sec/batch)
2016-02-04 01:45:59.525790: step 120680, loss = 0.52 (264.4 examples/sec; 0.484 sec/batch)
2016-02-04 01:46:04.271001: step 120690, loss = 0.84 (246.0 examples/sec; 0.520 sec/batch)
2016-02-04 01:46:08.944621: step 120700, loss = 0.59 (282.5 examples/sec; 0.453 sec/batch)
2016-02-04 01:46:14.166548: step 120710, loss = 0.76 (266.1 examples/sec; 0.481 sec/batch)
2016-02-04 01:46:18.891516: step 120720, loss = 0.75 (276.7 examples/sec; 0.463 sec/batch)
2016-02-04 01:46:23.646953: step 120730, loss = 0.71 (289.1 examples/sec; 0.443 sec/batch)
2016-02-04 01:46:28.359419: step 120740, loss = 0.80 (269.7 examples/sec; 0.475 sec/batch)
2016-02-04 01:46:33.091687: step 120750, loss = 0.76 (277.3 examples/sec; 0.462 sec/batch)
2016-02-04 01:46:37.786445: step 120760, loss = 0.58 (283.3 examples/sec; 0.452 sec/batch)
2016-02-04 01:46:42.427261: step 120770, loss = 0.62 (286.0 examples/sec; 0.448 sec/batch)
2016-02-04 01:46:47.003993: step 120780, loss = 0.54 (273.6 examples/sec; 0.468 sec/batch)
2016-02-04 01:46:51.756104: step 120790, loss = 0.75 (254.3 examples/sec; 0.503 sec/batch)
2016-02-04 01:46:56.391128: step 120800, loss = 0.71 (300.2 examples/sec; 0.426 sec/batch)
2016-02-04 01:47:01.627375: step 120810, loss = 0.80 (283.8 examples/sec; 0.451 sec/batch)
2016-02-04 01:47:06.365637: step 120820, loss = 0.62 (273.6 examples/sec; 0.468 sec/batch)
2016-02-04 01:47:11.090907: step 120830, loss = 0.71 (279.0 examples/sec; 0.459 sec/batch)
2016-02-04 01:47:15.718339: step 120840, loss = 0.65 (275.2 examples/sec; 0.465 sec/batch)
2016-02-04 01:47:20.410326: step 120850, loss = 0.72 (277.2 examples/sec; 0.462 sec/batch)
2016-02-04 01:47:25.089816: step 120860, loss = 0.85 (268.1 examples/sec; 0.477 sec/batch)
2016-02-04 01:47:29.748969: step 120870, loss = 0.75 (273.7 examples/sec; 0.468 sec/batch)
2016-02-04 01:47:34.429974: step 120880, loss = 0.66 (249.7 examples/sec; 0.513 sec/batch)
2016-02-04 01:47:39.140190: step 120890, loss = 0.67 (267.1 examples/sec; 0.479 sec/batch)
2016-02-04 01:47:43.800182: step 120900, loss = 0.73 (286.3 examples/sec; 0.447 sec/batch)
2016-02-04 01:47:49.090038: step 120910, loss = 0.80 (277.4 examples/sec; 0.462 sec/batch)
2016-02-04 01:47:53.780825: step 120920, loss = 0.56 (286.9 examples/sec; 0.446 sec/batch)
2016-02-04 01:47:58.551108: step 120930, loss = 0.77 (262.4 examples/sec; 0.488 sec/batch)
2016-02-04 01:48:03.216318: step 120940, loss = 0.60 (269.7 examples/sec; 0.475 sec/batch)
2016-02-04 01:48:07.956478: step 120950, loss = 0.83 (270.8 examples/sec; 0.473 sec/batch)
2016-02-04 01:48:12.695852: step 120960, loss = 0.60 (265.6 examples/sec; 0.482 sec/batch)
2016-02-04 01:48:17.407740: step 120970, loss = 0.71 (255.2 examples/sec; 0.502 sec/batch)
2016-02-04 01:48:22.111820: step 120980, loss = 0.66 (298.5 examples/sec; 0.429 sec/batch)
2016-02-04 01:48:26.766301: step 120990, loss = 0.65 (292.8 examples/sec; 0.437 sec/batch)
2016-02-04 01:48:31.571901: step 121000, loss = 0.55 (263.0 examples/sec; 0.487 sec/batch)
2016-02-04 01:48:36.829754: step 121010, loss = 0.83 (260.5 examples/sec; 0.491 sec/batch)
2016-02-04 01:48:41.427246: step 121020, loss = 0.78 (302.3 examples/sec; 0.423 sec/batch)
2016-02-04 01:48:46.226917: step 121030, loss = 0.63 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 01:48:50.962319: step 121040, loss = 0.59 (274.0 examples/sec; 0.467 sec/batch)
2016-02-04 01:48:55.709969: step 121050, loss = 0.74 (256.0 examples/sec; 0.500 sec/batch)
2016-02-04 01:49:00.366385: step 121060, loss = 0.71 (261.9 examples/sec; 0.489 sec/batch)
2016-02-04 01:49:05.070424: step 121070, loss = 0.64 (254.0 examples/sec; 0.504 sec/batch)
2016-02-04 01:49:09.700749: step 121080, loss = 0.62 (257.2 examples/sec; 0.498 sec/batch)
2016-02-04 01:49:14.468830: step 121090, loss = 0.90 (267.3 examples/sec; 0.479 sec/batch)
2016-02-04 01:49:19.210718: step 121100, loss = 0.60 (254.6 examples/sec; 0.503 sec/batch)
2016-02-04 01:49:24.403750: step 121110, loss = 0.65 (257.8 examples/sec; 0.497 sec/batch)
2016-02-04 01:49:29.044826: step 121120, loss = 0.83 (266.0 examples/sec; 0.481 sec/batch)
2016-02-04 01:49:33.791483: step 121130, loss = 0.73 (282.8 examples/sec; 0.453 sec/batch)
2016-02-04 01:49:38.528852: step 121140, loss = 0.59 (284.0 examples/sec; 0.451 sec/batch)
2016-02-04 01:49:43.259595: step 121150, loss = 0.71 (260.4 examples/sec; 0.492 sec/batch)
2016-02-04 01:49:47.962262: step 121160, loss = 0.64 (275.3 examples/sec; 0.465 sec/batch)
2016-02-04 01:49:52.652791: step 121170, loss = 0.66 (276.9 examples/sec; 0.462 sec/batch)
2016-02-04 01:49:57.397656: step 121180, loss = 0.67 (261.8 examples/sec; 0.489 sec/batch)
2016-02-04 01:50:02.148049: step 121190, loss = 0.78 (251.1 examples/sec; 0.510 sec/batch)
2016-02-04 01:50:06.843486: step 121200, loss = 0.66 (268.0 examples/sec; 0.478 sec/batch)
2016-02-04 01:50:12.153424: step 121210, loss = 0.61 (263.0 examples/sec; 0.487 sec/batch)
2016-02-04 01:50:16.837447: step 121220, loss = 0.68 (276.2 examples/sec; 0.463 sec/batch)
2016-02-04 01:50:21.478326: step 121230, loss = 0.79 (298.1 examples/sec; 0.429 sec/batch)
2016-02-04 01:50:26.242402: step 121240, loss = 0.53 (310.4 examples/sec; 0.412 sec/batch)
2016-02-04 01:50:30.987747: step 121250, loss = 0.61 (285.1 examples/sec; 0.449 sec/batch)
2016-02-04 01:50:35.751594: step 121260, loss = 0.70 (300.1 examples/sec; 0.427 sec/batch)
2016-02-04 01:50:40.394038: step 121270, loss = 0.68 (268.4 examples/sec; 0.477 sec/batch)
2016-02-04 01:50:45.095165: step 121280, loss = 0.68 (257.6 examples/sec; 0.497 sec/batch)
2016-02-04 01:50:49.820365: step 121290, loss = 0.61 (269.3 examples/sec; 0.475 sec/batch)
2016-02-04 01:50:54.534252: step 121300, loss = 0.72 (284.5 examples/sec; 0.450 sec/batch)
2016-02-04 01:50:59.686314: step 121310, loss = 0.67 (278.8 examples/sec; 0.459 sec/batch)
2016-02-04 01:51:04.447688: step 121320, loss = 0.81 (284.8 examples/sec; 0.449 sec/batch)
2016-02-04 01:51:09.125067: step 121330, loss = 0.81 (289.4 examples/sec; 0.442 sec/batch)
2016-02-04 01:51:13.888838: step 121340, loss = 0.79 (263.2 examples/sec; 0.486 sec/batch)
2016-02-04 01:51:18.600594: step 121350, loss = 0.78 (295.7 examples/sec; 0.433 sec/batch)
2016-02-04 01:51:23.298637: step 121360, loss = 0.66 (269.3 examples/sec; 0.475 sec/batch)
2016-02-04 01:51:28.007037: step 121370, loss = 0.63 (279.4 examples/sec; 0.458 sec/batch)
2016-02-04 01:51:32.677276: step 121380, loss = 0.68 (277.3 examples/sec; 0.462 sec/batch)
2016-02-04 01:51:37.435264: step 121390, loss = 0.82 (253.8 examples/sec; 0.504 sec/batch)
2016-02-04 01:51:42.155071: step 121400, loss = 0.58 (266.2 examples/sec; 0.481 sec/batch)
2016-02-04 01:51:47.381364: step 121410, loss = 0.64 (281.9 examples/sec; 0.454 sec/batch)
2016-02-04 01:51:52.079379: step 121420, loss = 0.67 (270.9 examples/sec; 0.473 sec/batch)
2016-02-04 01:51:56.779831: step 121430, loss = 0.75 (302.6 examples/sec; 0.423 sec/batch)
2016-02-04 01:52:01.471681: step 121440, loss = 0.75 (253.5 examples/sec; 0.505 sec/batch)
2016-02-04 01:52:06.132549: step 121450, loss = 0.78 (277.7 examples/sec; 0.461 sec/batch)
2016-02-04 01:52:10.792746: step 121460, loss = 0.66 (258.8 examples/sec; 0.495 sec/batch)
2016-02-04 01:52:15.559159: step 121470, loss = 0.64 (255.4 examples/sec; 0.501 sec/batch)
2016-02-04 01:52:20.298780: step 121480, loss = 0.67 (256.2 examples/sec; 0.500 sec/batch)
2016-02-04 01:52:24.882738: step 121490, loss = 0.80 (288.8 examples/sec; 0.443 sec/batch)
2016-02-04 01:52:29.505719: step 121500, loss = 0.57 (295.2 examples/sec; 0.434 sec/batch)
2016-02-04 01:52:34.743748: step 121510, loss = 0.75 (260.7 examples/sec; 0.491 sec/batch)
2016-02-04 01:52:39.449359: step 121520, loss = 0.71 (286.4 examples/sec; 0.447 sec/batch)
2016-02-04 01:52:44.088978: step 121530, loss = 0.66 (272.9 examples/sec; 0.469 sec/batch)
2016-02-04 01:52:48.868205: step 121540, loss = 0.86 (281.7 examples/sec; 0.454 sec/batch)
2016-02-04 01:52:53.630345: step 121550, loss = 0.78 (263.9 examples/sec; 0.485 sec/batch)
2016-02-04 01:52:58.330883: step 121560, loss = 0.66 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 01:53:03.044606: step 121570, loss = 0.65 (257.6 examples/sec; 0.497 sec/batch)
2016-02-04 01:53:07.687096: step 121580, loss = 0.69 (281.3 examples/sec; 0.455 sec/batch)
2016-02-04 01:53:12.437830: step 121590, loss = 0.71 (282.0 examples/sec; 0.454 sec/batch)
2016-02-04 01:53:17.133977: step 121600, loss = 0.59 (299.9 examples/sec; 0.427 sec/batch)
2016-02-04 01:53:22.302773: step 121610, loss = 0.73 (291.0 examples/sec; 0.440 sec/batch)
2016-02-04 01:53:26.977670: step 121620, loss = 0.77 (276.8 examples/sec; 0.462 sec/batch)
2016-02-04 01:53:31.657448: step 121630, loss = 0.83 (262.3 examples/sec; 0.488 sec/batch)
2016-02-04 01:53:36.337810: step 121640, loss = 0.65 (283.6 examples/sec; 0.451 sec/batch)
2016-02-04 01:53:41.043415: step 121650, loss = 0.80 (258.6 examples/sec; 0.495 sec/batch)
2016-02-04 01:53:45.750055: step 121660, loss = 0.71 (267.3 examples/sec; 0.479 sec/batch)
2016-02-04 01:53:50.445011: step 121670, loss = 0.69 (276.5 examples/sec; 0.463 sec/batch)
2016-02-04 01:53:55.141302: step 121680, loss = 0.66 (292.3 examples/sec; 0.438 sec/batch)
2016-02-04 01:53:59.889032: step 121690, loss = 0.59 (290.8 examples/sec; 0.440 sec/batch)
2016-02-04 01:54:04.629719: step 121700, loss = 0.68 (245.5 examples/sec; 0.521 sec/batch)
2016-02-04 01:54:09.844063: step 121710, loss = 0.75 (251.4 examples/sec; 0.509 sec/batch)
2016-02-04 01:54:14.613785: step 121720, loss = 0.72 (264.4 examples/sec; 0.484 sec/batch)
2016-02-04 01:54:19.264973: step 121730, loss = 0.69 (263.9 examples/sec; 0.485 sec/batch)
2016-02-04 01:54:23.983102: step 121740, loss = 0.83 (280.7 examples/sec; 0.456 sec/batch)
2016-02-04 01:54:28.747341: step 121750, loss = 0.72 (262.2 examples/sec; 0.488 sec/batch)
2016-02-04 01:54:33.433655: step 121760, loss = 0.79 (254.1 examples/sec; 0.504 sec/batch)
2016-02-04 01:54:38.104843: step 121770, loss = 0.55 (278.1 examples/sec; 0.460 sec/batch)
2016-02-04 01:54:42.852104: step 121780, loss = 0.67 (275.4 examples/sec; 0.465 sec/batch)
2016-02-04 01:54:47.538234: step 121790, loss = 0.67 (262.6 examples/sec; 0.487 sec/batch)
2016-02-04 01:54:52.185512: step 121800, loss = 0.69 (277.5 examples/sec; 0.461 sec/batch)
2016-02-04 01:54:57.362947: step 121810, loss = 0.62 (280.7 examples/sec; 0.456 sec/batch)
2016-02-04 01:55:02.113227: step 121820, loss = 0.73 (269.2 examples/sec; 0.475 sec/batch)
2016-02-04 01:55:06.830801: step 121830, loss = 0.66 (269.2 examples/sec; 0.476 sec/batch)
2016-02-04 01:55:11.570710: step 121840, loss = 0.78 (263.8 examples/sec; 0.485 sec/batch)
2016-02-04 01:55:16.242645: step 121850, loss = 0.65 (292.3 examples/sec; 0.438 sec/batch)
2016-02-04 01:55:20.956446: step 121860, loss = 0.64 (270.0 examples/sec; 0.474 sec/batch)
2016-02-04 01:55:25.710990: step 121870, loss = 0.57 (257.1 examples/sec; 0.498 sec/batch)
2016-02-04 01:55:30.404831: step 121880, loss = 0.78 (280.6 examples/sec; 0.456 sec/batch)
2016-02-04 01:55:35.066647: step 121890, loss = 0.61 (267.3 examples/sec; 0.479 sec/batch)
2016-02-04 01:55:39.730217: step 121900, loss = 0.64 (289.0 examples/sec; 0.443 sec/batch)
2016-02-04 01:55:44.916540: step 121910, loss = 0.72 (297.1 examples/sec; 0.431 sec/batch)
2016-02-04 01:55:49.626408: step 121920, loss = 0.70 (264.2 examples/sec; 0.484 sec/batch)
2016-02-04 01:55:54.358019: step 121930, loss = 0.67 (273.2 examples/sec; 0.469 sec/batch)
2016-02-04 01:55:59.007600: step 121940, loss = 0.73 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 01:56:03.770762: step 121950, loss = 0.67 (264.4 examples/sec; 0.484 sec/batch)
2016-02-04 01:56:08.472045: step 121960, loss = 0.80 (259.7 examples/sec; 0.493 sec/batch)
2016-02-04 01:56:13.161983: step 121970, loss = 0.70 (273.6 examples/sec; 0.468 sec/batch)
2016-02-04 01:56:17.760842: step 121980, loss = 0.78 (292.5 examples/sec; 0.438 sec/batch)
2016-02-04 01:56:22.397950: step 121990, loss = 0.60 (264.9 examples/sec; 0.483 sec/batch)
2016-02-04 01:56:27.065722: step 122000, loss = 0.62 (256.5 examples/sec; 0.499 sec/batch)
2016-02-04 01:56:32.349552: step 122010, loss = 0.72 (264.4 examples/sec; 0.484 sec/batch)
2016-02-04 01:56:37.094169: step 122020, loss = 0.88 (266.9 examples/sec; 0.480 sec/batch)
2016-02-04 01:56:41.775688: step 122030, loss = 0.67 (281.7 examples/sec; 0.454 sec/batch)
2016-02-04 01:56:46.472266: step 122040, loss = 0.55 (286.9 examples/sec; 0.446 sec/batch)
2016-02-04 01:56:51.197182: step 122050, loss = 0.66 (274.5 examples/sec; 0.466 sec/batch)
2016-02-04 01:56:55.823558: step 122060, loss = 0.66 (298.4 examples/sec; 0.429 sec/batch)
2016-02-04 01:57:00.655566: step 122070, loss = 0.59 (263.4 examples/sec; 0.486 sec/batch)
2016-02-04 01:57:05.363798: step 122080, loss = 0.82 (271.3 examples/sec; 0.472 sec/batch)
2016-02-04 01:57:09.988076: step 122090, loss = 0.59 (284.9 examples/sec; 0.449 sec/batch)
2016-02-04 01:57:14.648958: step 122100, loss = 0.75 (285.8 examples/sec; 0.448 sec/batch)
2016-02-04 01:57:19.965750: step 122110, loss = 0.68 (253.3 examples/sec; 0.505 sec/batch)
2016-02-04 01:57:24.622452: step 122120, loss = 0.60 (299.9 examples/sec; 0.427 sec/batch)
2016-02-04 01:57:29.370913: step 122130, loss = 0.68 (281.4 examples/sec; 0.455 sec/batch)
2016-02-04 01:57:34.145219: step 122140, loss = 0.63 (253.0 examples/sec; 0.506 sec/batch)
2016-02-04 01:57:38.853189: step 122150, loss = 0.67 (263.9 examples/sec; 0.485 sec/batch)
2016-02-04 01:57:43.472787: step 122160, loss = 0.73 (288.8 examples/sec; 0.443 sec/batch)
2016-02-04 01:57:48.226369: step 122170, loss = 0.58 (258.7 examples/sec; 0.495 sec/batch)
2016-02-04 01:57:52.917445: step 122180, loss = 0.73 (303.8 examples/sec; 0.421 sec/batch)
2016-02-04 01:57:57.670717: step 122190, loss = 0.80 (279.9 examples/sec; 0.457 sec/batch)
2016-02-04 01:58:02.348604: step 122200, loss = 0.69 (277.8 examples/sec; 0.461 sec/batch)
2016-02-04 01:58:07.668191: step 122210, loss = 0.64 (268.1 examples/sec; 0.478 sec/batch)
2016-02-04 01:58:12.428331: step 122220, loss = 0.70 (249.7 examples/sec; 0.513 sec/batch)
2016-02-04 01:58:17.108921: step 122230, loss = 0.71 (285.3 examples/sec; 0.449 sec/batch)
2016-02-04 01:58:21.812790: step 122240, loss = 0.82 (276.4 examples/sec; 0.463 sec/batch)
2016-02-04 01:58:26.564203: step 122250, loss = 0.69 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 01:58:31.336042: step 122260, loss = 0.66 (250.6 examples/sec; 0.511 sec/batch)
2016-02-04 01:58:36.036646: step 122270, loss = 0.59 (248.5 examples/sec; 0.515 sec/batch)
2016-02-04 01:58:40.695021: step 122280, loss = 0.67 (274.5 examples/sec; 0.466 sec/batch)
2016-02-04 01:58:45.448339: step 122290, loss = 0.60 (271.3 examples/sec; 0.472 sec/batch)
2016-02-04 01:58:50.143832: step 122300, loss = 0.60 (258.8 examples/sec; 0.495 sec/batch)
2016-02-04 01:58:55.440446: step 122310, loss = 0.73 (288.0 examples/sec; 0.444 sec/batch)
2016-02-04 01:59:00.135232: step 122320, loss = 0.59 (272.6 examples/sec; 0.470 sec/batch)
2016-02-04 01:59:04.933975: step 122330, loss = 0.71 (256.8 examples/sec; 0.499 sec/batch)
2016-02-04 01:59:09.678659: step 122340, loss = 0.65 (268.5 examples/sec; 0.477 sec/batch)
2016-02-04 01:59:14.407395: step 122350, loss = 0.68 (273.8 examples/sec; 0.467 sec/batch)
2016-02-04 01:59:19.110983: step 122360, loss = 0.69 (271.5 examples/sec; 0.471 sec/batch)
2016-02-04 01:59:23.825508: step 122370, loss = 0.72 (265.9 examples/sec; 0.481 sec/batch)
2016-02-04 01:59:28.581641: step 122380, loss = 0.59 (253.3 examples/sec; 0.505 sec/batch)
2016-02-04 01:59:33.272828: step 122390, loss = 0.66 (280.3 examples/sec; 0.457 sec/batch)
2016-02-04 01:59:37.946103: step 122400, loss = 0.83 (278.6 examples/sec; 0.459 sec/batch)
2016-02-04 01:59:43.158671: step 122410, loss = 0.77 (279.9 examples/sec; 0.457 sec/batch)
2016-02-04 01:59:47.827565: step 122420, loss = 0.78 (274.1 examples/sec; 0.467 sec/batch)
2016-02-04 01:59:52.580973: step 122430, loss = 0.67 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 01:59:57.169010: step 122440, loss = 0.72 (290.2 examples/sec; 0.441 sec/batch)
2016-02-04 02:00:01.831619: step 122450, loss = 0.80 (275.6 examples/sec; 0.464 sec/batch)
2016-02-04 02:00:06.475934: step 122460, loss = 0.89 (274.9 examples/sec; 0.466 sec/batch)
2016-02-04 02:00:11.175796: step 122470, loss = 0.59 (263.6 examples/sec; 0.486 sec/batch)
2016-02-04 02:00:15.889121: step 122480, loss = 0.65 (301.7 examples/sec; 0.424 sec/batch)
2016-02-04 02:00:20.676202: step 122490, loss = 0.77 (274.1 examples/sec; 0.467 sec/batch)
2016-02-04 02:00:25.396202: step 122500, loss = 0.62 (281.8 examples/sec; 0.454 sec/batch)
2016-02-04 02:00:30.615149: step 122510, loss = 0.66 (297.1 examples/sec; 0.431 sec/batch)
2016-02-04 02:00:35.396463: step 122520, loss = 0.75 (251.9 examples/sec; 0.508 sec/batch)
2016-02-04 02:00:40.020068: step 122530, loss = 0.81 (274.5 examples/sec; 0.466 sec/batch)
2016-02-04 02:00:44.733648: step 122540, loss = 0.63 (288.3 examples/sec; 0.444 sec/batch)
2016-02-04 02:00:49.483224: step 122550, loss = 0.72 (280.5 examples/sec; 0.456 sec/batch)
2016-02-04 02:00:54.097154: step 122560, loss = 0.75 (282.7 examples/sec; 0.453 sec/batch)
2016-02-04 02:00:58.807310: step 122570, loss = 0.57 (278.6 examples/sec; 0.459 sec/batch)
2016-02-04 02:01:03.496584: step 122580, loss = 0.74 (257.1 examples/sec; 0.498 sec/batch)
2016-02-04 02:01:08.164173: step 122590, loss = 0.50 (255.5 examples/sec; 0.501 sec/batch)
2016-02-04 02:01:12.803381: step 122600, loss = 0.68 (263.5 examples/sec; 0.486 sec/batch)
2016-02-04 02:01:17.988418: step 122610, loss = 0.74 (262.4 examples/sec; 0.488 sec/batch)
2016-02-04 02:01:22.642641: step 122620, loss = 0.59 (277.0 examples/sec; 0.462 sec/batch)
2016-02-04 02:01:27.401927: step 122630, loss = 0.67 (243.9 examples/sec; 0.525 sec/batch)
2016-02-04 02:01:32.081199: step 122640, loss = 0.69 (286.6 examples/sec; 0.447 sec/batch)
2016-02-04 02:01:36.818676: step 122650, loss = 0.65 (250.0 examples/sec; 0.512 sec/batch)
2016-02-04 02:01:41.508141: step 122660, loss = 0.63 (274.0 examples/sec; 0.467 sec/batch)
2016-02-04 02:01:46.270274: step 122670, loss = 0.96 (290.8 examples/sec; 0.440 sec/batch)
2016-02-04 02:01:50.987801: step 122680, loss = 0.78 (284.2 examples/sec; 0.450 sec/batch)
2016-02-04 02:01:55.655690: step 122690, loss = 0.63 (288.7 examples/sec; 0.443 sec/batch)
2016-02-04 02:02:00.372899: step 122700, loss = 0.84 (271.2 examples/sec; 0.472 sec/batch)
2016-02-04 02:02:05.565598: step 122710, loss = 0.64 (264.2 examples/sec; 0.484 sec/batch)
2016-02-04 02:02:10.203025: step 122720, loss = 0.73 (265.1 examples/sec; 0.483 sec/batch)
2016-02-04 02:02:14.922479: step 122730, loss = 0.72 (275.3 examples/sec; 0.465 sec/batch)
2016-02-04 02:02:19.531864: step 122740, loss = 0.73 (278.1 examples/sec; 0.460 sec/batch)
2016-02-04 02:02:24.145411: step 122750, loss = 0.83 (294.1 examples/sec; 0.435 sec/batch)
2016-02-04 02:02:28.755296: step 122760, loss = 0.71 (281.0 examples/sec; 0.456 sec/batch)
2016-02-04 02:02:33.486789: step 122770, loss = 0.75 (301.2 examples/sec; 0.425 sec/batch)
2016-02-04 02:02:38.195558: step 122780, loss = 0.63 (276.2 examples/sec; 0.463 sec/batch)
2016-02-04 02:02:42.868379: step 122790, loss = 0.83 (261.9 examples/sec; 0.489 sec/batch)
2016-02-04 02:02:47.525939: step 122800, loss = 0.77 (275.8 examples/sec; 0.464 sec/batch)
2016-02-04 02:02:52.686999: step 122810, loss = 0.72 (284.2 examples/sec; 0.450 sec/batch)
2016-02-04 02:02:57.402406: step 122820, loss = 0.81 (289.5 examples/sec; 0.442 sec/batch)
2016-02-04 02:03:02.099666: step 122830, loss = 0.52 (278.6 examples/sec; 0.459 sec/batch)
2016-02-04 02:03:06.781914: step 122840, loss = 0.64 (280.4 examples/sec; 0.457 sec/batch)
2016-02-04 02:03:11.491570: step 122850, loss = 0.71 (253.6 examples/sec; 0.505 sec/batch)
2016-02-04 02:03:16.209692: step 122860, loss = 0.62 (267.3 examples/sec; 0.479 sec/batch)
2016-02-04 02:03:20.888198: step 122870, loss = 0.55 (280.9 examples/sec; 0.456 sec/batch)
2016-02-04 02:03:25.612611: step 122880, loss = 0.69 (265.1 examples/sec; 0.483 sec/batch)
2016-02-04 02:03:30.297291: step 122890, loss = 0.66 (275.4 examples/sec; 0.465 sec/batch)
2016-02-04 02:03:35.020521: step 122900, loss = 0.66 (258.9 examples/sec; 0.494 sec/batch)
2016-02-04 02:03:40.268851: step 122910, loss = 0.64 (263.2 examples/sec; 0.486 sec/batch)
2016-02-04 02:03:44.943452: step 122920, loss = 0.70 (285.1 examples/sec; 0.449 sec/batch)
2016-02-04 02:03:49.636646: step 122930, loss = 0.78 (284.7 examples/sec; 0.450 sec/batch)
2016-02-04 02:03:54.330786: step 122940, loss = 0.61 (300.7 examples/sec; 0.426 sec/batch)
2016-02-04 02:03:59.081174: step 122950, loss = 0.66 (271.6 examples/sec; 0.471 sec/batch)
2016-02-04 02:04:03.800238: step 122960, loss = 0.65 (261.0 examples/sec; 0.490 sec/batch)
2016-02-04 02:04:08.595286: step 122970, loss = 0.69 (252.6 examples/sec; 0.507 sec/batch)
2016-02-04 02:04:13.284123: step 122980, loss = 0.61 (276.3 examples/sec; 0.463 sec/batch)
2016-02-04 02:04:17.983611: step 122990, loss = 0.57 (287.8 examples/sec; 0.445 sec/batch)
2016-02-04 02:04:22.784804: step 123000, loss = 0.84 (258.8 examples/sec; 0.495 sec/batch)
2016-02-04 02:04:28.025470: step 123010, loss = 0.58 (255.5 examples/sec; 0.501 sec/batch)
2016-02-04 02:04:32.656822: step 123020, loss = 0.64 (263.2 examples/sec; 0.486 sec/batch)
2016-02-04 02:04:37.374828: step 123030, loss = 0.64 (254.5 examples/sec; 0.503 sec/batch)
2016-02-04 02:04:42.150766: step 123040, loss = 0.67 (258.4 examples/sec; 0.495 sec/batch)
2016-02-04 02:04:46.879170: step 123050, loss = 0.73 (281.7 examples/sec; 0.454 sec/batch)
2016-02-04 02:04:51.611732: step 123060, loss = 0.72 (267.5 examples/sec; 0.478 sec/batch)
2016-02-04 02:04:56.377868: step 123070, loss = 0.80 (268.2 examples/sec; 0.477 sec/batch)
2016-02-04 02:05:01.105282: step 123080, loss = 0.54 (290.5 examples/sec; 0.441 sec/batch)
2016-02-04 02:05:05.854738: step 123090, loss = 0.70 (268.6 examples/sec; 0.477 sec/batch)
2016-02-04 02:05:10.493191: step 123100, loss = 0.59 (289.6 examples/sec; 0.442 sec/batch)
2016-02-04 02:05:15.647339: step 123110, loss = 0.67 (259.0 examples/sec; 0.494 sec/batch)
2016-02-04 02:05:20.368552: step 123120, loss = 0.69 (261.1 examples/sec; 0.490 sec/batch)
2016-02-04 02:05:25.004684: step 123130, loss = 0.57 (281.6 examples/sec; 0.455 sec/batch)
2016-02-04 02:05:29.733444: step 123140, loss = 0.66 (277.6 examples/sec; 0.461 sec/batch)
2016-02-04 02:05:34.470794: step 123150, loss = 0.78 (254.5 examples/sec; 0.503 sec/batch)
2016-02-04 02:05:39.193279: step 123160, loss = 0.81 (261.6 examples/sec; 0.489 sec/batch)
2016-02-04 02:05:43.997795: step 123170, loss = 0.69 (245.4 examples/sec; 0.522 sec/batch)
2016-02-04 02:05:48.769055: step 123180, loss = 0.67 (272.8 examples/sec; 0.469 sec/batch)
2016-02-04 02:05:53.451048: step 123190, loss = 0.68 (279.3 examples/sec; 0.458 sec/batch)
2016-02-04 02:05:58.199429: step 123200, loss = 0.63 (269.8 examples/sec; 0.474 sec/batch)
2016-02-04 02:06:03.359256: step 123210, loss = 0.85 (281.3 examples/sec; 0.455 sec/batch)
2016-02-04 02:06:08.014305: step 123220, loss = 0.62 (277.2 examples/sec; 0.462 sec/batch)
2016-02-04 02:06:12.755074: step 123230, loss = 0.59 (268.0 examples/sec; 0.478 sec/batch)
2016-02-04 02:06:17.558629: step 123240, loss = 0.84 (264.9 examples/sec; 0.483 sec/batch)
2016-02-04 02:06:22.225064: step 123250, loss = 0.72 (267.4 examples/sec; 0.479 sec/batch)
2016-02-04 02:06:26.939160: step 123260, loss = 0.67 (247.7 examples/sec; 0.517 sec/batch)
2016-02-04 02:06:31.549985: step 123270, loss = 0.66 (268.1 examples/sec; 0.477 sec/batch)
2016-02-04 02:06:36.244573: step 123280, loss = 0.68 (282.7 examples/sec; 0.453 sec/batch)
2016-02-04 02:06:40.942289: step 123290, loss = 0.76 (275.9 examples/sec; 0.464 sec/batch)
2016-02-04 02:06:45.659199: step 123300, loss = 0.69 (270.3 examples/sec; 0.474 sec/batch)
2016-02-04 02:06:50.802963: step 123310, loss = 0.56 (272.0 examples/sec; 0.471 sec/batch)
2016-02-04 02:06:55.491552: step 123320, loss = 0.68 (281.3 examples/sec; 0.455 sec/batch)
2016-02-04 02:07:00.257739: step 123330, loss = 0.63 (263.7 examples/sec; 0.485 sec/batch)
2016-02-04 02:07:05.020901: step 123340, loss = 0.68 (286.9 examples/sec; 0.446 sec/batch)
2016-02-04 02:07:09.759612: step 123350, loss = 0.76 (266.9 examples/sec; 0.480 sec/batch)
2016-02-04 02:07:14.469022: step 123360, loss = 0.64 (286.4 examples/sec; 0.447 sec/batch)
2016-02-04 02:07:19.078920: step 123370, loss = 0.72 (308.5 examples/sec; 0.415 sec/batch)
2016-02-04 02:07:23.781794: step 123380, loss = 0.67 (279.0 examples/sec; 0.459 sec/batch)
2016-02-04 02:07:28.480065: step 123390, loss = 0.75 (284.5 examples/sec; 0.450 sec/batch)
2016-02-04 02:07:33.169475: step 123400, loss = 0.60 (281.7 examples/sec; 0.454 sec/batch)
2016-02-04 02:07:38.450855: step 123410, loss = 0.62 (273.9 examples/sec; 0.467 sec/batch)
2016-02-04 02:07:43.167921: step 123420, loss = 0.61 (260.7 examples/sec; 0.491 sec/batch)
2016-02-04 02:07:47.871650: step 123430, loss = 0.66 (284.6 examples/sec; 0.450 sec/batch)
2016-02-04 02:07:52.645835: step 123440, loss = 0.63 (265.5 examples/sec; 0.482 sec/batch)
2016-02-04 02:07:57.311770: step 123450, loss = 0.68 (283.3 examples/sec; 0.452 sec/batch)
2016-02-04 02:08:02.065321: step 123460, loss = 0.68 (250.1 examples/sec; 0.512 sec/batch)
2016-02-04 02:08:06.668039: step 123470, loss = 0.64 (292.9 examples/sec; 0.437 sec/batch)
2016-02-04 02:08:11.341570: step 123480, loss = 0.61 (271.7 examples/sec; 0.471 sec/batch)
2016-02-04 02:08:16.054076: step 123490, loss = 0.67 (286.4 examples/sec; 0.447 sec/batch)
2016-02-04 02:08:20.785354: step 123500, loss = 0.68 (262.8 examples/sec; 0.487 sec/batch)
2016-02-04 02:08:25.990877: step 123510, loss = 0.78 (296.1 examples/sec; 0.432 sec/batch)
2016-02-04 02:08:30.755406: step 123520, loss = 0.65 (281.3 examples/sec; 0.455 sec/batch)
2016-02-04 02:08:35.413588: step 123530, loss = 0.66 (251.5 examples/sec; 0.509 sec/batch)
2016-02-04 02:08:40.080901: step 123540, loss = 0.63 (265.0 examples/sec; 0.483 sec/batch)
2016-02-04 02:08:44.788991: step 123550, loss = 0.69 (263.9 examples/sec; 0.485 sec/batch)
2016-02-04 02:08:49.442418: step 123560, loss = 0.72 (259.0 examples/sec; 0.494 sec/batch)
2016-02-04 02:08:54.078612: step 123570, loss = 0.51 (291.3 examples/sec; 0.439 sec/batch)
2016-02-04 02:08:58.759927: step 123580, loss = 0.73 (265.9 examples/sec; 0.481 sec/batch)
2016-02-04 02:09:03.499113: step 123590, loss = 0.79 (274.3 examples/sec; 0.467 sec/batch)
2016-02-04 02:09:08.206343: step 123600, loss = 0.55 (270.7 examples/sec; 0.473 sec/batch)
2016-02-04 02:09:13.434288: step 123610, loss = 0.66 (275.6 examples/sec; 0.464 sec/batch)
2016-02-04 02:09:18.114663: step 123620, loss = 0.67 (258.5 examples/sec; 0.495 sec/batch)
2016-02-04 02:09:22.739263: step 123630, loss = 0.64 (297.7 examples/sec; 0.430 sec/batch)
2016-02-04 02:09:27.482884: step 123640, loss = 0.67 (266.1 examples/sec; 0.481 sec/batch)
2016-02-04 02:09:32.111068: step 123650, loss = 0.66 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 02:09:36.820089: step 123660, loss = 0.57 (279.8 examples/sec; 0.457 sec/batch)
2016-02-04 02:09:41.453551: step 123670, loss = 0.69 (282.1 examples/sec; 0.454 sec/batch)
2016-02-04 02:09:46.164312: step 123680, loss = 0.85 (283.2 examples/sec; 0.452 sec/batch)
2016-02-04 02:09:50.874283: step 123690, loss = 0.73 (286.3 examples/sec; 0.447 sec/batch)
2016-02-04 02:09:55.518561: step 123700, loss = 0.62 (276.1 examples/sec; 0.464 sec/batch)
2016-02-04 02:10:00.826021: step 123710, loss = 0.74 (269.0 examples/sec; 0.476 sec/batch)
2016-02-04 02:10:05.371492: step 123720, loss = 0.78 (281.7 examples/sec; 0.454 sec/batch)
2016-02-04 02:10:10.107874: step 123730, loss = 0.77 (280.5 examples/sec; 0.456 sec/batch)
2016-02-04 02:10:14.741039: step 123740, loss = 0.66 (278.2 examples/sec; 0.460 sec/batch)
2016-02-04 02:10:19.472738: step 123750, loss = 0.81 (258.7 examples/sec; 0.495 sec/batch)
2016-02-04 02:10:24.162340: step 123760, loss = 0.74 (258.3 examples/sec; 0.495 sec/batch)
2016-02-04 02:10:28.838939: step 123770, loss = 0.64 (275.3 examples/sec; 0.465 sec/batch)
2016-02-04 02:10:33.519138: step 123780, loss = 0.69 (279.1 examples/sec; 0.459 sec/batch)
2016-02-04 02:10:38.228238: step 123790, loss = 0.76 (291.6 examples/sec; 0.439 sec/batch)
2016-02-04 02:10:42.981461: step 123800, loss = 0.63 (248.6 examples/sec; 0.515 sec/batch)
2016-02-04 02:10:48.247074: step 123810, loss = 0.68 (263.1 examples/sec; 0.486 sec/batch)
2016-02-04 02:10:52.959635: step 123820, loss = 0.62 (285.2 examples/sec; 0.449 sec/batch)
2016-02-04 02:10:57.610121: step 123830, loss = 0.77 (273.3 examples/sec; 0.468 sec/batch)
2016-02-04 02:11:02.381614: step 123840, loss = 0.65 (287.1 examples/sec; 0.446 sec/batch)
2016-02-04 02:11:07.145485: step 123850, loss = 0.68 (272.4 examples/sec; 0.470 sec/batch)
2016-02-04 02:11:11.840910: step 123860, loss = 0.77 (258.2 examples/sec; 0.496 sec/batch)
2016-02-04 02:11:16.539732: step 123870, loss = 0.73 (268.2 examples/sec; 0.477 sec/batch)
2016-02-04 02:11:21.217640: step 123880, loss = 0.60 (318.2 examples/sec; 0.402 sec/batch)
2016-02-04 02:11:25.968063: step 123890, loss = 0.60 (279.8 examples/sec; 0.457 sec/batch)
2016-02-04 02:11:30.795065: step 123900, loss = 0.70 (255.4 examples/sec; 0.501 sec/batch)
2016-02-04 02:11:35.996962: step 123910, loss = 0.77 (298.0 examples/sec; 0.429 sec/batch)
2016-02-04 02:11:40.631356: step 123920, loss = 0.67 (303.1 examples/sec; 0.422 sec/batch)
2016-02-04 02:11:45.296579: step 123930, loss = 0.91 (278.4 examples/sec; 0.460 sec/batch)
2016-02-04 02:11:49.997991: step 123940, loss = 0.58 (286.6 examples/sec; 0.447 sec/batch)
2016-02-04 02:11:54.756783: step 123950, loss = 0.77 (290.9 examples/sec; 0.440 sec/batch)
2016-02-04 02:11:59.428865: step 123960, loss = 0.61 (311.4 examples/sec; 0.411 sec/batch)
2016-02-04 02:12:04.135163: step 123970, loss = 0.55 (282.4 examples/sec; 0.453 sec/batch)
2016-02-04 02:12:08.817113: step 123980, loss = 0.76 (260.3 examples/sec; 0.492 sec/batch)
2016-02-04 02:12:13.449212: step 123990, loss = 0.80 (276.4 examples/sec; 0.463 sec/batch)
2016-02-04 02:12:18.168114: step 124000, loss = 0.68 (269.7 examples/sec; 0.475 sec/batch)
2016-02-04 02:12:23.374117: step 124010, loss = 0.74 (248.5 examples/sec; 0.515 sec/batch)
2016-02-04 02:12:28.001363: step 124020, loss = 0.68 (292.4 examples/sec; 0.438 sec/batch)
2016-02-04 02:12:32.810337: step 124030, loss = 0.56 (250.1 examples/sec; 0.512 sec/batch)
2016-02-04 02:12:37.508086: step 124040, loss = 0.75 (263.5 examples/sec; 0.486 sec/batch)
2016-02-04 02:12:42.187817: step 124050, loss = 0.76 (292.6 examples/sec; 0.438 sec/batch)
2016-02-04 02:12:46.952717: step 124060, loss = 0.63 (251.6 examples/sec; 0.509 sec/batch)
2016-02-04 02:12:51.664731: step 124070, loss = 0.65 (260.0 examples/sec; 0.492 sec/batch)
2016-02-04 02:12:56.379378: step 124080, loss = 0.67 (284.3 examples/sec; 0.450 sec/batch)
2016-02-04 02:13:01.084193: step 124090, loss = 0.60 (272.6 examples/sec; 0.469 sec/batch)
2016-02-04 02:13:05.766495: step 124100, loss = 0.70 (272.5 examples/sec; 0.470 sec/batch)
2016-02-04 02:13:10.992338: step 124110, loss = 0.77 (286.0 examples/sec; 0.448 sec/batch)
2016-02-04 02:13:15.687961: step 124120, loss = 0.59 (259.4 examples/sec; 0.494 sec/batch)
2016-02-04 02:13:20.370701: step 124130, loss = 0.58 (266.4 examples/sec; 0.480 sec/batch)
2016-02-04 02:13:25.122720: step 124140, loss = 0.62 (265.7 examples/sec; 0.482 sec/batch)
2016-02-04 02:13:29.924650: step 124150, loss = 0.72 (275.8 examples/sec; 0.464 sec/batch)
2016-02-04 02:13:34.571491: step 124160, loss = 0.69 (272.8 examples/sec; 0.469 sec/batch)
2016-02-04 02:13:39.242735: step 124170, loss = 0.71 (267.8 examples/sec; 0.478 sec/batch)
2016-02-04 02:13:43.942048: step 124180, loss = 0.74 (282.9 examples/sec; 0.452 sec/batch)
2016-02-04 02:13:48.633370: step 124190, loss = 0.58 (281.6 examples/sec; 0.455 sec/batch)
2016-02-04 02:13:53.484498: step 124200, loss = 0.59 (237.6 examples/sec; 0.539 sec/batch)
2016-02-04 02:13:58.678510: step 124210, loss = 0.71 (291.4 examples/sec; 0.439 sec/batch)
2016-02-04 02:14:03.431945: step 124220, loss = 0.80 (255.1 examples/sec; 0.502 sec/batch)
2016-02-04 02:14:08.095144: step 124230, loss = 0.76 (257.2 examples/sec; 0.498 sec/batch)
2016-02-04 02:14:12.804290: step 124240, loss = 0.59 (277.2 examples/sec; 0.462 sec/batch)
2016-02-04 02:14:17.513877: step 124250, loss = 0.60 (272.4 examples/sec; 0.470 sec/batch)
2016-02-04 02:14:22.193327: step 124260, loss = 0.58 (281.2 examples/sec; 0.455 sec/batch)
2016-02-04 02:14:26.898715: step 124270, loss = 0.66 (305.1 examples/sec; 0.420 sec/batch)
2016-02-04 02:14:31.671714: step 124280, loss = 0.60 (273.2 examples/sec; 0.469 sec/batch)
2016-02-04 02:14:36.380820: step 124290, loss = 0.66 (271.3 examples/sec; 0.472 sec/batch)
2016-02-04 02:14:41.107517: step 124300, loss = 0.65 (277.3 examples/sec; 0.462 sec/batch)
2016-02-04 02:14:46.357425: step 124310, loss = 0.65 (282.0 examples/sec; 0.454 sec/batch)
2016-02-04 02:14:51.110614: step 124320, loss = 0.73 (264.1 examples/sec; 0.485 sec/batch)
2016-02-04 02:14:55.825269: step 124330, loss = 0.75 (270.3 examples/sec; 0.474 sec/batch)
2016-02-04 02:15:00.558608: step 124340, loss = 0.76 (277.1 examples/sec; 0.462 sec/batch)
2016-02-04 02:15:05.250664: step 124350, loss = 0.70 (281.6 examples/sec; 0.455 sec/batch)
2016-02-04 02:15:09.902344: step 124360, loss = 0.68 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 02:15:14.578595: step 124370, loss = 0.65 (259.1 examples/sec; 0.494 sec/batch)
2016-02-04 02:15:19.274111: step 124380, loss = 0.65 (247.0 examples/sec; 0.518 sec/batch)
2016-02-04 02:15:23.865625: step 124390, loss = 0.79 (264.4 examples/sec; 0.484 sec/batch)
2016-02-04 02:15:28.608757: step 124400, loss = 0.82 (252.6 examples/sec; 0.507 sec/batch)
2016-02-04 02:15:33.872315: step 124410, loss = 0.69 (239.5 examples/sec; 0.534 sec/batch)
2016-02-04 02:15:38.578377: step 124420, loss = 0.80 (264.9 examples/sec; 0.483 sec/batch)
2016-02-04 02:15:43.268751: step 124430, loss = 0.85 (267.2 examples/sec; 0.479 sec/batch)
2016-02-04 02:15:47.896123: step 124440, loss = 0.68 (261.3 examples/sec; 0.490 sec/batch)
2016-02-04 02:15:52.652651: step 124450, loss = 0.77 (250.0 examples/sec; 0.512 sec/batch)
2016-02-04 02:15:57.348469: step 124460, loss = 0.63 (262.7 examples/sec; 0.487 sec/batch)
2016-02-04 02:16:01.985514: step 124470, loss = 0.64 (259.3 examples/sec; 0.494 sec/batch)
2016-02-04 02:16:06.613760: step 124480, loss = 0.72 (296.9 examples/sec; 0.431 sec/batch)
2016-02-04 02:16:11.362209: step 124490, loss = 0.65 (280.7 examples/sec; 0.456 sec/batch)
2016-02-04 02:16:16.099038: step 124500, loss = 0.74 (270.3 examples/sec; 0.474 sec/batch)
2016-02-04 02:16:21.200698: step 124510, loss = 0.59 (289.8 examples/sec; 0.442 sec/batch)
2016-02-04 02:16:25.936977: step 124520, loss = 0.78 (284.9 examples/sec; 0.449 sec/batch)
2016-02-04 02:16:30.665657: step 124530, loss = 0.78 (274.2 examples/sec; 0.467 sec/batch)
2016-02-04 02:16:35.432185: step 124540, loss = 0.72 (244.6 examples/sec; 0.523 sec/batch)
2016-02-04 02:16:40.041816: step 124550, loss = 0.66 (275.4 examples/sec; 0.465 sec/batch)
2016-02-04 02:16:44.754095: step 124560, loss = 0.61 (287.4 examples/sec; 0.445 sec/batch)
2016-02-04 02:16:49.403893: step 124570, loss = 0.68 (263.7 examples/sec; 0.485 sec/batch)
2016-02-04 02:16:54.101698: step 124580, loss = 0.71 (287.2 examples/sec; 0.446 sec/batch)
2016-02-04 02:16:58.857157: step 124590, loss = 0.95 (242.7 examples/sec; 0.527 sec/batch)
2016-02-04 02:17:03.599372: step 124600, loss = 0.70 (284.4 examples/sec; 0.450 sec/batch)
2016-02-04 02:17:08.998357: step 124610, loss = 0.70 (258.2 examples/sec; 0.496 sec/batch)
2016-02-04 02:17:13.657420: step 124620, loss = 0.69 (284.7 examples/sec; 0.450 sec/batch)
2016-02-04 02:17:18.360834: step 124630, loss = 0.61 (266.1 examples/sec; 0.481 sec/batch)
2016-02-04 02:17:23.067959: step 124640, loss = 0.72 (302.6 examples/sec; 0.423 sec/batch)
2016-02-04 02:17:27.750497: step 124650, loss = 0.67 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 02:17:32.428131: step 124660, loss = 0.68 (256.6 examples/sec; 0.499 sec/batch)
2016-02-04 02:17:37.159898: step 124670, loss = 0.72 (273.3 examples/sec; 0.468 sec/batch)
2016-02-04 02:17:41.902528: step 124680, loss = 0.67 (275.3 examples/sec; 0.465 sec/batch)
2016-02-04 02:17:46.525542: step 124690, loss = 0.61 (279.0 examples/sec; 0.459 sec/batch)
2016-02-04 02:17:51.253004: step 124700, loss = 0.74 (250.7 examples/sec; 0.511 sec/batch)
2016-02-04 02:17:56.497832: step 124710, loss = 0.85 (282.6 examples/sec; 0.453 sec/batch)
2016-02-04 02:18:01.230717: step 124720, loss = 0.80 (268.9 examples/sec; 0.476 sec/batch)
2016-02-04 02:18:05.945335: step 124730, loss = 0.70 (284.3 examples/sec; 0.450 sec/batch)
2016-02-04 02:18:10.668283: step 124740, loss = 0.67 (271.8 examples/sec; 0.471 sec/batch)
2016-02-04 02:18:15.396771: step 124750, loss = 0.63 (271.6 examples/sec; 0.471 sec/batch)
2016-02-04 02:18:20.128027: step 124760, loss = 0.67 (296.4 examples/sec; 0.432 sec/batch)
2016-02-04 02:18:24.970280: step 124770, loss = 0.58 (255.8 examples/sec; 0.500 sec/batch)
2016-02-04 02:18:29.639188: step 124780, loss = 0.73 (293.3 examples/sec; 0.436 sec/batch)
2016-02-04 02:18:34.387765: step 124790, loss = 0.76 (293.3 examples/sec; 0.436 sec/batch)
2016-02-04 02:18:39.112250: step 124800, loss = 0.74 (272.3 examples/sec; 0.470 sec/batch)
2016-02-04 02:18:44.365835: step 124810, loss = 0.68 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 02:18:49.068270: step 124820, loss = 0.73 (271.1 examples/sec; 0.472 sec/batch)
2016-02-04 02:18:53.854136: step 124830, loss = 0.70 (288.8 examples/sec; 0.443 sec/batch)
2016-02-04 02:18:58.598644: step 124840, loss = 0.70 (267.8 examples/sec; 0.478 sec/batch)
2016-02-04 02:19:03.273936: step 124850, loss = 0.75 (274.8 examples/sec; 0.466 sec/batch)
2016-02-04 02:19:07.950622: step 124860, loss = 0.59 (276.1 examples/sec; 0.464 sec/batch)
2016-02-04 02:19:12.705273: step 124870, loss = 0.62 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 02:19:17.447673: step 124880, loss = 0.89 (263.2 examples/sec; 0.486 sec/batch)
2016-02-04 02:19:22.121232: step 124890, loss = 0.70 (263.8 examples/sec; 0.485 sec/batch)
2016-02-04 02:19:26.843788: step 124900, loss = 0.66 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 02:19:32.074806: step 124910, loss = 0.63 (265.2 examples/sec; 0.483 sec/batch)
2016-02-04 02:19:36.812487: step 124920, loss = 0.65 (259.1 examples/sec; 0.494 sec/batch)
2016-02-04 02:19:41.580200: step 124930, loss = 0.75 (264.4 examples/sec; 0.484 sec/batch)
2016-02-04 02:19:46.241725: step 124940, loss = 0.61 (281.1 examples/sec; 0.455 sec/batch)
2016-02-04 02:19:50.928725: step 124950, loss = 0.66 (290.0 examples/sec; 0.441 sec/batch)
2016-02-04 02:19:55.718491: step 124960, loss = 0.66 (281.1 examples/sec; 0.455 sec/batch)
2016-02-04 02:20:00.417389: step 124970, loss = 0.74 (308.0 examples/sec; 0.416 sec/batch)
2016-02-04 02:20:05.129205: step 124980, loss = 0.64 (276.3 examples/sec; 0.463 sec/batch)
2016-02-04 02:20:09.784708: step 124990, loss = 0.68 (284.3 examples/sec; 0.450 sec/batch)
2016-02-04 02:20:14.417661: step 125000, loss = 0.75 (279.9 examples/sec; 0.457 sec/batch)
2016-02-04 02:20:19.578615: step 125010, loss = 0.59 (298.8 examples/sec; 0.428 sec/batch)
2016-02-04 02:20:24.311290: step 125020, loss = 0.68 (265.5 examples/sec; 0.482 sec/batch)
2016-02-04 02:20:28.963027: step 125030, loss = 0.72 (280.0 examples/sec; 0.457 sec/batch)
2016-02-04 02:20:33.617905: step 125040, loss = 0.72 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 02:20:38.319687: step 125050, loss = 0.61 (278.4 examples/sec; 0.460 sec/batch)
2016-02-04 02:20:43.025588: step 125060, loss = 0.57 (285.4 examples/sec; 0.448 sec/batch)
2016-02-04 02:20:47.743923: step 125070, loss = 0.68 (283.1 examples/sec; 0.452 sec/batch)
2016-02-04 02:20:52.398164: step 125080, loss = 0.56 (247.6 examples/sec; 0.517 sec/batch)
2016-02-04 02:20:57.147743: step 125090, loss = 0.53 (271.8 examples/sec; 0.471 sec/batch)
2016-02-04 02:21:01.862562: step 125100, loss = 0.59 (265.5 examples/sec; 0.482 sec/batch)
2016-02-04 02:21:07.136580: step 125110, loss = 0.61 (252.9 examples/sec; 0.506 sec/batch)
2016-02-04 02:21:11.872708: step 125120, loss = 0.64 (282.8 examples/sec; 0.453 sec/batch)
2016-02-04 02:21:16.572749: step 125130, loss = 0.70 (287.3 examples/sec; 0.446 sec/batch)
2016-02-04 02:21:21.305400: step 125140, loss = 0.70 (280.0 examples/sec; 0.457 sec/batch)
2016-02-04 02:21:26.035759: step 125150, loss = 0.76 (257.5 examples/sec; 0.497 sec/batch)
2016-02-04 02:21:30.757077: step 125160, loss = 0.77 (254.6 examples/sec; 0.503 sec/batch)
2016-02-04 02:21:35.461103: step 125170, loss = 0.73 (269.1 examples/sec; 0.476 sec/batch)
2016-02-04 02:21:40.138239: step 125180, loss = 0.74 (274.5 examples/sec; 0.466 sec/batch)
2016-02-04 02:21:44.939807: step 125190, loss = 0.72 (264.9 examples/sec; 0.483 sec/batch)
2016-02-04 02:21:49.639046: step 125200, loss = 0.77 (281.6 examples/sec; 0.455 sec/batch)
2016-02-04 02:21:54.856546: step 125210, loss = 0.70 (269.4 examples/sec; 0.475 sec/batch)
2016-02-04 02:21:59.526725: step 125220, loss = 0.69 (265.2 examples/sec; 0.483 sec/batch)
2016-02-04 02:22:04.194465: step 125230, loss = 0.72 (279.7 examples/sec; 0.458 sec/batch)
2016-02-04 02:22:08.927503: step 125240, loss = 0.75 (288.1 examples/sec; 0.444 sec/batch)
2016-02-04 02:22:13.685912: step 125250, loss = 0.85 (273.1 examples/sec; 0.469 sec/batch)
2016-02-04 02:22:18.420053: step 125260, loss = 0.65 (288.5 examples/sec; 0.444 sec/batch)
2016-02-04 02:22:23.143285: step 125270, loss = 0.68 (270.7 examples/sec; 0.473 sec/batch)
2016-02-04 02:22:27.833101: step 125280, loss = 0.69 (282.8 examples/sec; 0.453 sec/batch)
2016-02-04 02:22:32.490433: step 125290, loss = 0.70 (263.7 examples/sec; 0.485 sec/batch)
2016-02-04 02:22:37.152727: step 125300, loss = 0.78 (279.7 examples/sec; 0.458 sec/batch)
2016-02-04 02:22:42.425982: step 125310, loss = 0.59 (239.1 examples/sec; 0.535 sec/batch)
2016-02-04 02:22:47.151725: step 125320, loss = 0.66 (271.0 examples/sec; 0.472 sec/batch)
2016-02-04 02:22:51.857393: step 125330, loss = 0.63 (264.8 examples/sec; 0.483 sec/batch)
2016-02-04 02:22:56.597923: step 125340, loss = 0.68 (266.8 examples/sec; 0.480 sec/batch)
2016-02-04 02:23:01.295627: step 125350, loss = 0.71 (269.2 examples/sec; 0.475 sec/batch)
2016-02-04 02:23:06.000865: step 125360, loss = 0.84 (267.8 examples/sec; 0.478 sec/batch)
2016-02-04 02:23:10.721090: step 125370, loss = 0.66 (268.0 examples/sec; 0.478 sec/batch)
2016-02-04 02:23:15.407990: step 125380, loss = 0.67 (278.1 examples/sec; 0.460 sec/batch)
2016-02-04 02:23:20.036983: step 125390, loss = 0.76 (312.0 examples/sec; 0.410 sec/batch)
2016-02-04 02:23:24.876013: step 125400, loss = 0.73 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 02:23:30.116416: step 125410, loss = 0.63 (284.6 examples/sec; 0.450 sec/batch)
2016-02-04 02:23:34.794899: step 125420, loss = 0.74 (271.1 examples/sec; 0.472 sec/batch)
2016-02-04 02:23:39.479779: step 125430, loss = 0.82 (274.3 examples/sec; 0.467 sec/batch)
2016-02-04 02:23:44.265132: step 125440, loss = 0.77 (259.3 examples/sec; 0.494 sec/batch)
2016-02-04 02:23:48.953778: step 125450, loss = 0.68 (282.5 examples/sec; 0.453 sec/batch)
2016-02-04 02:23:53.743326: step 125460, loss = 0.57 (243.8 examples/sec; 0.525 sec/batch)
2016-02-04 02:23:58.477836: step 125470, loss = 0.64 (272.8 examples/sec; 0.469 sec/batch)
2016-02-04 02:24:03.157337: step 125480, loss = 0.55 (260.2 examples/sec; 0.492 sec/batch)
2016-02-04 02:24:07.888713: step 125490, loss = 0.72 (277.4 examples/sec; 0.461 sec/batch)
2016-02-04 02:24:12.599139: step 125500, loss = 0.67 (287.5 examples/sec; 0.445 sec/batch)
2016-02-04 02:24:18.000611: step 125510, loss = 0.64 (261.9 examples/sec; 0.489 sec/batch)
2016-02-04 02:24:22.728246: step 125520, loss = 0.64 (302.7 examples/sec; 0.423 sec/batch)
2016-02-04 02:24:27.403065: step 125530, loss = 0.77 (279.2 examples/sec; 0.458 sec/batch)
2016-02-04 02:24:32.125267: step 125540, loss = 0.66 (257.3 examples/sec; 0.497 sec/batch)
2016-02-04 02:24:36.874630: step 125550, loss = 0.66 (287.4 examples/sec; 0.445 sec/batch)
2016-02-04 02:24:41.625667: step 125560, loss = 0.76 (265.7 examples/sec; 0.482 sec/batch)
2016-02-04 02:24:46.341996: step 125570, loss = 0.73 (262.6 examples/sec; 0.487 sec/batch)
2016-02-04 02:24:50.995146: step 125580, loss = 0.66 (277.0 examples/sec; 0.462 sec/batch)
2016-02-04 02:24:55.645861: step 125590, loss = 0.66 (287.2 examples/sec; 0.446 sec/batch)
2016-02-04 02:25:00.292454: step 125600, loss = 0.72 (272.9 examples/sec; 0.469 sec/batch)
2016-02-04 02:25:05.477217: step 125610, loss = 0.72 (272.7 examples/sec; 0.469 sec/batch)
2016-02-04 02:25:10.109520: step 125620, loss = 0.67 (293.1 examples/sec; 0.437 sec/batch)
2016-02-04 02:25:14.845144: step 125630, loss = 0.62 (286.4 examples/sec; 0.447 sec/batch)
2016-02-04 02:25:19.551537: step 125640, loss = 0.82 (279.4 examples/sec; 0.458 sec/batch)
2016-02-04 02:25:24.362330: step 125650, loss = 0.62 (258.2 examples/sec; 0.496 sec/batch)
2016-02-04 02:25:29.094924: step 125660, loss = 0.69 (291.9 examples/sec; 0.439 sec/batch)
2016-02-04 02:25:33.834256: step 125670, loss = 0.71 (275.0 examples/sec; 0.465 sec/batch)
2016-02-04 02:25:38.503165: step 125680, loss = 0.66 (279.7 examples/sec; 0.458 sec/batch)
2016-02-04 02:25:43.188370: step 125690, loss = 0.70 (261.2 examples/sec; 0.490 sec/batch)
2016-02-04 02:25:47.929391: step 125700, loss = 0.77 (272.5 examples/sec; 0.470 sec/batch)
2016-02-04 02:25:53.165500: step 125710, loss = 0.78 (285.4 examples/sec; 0.448 sec/batch)
2016-02-04 02:25:57.920908: step 125720, loss = 0.83 (252.7 examples/sec; 0.506 sec/batch)
2016-02-04 02:26:02.598623: step 125730, loss = 0.80 (269.5 examples/sec; 0.475 sec/batch)
2016-02-04 02:26:07.300605: step 125740, loss = 0.71 (269.2 examples/sec; 0.475 sec/batch)
2016-02-04 02:26:11.950473: step 125750, loss = 0.60 (258.9 examples/sec; 0.494 sec/batch)
2016-02-04 02:26:16.676473: step 125760, loss = 0.60 (261.6 examples/sec; 0.489 sec/batch)
2016-02-04 02:26:21.393714: step 125770, loss = 0.61 (277.2 examples/sec; 0.462 sec/batch)
2016-02-04 02:26:26.062350: step 125780, loss = 0.81 (288.8 examples/sec; 0.443 sec/batch)
2016-02-04 02:26:30.761061: step 125790, loss = 0.79 (280.3 examples/sec; 0.457 sec/batch)
2016-02-04 02:26:35.513665: step 125800, loss = 0.59 (244.1 examples/sec; 0.524 sec/batch)
2016-02-04 02:26:40.671084: step 125810, loss = 0.75 (272.2 examples/sec; 0.470 sec/batch)
2016-02-04 02:26:45.379614: step 125820, loss = 0.56 (290.7 examples/sec; 0.440 sec/batch)
2016-02-04 02:26:50.158205: step 125830, loss = 0.74 (255.6 examples/sec; 0.501 sec/batch)
2016-02-04 02:26:54.882119: step 125840, loss = 0.65 (256.8 examples/sec; 0.498 sec/batch)
2016-02-04 02:26:59.584278: step 125850, loss = 0.72 (275.2 examples/sec; 0.465 sec/batch)
2016-02-04 02:27:04.311248: step 125860, loss = 0.65 (256.5 examples/sec; 0.499 sec/batch)
2016-02-04 02:27:08.974623: step 125870, loss = 0.78 (259.6 examples/sec; 0.493 sec/batch)
2016-02-04 02:27:13.678096: step 125880, loss = 0.72 (259.5 examples/sec; 0.493 sec/batch)
2016-02-04 02:27:18.333311: step 125890, loss = 0.67 (274.3 examples/sec; 0.467 sec/batch)
2016-02-04 02:27:23.088664: step 125900, loss = 0.59 (269.6 examples/sec; 0.475 sec/batch)
2016-02-04 02:27:28.267957: step 125910, loss = 0.76 (280.5 examples/sec; 0.456 sec/batch)
2016-02-04 02:27:33.015456: step 125920, loss = 0.69 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 02:27:37.737947: step 125930, loss = 0.69 (288.7 examples/sec; 0.443 sec/batch)
2016-02-04 02:27:42.552016: step 125940, loss = 0.64 (266.7 examples/sec; 0.480 sec/batch)
2016-02-04 02:27:47.354780: step 125950, loss = 0.62 (253.7 examples/sec; 0.505 sec/batch)
2016-02-04 02:27:52.088492: step 125960, loss = 0.79 (305.9 examples/sec; 0.418 sec/batch)
2016-02-04 02:27:56.787921: step 125970, loss = 0.60 (281.2 examples/sec; 0.455 sec/batch)
2016-02-04 02:28:01.484228: step 125980, loss = 0.77 (264.3 examples/sec; 0.484 sec/batch)
2016-02-04 02:28:06.198644: step 125990, loss = 0.62 (268.8 examples/sec; 0.476 sec/batch)
2016-02-04 02:28:10.876196: step 126000, loss = 0.68 (264.8 examples/sec; 0.483 sec/batch)
2016-02-04 02:28:16.118068: step 126010, loss = 0.73 (264.3 examples/sec; 0.484 sec/batch)
2016-02-04 02:28:20.802203: step 126020, loss = 0.69 (273.7 examples/sec; 0.468 sec/batch)
2016-02-04 02:28:25.402918: step 126030, loss = 0.62 (286.4 examples/sec; 0.447 sec/batch)
2016-02-04 02:28:30.133654: step 126040, loss = 0.68 (280.7 examples/sec; 0.456 sec/batch)
2016-02-04 02:28:34.841432: step 126050, loss = 0.60 (261.1 examples/sec; 0.490 sec/batch)
2016-02-04 02:28:39.559663: step 126060, loss = 0.75 (288.6 examples/sec; 0.444 sec/batch)
2016-02-04 02:28:44.307197: step 126070, loss = 0.68 (272.3 examples/sec; 0.470 sec/batch)
2016-02-04 02:28:49.070583: step 126080, loss = 0.77 (268.9 examples/sec; 0.476 sec/batch)
2016-02-04 02:28:53.793458: step 126090, loss = 0.62 (284.0 examples/sec; 0.451 sec/batch)
2016-02-04 02:28:58.542437: step 126100, loss = 0.72 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 02:29:03.796391: step 126110, loss = 0.60 (264.6 examples/sec; 0.484 sec/batch)
2016-02-04 02:29:08.550870: step 126120, loss = 0.81 (252.0 examples/sec; 0.508 sec/batch)
2016-02-04 02:29:13.324077: step 126130, loss = 0.78 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 02:29:17.966714: step 126140, loss = 0.62 (261.0 examples/sec; 0.490 sec/batch)
2016-02-04 02:29:22.667995: step 126150, loss = 0.84 (296.0 examples/sec; 0.432 sec/batch)
2016-02-04 02:29:27.406209: step 126160, loss = 0.69 (245.5 examples/sec; 0.521 sec/batch)
2016-02-04 02:29:32.117960: step 126170, loss = 0.79 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 02:29:36.851915: step 126180, loss = 0.71 (251.3 examples/sec; 0.509 sec/batch)
2016-02-04 02:29:41.533491: step 126190, loss = 0.55 (290.5 examples/sec; 0.441 sec/batch)
2016-02-04 02:29:46.269504: step 126200, loss = 0.62 (238.2 examples/sec; 0.537 sec/batch)
2016-02-04 02:29:51.456352: step 126210, loss = 0.60 (268.8 examples/sec; 0.476 sec/batch)
2016-02-04 02:29:56.201467: step 126220, loss = 0.69 (256.4 examples/sec; 0.499 sec/batch)
2016-02-04 02:30:01.005032: step 126230, loss = 0.65 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 02:30:05.830270: step 126240, loss = 0.63 (249.5 examples/sec; 0.513 sec/batch)
2016-02-04 02:30:10.463975: step 126250, loss = 0.73 (281.8 examples/sec; 0.454 sec/batch)
2016-02-04 02:30:15.206638: step 126260, loss = 0.72 (276.3 examples/sec; 0.463 sec/batch)
2016-02-04 02:30:20.010181: step 126270, loss = 0.64 (287.0 examples/sec; 0.446 sec/batch)
2016-02-04 02:30:24.737958: step 126280, loss = 0.65 (256.1 examples/sec; 0.500 sec/batch)
2016-02-04 02:30:29.500066: step 126290, loss = 0.66 (268.2 examples/sec; 0.477 sec/batch)
2016-02-04 02:30:34.225699: step 126300, loss = 0.66 (286.6 examples/sec; 0.447 sec/batch)
2016-02-04 02:30:39.444129: step 126310, loss = 0.64 (277.9 examples/sec; 0.461 sec/batch)
2016-02-04 02:30:44.201252: step 126320, loss = 0.73 (256.0 examples/sec; 0.500 sec/batch)
2016-02-04 02:30:49.001175: step 126330, loss = 0.64 (251.2 examples/sec; 0.509 sec/batch)
2016-02-04 02:30:53.723959: step 126340, loss = 0.71 (287.2 examples/sec; 0.446 sec/batch)
2016-02-04 02:30:58.427854: step 126350, loss = 0.71 (294.4 examples/sec; 0.435 sec/batch)
2016-02-04 02:31:03.225627: step 126360, loss = 0.68 (257.3 examples/sec; 0.497 sec/batch)
2016-02-04 02:31:07.929173: step 126370, loss = 0.74 (266.7 examples/sec; 0.480 sec/batch)
2016-02-04 02:31:12.589674: step 126380, loss = 0.83 (286.3 examples/sec; 0.447 sec/batch)
2016-02-04 02:31:17.306587: step 126390, loss = 0.62 (266.4 examples/sec; 0.480 sec/batch)
2016-02-04 02:31:22.013294: step 126400, loss = 0.63 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 02:31:27.193283: step 126410, loss = 0.81 (283.2 examples/sec; 0.452 sec/batch)
2016-02-04 02:31:32.005904: step 126420, loss = 0.69 (251.9 examples/sec; 0.508 sec/batch)
2016-02-04 02:31:36.699989: step 126430, loss = 0.70 (263.7 examples/sec; 0.485 sec/batch)
2016-02-04 02:31:41.307062: step 126440, loss = 0.65 (264.9 examples/sec; 0.483 sec/batch)
2016-02-04 02:31:46.008932: step 126450, loss = 0.63 (291.9 examples/sec; 0.439 sec/batch)
2016-02-04 02:31:50.711801: step 126460, loss = 0.71 (281.7 examples/sec; 0.454 sec/batch)
2016-02-04 02:31:55.441709: step 126470, loss = 0.85 (267.0 examples/sec; 0.479 sec/batch)
2016-02-04 02:32:00.113880: step 126480, loss = 0.65 (269.7 examples/sec; 0.475 sec/batch)
2016-02-04 02:32:04.797933: step 126490, loss = 0.60 (270.0 examples/sec; 0.474 sec/batch)
2016-02-04 02:32:09.468007: step 126500, loss = 0.60 (259.0 examples/sec; 0.494 sec/batch)
2016-02-04 02:32:14.681770: step 126510, loss = 0.67 (259.7 examples/sec; 0.493 sec/batch)
2016-02-04 02:32:19.326382: step 126520, loss = 0.78 (284.2 examples/sec; 0.450 sec/batch)
2016-02-04 02:32:24.037828: step 126530, loss = 0.71 (272.8 examples/sec; 0.469 sec/batch)
2016-02-04 02:32:28.669955: step 126540, loss = 0.82 (264.0 examples/sec; 0.485 sec/batch)
2016-02-04 02:32:33.313807: step 126550, loss = 0.97 (283.3 examples/sec; 0.452 sec/batch)
2016-02-04 02:32:38.010165: step 126560, loss = 0.64 (248.9 examples/sec; 0.514 sec/batch)
2016-02-04 02:32:42.678568: step 126570, loss = 0.76 (284.3 examples/sec; 0.450 sec/batch)
2016-02-04 02:32:47.372846: step 126580, loss = 0.63 (287.4 examples/sec; 0.445 sec/batch)
2016-02-04 02:32:52.022878: step 126590, loss = 0.79 (298.2 examples/sec; 0.429 sec/batch)
2016-02-04 02:32:56.828665: step 126600, loss = 0.80 (257.8 examples/sec; 0.496 sec/batch)
2016-02-04 02:33:02.045659: step 126610, loss = 0.73 (251.5 examples/sec; 0.509 sec/batch)
2016-02-04 02:33:06.700690: step 126620, loss = 0.68 (289.2 examples/sec; 0.443 sec/batch)
2016-02-04 02:33:11.519000: step 126630, loss = 0.58 (256.4 examples/sec; 0.499 sec/batch)
2016-02-04 02:33:16.096327: step 126640, loss = 0.69 (293.9 examples/sec; 0.436 sec/batch)
2016-02-04 02:33:20.832707: step 126650, loss = 0.68 (254.3 examples/sec; 0.503 sec/batch)
2016-02-04 02:33:25.498804: step 126660, loss = 0.51 (275.0 examples/sec; 0.466 sec/batch)
2016-02-04 02:33:30.161561: step 126670, loss = 0.71 (307.1 examples/sec; 0.417 sec/batch)
2016-02-04 02:33:34.769755: step 126680, loss = 0.56 (291.2 examples/sec; 0.440 sec/batch)
2016-02-04 02:33:39.500112: step 126690, loss = 0.89 (263.1 examples/sec; 0.487 sec/batch)
2016-02-04 02:33:44.065964: step 126700, loss = 0.72 (258.2 examples/sec; 0.496 sec/batch)
2016-02-04 02:33:49.293840: step 126710, loss = 0.68 (270.9 examples/sec; 0.472 sec/batch)
2016-02-04 02:33:53.977755: step 126720, loss = 0.62 (310.4 examples/sec; 0.412 sec/batch)
2016-02-04 02:33:58.640131: step 126730, loss = 0.69 (279.1 examples/sec; 0.459 sec/batch)
2016-02-04 02:34:03.347024: step 126740, loss = 0.66 (262.2 examples/sec; 0.488 sec/batch)
2016-02-04 02:34:08.088706: step 126750, loss = 0.74 (280.7 examples/sec; 0.456 sec/batch)
2016-02-04 02:34:12.790707: step 126760, loss = 0.55 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 02:34:17.448198: step 126770, loss = 0.73 (264.7 examples/sec; 0.483 sec/batch)
2016-02-04 02:34:22.183506: step 126780, loss = 0.65 (278.7 examples/sec; 0.459 sec/batch)
2016-02-04 02:34:26.874490: step 126790, loss = 0.75 (272.9 examples/sec; 0.469 sec/batch)
2016-02-04 02:34:31.640079: step 126800, loss = 0.80 (253.1 examples/sec; 0.506 sec/batch)
2016-02-04 02:34:36.899409: step 126810, loss = 0.66 (270.3 examples/sec; 0.474 sec/batch)
2016-02-04 02:34:41.608161: step 126820, loss = 0.64 (284.3 examples/sec; 0.450 sec/batch)
2016-02-04 02:34:46.326677: step 126830, loss = 0.90 (274.2 examples/sec; 0.467 sec/batch)
2016-02-04 02:34:51.042173: step 126840, loss = 0.76 (265.8 examples/sec; 0.482 sec/batch)
2016-02-04 02:34:55.728021: step 126850, loss = 0.52 (264.1 examples/sec; 0.485 sec/batch)
2016-02-04 02:35:00.468489: step 126860, loss = 0.60 (261.7 examples/sec; 0.489 sec/batch)
2016-02-04 02:35:05.263969: step 126870, loss = 0.74 (268.8 examples/sec; 0.476 sec/batch)
2016-02-04 02:35:09.984451: step 126880, loss = 0.76 (297.4 examples/sec; 0.430 sec/batch)
2016-02-04 02:35:14.770012: step 126890, loss = 0.62 (270.8 examples/sec; 0.473 sec/batch)
2016-02-04 02:35:19.448986: step 126900, loss = 0.61 (293.6 examples/sec; 0.436 sec/batch)
2016-02-04 02:35:24.646435: step 126910, loss = 0.53 (277.0 examples/sec; 0.462 sec/batch)
2016-02-04 02:35:29.307549: step 126920, loss = 0.60 (293.6 examples/sec; 0.436 sec/batch)
2016-02-04 02:35:34.075830: step 126930, loss = 0.67 (264.9 examples/sec; 0.483 sec/batch)
2016-02-04 02:35:38.845127: step 126940, loss = 0.83 (249.0 examples/sec; 0.514 sec/batch)
2016-02-04 02:35:43.556889: step 126950, loss = 0.59 (274.4 examples/sec; 0.466 sec/batch)
2016-02-04 02:35:48.317679: step 126960, loss = 0.81 (279.1 examples/sec; 0.459 sec/batch)
2016-02-04 02:35:53.054509: step 126970, loss = 0.76 (264.6 examples/sec; 0.484 sec/batch)
2016-02-04 02:35:57.735387: step 126980, loss = 0.78 (277.0 examples/sec; 0.462 sec/batch)
2016-02-04 02:36:02.360042: step 126990, loss = 0.63 (291.3 examples/sec; 0.439 sec/batch)
2016-02-04 02:36:07.054724: step 127000, loss = 0.66 (265.7 examples/sec; 0.482 sec/batch)
2016-02-04 02:36:12.297335: step 127010, loss = 0.75 (268.1 examples/sec; 0.477 sec/batch)
2016-02-04 02:36:16.949994: step 127020, loss = 0.55 (266.5 examples/sec; 0.480 sec/batch)
2016-02-04 02:36:21.678147: step 127030, loss = 0.64 (259.7 examples/sec; 0.493 sec/batch)
2016-02-04 02:36:26.391504: step 127040, loss = 0.69 (271.0 examples/sec; 0.472 sec/batch)
2016-02-04 02:36:31.095005: step 127050, loss = 0.58 (264.4 examples/sec; 0.484 sec/batch)
2016-02-04 02:36:35.766292: step 127060, loss = 0.76 (258.1 examples/sec; 0.496 sec/batch)
2016-02-04 02:36:40.424235: step 127070, loss = 0.70 (273.1 examples/sec; 0.469 sec/batch)
2016-02-04 02:36:45.105254: step 127080, loss = 0.72 (278.0 examples/sec; 0.460 sec/batch)
2016-02-04 02:36:49.807134: step 127090, loss = 0.72 (276.8 examples/sec; 0.463 sec/batch)
2016-02-04 02:36:54.531636: step 127100, loss = 0.67 (273.7 examples/sec; 0.468 sec/batch)
2016-02-04 02:36:59.660270: step 127110, loss = 0.56 (267.6 examples/sec; 0.478 sec/batch)
2016-02-04 02:37:04.352509: step 127120, loss = 0.60 (267.4 examples/sec; 0.479 sec/batch)
2016-02-04 02:37:09.059900: step 127130, loss = 0.66 (277.9 examples/sec; 0.461 sec/batch)
2016-02-04 02:37:13.819011: step 127140, loss = 0.63 (306.7 examples/sec; 0.417 sec/batch)
2016-02-04 02:37:18.375689: step 127150, loss = 0.66 (277.9 examples/sec; 0.461 sec/batch)
2016-02-04 02:37:23.083080: step 127160, loss = 0.65 (246.6 examples/sec; 0.519 sec/batch)
2016-02-04 02:37:27.870043: step 127170, loss = 0.65 (250.0 examples/sec; 0.512 sec/batch)
2016-02-04 02:37:32.566319: step 127180, loss = 0.61 (287.2 examples/sec; 0.446 sec/batch)
2016-02-04 02:37:37.286764: step 127190, loss = 0.63 (281.0 examples/sec; 0.456 sec/batch)
2016-02-04 02:37:42.063217: step 127200, loss = 0.64 (256.7 examples/sec; 0.499 sec/batch)
2016-02-04 02:37:47.283979: step 127210, loss = 0.73 (265.5 examples/sec; 0.482 sec/batch)
2016-02-04 02:37:51.998476: step 127220, loss = 0.57 (253.5 examples/sec; 0.505 sec/batch)
2016-02-04 02:37:56.746038: step 127230, loss = 0.74 (273.1 examples/sec; 0.469 sec/batch)
2016-02-04 02:38:01.600480: step 127240, loss = 0.58 (237.1 examples/sec; 0.540 sec/batch)
2016-02-04 02:38:06.242669: step 127250, loss = 0.74 (297.0 examples/sec; 0.431 sec/batch)
2016-02-04 02:38:10.966787: step 127260, loss = 0.63 (271.2 examples/sec; 0.472 sec/batch)
2016-02-04 02:38:15.663524: step 127270, loss = 0.69 (262.3 examples/sec; 0.488 sec/batch)
2016-02-04 02:38:20.354211: step 127280, loss = 0.79 (264.7 examples/sec; 0.483 sec/batch)
2016-02-04 02:38:25.137413: step 127290, loss = 0.58 (259.9 examples/sec; 0.493 sec/batch)
2016-02-04 02:38:29.927697: step 127300, loss = 0.64 (266.8 examples/sec; 0.480 sec/batch)
2016-02-04 02:38:35.156223: step 127310, loss = 0.70 (258.8 examples/sec; 0.495 sec/batch)
2016-02-04 02:38:39.901812: step 127320, loss = 0.69 (276.4 examples/sec; 0.463 sec/batch)
2016-02-04 02:38:44.595916: step 127330, loss = 0.67 (274.6 examples/sec; 0.466 sec/batch)
2016-02-04 02:38:49.307963: step 127340, loss = 0.69 (286.3 examples/sec; 0.447 sec/batch)
2016-02-04 02:38:53.999081: step 127350, loss = 0.71 (261.0 examples/sec; 0.490 sec/batch)
2016-02-04 02:38:58.763833: step 127360, loss = 0.83 (291.3 examples/sec; 0.439 sec/batch)
2016-02-04 02:39:03.435866: step 127370, loss = 0.76 (268.1 examples/sec; 0.477 sec/batch)
2016-02-04 02:39:08.120821: step 127380, loss = 0.67 (275.2 examples/sec; 0.465 sec/batch)
2016-02-04 02:39:12.811466: step 127390, loss = 0.78 (277.3 examples/sec; 0.462 sec/batch)
2016-02-04 02:39:17.595580: step 127400, loss = 0.67 (262.1 examples/sec; 0.488 sec/batch)
2016-02-04 02:39:22.798702: step 127410, loss = 0.66 (261.1 examples/sec; 0.490 sec/batch)
2016-02-04 02:39:27.481642: step 127420, loss = 0.59 (267.7 examples/sec; 0.478 sec/batch)
2016-02-04 02:39:32.147697: step 127430, loss = 0.63 (284.4 examples/sec; 0.450 sec/batch)
2016-02-04 02:39:36.857170: step 127440, loss = 0.87 (264.6 examples/sec; 0.484 sec/batch)
2016-02-04 02:39:41.650146: step 127450, loss = 0.67 (255.7 examples/sec; 0.501 sec/batch)
2016-02-04 02:39:46.313752: step 127460, loss = 0.73 (269.5 examples/sec; 0.475 sec/batch)
2016-02-04 02:39:50.997255: step 127470, loss = 0.73 (274.9 examples/sec; 0.466 sec/batch)
2016-02-04 02:39:55.750731: step 127480, loss = 0.70 (279.9 examples/sec; 0.457 sec/batch)
2016-02-04 02:40:00.482906: step 127490, loss = 0.68 (271.1 examples/sec; 0.472 sec/batch)
2016-02-04 02:40:05.187512: step 127500, loss = 0.78 (247.1 examples/sec; 0.518 sec/batch)
2016-02-04 02:40:10.428335: step 127510, loss = 0.65 (272.0 examples/sec; 0.471 sec/batch)
2016-02-04 02:40:15.120756: step 127520, loss = 0.61 (266.9 examples/sec; 0.480 sec/batch)
2016-02-04 02:40:19.928350: step 127530, loss = 0.59 (272.4 examples/sec; 0.470 sec/batch)
2016-02-04 02:40:24.657154: step 127540, loss = 0.63 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 02:40:29.323291: step 127550, loss = 0.55 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 02:40:34.068247: step 127560, loss = 0.80 (265.0 examples/sec; 0.483 sec/batch)
2016-02-04 02:40:38.814796: step 127570, loss = 0.60 (247.4 examples/sec; 0.517 sec/batch)
2016-02-04 02:40:43.536811: step 127580, loss = 0.75 (274.8 examples/sec; 0.466 sec/batch)
2016-02-04 02:40:48.298103: step 127590, loss = 0.76 (257.4 examples/sec; 0.497 sec/batch)
2016-02-04 02:40:52.983459: step 127600, loss = 0.71 (298.9 examples/sec; 0.428 sec/batch)
2016-02-04 02:40:58.244643: step 127610, loss = 0.61 (288.8 examples/sec; 0.443 sec/batch)
2016-02-04 02:41:02.986527: step 127620, loss = 0.56 (278.4 examples/sec; 0.460 sec/batch)
2016-02-04 02:41:07.759913: step 127630, loss = 0.68 (285.3 examples/sec; 0.449 sec/batch)
2016-02-04 02:41:12.475156: step 127640, loss = 0.79 (303.0 examples/sec; 0.423 sec/batch)
2016-02-04 02:41:17.220915: step 127650, loss = 0.63 (272.5 examples/sec; 0.470 sec/batch)
2016-02-04 02:41:21.848118: step 127660, loss = 0.71 (290.5 examples/sec; 0.441 sec/batch)
2016-02-04 02:41:26.567550: step 127670, loss = 0.53 (281.2 examples/sec; 0.455 sec/batch)
2016-02-04 02:41:31.232745: step 127680, loss = 0.70 (270.1 examples/sec; 0.474 sec/batch)
2016-02-04 02:41:35.903013: step 127690, loss = 0.83 (293.5 examples/sec; 0.436 sec/batch)
2016-02-04 02:41:40.648680: step 127700, loss = 0.65 (274.5 examples/sec; 0.466 sec/batch)
2016-02-04 02:41:45.866599: step 127710, loss = 0.80 (301.1 examples/sec; 0.425 sec/batch)
2016-02-04 02:41:50.624724: step 127720, loss = 0.63 (262.2 examples/sec; 0.488 sec/batch)
2016-02-04 02:41:55.262337: step 127730, loss = 0.56 (254.2 examples/sec; 0.504 sec/batch)
2016-02-04 02:41:59.956192: step 127740, loss = 0.68 (287.5 examples/sec; 0.445 sec/batch)
2016-02-04 02:42:04.704037: step 127750, loss = 0.74 (263.7 examples/sec; 0.485 sec/batch)
2016-02-04 02:42:09.387894: step 127760, loss = 0.74 (282.7 examples/sec; 0.453 sec/batch)
2016-02-04 02:42:14.114562: step 127770, loss = 0.67 (271.8 examples/sec; 0.471 sec/batch)
2016-02-04 02:42:18.820032: step 127780, loss = 0.87 (300.2 examples/sec; 0.426 sec/batch)
2016-02-04 02:42:23.401951: step 127790, loss = 0.57 (309.8 examples/sec; 0.413 sec/batch)
2016-02-04 02:42:28.140246: step 127800, loss = 0.56 (263.2 examples/sec; 0.486 sec/batch)
2016-02-04 02:42:33.387256: step 127810, loss = 0.65 (271.7 examples/sec; 0.471 sec/batch)
2016-02-04 02:42:38.102819: step 127820, loss = 0.52 (256.8 examples/sec; 0.498 sec/batch)
2016-02-04 02:42:42.815456: step 127830, loss = 0.59 (253.0 examples/sec; 0.506 sec/batch)
2016-02-04 02:42:47.498946: step 127840, loss = 0.65 (263.0 examples/sec; 0.487 sec/batch)
2016-02-04 02:42:52.249137: step 127850, loss = 0.72 (245.7 examples/sec; 0.521 sec/batch)
2016-02-04 02:42:56.919024: step 127860, loss = 0.66 (311.1 examples/sec; 0.411 sec/batch)
2016-02-04 02:43:01.632316: step 127870, loss = 0.78 (277.5 examples/sec; 0.461 sec/batch)
2016-02-04 02:43:06.291274: step 127880, loss = 0.59 (291.9 examples/sec; 0.438 sec/batch)
2016-02-04 02:43:11.041705: step 127890, loss = 0.57 (262.7 examples/sec; 0.487 sec/batch)
2016-02-04 02:43:15.733176: step 127900, loss = 0.60 (295.2 examples/sec; 0.434 sec/batch)
2016-02-04 02:43:20.973661: step 127910, loss = 0.60 (260.4 examples/sec; 0.492 sec/batch)
2016-02-04 02:43:25.703646: step 127920, loss = 0.66 (271.5 examples/sec; 0.471 sec/batch)
2016-02-04 02:43:30.423081: step 127930, loss = 0.75 (290.3 examples/sec; 0.441 sec/batch)
2016-02-04 02:43:35.179348: step 127940, loss = 0.70 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 02:43:39.867613: step 127950, loss = 0.58 (261.4 examples/sec; 0.490 sec/batch)
2016-02-04 02:43:44.588604: step 127960, loss = 0.68 (301.3 examples/sec; 0.425 sec/batch)
2016-02-04 02:43:49.295922: step 127970, loss = 0.76 (267.0 examples/sec; 0.479 sec/batch)
2016-02-04 02:43:53.896309: step 127980, loss = 0.62 (282.4 examples/sec; 0.453 sec/batch)
2016-02-04 02:43:58.544699: step 127990, loss = 0.67 (280.2 examples/sec; 0.457 sec/batch)
2016-02-04 02:44:03.147011: step 128000, loss = 0.61 (250.6 examples/sec; 0.511 sec/batch)
2016-02-04 02:44:08.285563: step 128010, loss = 0.66 (269.3 examples/sec; 0.475 sec/batch)
2016-02-04 02:44:12.914738: step 128020, loss = 0.55 (282.1 examples/sec; 0.454 sec/batch)
2016-02-04 02:44:17.621805: step 128030, loss = 0.63 (255.8 examples/sec; 0.500 sec/batch)
2016-02-04 02:44:22.290970: step 128040, loss = 0.58 (299.4 examples/sec; 0.428 sec/batch)
2016-02-04 02:44:27.063433: step 128050, loss = 0.80 (263.6 examples/sec; 0.486 sec/batch)
2016-02-04 02:44:31.781736: step 128060, loss = 0.66 (276.0 examples/sec; 0.464 sec/batch)
2016-02-04 02:44:36.529336: step 128070, loss = 0.66 (281.8 examples/sec; 0.454 sec/batch)
2016-02-04 02:44:41.179139: step 128080, loss = 0.77 (268.6 examples/sec; 0.477 sec/batch)
2016-02-04 02:44:45.893103: step 128090, loss = 0.50 (291.4 examples/sec; 0.439 sec/batch)
2016-02-04 02:44:50.595442: step 128100, loss = 0.65 (270.1 examples/sec; 0.474 sec/batch)
2016-02-04 02:44:55.728261: step 128110, loss = 0.71 (290.4 examples/sec; 0.441 sec/batch)
2016-02-04 02:45:00.432681: step 128120, loss = 0.67 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 02:45:05.083028: step 128130, loss = 0.68 (265.5 examples/sec; 0.482 sec/batch)
2016-02-04 02:45:09.802719: step 128140, loss = 0.68 (259.1 examples/sec; 0.494 sec/batch)
2016-02-04 02:45:14.405948: step 128150, loss = 0.74 (278.6 examples/sec; 0.460 sec/batch)
2016-02-04 02:45:19.185854: step 128160, loss = 0.51 (284.7 examples/sec; 0.450 sec/batch)
2016-02-04 02:45:23.905959: step 128170, loss = 0.85 (283.5 examples/sec; 0.452 sec/batch)
2016-02-04 02:45:28.673621: step 128180, loss = 0.59 (264.0 examples/sec; 0.485 sec/batch)
2016-02-04 02:45:33.331817: step 128190, loss = 0.70 (285.8 examples/sec; 0.448 sec/batch)
2016-02-04 02:45:38.140613: step 128200, loss = 0.68 (259.1 examples/sec; 0.494 sec/batch)
2016-02-04 02:45:43.436324: step 128210, loss = 0.78 (251.9 examples/sec; 0.508 sec/batch)
2016-02-04 02:45:48.121489: step 128220, loss = 0.56 (255.4 examples/sec; 0.501 sec/batch)
2016-02-04 02:45:52.795319: step 128230, loss = 0.70 (260.6 examples/sec; 0.491 sec/batch)
2016-02-04 02:45:57.506216: step 128240, loss = 0.71 (250.9 examples/sec; 0.510 sec/batch)
2016-02-04 02:46:02.235339: step 128250, loss = 0.63 (276.1 examples/sec; 0.464 sec/batch)
2016-02-04 02:46:06.977640: step 128260, loss = 0.73 (275.6 examples/sec; 0.465 sec/batch)
2016-02-04 02:46:11.625528: step 128270, loss = 0.70 (268.0 examples/sec; 0.478 sec/batch)
2016-02-04 02:46:16.337383: step 128280, loss = 0.64 (284.7 examples/sec; 0.450 sec/batch)
2016-02-04 02:46:21.022421: step 128290, loss = 0.60 (278.4 examples/sec; 0.460 sec/batch)
2016-02-04 02:46:25.688012: step 128300, loss = 0.55 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 02:46:30.841964: step 128310, loss = 0.77 (283.0 examples/sec; 0.452 sec/batch)
2016-02-04 02:46:35.531663: step 128320, loss = 0.73 (244.7 examples/sec; 0.523 sec/batch)
2016-02-04 02:46:40.247239: step 128330, loss = 0.63 (281.2 examples/sec; 0.455 sec/batch)
2016-02-04 02:46:44.984826: step 128340, loss = 0.76 (273.5 examples/sec; 0.468 sec/batch)
2016-02-04 02:46:49.731363: step 128350, loss = 0.66 (276.6 examples/sec; 0.463 sec/batch)
2016-02-04 02:46:54.439372: step 128360, loss = 0.72 (283.3 examples/sec; 0.452 sec/batch)
2016-02-04 02:46:59.188599: step 128370, loss = 0.72 (270.5 examples/sec; 0.473 sec/batch)
2016-02-04 02:47:03.960180: step 128380, loss = 0.57 (258.5 examples/sec; 0.495 sec/batch)
2016-02-04 02:47:08.675167: step 128390, loss = 0.59 (253.1 examples/sec; 0.506 sec/batch)
2016-02-04 02:47:13.365278: step 128400, loss = 0.64 (262.4 examples/sec; 0.488 sec/batch)
2016-02-04 02:47:18.578866: step 128410, loss = 0.65 (259.4 examples/sec; 0.493 sec/batch)
2016-02-04 02:47:23.241133: step 128420, loss = 0.64 (273.5 examples/sec; 0.468 sec/batch)
2016-02-04 02:47:27.901845: step 128430, loss = 0.74 (299.5 examples/sec; 0.427 sec/batch)
2016-02-04 02:47:32.689007: step 128440, loss = 0.67 (246.1 examples/sec; 0.520 sec/batch)
2016-02-04 02:47:37.335846: step 128450, loss = 0.69 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 02:47:42.025037: step 128460, loss = 0.72 (260.9 examples/sec; 0.491 sec/batch)
2016-02-04 02:47:46.688400: step 128470, loss = 0.81 (279.1 examples/sec; 0.459 sec/batch)
2016-02-04 02:47:51.387132: step 128480, loss = 0.81 (288.9 examples/sec; 0.443 sec/batch)
2016-02-04 02:47:56.175919: step 128490, loss = 0.77 (273.2 examples/sec; 0.469 sec/batch)
2016-02-04 02:48:00.900203: step 128500, loss = 0.69 (271.3 examples/sec; 0.472 sec/batch)
2016-02-04 02:48:06.187651: step 128510, loss = 0.64 (226.3 examples/sec; 0.566 sec/batch)
2016-02-04 02:48:10.854807: step 128520, loss = 0.74 (279.7 examples/sec; 0.458 sec/batch)
2016-02-04 02:48:15.634748: step 128530, loss = 0.76 (244.6 examples/sec; 0.523 sec/batch)
2016-02-04 02:48:20.405968: step 128540, loss = 0.73 (263.5 examples/sec; 0.486 sec/batch)
2016-02-04 02:48:25.120184: step 128550, loss = 0.71 (270.4 examples/sec; 0.473 sec/batch)
2016-02-04 02:48:29.903800: step 128560, loss = 0.90 (264.2 examples/sec; 0.484 sec/batch)
2016-02-04 02:48:34.676463: step 128570, loss = 0.63 (260.7 examples/sec; 0.491 sec/batch)
2016-02-04 02:48:39.367142: step 128580, loss = 0.70 (273.5 examples/sec; 0.468 sec/batch)
2016-02-04 02:48:44.045398: step 128590, loss = 0.61 (296.1 examples/sec; 0.432 sec/batch)
2016-02-04 02:48:48.742673: step 128600, loss = 0.68 (263.0 examples/sec; 0.487 sec/batch)
2016-02-04 02:48:54.069235: step 128610, loss = 0.54 (275.4 examples/sec; 0.465 sec/batch)
2016-02-04 02:48:58.773426: step 128620, loss = 0.63 (270.1 examples/sec; 0.474 sec/batch)
2016-02-04 02:49:03.483633: step 128630, loss = 0.74 (267.1 examples/sec; 0.479 sec/batch)
2016-02-04 02:49:08.241717: step 128640, loss = 0.66 (285.4 examples/sec; 0.448 sec/batch)
2016-02-04 02:49:12.853726: step 128650, loss = 0.52 (264.8 examples/sec; 0.483 sec/batch)
2016-02-04 02:49:17.624576: step 128660, loss = 0.63 (249.8 examples/sec; 0.512 sec/batch)
2016-02-04 02:49:22.290037: step 128670, loss = 0.63 (296.5 examples/sec; 0.432 sec/batch)
2016-02-04 02:49:27.010042: step 128680, loss = 0.68 (261.0 examples/sec; 0.490 sec/batch)
2016-02-04 02:49:31.785199: step 128690, loss = 0.67 (264.8 examples/sec; 0.483 sec/batch)
2016-02-04 02:49:36.537021: step 128700, loss = 0.57 (274.8 examples/sec; 0.466 sec/batch)
2016-02-04 02:49:41.781575: step 128710, loss = 0.57 (280.6 examples/sec; 0.456 sec/batch)
2016-02-04 02:49:46.494496: step 128720, loss = 0.73 (283.7 examples/sec; 0.451 sec/batch)
2016-02-04 02:49:51.240721: step 128730, loss = 0.56 (273.3 examples/sec; 0.468 sec/batch)
2016-02-04 02:49:55.979467: step 128740, loss = 0.61 (268.2 examples/sec; 0.477 sec/batch)
2016-02-04 02:50:00.706823: step 128750, loss = 0.69 (252.4 examples/sec; 0.507 sec/batch)
2016-02-04 02:50:05.365804: step 128760, loss = 0.71 (267.1 examples/sec; 0.479 sec/batch)
2016-02-04 02:50:10.080086: step 128770, loss = 0.66 (272.8 examples/sec; 0.469 sec/batch)
2016-02-04 02:50:14.791761: step 128780, loss = 0.66 (288.6 examples/sec; 0.444 sec/batch)
2016-02-04 02:50:19.574808: step 128790, loss = 0.77 (272.8 examples/sec; 0.469 sec/batch)
2016-02-04 02:50:24.294222: step 128800, loss = 0.70 (275.8 examples/sec; 0.464 sec/batch)
2016-02-04 02:50:29.555684: step 128810, loss = 0.77 (261.2 examples/sec; 0.490 sec/batch)
2016-02-04 02:50:34.229878: step 128820, loss = 0.71 (280.8 examples/sec; 0.456 sec/batch)
2016-02-04 02:50:38.908346: step 128830, loss = 0.85 (281.5 examples/sec; 0.455 sec/batch)
2016-02-04 02:50:43.602210: step 128840, loss = 0.64 (266.5 examples/sec; 0.480 sec/batch)
2016-02-04 02:50:48.309350: step 128850, loss = 0.74 (302.7 examples/sec; 0.423 sec/batch)
2016-02-04 02:50:53.100414: step 128860, loss = 0.55 (244.7 examples/sec; 0.523 sec/batch)
2016-02-04 02:50:57.826935: step 128870, loss = 0.67 (264.2 examples/sec; 0.485 sec/batch)
2016-02-04 02:51:02.506894: step 128880, loss = 0.78 (276.4 examples/sec; 0.463 sec/batch)
2016-02-04 02:51:07.208999: step 128890, loss = 0.65 (288.5 examples/sec; 0.444 sec/batch)
2016-02-04 02:51:11.939288: step 128900, loss = 0.62 (266.9 examples/sec; 0.480 sec/batch)
2016-02-04 02:51:17.223950: step 128910, loss = 0.67 (251.6 examples/sec; 0.509 sec/batch)
2016-02-04 02:51:21.844421: step 128920, loss = 0.70 (282.3 examples/sec; 0.453 sec/batch)
2016-02-04 02:51:26.534023: step 128930, loss = 0.74 (272.7 examples/sec; 0.469 sec/batch)
2016-02-04 02:51:31.234781: step 128940, loss = 0.62 (256.6 examples/sec; 0.499 sec/batch)
2016-02-04 02:51:35.969883: step 128950, loss = 0.64 (278.7 examples/sec; 0.459 sec/batch)
2016-02-04 02:51:40.732976: step 128960, loss = 0.68 (273.2 examples/sec; 0.469 sec/batch)
2016-02-04 02:51:45.431709: step 128970, loss = 0.71 (286.2 examples/sec; 0.447 sec/batch)
2016-02-04 02:51:50.197209: step 128980, loss = 0.61 (270.4 examples/sec; 0.473 sec/batch)
2016-02-04 02:51:54.864387: step 128990, loss = 0.65 (260.7 examples/sec; 0.491 sec/batch)
2016-02-04 02:51:59.631541: step 129000, loss = 0.64 (260.7 examples/sec; 0.491 sec/batch)
2016-02-04 02:52:04.894929: step 129010, loss = 0.79 (293.2 examples/sec; 0.437 sec/batch)
2016-02-04 02:52:09.549400: step 129020, loss = 0.65 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 02:52:14.276952: step 129030, loss = 0.62 (273.5 examples/sec; 0.468 sec/batch)
2016-02-04 02:52:18.936841: step 129040, loss = 0.70 (292.3 examples/sec; 0.438 sec/batch)
2016-02-04 02:52:23.738075: step 129050, loss = 0.51 (281.4 examples/sec; 0.455 sec/batch)
2016-02-04 02:52:28.434560: step 129060, loss = 0.70 (290.7 examples/sec; 0.440 sec/batch)
2016-02-04 02:52:33.153921: step 129070, loss = 0.71 (272.3 examples/sec; 0.470 sec/batch)
2016-02-04 02:52:37.793465: step 129080, loss = 0.66 (288.6 examples/sec; 0.444 sec/batch)
2016-02-04 02:52:42.508255: step 129090, loss = 0.80 (279.7 examples/sec; 0.458 sec/batch)
2016-02-04 02:52:47.252852: step 129100, loss = 0.72 (252.7 examples/sec; 0.507 sec/batch)
2016-02-04 02:52:52.503366: step 129110, loss = 0.79 (257.9 examples/sec; 0.496 sec/batch)
2016-02-04 02:52:57.248299: step 129120, loss = 0.69 (240.3 examples/sec; 0.533 sec/batch)
2016-02-04 02:53:01.970112: step 129130, loss = 0.63 (254.1 examples/sec; 0.504 sec/batch)
2016-02-04 02:53:06.695736: step 129140, loss = 0.64 (253.6 examples/sec; 0.505 sec/batch)
2016-02-04 02:53:11.464119: step 129150, loss = 0.71 (253.6 examples/sec; 0.505 sec/batch)
2016-02-04 02:53:16.101659: step 129160, loss = 0.57 (274.4 examples/sec; 0.466 sec/batch)
2016-02-04 02:53:20.753523: step 129170, loss = 0.65 (264.1 examples/sec; 0.485 sec/batch)
2016-02-04 02:53:25.495393: step 129180, loss = 0.73 (274.3 examples/sec; 0.467 sec/batch)
2016-02-04 02:53:30.307878: step 129190, loss = 0.73 (259.5 examples/sec; 0.493 sec/batch)
2016-02-04 02:53:34.960352: step 129200, loss = 0.84 (255.6 examples/sec; 0.501 sec/batch)
2016-02-04 02:53:40.233268: step 129210, loss = 0.75 (256.5 examples/sec; 0.499 sec/batch)
2016-02-04 02:53:45.050122: step 129220, loss = 0.73 (239.7 examples/sec; 0.534 sec/batch)
2016-02-04 02:53:49.817956: step 129230, loss = 0.62 (264.0 examples/sec; 0.485 sec/batch)
2016-02-04 02:53:54.510874: step 129240, loss = 0.81 (298.4 examples/sec; 0.429 sec/batch)
2016-02-04 02:53:59.268442: step 129250, loss = 0.64 (290.8 examples/sec; 0.440 sec/batch)
2016-02-04 02:54:03.856134: step 129260, loss = 0.75 (271.7 examples/sec; 0.471 sec/batch)
2016-02-04 02:54:08.596088: step 129270, loss = 0.55 (304.3 examples/sec; 0.421 sec/batch)
2016-02-04 02:54:13.288966: step 129280, loss = 0.69 (290.7 examples/sec; 0.440 sec/batch)
2016-02-04 02:54:18.043811: step 129290, loss = 0.53 (262.6 examples/sec; 0.488 sec/batch)
2016-02-04 02:54:22.692680: step 129300, loss = 0.64 (290.2 examples/sec; 0.441 sec/batch)
2016-02-04 02:54:27.905744: step 129310, loss = 0.70 (276.8 examples/sec; 0.462 sec/batch)
2016-02-04 02:54:32.568640: step 129320, loss = 0.74 (267.0 examples/sec; 0.479 sec/batch)
2016-02-04 02:54:37.259853: step 129330, loss = 0.66 (293.7 examples/sec; 0.436 sec/batch)
2016-02-04 02:54:41.970055: step 129340, loss = 0.44 (283.9 examples/sec; 0.451 sec/batch)
2016-02-04 02:54:46.737975: step 129350, loss = 0.82 (270.4 examples/sec; 0.473 sec/batch)
2016-02-04 02:54:51.396430: step 129360, loss = 0.63 (276.3 examples/sec; 0.463 sec/batch)
2016-02-04 02:54:56.060825: step 129370, loss = 0.63 (297.7 examples/sec; 0.430 sec/batch)
2016-02-04 02:55:00.752118: step 129380, loss = 0.59 (259.4 examples/sec; 0.493 sec/batch)
2016-02-04 02:55:05.420741: step 129390, loss = 0.78 (271.1 examples/sec; 0.472 sec/batch)
2016-02-04 02:55:10.188331: step 129400, loss = 0.88 (260.1 examples/sec; 0.492 sec/batch)
2016-02-04 02:55:15.411234: step 129410, loss = 0.77 (256.8 examples/sec; 0.498 sec/batch)
2016-02-04 02:55:20.130735: step 129420, loss = 0.80 (269.8 examples/sec; 0.474 sec/batch)
2016-02-04 02:55:24.901918: step 129430, loss = 0.65 (257.5 examples/sec; 0.497 sec/batch)
2016-02-04 02:55:29.544378: step 129440, loss = 0.72 (300.5 examples/sec; 0.426 sec/batch)
2016-02-04 02:55:34.320221: step 129450, loss = 0.68 (262.0 examples/sec; 0.489 sec/batch)
2016-02-04 02:55:39.031806: step 129460, loss = 0.54 (250.3 examples/sec; 0.511 sec/batch)
2016-02-04 02:55:43.730972: step 129470, loss = 0.78 (291.0 examples/sec; 0.440 sec/batch)
2016-02-04 02:55:48.540411: step 129480, loss = 0.66 (264.1 examples/sec; 0.485 sec/batch)
2016-02-04 02:55:53.165533: step 129490, loss = 0.66 (297.0 examples/sec; 0.431 sec/batch)
2016-02-04 02:55:57.864680: step 129500, loss = 0.76 (266.5 examples/sec; 0.480 sec/batch)
2016-02-04 02:56:02.995266: step 129510, loss = 0.67 (289.7 examples/sec; 0.442 sec/batch)
2016-02-04 02:56:07.707099: step 129520, loss = 0.63 (293.8 examples/sec; 0.436 sec/batch)
2016-02-04 02:56:12.383327: step 129530, loss = 0.70 (289.5 examples/sec; 0.442 sec/batch)
2016-02-04 02:56:17.100118: step 129540, loss = 0.61 (270.4 examples/sec; 0.473 sec/batch)
2016-02-04 02:56:21.765895: step 129550, loss = 0.66 (272.4 examples/sec; 0.470 sec/batch)
2016-02-04 02:56:26.412585: step 129560, loss = 0.82 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 02:56:31.188818: step 129570, loss = 0.79 (249.6 examples/sec; 0.513 sec/batch)
2016-02-04 02:56:35.905557: step 129580, loss = 0.81 (264.1 examples/sec; 0.485 sec/batch)
2016-02-04 02:56:40.585296: step 129590, loss = 0.60 (257.9 examples/sec; 0.496 sec/batch)
2016-02-04 02:56:45.316833: step 129600, loss = 0.68 (283.8 examples/sec; 0.451 sec/batch)
2016-02-04 02:56:50.539137: step 129610, loss = 0.64 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 02:56:55.242347: step 129620, loss = 0.85 (269.7 examples/sec; 0.475 sec/batch)
2016-02-04 02:57:00.008278: step 129630, loss = 0.73 (275.3 examples/sec; 0.465 sec/batch)
2016-02-04 02:57:04.655821: step 129640, loss = 0.75 (325.5 examples/sec; 0.393 sec/batch)
2016-02-04 02:57:09.311572: step 129650, loss = 0.59 (262.7 examples/sec; 0.487 sec/batch)
2016-02-04 02:57:14.078577: step 129660, loss = 0.57 (273.9 examples/sec; 0.467 sec/batch)
2016-02-04 02:57:18.700269: step 129670, loss = 0.80 (274.1 examples/sec; 0.467 sec/batch)
2016-02-04 02:57:23.398923: step 129680, loss = 0.73 (255.5 examples/sec; 0.501 sec/batch)
2016-02-04 02:57:28.083650: step 129690, loss = 0.74 (260.4 examples/sec; 0.492 sec/batch)
2016-02-04 02:57:32.762363: step 129700, loss = 0.67 (243.3 examples/sec; 0.526 sec/batch)
2016-02-04 02:57:37.933545: step 129710, loss = 0.68 (259.0 examples/sec; 0.494 sec/batch)
2016-02-04 02:57:42.657325: step 129720, loss = 0.90 (271.5 examples/sec; 0.471 sec/batch)
2016-02-04 02:57:47.380372: step 129730, loss = 0.62 (263.2 examples/sec; 0.486 sec/batch)
2016-02-04 02:57:52.087518: step 129740, loss = 0.58 (268.4 examples/sec; 0.477 sec/batch)
2016-02-04 02:57:56.824495: step 129750, loss = 0.71 (261.9 examples/sec; 0.489 sec/batch)
2016-02-04 02:58:01.531923: step 129760, loss = 0.56 (261.7 examples/sec; 0.489 sec/batch)
2016-02-04 02:58:06.267589: step 129770, loss = 0.71 (270.4 examples/sec; 0.473 sec/batch)
2016-02-04 02:58:11.000210: step 129780, loss = 0.53 (257.1 examples/sec; 0.498 sec/batch)
2016-02-04 02:58:15.756625: step 129790, loss = 0.80 (251.0 examples/sec; 0.510 sec/batch)
2016-02-04 02:58:20.483902: step 129800, loss = 0.66 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 02:58:25.796906: step 129810, loss = 0.66 (264.3 examples/sec; 0.484 sec/batch)
2016-02-04 02:58:30.478904: step 129820, loss = 0.58 (285.5 examples/sec; 0.448 sec/batch)
2016-02-04 02:58:35.208215: step 129830, loss = 0.59 (280.4 examples/sec; 0.456 sec/batch)
2016-02-04 02:58:39.843717: step 129840, loss = 0.64 (268.7 examples/sec; 0.476 sec/batch)
2016-02-04 02:58:44.558220: step 129850, loss = 0.66 (252.2 examples/sec; 0.507 sec/batch)
2016-02-04 02:58:49.236492: step 129860, loss = 0.82 (264.4 examples/sec; 0.484 sec/batch)
2016-02-04 02:58:53.885468: step 129870, loss = 0.77 (276.8 examples/sec; 0.462 sec/batch)
2016-02-04 02:58:58.583000: step 129880, loss = 0.72 (276.3 examples/sec; 0.463 sec/batch)
2016-02-04 02:59:03.306251: step 129890, loss = 0.74 (270.1 examples/sec; 0.474 sec/batch)
2016-02-04 02:59:07.997756: step 129900, loss = 0.66 (261.4 examples/sec; 0.490 sec/batch)
2016-02-04 02:59:13.176196: step 129910, loss = 0.53 (300.2 examples/sec; 0.426 sec/batch)
2016-02-04 02:59:17.904017: step 129920, loss = 0.68 (277.1 examples/sec; 0.462 sec/batch)
2016-02-04 02:59:22.625214: step 129930, loss = 0.54 (258.2 examples/sec; 0.496 sec/batch)
2016-02-04 02:59:27.364610: step 129940, loss = 0.58 (241.4 examples/sec; 0.530 sec/batch)
2016-02-04 02:59:32.042756: step 129950, loss = 0.59 (290.0 examples/sec; 0.441 sec/batch)
2016-02-04 02:59:36.813707: step 129960, loss = 0.69 (245.5 examples/sec; 0.521 sec/batch)
2016-02-04 02:59:41.529616: step 129970, loss = 0.58 (289.8 examples/sec; 0.442 sec/batch)
2016-02-04 02:59:46.257765: step 129980, loss = 0.62 (258.1 examples/sec; 0.496 sec/batch)
2016-02-04 02:59:51.003508: step 129990, loss = 0.57 (265.4 examples/sec; 0.482 sec/batch)
2016-02-04 02:59:55.699159: step 130000, loss = 0.74 (282.5 examples/sec; 0.453 sec/batch)
2016-02-04 03:00:01.001362: step 130010, loss = 0.67 (275.7 examples/sec; 0.464 sec/batch)
2016-02-04 03:00:05.632334: step 130020, loss = 0.69 (313.0 examples/sec; 0.409 sec/batch)
2016-02-04 03:00:10.405861: step 130030, loss = 0.56 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 03:00:15.094142: step 130040, loss = 0.63 (258.5 examples/sec; 0.495 sec/batch)
2016-02-04 03:00:19.783960: step 130050, loss = 0.77 (262.2 examples/sec; 0.488 sec/batch)
2016-02-04 03:00:24.513364: step 130060, loss = 0.76 (281.7 examples/sec; 0.454 sec/batch)
2016-02-04 03:00:29.232051: step 130070, loss = 0.78 (295.1 examples/sec; 0.434 sec/batch)
2016-02-04 03:00:33.903701: step 130080, loss = 0.61 (254.2 examples/sec; 0.504 sec/batch)
2016-02-04 03:00:38.605778: step 130090, loss = 0.76 (284.8 examples/sec; 0.449 sec/batch)
2016-02-04 03:00:43.389367: step 130100, loss = 0.81 (270.5 examples/sec; 0.473 sec/batch)
2016-02-04 03:00:48.694581: step 130110, loss = 0.54 (264.4 examples/sec; 0.484 sec/batch)
2016-02-04 03:00:53.377359: step 130120, loss = 0.60 (262.3 examples/sec; 0.488 sec/batch)
2016-02-04 03:00:58.088299: step 130130, loss = 0.60 (265.1 examples/sec; 0.483 sec/batch)
2016-02-04 03:01:02.847010: step 130140, loss = 0.62 (255.8 examples/sec; 0.500 sec/batch)
2016-02-04 03:01:07.566544: step 130150, loss = 0.69 (245.2 examples/sec; 0.522 sec/batch)
2016-02-04 03:01:12.318116: step 130160, loss = 0.72 (260.4 examples/sec; 0.492 sec/batch)
2016-02-04 03:01:17.001333: step 130170, loss = 0.59 (274.0 examples/sec; 0.467 sec/batch)
2016-02-04 03:01:21.636862: step 130180, loss = 0.71 (288.2 examples/sec; 0.444 sec/batch)
2016-02-04 03:01:26.444088: step 130190, loss = 0.60 (254.7 examples/sec; 0.503 sec/batch)
2016-02-04 03:01:31.110356: step 130200, loss = 0.65 (262.6 examples/sec; 0.487 sec/batch)
2016-02-04 03:01:36.378567: step 130210, loss = 0.79 (270.3 examples/sec; 0.474 sec/batch)
2016-02-04 03:01:41.115749: step 130220, loss = 0.66 (264.6 examples/sec; 0.484 sec/batch)
2016-02-04 03:01:45.837743: step 130230, loss = 0.67 (279.2 examples/sec; 0.458 sec/batch)
2016-02-04 03:01:50.501063: step 130240, loss = 0.66 (283.5 examples/sec; 0.452 sec/batch)
2016-02-04 03:01:55.268697: step 130250, loss = 0.74 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 03:01:59.950760: step 130260, loss = 0.66 (285.4 examples/sec; 0.449 sec/batch)
2016-02-04 03:02:04.663464: step 130270, loss = 0.65 (283.4 examples/sec; 0.452 sec/batch)
2016-02-04 03:02:09.473156: step 130280, loss = 0.64 (260.4 examples/sec; 0.491 sec/batch)
2016-02-04 03:02:14.161897: step 130290, loss = 0.79 (288.8 examples/sec; 0.443 sec/batch)
2016-02-04 03:02:18.914118: step 130300, loss = 0.62 (268.8 examples/sec; 0.476 sec/batch)
2016-02-04 03:02:24.084942: step 130310, loss = 0.63 (262.2 examples/sec; 0.488 sec/batch)
2016-02-04 03:02:28.800157: step 130320, loss = 0.69 (282.6 examples/sec; 0.453 sec/batch)
2016-02-04 03:02:33.501244: step 130330, loss = 0.72 (285.3 examples/sec; 0.449 sec/batch)
2016-02-04 03:02:38.217950: step 130340, loss = 0.89 (255.7 examples/sec; 0.501 sec/batch)
2016-02-04 03:02:42.791161: step 130350, loss = 0.73 (291.7 examples/sec; 0.439 sec/batch)
2016-02-04 03:02:47.509594: step 130360, loss = 0.52 (267.2 examples/sec; 0.479 sec/batch)
2016-02-04 03:02:52.259073: step 130370, loss = 0.69 (257.3 examples/sec; 0.497 sec/batch)
2016-02-04 03:02:56.993632: step 130380, loss = 0.67 (253.8 examples/sec; 0.504 sec/batch)
2016-02-04 03:03:01.689625: step 130390, loss = 0.72 (282.9 examples/sec; 0.453 sec/batch)
2016-02-04 03:03:06.368037: step 130400, loss = 0.63 (284.6 examples/sec; 0.450 sec/batch)
2016-02-04 03:03:11.625282: step 130410, loss = 0.77 (285.4 examples/sec; 0.449 sec/batch)
2016-02-04 03:03:16.330245: step 130420, loss = 0.75 (275.8 examples/sec; 0.464 sec/batch)
2016-02-04 03:03:21.060506: step 130430, loss = 0.82 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 03:03:25.791061: step 130440, loss = 0.56 (265.0 examples/sec; 0.483 sec/batch)
2016-02-04 03:03:30.479931: step 130450, loss = 0.70 (283.6 examples/sec; 0.451 sec/batch)
2016-02-04 03:03:35.224826: step 130460, loss = 0.57 (254.8 examples/sec; 0.502 sec/batch)
2016-02-04 03:03:39.927862: step 130470, loss = 0.84 (243.5 examples/sec; 0.526 sec/batch)
2016-02-04 03:03:44.611710: step 130480, loss = 0.57 (283.5 examples/sec; 0.451 sec/batch)
2016-02-04 03:03:49.395424: step 130490, loss = 0.55 (271.0 examples/sec; 0.472 sec/batch)
2016-02-04 03:03:54.236580: step 130500, loss = 0.86 (247.8 examples/sec; 0.517 sec/batch)
2016-02-04 03:03:59.396093: step 130510, loss = 0.68 (260.3 examples/sec; 0.492 sec/batch)
2016-02-04 03:04:04.097886: step 130520, loss = 0.73 (264.7 examples/sec; 0.484 sec/batch)
2016-02-04 03:04:08.874939: step 130530, loss = 0.76 (272.1 examples/sec; 0.470 sec/batch)
2016-02-04 03:04:13.595902: step 130540, loss = 0.64 (278.2 examples/sec; 0.460 sec/batch)
2016-02-04 03:04:18.369568: step 130550, loss = 0.58 (260.1 examples/sec; 0.492 sec/batch)
2016-02-04 03:04:23.151626: step 130560, loss = 0.76 (279.4 examples/sec; 0.458 sec/batch)
2016-02-04 03:04:27.847930: step 130570, loss = 0.63 (291.3 examples/sec; 0.439 sec/batch)
2016-02-04 03:04:32.563500: step 130580, loss = 0.54 (289.0 examples/sec; 0.443 sec/batch)
2016-02-04 03:04:37.311119: step 130590, loss = 0.67 (279.1 examples/sec; 0.459 sec/batch)
2016-02-04 03:04:42.037006: step 130600, loss = 0.71 (244.0 examples/sec; 0.525 sec/batch)
2016-02-04 03:04:47.287287: step 130610, loss = 0.76 (268.2 examples/sec; 0.477 sec/batch)
2016-02-04 03:04:52.030374: step 130620, loss = 0.65 (290.1 examples/sec; 0.441 sec/batch)
2016-02-04 03:04:56.740474: step 130630, loss = 0.63 (285.8 examples/sec; 0.448 sec/batch)
2016-02-04 03:05:01.485881: step 130640, loss = 0.67 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 03:05:06.219930: step 130650, loss = 0.71 (275.9 examples/sec; 0.464 sec/batch)
2016-02-04 03:05:10.916149: step 130660, loss = 0.57 (300.8 examples/sec; 0.426 sec/batch)
2016-02-04 03:05:15.708232: step 130670, loss = 0.74 (249.9 examples/sec; 0.512 sec/batch)
2016-02-04 03:05:20.441958: step 130680, loss = 0.61 (270.9 examples/sec; 0.472 sec/batch)
2016-02-04 03:05:25.240519: step 130690, loss = 0.65 (260.4 examples/sec; 0.492 sec/batch)
2016-02-04 03:05:29.971040: step 130700, loss = 0.59 (262.6 examples/sec; 0.487 sec/batch)
2016-02-04 03:05:35.200379: step 130710, loss = 0.58 (266.5 examples/sec; 0.480 sec/batch)
2016-02-04 03:05:39.909861: step 130720, loss = 0.74 (281.3 examples/sec; 0.455 sec/batch)
2016-02-04 03:05:44.680366: step 130730, loss = 0.77 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 03:05:49.426485: step 130740, loss = 0.70 (267.3 examples/sec; 0.479 sec/batch)
2016-02-04 03:05:54.111648: step 130750, loss = 0.75 (287.8 examples/sec; 0.445 sec/batch)
2016-02-04 03:05:58.877662: step 130760, loss = 0.66 (276.4 examples/sec; 0.463 sec/batch)
2016-02-04 03:06:03.584722: step 130770, loss = 0.69 (265.5 examples/sec; 0.482 sec/batch)
2016-02-04 03:06:08.260453: step 130780, loss = 0.64 (253.2 examples/sec; 0.506 sec/batch)
2016-02-04 03:06:13.041611: step 130790, loss = 0.69 (275.0 examples/sec; 0.466 sec/batch)
2016-02-04 03:06:17.768047: step 130800, loss = 0.57 (253.2 examples/sec; 0.506 sec/batch)
2016-02-04 03:06:22.985934: step 130810, loss = 0.87 (273.3 examples/sec; 0.468 sec/batch)
2016-02-04 03:06:27.702964: step 130820, loss = 0.66 (265.0 examples/sec; 0.483 sec/batch)
2016-02-04 03:06:32.456327: step 130830, loss = 0.55 (258.8 examples/sec; 0.495 sec/batch)
2016-02-04 03:06:37.190790: step 130840, loss = 0.71 (248.6 examples/sec; 0.515 sec/batch)
2016-02-04 03:06:41.926024: step 130850, loss = 0.63 (296.1 examples/sec; 0.432 sec/batch)
2016-02-04 03:06:46.653825: step 130860, loss = 0.81 (268.6 examples/sec; 0.477 sec/batch)
2016-02-04 03:06:51.431421: step 130870, loss = 0.60 (271.4 examples/sec; 0.472 sec/batch)
2016-02-04 03:06:56.141221: step 130880, loss = 0.74 (262.3 examples/sec; 0.488 sec/batch)
2016-02-04 03:07:00.815730: step 130890, loss = 0.73 (313.7 examples/sec; 0.408 sec/batch)
2016-02-04 03:07:05.500449: step 130900, loss = 0.71 (273.4 examples/sec; 0.468 sec/batch)
2016-02-04 03:07:10.795169: step 130910, loss = 0.76 (286.2 examples/sec; 0.447 sec/batch)
2016-02-04 03:07:15.474968: step 130920, loss = 0.76 (272.8 examples/sec; 0.469 sec/batch)
2016-02-04 03:07:20.340972: step 130930, loss = 0.58 (262.7 examples/sec; 0.487 sec/batch)
2016-02-04 03:07:25.059237: step 130940, loss = 0.70 (281.7 examples/sec; 0.454 sec/batch)
2016-02-04 03:07:29.816509: step 130950, loss = 0.73 (250.4 examples/sec; 0.511 sec/batch)
2016-02-04 03:07:34.504355: step 130960, loss = 0.66 (275.5 examples/sec; 0.465 sec/batch)
2016-02-04 03:07:39.223147: step 130970, loss = 0.72 (259.1 examples/sec; 0.494 sec/batch)
2016-02-04 03:07:43.894437: step 130980, loss = 0.58 (263.1 examples/sec; 0.486 sec/batch)
2016-02-04 03:07:48.561218: step 130990, loss = 0.62 (300.5 examples/sec; 0.426 sec/batch)
2016-02-04 03:07:53.311642: step 131000, loss = 0.71 (270.2 examples/sec; 0.474 sec/batch)
2016-02-04 03:07:58.533428: step 131010, loss = 0.66 (256.3 examples/sec; 0.499 sec/batch)
2016-02-04 03:08:03.155203: step 131020, loss = 0.54 (295.2 examples/sec; 0.434 sec/batch)
2016-02-04 03:08:07.924515: step 131030, loss = 0.67 (254.9 examples/sec; 0.502 sec/batch)
2016-02-04 03:08:12.650507: step 131040, loss = 0.70 (254.6 examples/sec; 0.503 sec/batch)
2016-02-04 03:08:17.289273: step 131050, loss = 0.55 (267.4 examples/sec; 0.479 sec/batch)
2016-02-04 03:08:22.025658: step 131060, loss = 0.71 (280.5 examples/sec; 0.456 sec/batch)
2016-02-04 03:08:26.725533: step 131070, loss = 0.68 (269.2 examples/sec; 0.476 sec/batch)
2016-02-04 03:08:31.375568: step 131080, loss = 0.67 (280.8 examples/sec; 0.456 sec/batch)
2016-02-04 03:08:36.065394: step 131090, loss = 0.67 (274.9 examples/sec; 0.466 sec/batch)
2016-02-04 03:08:40.793990: step 131100, loss = 0.73 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 03:08:45.972853: step 131110, loss = 0.63 (275.7 examples/sec; 0.464 sec/batch)
2016-02-04 03:08:50.668602: step 131120, loss = 0.73 (263.8 examples/sec; 0.485 sec/batch)
2016-02-04 03:08:55.440295: step 131130, loss = 0.66 (257.6 examples/sec; 0.497 sec/batch)
2016-02-04 03:09:00.125635: step 131140, loss = 0.63 (271.8 examples/sec; 0.471 sec/batch)
2016-02-04 03:09:04.730957: step 131150, loss = 0.65 (261.5 examples/sec; 0.489 sec/batch)
2016-02-04 03:09:09.434034: step 131160, loss = 0.81 (272.6 examples/sec; 0.470 sec/batch)
2016-02-04 03:09:14.078443: step 131170, loss = 0.63 (265.7 examples/sec; 0.482 sec/batch)
2016-02-04 03:09:18.717006: step 131180, loss = 0.58 (263.5 examples/sec; 0.486 sec/batch)
2016-02-04 03:09:23.357873: step 131190, loss = 0.62 (269.1 examples/sec; 0.476 sec/batch)
2016-02-04 03:09:28.090006: step 131200, loss = 0.68 (245.1 examples/sec; 0.522 sec/batch)
2016-02-04 03:09:33.346445: step 131210, loss = 0.76 (275.0 examples/sec; 0.466 sec/batch)
2016-02-04 03:09:37.995588: step 131220, loss = 0.70 (272.6 examples/sec; 0.470 sec/batch)
2016-02-04 03:09:42.681345: step 131230, loss = 0.59 (266.8 examples/sec; 0.480 sec/batch)
2016-02-04 03:09:47.391447: step 131240, loss = 0.68 (276.7 examples/sec; 0.463 sec/batch)
2016-02-04 03:09:51.979311: step 131250, loss = 0.68 (293.0 examples/sec; 0.437 sec/batch)
2016-02-04 03:09:56.711618: step 131260, loss = 0.65 (270.0 examples/sec; 0.474 sec/batch)
2016-02-04 03:10:01.398763: step 131270, loss = 0.58 (284.2 examples/sec; 0.450 sec/batch)
2016-02-04 03:10:06.166705: step 131280, loss = 0.74 (256.8 examples/sec; 0.499 sec/batch)
2016-02-04 03:10:10.904226: step 131290, loss = 0.74 (263.1 examples/sec; 0.486 sec/batch)
2016-02-04 03:10:15.608455: step 131300, loss = 0.56 (283.9 examples/sec; 0.451 sec/batch)
2016-02-04 03:10:20.823770: step 131310, loss = 0.83 (267.2 examples/sec; 0.479 sec/batch)
2016-02-04 03:10:25.572480: step 131320, loss = 0.69 (262.0 examples/sec; 0.489 sec/batch)
2016-02-04 03:10:30.301915: step 131330, loss = 0.62 (301.6 examples/sec; 0.424 sec/batch)
2016-02-04 03:10:35.058503: step 131340, loss = 0.70 (256.8 examples/sec; 0.499 sec/batch)
2016-02-04 03:10:39.773425: step 131350, loss = 0.63 (269.7 examples/sec; 0.475 sec/batch)
2016-02-04 03:10:44.577177: step 131360, loss = 0.73 (269.3 examples/sec; 0.475 sec/batch)
2016-02-04 03:10:49.347454: step 131370, loss = 0.53 (284.3 examples/sec; 0.450 sec/batch)
2016-02-04 03:10:53.985036: step 131380, loss = 0.77 (305.2 examples/sec; 0.419 sec/batch)
2016-02-04 03:10:58.669199: step 131390, loss = 0.60 (284.5 examples/sec; 0.450 sec/batch)
2016-02-04 03:11:03.366853: step 131400, loss = 0.61 (265.5 examples/sec; 0.482 sec/batch)
2016-02-04 03:11:08.656777: step 131410, loss = 0.67 (277.9 examples/sec; 0.461 sec/batch)
2016-02-04 03:11:13.390369: step 131420, loss = 0.73 (256.8 examples/sec; 0.498 sec/batch)
2016-02-04 03:11:18.135753: step 131430, loss = 0.81 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 03:11:22.877904: step 131440, loss = 0.54 (270.0 examples/sec; 0.474 sec/batch)
2016-02-04 03:11:27.575029: step 131450, loss = 0.67 (298.4 examples/sec; 0.429 sec/batch)
2016-02-04 03:11:32.309512: step 131460, loss = 0.73 (298.9 examples/sec; 0.428 sec/batch)
2016-02-04 03:11:37.057559: step 131470, loss = 0.82 (268.2 examples/sec; 0.477 sec/batch)
2016-02-04 03:11:41.839467: step 131480, loss = 0.60 (275.0 examples/sec; 0.465 sec/batch)
2016-02-04 03:11:46.579652: step 131490, loss = 0.69 (265.2 examples/sec; 0.483 sec/batch)
2016-02-04 03:11:51.271932: step 131500, loss = 0.75 (291.4 examples/sec; 0.439 sec/batch)
2016-02-04 03:11:56.620119: step 131510, loss = 0.59 (261.9 examples/sec; 0.489 sec/batch)
2016-02-04 03:12:01.322076: step 131520, loss = 0.65 (274.7 examples/sec; 0.466 sec/batch)
2016-02-04 03:12:06.061531: step 131530, loss = 0.68 (245.9 examples/sec; 0.521 sec/batch)
2016-02-04 03:12:10.784848: step 131540, loss = 0.65 (279.7 examples/sec; 0.458 sec/batch)
2016-02-04 03:12:15.459008: step 131550, loss = 0.65 (269.8 examples/sec; 0.474 sec/batch)
2016-02-04 03:12:20.210697: step 131560, loss = 0.70 (268.2 examples/sec; 0.477 sec/batch)
2016-02-04 03:12:24.876019: step 131570, loss = 0.65 (270.2 examples/sec; 0.474 sec/batch)
2016-02-04 03:12:29.551799: step 131580, loss = 0.71 (277.0 examples/sec; 0.462 sec/batch)
2016-02-04 03:12:34.206949: step 131590, loss = 0.67 (261.6 examples/sec; 0.489 sec/batch)
2016-02-04 03:12:38.839360: step 131600, loss = 0.80 (274.4 examples/sec; 0.466 sec/batch)
2016-02-04 03:12:44.121560: step 131610, loss = 0.53 (261.1 examples/sec; 0.490 sec/batch)
2016-02-04 03:12:48.845483: step 131620, loss = 0.66 (281.6 examples/sec; 0.454 sec/batch)
2016-02-04 03:12:53.547642: step 131630, loss = 0.68 (252.6 examples/sec; 0.507 sec/batch)
2016-02-04 03:12:58.272692: step 131640, loss = 0.52 (270.4 examples/sec; 0.473 sec/batch)
2016-02-04 03:13:02.888634: step 131650, loss = 0.75 (262.1 examples/sec; 0.488 sec/batch)
2016-02-04 03:13:07.634958: step 131660, loss = 0.74 (276.4 examples/sec; 0.463 sec/batch)
2016-02-04 03:13:12.401177: step 131670, loss = 0.60 (271.6 examples/sec; 0.471 sec/batch)
2016-02-04 03:13:17.142502: step 131680, loss = 0.67 (279.0 examples/sec; 0.459 sec/batch)
2016-02-04 03:13:21.835135: step 131690, loss = 0.62 (273.2 examples/sec; 0.469 sec/batch)
2016-02-04 03:13:26.445092: step 131700, loss = 0.68 (283.5 examples/sec; 0.452 sec/batch)
2016-02-04 03:13:31.689180: step 131710, loss = 0.72 (272.2 examples/sec; 0.470 sec/batch)
2016-02-04 03:13:36.417686: step 131720, loss = 0.62 (263.4 examples/sec; 0.486 sec/batch)
2016-02-04 03:13:41.188331: step 131730, loss = 0.76 (256.2 examples/sec; 0.500 sec/batch)
2016-02-04 03:13:45.939161: step 131740, loss = 0.73 (260.2 examples/sec; 0.492 sec/batch)
2016-02-04 03:13:50.573994: step 131750, loss = 0.47 (286.2 examples/sec; 0.447 sec/batch)
2016-02-04 03:13:55.346105: step 131760, loss = 0.68 (272.1 examples/sec; 0.470 sec/batch)
2016-02-04 03:14:00.118313: step 131770, loss = 0.68 (278.0 examples/sec; 0.460 sec/batch)
2016-02-04 03:14:04.737330: step 131780, loss = 0.60 (272.9 examples/sec; 0.469 sec/batch)
2016-02-04 03:14:09.516254: step 131790, loss = 0.72 (262.0 examples/sec; 0.488 sec/batch)
2016-02-04 03:14:14.232122: step 131800, loss = 0.54 (269.2 examples/sec; 0.476 sec/batch)
2016-02-04 03:14:19.479276: step 131810, loss = 0.66 (274.7 examples/sec; 0.466 sec/batch)
2016-02-04 03:14:24.181945: step 131820, loss = 0.79 (288.6 examples/sec; 0.444 sec/batch)
2016-02-04 03:14:28.943596: step 131830, loss = 0.74 (274.1 examples/sec; 0.467 sec/batch)
2016-02-04 03:14:33.565692: step 131840, loss = 0.72 (263.0 examples/sec; 0.487 sec/batch)
2016-02-04 03:14:38.321747: step 131850, loss = 0.56 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 03:14:42.996438: step 131860, loss = 0.65 (309.1 examples/sec; 0.414 sec/batch)
2016-02-04 03:14:47.713444: step 131870, loss = 0.69 (277.8 examples/sec; 0.461 sec/batch)
2016-02-04 03:14:52.416041: step 131880, loss = 0.65 (249.8 examples/sec; 0.512 sec/batch)
2016-02-04 03:14:57.170373: step 131890, loss = 0.68 (272.9 examples/sec; 0.469 sec/batch)
2016-02-04 03:15:01.893602: step 131900, loss = 0.62 (251.7 examples/sec; 0.509 sec/batch)
2016-02-04 03:15:07.073241: step 131910, loss = 0.70 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 03:15:11.704632: step 131920, loss = 0.72 (268.2 examples/sec; 0.477 sec/batch)
2016-02-04 03:15:16.423988: step 131930, loss = 0.65 (270.9 examples/sec; 0.473 sec/batch)
2016-02-04 03:15:21.141749: step 131940, loss = 0.64 (264.8 examples/sec; 0.483 sec/batch)
2016-02-04 03:15:25.879668: step 131950, loss = 0.51 (262.9 examples/sec; 0.487 sec/batch)
2016-02-04 03:15:30.524035: step 131960, loss = 0.62 (290.0 examples/sec; 0.441 sec/batch)
2016-02-04 03:15:35.298047: step 131970, loss = 0.55 (262.9 examples/sec; 0.487 sec/batch)
2016-02-04 03:15:40.036930: step 131980, loss = 0.69 (268.9 examples/sec; 0.476 sec/batch)
2016-02-04 03:15:44.784509: step 131990, loss = 0.70 (272.1 examples/sec; 0.470 sec/batch)
2016-02-04 03:15:49.473389: step 132000, loss = 0.83 (273.8 examples/sec; 0.467 sec/batch)
2016-02-04 03:15:54.638850: step 132010, loss = 0.70 (277.0 examples/sec; 0.462 sec/batch)
2016-02-04 03:15:59.296767: step 132020, loss = 0.67 (282.9 examples/sec; 0.452 sec/batch)
2016-02-04 03:16:03.953564: step 132030, loss = 0.68 (268.3 examples/sec; 0.477 sec/batch)
2016-02-04 03:16:08.554849: step 132040, loss = 0.65 (264.6 examples/sec; 0.484 sec/batch)
2016-02-04 03:16:13.065649: step 132050, loss = 0.63 (294.6 examples/sec; 0.434 sec/batch)
2016-02-04 03:16:17.617058: step 132060, loss = 0.97 (274.1 examples/sec; 0.467 sec/batch)
2016-02-04 03:16:22.250624: step 132070, loss = 0.64 (282.0 examples/sec; 0.454 sec/batch)
2016-02-04 03:16:26.930414: step 132080, loss = 0.74 (305.6 examples/sec; 0.419 sec/batch)
2016-02-04 03:16:31.608486: step 132090, loss = 0.80 (255.1 examples/sec; 0.502 sec/batch)
2016-02-04 03:16:36.286708: step 132100, loss = 0.77 (281.8 examples/sec; 0.454 sec/batch)
2016-02-04 03:16:41.503193: step 132110, loss = 0.70 (289.4 examples/sec; 0.442 sec/batch)
2016-02-04 03:16:46.032382: step 132120, loss = 0.69 (294.2 examples/sec; 0.435 sec/batch)
2016-02-04 03:16:50.637748: step 132130, loss = 0.59 (294.7 examples/sec; 0.434 sec/batch)
2016-02-04 03:16:55.331549: step 132140, loss = 0.68 (292.2 examples/sec; 0.438 sec/batch)
2016-02-04 03:17:00.082204: step 132150, loss = 0.70 (268.0 examples/sec; 0.478 sec/batch)
2016-02-04 03:17:04.810876: step 132160, loss = 0.56 (284.8 examples/sec; 0.449 sec/batch)
2016-02-04 03:17:09.584253: step 132170, loss = 0.94 (263.8 examples/sec; 0.485 sec/batch)
2016-02-04 03:17:14.253444: step 132180, loss = 0.64 (281.3 examples/sec; 0.455 sec/batch)
2016-02-04 03:17:18.916090: step 132190, loss = 0.80 (277.1 examples/sec; 0.462 sec/batch)
2016-02-04 03:17:23.711191: step 132200, loss = 0.63 (283.7 examples/sec; 0.451 sec/batch)
2016-02-04 03:17:28.998021: step 132210, loss = 0.65 (276.6 examples/sec; 0.463 sec/batch)
2016-02-04 03:17:33.604300: step 132220, loss = 0.58 (272.0 examples/sec; 0.471 sec/batch)
2016-02-04 03:17:38.285779: step 132230, loss = 0.69 (245.0 examples/sec; 0.522 sec/batch)
2016-02-04 03:17:42.898314: step 132240, loss = 0.69 (269.7 examples/sec; 0.475 sec/batch)
2016-02-04 03:17:47.618575: step 132250, loss = 0.60 (255.0 examples/sec; 0.502 sec/batch)
2016-02-04 03:17:52.356784: step 132260, loss = 0.68 (270.1 examples/sec; 0.474 sec/batch)
2016-02-04 03:17:57.002995: step 132270, loss = 0.68 (277.7 examples/sec; 0.461 sec/batch)
2016-02-04 03:18:01.660338: step 132280, loss = 0.63 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 03:18:06.226211: step 132290, loss = 0.53 (281.5 examples/sec; 0.455 sec/batch)
2016-02-04 03:18:10.904147: step 132300, loss = 0.84 (273.8 examples/sec; 0.468 sec/batch)
2016-02-04 03:18:16.108735: step 132310, loss = 0.72 (285.6 examples/sec; 0.448 sec/batch)
2016-02-04 03:18:20.800690: step 132320, loss = 0.71 (246.2 examples/sec; 0.520 sec/batch)
2016-02-04 03:18:25.364696: step 132330, loss = 0.70 (293.5 examples/sec; 0.436 sec/batch)
2016-02-04 03:18:30.160858: step 132340, loss = 0.66 (276.6 examples/sec; 0.463 sec/batch)
2016-02-04 03:18:34.912409: step 132350, loss = 0.80 (280.9 examples/sec; 0.456 sec/batch)
2016-02-04 03:18:39.732360: step 132360, loss = 0.63 (266.5 examples/sec; 0.480 sec/batch)
2016-02-04 03:18:44.452705: step 132370, loss = 0.65 (291.6 examples/sec; 0.439 sec/batch)
2016-02-04 03:18:49.172625: step 132380, loss = 0.84 (276.6 examples/sec; 0.463 sec/batch)
2016-02-04 03:18:53.955333: step 132390, loss = 0.56 (281.3 examples/sec; 0.455 sec/batch)
2016-02-04 03:18:58.555298: step 132400, loss = 0.58 (282.3 examples/sec; 0.453 sec/batch)
2016-02-04 03:19:03.778784: step 132410, loss = 0.81 (265.2 examples/sec; 0.483 sec/batch)
2016-02-04 03:19:08.442883: step 132420, loss = 0.66 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 03:19:13.115804: step 132430, loss = 0.77 (302.6 examples/sec; 0.423 sec/batch)
2016-02-04 03:19:17.868176: step 132440, loss = 0.58 (278.8 examples/sec; 0.459 sec/batch)
2016-02-04 03:19:22.580606: step 132450, loss = 0.77 (282.6 examples/sec; 0.453 sec/batch)
2016-02-04 03:19:27.193862: step 132460, loss = 0.79 (279.3 examples/sec; 0.458 sec/batch)
2016-02-04 03:19:31.936197: step 132470, loss = 0.62 (285.4 examples/sec; 0.448 sec/batch)
2016-02-04 03:19:36.617058: step 132480, loss = 0.67 (282.8 examples/sec; 0.453 sec/batch)
2016-02-04 03:19:41.420362: step 132490, loss = 0.76 (260.9 examples/sec; 0.491 sec/batch)
2016-02-04 03:19:46.048536: step 132500, loss = 0.79 (279.9 examples/sec; 0.457 sec/batch)
2016-02-04 03:19:51.248383: step 132510, loss = 0.66 (264.3 examples/sec; 0.484 sec/batch)
2016-02-04 03:19:55.986191: step 132520, loss = 0.76 (290.1 examples/sec; 0.441 sec/batch)
2016-02-04 03:20:00.730637: step 132530, loss = 0.67 (254.2 examples/sec; 0.503 sec/batch)
2016-02-04 03:20:05.446963: step 132540, loss = 0.61 (283.0 examples/sec; 0.452 sec/batch)
2016-02-04 03:20:10.192666: step 132550, loss = 0.64 (253.6 examples/sec; 0.505 sec/batch)
2016-02-04 03:20:14.934170: step 132560, loss = 0.87 (274.0 examples/sec; 0.467 sec/batch)
2016-02-04 03:20:19.721076: step 132570, loss = 0.72 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 03:20:24.378649: step 132580, loss = 0.78 (285.6 examples/sec; 0.448 sec/batch)
2016-02-04 03:20:29.156862: step 132590, loss = 0.80 (265.7 examples/sec; 0.482 sec/batch)
2016-02-04 03:20:33.924935: step 132600, loss = 0.65 (255.3 examples/sec; 0.501 sec/batch)
2016-02-04 03:20:39.145385: step 132610, loss = 0.80 (266.0 examples/sec; 0.481 sec/batch)
2016-02-04 03:20:43.863123: step 132620, loss = 0.55 (280.6 examples/sec; 0.456 sec/batch)
2016-02-04 03:20:48.548592: step 132630, loss = 0.63 (260.2 examples/sec; 0.492 sec/batch)
2016-02-04 03:20:53.194380: step 132640, loss = 0.72 (289.1 examples/sec; 0.443 sec/batch)
2016-02-04 03:20:57.916993: step 132650, loss = 0.82 (264.9 examples/sec; 0.483 sec/batch)
2016-02-04 03:21:02.648718: step 132660, loss = 0.79 (270.3 examples/sec; 0.474 sec/batch)
2016-02-04 03:21:07.415782: step 132670, loss = 0.72 (263.3 examples/sec; 0.486 sec/batch)
2016-02-04 03:21:12.096835: step 132680, loss = 0.64 (264.2 examples/sec; 0.484 sec/batch)
2016-02-04 03:21:16.766458: step 132690, loss = 0.55 (268.1 examples/sec; 0.478 sec/batch)
2016-02-04 03:21:21.376864: step 132700, loss = 0.71 (279.2 examples/sec; 0.458 sec/batch)
2016-02-04 03:21:26.590810: step 132710, loss = 0.65 (272.2 examples/sec; 0.470 sec/batch)
2016-02-04 03:21:31.281985: step 132720, loss = 0.77 (275.7 examples/sec; 0.464 sec/batch)
2016-02-04 03:21:35.979860: step 132730, loss = 0.69 (284.7 examples/sec; 0.450 sec/batch)
2016-02-04 03:21:40.742050: step 132740, loss = 0.57 (267.1 examples/sec; 0.479 sec/batch)
2016-02-04 03:21:45.462148: step 132750, loss = 0.63 (299.6 examples/sec; 0.427 sec/batch)
2016-02-04 03:21:50.268829: step 132760, loss = 0.68 (282.8 examples/sec; 0.453 sec/batch)
2016-02-04 03:21:54.963062: step 132770, loss = 0.58 (253.2 examples/sec; 0.505 sec/batch)
2016-02-04 03:21:59.579160: step 132780, loss = 0.65 (284.6 examples/sec; 0.450 sec/batch)
2016-02-04 03:22:04.323090: step 132790, loss = 0.71 (272.2 examples/sec; 0.470 sec/batch)
2016-02-04 03:22:08.947712: step 132800, loss = 0.85 (270.4 examples/sec; 0.473 sec/batch)
2016-02-04 03:22:14.191279: step 132810, loss = 0.67 (295.3 examples/sec; 0.433 sec/batch)
2016-02-04 03:22:18.887989: step 132820, loss = 0.67 (272.3 examples/sec; 0.470 sec/batch)
2016-02-04 03:22:23.608572: step 132830, loss = 0.63 (267.8 examples/sec; 0.478 sec/batch)
2016-02-04 03:22:28.275863: step 132840, loss = 0.63 (264.7 examples/sec; 0.484 sec/batch)
2016-02-04 03:22:32.953157: step 132850, loss = 0.70 (279.2 examples/sec; 0.458 sec/batch)
2016-02-04 03:22:37.717209: step 132860, loss = 0.66 (272.8 examples/sec; 0.469 sec/batch)
2016-02-04 03:22:42.389292: step 132870, loss = 0.63 (292.4 examples/sec; 0.438 sec/batch)
2016-02-04 03:22:47.077320: step 132880, loss = 0.68 (296.4 examples/sec; 0.432 sec/batch)
2016-02-04 03:22:51.801384: step 132890, loss = 0.68 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 03:22:56.556674: step 132900, loss = 0.79 (267.1 examples/sec; 0.479 sec/batch)
2016-02-04 03:23:01.783213: step 132910, loss = 0.81 (264.7 examples/sec; 0.484 sec/batch)
2016-02-04 03:23:06.466202: step 132920, loss = 0.52 (279.0 examples/sec; 0.459 sec/batch)
2016-02-04 03:23:11.080867: step 132930, loss = 0.76 (281.3 examples/sec; 0.455 sec/batch)
2016-02-04 03:23:15.750663: step 132940, loss = 0.55 (281.5 examples/sec; 0.455 sec/batch)
2016-02-04 03:23:20.464346: step 132950, loss = 0.72 (273.3 examples/sec; 0.468 sec/batch)
2016-02-04 03:23:25.152646: step 132960, loss = 0.76 (281.8 examples/sec; 0.454 sec/batch)
2016-02-04 03:23:29.871384: step 132970, loss = 0.58 (251.6 examples/sec; 0.509 sec/batch)
2016-02-04 03:23:34.580413: step 132980, loss = 0.71 (283.8 examples/sec; 0.451 sec/batch)
2016-02-04 03:23:39.321936: step 132990, loss = 0.59 (265.9 examples/sec; 0.481 sec/batch)
2016-02-04 03:23:44.043431: step 133000, loss = 0.76 (279.7 examples/sec; 0.458 sec/batch)
2016-02-04 03:23:49.260096: step 133010, loss = 0.67 (272.8 examples/sec; 0.469 sec/batch)
2016-02-04 03:23:53.913827: step 133020, loss = 0.65 (252.5 examples/sec; 0.507 sec/batch)
2016-02-04 03:23:58.554361: step 133030, loss = 0.63 (310.5 examples/sec; 0.412 sec/batch)
2016-02-04 03:24:03.184791: step 133040, loss = 0.72 (270.0 examples/sec; 0.474 sec/batch)
2016-02-04 03:24:07.823259: step 133050, loss = 0.75 (281.2 examples/sec; 0.455 sec/batch)
2016-02-04 03:24:12.544988: step 133060, loss = 0.60 (287.9 examples/sec; 0.445 sec/batch)
2016-02-04 03:24:17.260813: step 133070, loss = 0.71 (274.4 examples/sec; 0.466 sec/batch)
2016-02-04 03:24:21.937725: step 133080, loss = 0.71 (272.1 examples/sec; 0.471 sec/batch)
2016-02-04 03:24:26.684215: step 133090, loss = 0.68 (267.6 examples/sec; 0.478 sec/batch)
2016-02-04 03:24:31.348934: step 133100, loss = 0.64 (276.8 examples/sec; 0.462 sec/batch)
2016-02-04 03:24:36.595381: step 133110, loss = 0.64 (262.4 examples/sec; 0.488 sec/batch)
2016-02-04 03:24:41.277723: step 133120, loss = 0.54 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 03:24:45.960499: step 133130, loss = 0.60 (254.2 examples/sec; 0.504 sec/batch)
2016-02-04 03:24:50.555622: step 133140, loss = 0.68 (286.9 examples/sec; 0.446 sec/batch)
2016-02-04 03:24:55.308456: step 133150, loss = 0.80 (270.4 examples/sec; 0.473 sec/batch)
2016-02-04 03:25:00.020209: step 133160, loss = 0.61 (270.9 examples/sec; 0.473 sec/batch)
2016-02-04 03:25:04.720366: step 133170, loss = 0.79 (266.0 examples/sec; 0.481 sec/batch)
2016-02-04 03:25:09.377035: step 133180, loss = 0.66 (285.1 examples/sec; 0.449 sec/batch)
2016-02-04 03:25:14.055621: step 133190, loss = 0.69 (268.9 examples/sec; 0.476 sec/batch)
2016-02-04 03:25:18.718696: step 133200, loss = 0.64 (291.6 examples/sec; 0.439 sec/batch)
2016-02-04 03:25:23.974310: step 133210, loss = 0.84 (269.3 examples/sec; 0.475 sec/batch)
2016-02-04 03:25:28.675319: step 133220, loss = 0.69 (245.9 examples/sec; 0.521 sec/batch)
2016-02-04 03:25:33.361222: step 133230, loss = 0.73 (261.4 examples/sec; 0.490 sec/batch)
2016-02-04 03:25:38.157352: step 133240, loss = 0.66 (279.7 examples/sec; 0.458 sec/batch)
2016-02-04 03:25:42.886488: step 133250, loss = 0.78 (271.0 examples/sec; 0.472 sec/batch)
2016-02-04 03:25:47.588164: step 133260, loss = 0.64 (273.4 examples/sec; 0.468 sec/batch)
2016-02-04 03:25:52.325806: step 133270, loss = 0.70 (260.5 examples/sec; 0.491 sec/batch)
2016-02-04 03:25:56.901532: step 133280, loss = 0.51 (295.0 examples/sec; 0.434 sec/batch)
2016-02-04 03:26:01.624063: step 133290, loss = 0.77 (289.4 examples/sec; 0.442 sec/batch)
2016-02-04 03:26:06.339088: step 133300, loss = 0.58 (267.7 examples/sec; 0.478 sec/batch)
2016-02-04 03:26:11.446477: step 133310, loss = 0.77 (245.5 examples/sec; 0.521 sec/batch)
2016-02-04 03:26:16.176094: step 133320, loss = 0.74 (282.9 examples/sec; 0.452 sec/batch)
2016-02-04 03:26:20.877226: step 133330, loss = 0.60 (261.8 examples/sec; 0.489 sec/batch)
2016-02-04 03:26:25.585947: step 133340, loss = 0.59 (251.5 examples/sec; 0.509 sec/batch)
2016-02-04 03:26:30.298174: step 133350, loss = 0.68 (288.0 examples/sec; 0.444 sec/batch)
2016-02-04 03:26:34.983186: step 133360, loss = 0.65 (263.4 examples/sec; 0.486 sec/batch)
2016-02-04 03:26:39.562125: step 133370, loss = 0.66 (283.4 examples/sec; 0.452 sec/batch)
2016-02-04 03:26:44.240856: step 133380, loss = 0.65 (254.3 examples/sec; 0.503 sec/batch)
2016-02-04 03:26:48.945557: step 133390, loss = 0.79 (273.7 examples/sec; 0.468 sec/batch)
2016-02-04 03:26:53.707643: step 133400, loss = 0.64 (277.9 examples/sec; 0.461 sec/batch)
2016-02-04 03:26:59.015164: step 133410, loss = 0.64 (265.3 examples/sec; 0.482 sec/batch)
2016-02-04 03:27:03.717054: step 133420, loss = 0.80 (262.7 examples/sec; 0.487 sec/batch)
2016-02-04 03:27:08.447529: step 133430, loss = 0.64 (268.0 examples/sec; 0.478 sec/batch)
2016-02-04 03:27:13.164481: step 133440, loss = 0.52 (274.2 examples/sec; 0.467 sec/batch)
2016-02-04 03:27:17.885603: step 133450, loss = 0.62 (269.5 examples/sec; 0.475 sec/batch)
2016-02-04 03:27:22.527294: step 133460, loss = 0.58 (285.5 examples/sec; 0.448 sec/batch)
2016-02-04 03:27:27.300008: step 133470, loss = 0.81 (241.8 examples/sec; 0.529 sec/batch)
2016-02-04 03:27:31.936052: step 133480, loss = 0.73 (254.9 examples/sec; 0.502 sec/batch)
2016-02-04 03:27:36.633632: step 133490, loss = 0.76 (268.5 examples/sec; 0.477 sec/batch)
2016-02-04 03:27:41.261558: step 133500, loss = 0.76 (263.1 examples/sec; 0.487 sec/batch)
2016-02-04 03:27:46.434275: step 133510, loss = 0.60 (306.6 examples/sec; 0.417 sec/batch)
2016-02-04 03:27:51.158749: step 133520, loss = 0.68 (271.1 examples/sec; 0.472 sec/batch)
2016-02-04 03:27:55.906809: step 133530, loss = 0.72 (260.3 examples/sec; 0.492 sec/batch)
2016-02-04 03:28:00.632090: step 133540, loss = 0.76 (274.9 examples/sec; 0.466 sec/batch)
2016-02-04 03:28:05.352265: step 133550, loss = 0.62 (293.1 examples/sec; 0.437 sec/batch)
2016-02-04 03:28:10.082788: step 133560, loss = 0.63 (255.7 examples/sec; 0.501 sec/batch)
2016-02-04 03:28:14.752513: step 133570, loss = 0.79 (264.7 examples/sec; 0.484 sec/batch)
2016-02-04 03:28:19.519348: step 133580, loss = 0.70 (293.5 examples/sec; 0.436 sec/batch)
2016-02-04 03:28:24.159766: step 133590, loss = 0.68 (258.5 examples/sec; 0.495 sec/batch)
2016-02-04 03:28:28.917791: step 133600, loss = 0.81 (262.5 examples/sec; 0.488 sec/batch)
2016-02-04 03:28:34.096966: step 133610, loss = 0.68 (291.6 examples/sec; 0.439 sec/batch)
2016-02-04 03:28:38.807072: step 133620, loss = 0.76 (288.2 examples/sec; 0.444 sec/batch)
2016-02-04 03:28:43.563219: step 133630, loss = 0.56 (247.7 examples/sec; 0.517 sec/batch)
2016-02-04 03:28:48.273178: step 133640, loss = 0.66 (282.4 examples/sec; 0.453 sec/batch)
2016-02-04 03:28:53.032227: step 133650, loss = 0.62 (263.9 examples/sec; 0.485 sec/batch)
2016-02-04 03:28:57.750469: step 133660, loss = 0.73 (270.9 examples/sec; 0.473 sec/batch)
2016-02-04 03:29:02.545156: step 133670, loss = 0.70 (275.9 examples/sec; 0.464 sec/batch)
2016-02-04 03:29:07.301437: step 133680, loss = 0.65 (245.2 examples/sec; 0.522 sec/batch)
2016-02-04 03:29:12.051032: step 133690, loss = 0.73 (259.0 examples/sec; 0.494 sec/batch)
2016-02-04 03:29:16.834200: step 133700, loss = 0.58 (261.3 examples/sec; 0.490 sec/batch)
2016-02-04 03:29:21.988363: step 133710, loss = 0.69 (295.1 examples/sec; 0.434 sec/batch)
2016-02-04 03:29:26.742232: step 133720, loss = 0.67 (278.7 examples/sec; 0.459 sec/batch)
2016-02-04 03:29:31.465476: step 133730, loss = 0.79 (280.6 examples/sec; 0.456 sec/batch)
2016-02-04 03:29:36.206311: step 133740, loss = 0.69 (274.1 examples/sec; 0.467 sec/batch)
2016-02-04 03:29:41.022801: step 133750, loss = 0.60 (249.4 examples/sec; 0.513 sec/batch)
2016-02-04 03:29:45.721819: step 133760, loss = 0.66 (299.8 examples/sec; 0.427 sec/batch)
2016-02-04 03:29:50.442083: step 133770, loss = 0.56 (301.0 examples/sec; 0.425 sec/batch)
2016-02-04 03:29:55.223573: step 133780, loss = 0.65 (262.5 examples/sec; 0.488 sec/batch)
2016-02-04 03:29:59.898760: step 133790, loss = 0.70 (266.9 examples/sec; 0.480 sec/batch)
2016-02-04 03:30:04.636463: step 133800, loss = 0.60 (279.0 examples/sec; 0.459 sec/batch)
2016-02-04 03:30:09.812363: step 133810, loss = 0.60 (276.4 examples/sec; 0.463 sec/batch)
2016-02-04 03:30:14.514430: step 133820, loss = 0.63 (290.8 examples/sec; 0.440 sec/batch)
2016-02-04 03:30:19.281347: step 133830, loss = 0.73 (263.8 examples/sec; 0.485 sec/batch)
2016-02-04 03:30:23.924985: step 133840, loss = 0.77 (272.4 examples/sec; 0.470 sec/batch)
2016-02-04 03:30:28.657958: step 133850, loss = 0.66 (279.3 examples/sec; 0.458 sec/batch)
2016-02-04 03:30:33.413487: step 133860, loss = 0.64 (287.1 examples/sec; 0.446 sec/batch)
2016-02-04 03:30:38.101648: step 133870, loss = 0.67 (289.2 examples/sec; 0.443 sec/batch)
2016-02-04 03:30:42.847457: step 133880, loss = 0.58 (252.1 examples/sec; 0.508 sec/batch)
2016-02-04 03:30:47.603374: step 133890, loss = 0.65 (276.7 examples/sec; 0.463 sec/batch)
2016-02-04 03:30:52.315418: step 133900, loss = 0.62 (274.9 examples/sec; 0.466 sec/batch)
2016-02-04 03:30:57.495051: step 133910, loss = 0.69 (256.7 examples/sec; 0.499 sec/batch)
2016-02-04 03:31:02.234124: step 133920, loss = 0.68 (261.9 examples/sec; 0.489 sec/batch)
2016-02-04 03:31:06.962536: step 133930, loss = 0.59 (273.7 examples/sec; 0.468 sec/batch)
2016-02-04 03:31:11.686576: step 133940, loss = 0.63 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 03:31:16.369889: step 133950, loss = 0.71 (258.5 examples/sec; 0.495 sec/batch)
2016-02-04 03:31:21.014601: step 133960, loss = 0.64 (288.4 examples/sec; 0.444 sec/batch)
2016-02-04 03:31:25.794592: step 133970, loss = 0.68 (265.4 examples/sec; 0.482 sec/batch)
2016-02-04 03:31:30.486747: step 133980, loss = 0.62 (299.8 examples/sec; 0.427 sec/batch)
2016-02-04 03:31:35.253039: step 133990, loss = 0.56 (265.9 examples/sec; 0.481 sec/batch)
2016-02-04 03:31:39.858942: step 134000, loss = 0.69 (288.4 examples/sec; 0.444 sec/batch)
2016-02-04 03:31:45.187832: step 134010, loss = 0.68 (274.7 examples/sec; 0.466 sec/batch)
2016-02-04 03:31:49.912842: step 134020, loss = 0.64 (272.0 examples/sec; 0.471 sec/batch)
2016-02-04 03:31:54.631973: step 134030, loss = 0.67 (269.5 examples/sec; 0.475 sec/batch)
2016-02-04 03:31:59.380762: step 134040, loss = 0.67 (261.6 examples/sec; 0.489 sec/batch)
2016-02-04 03:32:04.066585: step 134050, loss = 0.64 (275.2 examples/sec; 0.465 sec/batch)
2016-02-04 03:32:08.725636: step 134060, loss = 0.60 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 03:32:13.499003: step 134070, loss = 0.57 (272.7 examples/sec; 0.469 sec/batch)
2016-02-04 03:32:18.248629: step 134080, loss = 0.70 (265.4 examples/sec; 0.482 sec/batch)
2016-02-04 03:32:22.975904: step 134090, loss = 0.67 (269.8 examples/sec; 0.474 sec/batch)
2016-02-04 03:32:27.682247: step 134100, loss = 0.84 (258.6 examples/sec; 0.495 sec/batch)
2016-02-04 03:32:32.907744: step 134110, loss = 0.68 (275.6 examples/sec; 0.464 sec/batch)
2016-02-04 03:32:37.650983: step 134120, loss = 0.76 (258.5 examples/sec; 0.495 sec/batch)
2016-02-04 03:32:42.399886: step 134130, loss = 0.62 (257.8 examples/sec; 0.496 sec/batch)
2016-02-04 03:32:47.101709: step 134140, loss = 0.67 (261.7 examples/sec; 0.489 sec/batch)
2016-02-04 03:32:51.771749: step 134150, loss = 0.77 (288.6 examples/sec; 0.444 sec/batch)
2016-02-04 03:32:56.534699: step 134160, loss = 0.76 (279.4 examples/sec; 0.458 sec/batch)
2016-02-04 03:33:01.299734: step 134170, loss = 0.85 (270.4 examples/sec; 0.473 sec/batch)
2016-02-04 03:33:05.981863: step 134180, loss = 0.70 (295.6 examples/sec; 0.433 sec/batch)
2016-02-04 03:33:10.691034: step 134190, loss = 0.67 (257.5 examples/sec; 0.497 sec/batch)
2016-02-04 03:33:15.457068: step 134200, loss = 0.64 (267.6 examples/sec; 0.478 sec/batch)
2016-02-04 03:33:20.603163: step 134210, loss = 0.51 (265.8 examples/sec; 0.482 sec/batch)
2016-02-04 03:33:25.350883: step 134220, loss = 0.81 (285.0 examples/sec; 0.449 sec/batch)
2016-02-04 03:33:30.014399: step 134230, loss = 0.75 (305.9 examples/sec; 0.418 sec/batch)
2016-02-04 03:33:34.774403: step 134240, loss = 0.76 (257.2 examples/sec; 0.498 sec/batch)
2016-02-04 03:33:39.476713: step 134250, loss = 0.81 (294.0 examples/sec; 0.435 sec/batch)
2016-02-04 03:33:44.251823: step 134260, loss = 0.67 (252.6 examples/sec; 0.507 sec/batch)
2016-02-04 03:33:48.932713: step 134270, loss = 0.67 (281.4 examples/sec; 0.455 sec/batch)
2016-02-04 03:33:53.673942: step 134280, loss = 0.59 (273.4 examples/sec; 0.468 sec/batch)
2016-02-04 03:33:58.308495: step 134290, loss = 0.52 (284.5 examples/sec; 0.450 sec/batch)
2016-02-04 03:34:03.138942: step 134300, loss = 0.77 (272.9 examples/sec; 0.469 sec/batch)
2016-02-04 03:34:08.401783: step 134310, loss = 0.61 (287.1 examples/sec; 0.446 sec/batch)
2016-02-04 03:34:13.168142: step 134320, loss = 0.67 (242.4 examples/sec; 0.528 sec/batch)
2016-02-04 03:34:17.878520: step 134330, loss = 0.80 (315.5 examples/sec; 0.406 sec/batch)
2016-02-04 03:34:22.596597: step 134340, loss = 0.78 (281.8 examples/sec; 0.454 sec/batch)
2016-02-04 03:34:27.323715: step 134350, loss = 0.70 (260.4 examples/sec; 0.492 sec/batch)
2016-02-04 03:34:31.951839: step 134360, loss = 0.77 (271.2 examples/sec; 0.472 sec/batch)
2016-02-04 03:34:36.685990: step 134370, loss = 0.79 (263.5 examples/sec; 0.486 sec/batch)
2016-02-04 03:34:41.408107: step 134380, loss = 0.73 (263.5 examples/sec; 0.486 sec/batch)
2016-02-04 03:34:46.166659: step 134390, loss = 0.80 (274.7 examples/sec; 0.466 sec/batch)
2016-02-04 03:34:50.829313: step 134400, loss = 0.56 (294.2 examples/sec; 0.435 sec/batch)
2016-02-04 03:34:56.047132: step 134410, loss = 0.57 (282.3 examples/sec; 0.453 sec/batch)
2016-02-04 03:35:00.666127: step 134420, loss = 0.76 (264.4 examples/sec; 0.484 sec/batch)
2016-02-04 03:35:05.398554: step 134430, loss = 0.83 (269.5 examples/sec; 0.475 sec/batch)
2016-02-04 03:35:10.149357: step 134440, loss = 0.62 (274.6 examples/sec; 0.466 sec/batch)
2016-02-04 03:35:14.841206: step 134450, loss = 0.68 (269.5 examples/sec; 0.475 sec/batch)
2016-02-04 03:35:19.633363: step 134460, loss = 0.62 (249.2 examples/sec; 0.514 sec/batch)
2016-02-04 03:35:24.380186: step 134470, loss = 0.58 (262.6 examples/sec; 0.487 sec/batch)
2016-02-04 03:35:29.083811: step 134480, loss = 0.71 (274.2 examples/sec; 0.467 sec/batch)
2016-02-04 03:35:33.761356: step 134490, loss = 0.62 (268.0 examples/sec; 0.478 sec/batch)
2016-02-04 03:35:38.480268: step 134500, loss = 0.65 (282.7 examples/sec; 0.453 sec/batch)
2016-02-04 03:35:43.814486: step 134510, loss = 0.65 (245.2 examples/sec; 0.522 sec/batch)
2016-02-04 03:35:48.518889: step 134520, loss = 0.63 (306.4 examples/sec; 0.418 sec/batch)
2016-02-04 03:35:53.256313: step 134530, loss = 0.73 (284.4 examples/sec; 0.450 sec/batch)
2016-02-04 03:35:58.011879: step 134540, loss = 0.70 (284.0 examples/sec; 0.451 sec/batch)
2016-02-04 03:36:02.759386: step 134550, loss = 0.69 (254.0 examples/sec; 0.504 sec/batch)
2016-02-04 03:36:07.394478: step 134560, loss = 0.61 (282.8 examples/sec; 0.453 sec/batch)
2016-02-04 03:36:12.094887: step 134570, loss = 0.64 (267.6 examples/sec; 0.478 sec/batch)
2016-02-04 03:36:16.881521: step 134580, loss = 0.62 (269.8 examples/sec; 0.474 sec/batch)
2016-02-04 03:36:21.598553: step 134590, loss = 0.64 (253.6 examples/sec; 0.505 sec/batch)
2016-02-04 03:36:26.257329: step 134600, loss = 0.59 (284.4 examples/sec; 0.450 sec/batch)
2016-02-04 03:36:31.445728: step 134610, loss = 0.57 (262.0 examples/sec; 0.489 sec/batch)
2016-02-04 03:36:36.118175: step 134620, loss = 0.76 (259.7 examples/sec; 0.493 sec/batch)
2016-02-04 03:36:40.806407: step 134630, loss = 0.69 (279.0 examples/sec; 0.459 sec/batch)
2016-02-04 03:36:45.594389: step 134640, loss = 0.68 (270.4 examples/sec; 0.473 sec/batch)
2016-02-04 03:36:50.305569: step 134650, loss = 0.81 (258.2 examples/sec; 0.496 sec/batch)
2016-02-04 03:36:55.054042: step 134660, loss = 0.62 (268.2 examples/sec; 0.477 sec/batch)
2016-02-04 03:36:59.742112: step 134670, loss = 0.64 (268.3 examples/sec; 0.477 sec/batch)
2016-02-04 03:37:04.471819: step 134680, loss = 0.69 (268.3 examples/sec; 0.477 sec/batch)
2016-02-04 03:37:09.191536: step 134690, loss = 0.60 (277.4 examples/sec; 0.461 sec/batch)
2016-02-04 03:37:13.978287: step 134700, loss = 0.65 (261.5 examples/sec; 0.489 sec/batch)
2016-02-04 03:37:19.181558: step 134710, loss = 0.52 (251.5 examples/sec; 0.509 sec/batch)
2016-02-04 03:37:23.848549: step 134720, loss = 0.65 (276.3 examples/sec; 0.463 sec/batch)
2016-02-04 03:37:28.582008: step 134730, loss = 0.65 (256.9 examples/sec; 0.498 sec/batch)
2016-02-04 03:37:33.282696: step 134740, loss = 0.78 (251.0 examples/sec; 0.510 sec/batch)
2016-02-04 03:37:37.924296: step 134750, loss = 0.66 (273.4 examples/sec; 0.468 sec/batch)
2016-02-04 03:37:42.619875: step 134760, loss = 0.68 (285.9 examples/sec; 0.448 sec/batch)
2016-02-04 03:37:47.296763: step 134770, loss = 0.54 (264.0 examples/sec; 0.485 sec/batch)
2016-02-04 03:37:51.955963: step 134780, loss = 0.70 (258.9 examples/sec; 0.494 sec/batch)
2016-02-04 03:37:56.611802: step 134790, loss = 0.63 (255.8 examples/sec; 0.500 sec/batch)
2016-02-04 03:38:01.312357: step 134800, loss = 0.72 (263.1 examples/sec; 0.487 sec/batch)
2016-02-04 03:38:06.450996: step 134810, loss = 0.74 (288.5 examples/sec; 0.444 sec/batch)
2016-02-04 03:38:11.186264: step 134820, loss = 0.72 (285.6 examples/sec; 0.448 sec/batch)
2016-02-04 03:38:15.878964: step 134830, loss = 0.73 (259.7 examples/sec; 0.493 sec/batch)
2016-02-04 03:38:20.646262: step 134840, loss = 0.56 (290.3 examples/sec; 0.441 sec/batch)
2016-02-04 03:38:25.356212: step 134850, loss = 0.72 (265.1 examples/sec; 0.483 sec/batch)
2016-02-04 03:38:30.006724: step 134860, loss = 0.54 (282.1 examples/sec; 0.454 sec/batch)
2016-02-04 03:38:34.736399: step 134870, loss = 0.71 (246.9 examples/sec; 0.518 sec/batch)
2016-02-04 03:38:39.320018: step 134880, loss = 0.64 (289.8 examples/sec; 0.442 sec/batch)
2016-02-04 03:38:44.120949: step 134890, loss = 0.56 (257.9 examples/sec; 0.496 sec/batch)
2016-02-04 03:38:48.782308: step 134900, loss = 0.57 (284.3 examples/sec; 0.450 sec/batch)
2016-02-04 03:38:54.098087: step 134910, loss = 0.68 (253.8 examples/sec; 0.504 sec/batch)
2016-02-04 03:38:58.697430: step 134920, loss = 0.57 (273.9 examples/sec; 0.467 sec/batch)
2016-02-04 03:39:03.336208: step 134930, loss = 0.60 (268.1 examples/sec; 0.478 sec/batch)
2016-02-04 03:39:07.946185: step 134940, loss = 0.61 (311.7 examples/sec; 0.411 sec/batch)
2016-02-04 03:39:12.664957: step 134950, loss = 0.69 (264.9 examples/sec; 0.483 sec/batch)
2016-02-04 03:39:17.334525: step 134960, loss = 0.70 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 03:39:21.845259: step 134970, loss = 0.68 (278.9 examples/sec; 0.459 sec/batch)
2016-02-04 03:39:26.510107: step 134980, loss = 0.58 (294.8 examples/sec; 0.434 sec/batch)
2016-02-04 03:39:31.260694: step 134990, loss = 0.78 (282.8 examples/sec; 0.453 sec/batch)
2016-02-04 03:39:35.985706: step 135000, loss = 0.77 (297.8 examples/sec; 0.430 sec/batch)
2016-02-04 03:39:41.193473: step 135010, loss = 0.79 (287.5 examples/sec; 0.445 sec/batch)
2016-02-04 03:39:45.915192: step 135020, loss = 0.63 (260.9 examples/sec; 0.491 sec/batch)
2016-02-04 03:39:50.624954: step 135030, loss = 0.77 (268.6 examples/sec; 0.477 sec/batch)
2016-02-04 03:39:55.398074: step 135040, loss = 0.71 (250.8 examples/sec; 0.510 sec/batch)
2016-02-04 03:40:00.005105: step 135050, loss = 0.75 (313.8 examples/sec; 0.408 sec/batch)
2016-02-04 03:40:04.690956: step 135060, loss = 0.81 (258.5 examples/sec; 0.495 sec/batch)
2016-02-04 03:40:09.248699: step 135070, loss = 0.59 (279.2 examples/sec; 0.458 sec/batch)
2016-02-04 03:40:13.991639: step 135080, loss = 0.74 (247.5 examples/sec; 0.517 sec/batch)
2016-02-04 03:40:18.687444: step 135090, loss = 0.76 (277.7 examples/sec; 0.461 sec/batch)
2016-02-04 03:40:23.391324: step 135100, loss = 0.61 (275.2 examples/sec; 0.465 sec/batch)
2016-02-04 03:40:28.618445: step 135110, loss = 0.63 (256.4 examples/sec; 0.499 sec/batch)
2016-02-04 03:40:33.346198: step 135120, loss = 0.67 (258.8 examples/sec; 0.495 sec/batch)
2016-02-04 03:40:37.963548: step 135130, loss = 0.64 (282.4 examples/sec; 0.453 sec/batch)
2016-02-04 03:40:42.567242: step 135140, loss = 0.58 (274.0 examples/sec; 0.467 sec/batch)
2016-02-04 03:40:47.146950: step 135150, loss = 0.74 (268.0 examples/sec; 0.478 sec/batch)
2016-02-04 03:40:51.876371: step 135160, loss = 0.66 (246.6 examples/sec; 0.519 sec/batch)
2016-02-04 03:40:56.539786: step 135170, loss = 0.66 (249.1 examples/sec; 0.514 sec/batch)
2016-02-04 03:41:01.193319: step 135180, loss = 0.58 (259.8 examples/sec; 0.493 sec/batch)
2016-02-04 03:41:05.940806: step 135190, loss = 0.70 (271.2 examples/sec; 0.472 sec/batch)
2016-02-04 03:41:10.632857: step 135200, loss = 0.64 (257.7 examples/sec; 0.497 sec/batch)
2016-02-04 03:41:15.814308: step 135210, loss = 0.73 (259.1 examples/sec; 0.494 sec/batch)
2016-02-04 03:41:20.545441: step 135220, loss = 0.73 (273.6 examples/sec; 0.468 sec/batch)
2016-02-04 03:41:25.230520: step 135230, loss = 0.68 (255.3 examples/sec; 0.501 sec/batch)
2016-02-04 03:41:29.972274: step 135240, loss = 0.73 (255.6 examples/sec; 0.501 sec/batch)
2016-02-04 03:41:34.537006: step 135250, loss = 0.65 (285.3 examples/sec; 0.449 sec/batch)
2016-02-04 03:41:39.144782: step 135260, loss = 0.74 (293.6 examples/sec; 0.436 sec/batch)
2016-02-04 03:41:43.755296: step 135270, loss = 0.71 (272.5 examples/sec; 0.470 sec/batch)
2016-02-04 03:41:48.427792: step 135280, loss = 0.67 (257.4 examples/sec; 0.497 sec/batch)
2016-02-04 03:41:53.080451: step 135290, loss = 0.63 (294.5 examples/sec; 0.435 sec/batch)
2016-02-04 03:41:57.811167: step 135300, loss = 0.82 (283.7 examples/sec; 0.451 sec/batch)
2016-02-04 03:42:02.996321: step 135310, loss = 0.78 (290.1 examples/sec; 0.441 sec/batch)
2016-02-04 03:42:07.691364: step 135320, loss = 0.68 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 03:42:12.319802: step 135330, loss = 0.56 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 03:42:17.007842: step 135340, loss = 0.68 (289.1 examples/sec; 0.443 sec/batch)
2016-02-04 03:42:21.698337: step 135350, loss = 0.84 (273.9 examples/sec; 0.467 sec/batch)
2016-02-04 03:42:26.393698: step 135360, loss = 0.69 (255.3 examples/sec; 0.501 sec/batch)
2016-02-04 03:42:31.106154: step 135370, loss = 0.61 (250.4 examples/sec; 0.511 sec/batch)
2016-02-04 03:42:35.860181: step 135380, loss = 0.74 (289.5 examples/sec; 0.442 sec/batch)
2016-02-04 03:42:40.532843: step 135390, loss = 0.61 (284.5 examples/sec; 0.450 sec/batch)
2016-02-04 03:42:45.249508: step 135400, loss = 0.75 (277.3 examples/sec; 0.462 sec/batch)
2016-02-04 03:42:50.524761: step 135410, loss = 0.72 (276.5 examples/sec; 0.463 sec/batch)
2016-02-04 03:42:55.169920: step 135420, loss = 0.58 (265.1 examples/sec; 0.483 sec/batch)
2016-02-04 03:42:59.859137: step 135430, loss = 0.66 (262.6 examples/sec; 0.488 sec/batch)
2016-02-04 03:43:04.658770: step 135440, loss = 0.61 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 03:43:09.407971: step 135450, loss = 0.68 (255.2 examples/sec; 0.502 sec/batch)
2016-02-04 03:43:14.091466: step 135460, loss = 0.63 (251.2 examples/sec; 0.510 sec/batch)
2016-02-04 03:43:18.696495: step 135470, loss = 0.64 (279.7 examples/sec; 0.458 sec/batch)
2016-02-04 03:43:23.391618: step 135480, loss = 0.80 (262.7 examples/sec; 0.487 sec/batch)
2016-02-04 03:43:28.060394: step 135490, loss = 0.69 (285.6 examples/sec; 0.448 sec/batch)
2016-02-04 03:43:32.734714: step 135500, loss = 0.60 (246.3 examples/sec; 0.520 sec/batch)
2016-02-04 03:43:37.917841: step 135510, loss = 0.54 (271.1 examples/sec; 0.472 sec/batch)
2016-02-04 03:43:42.568817: step 135520, loss = 0.69 (276.9 examples/sec; 0.462 sec/batch)
2016-02-04 03:43:47.193542: step 135530, loss = 0.63 (279.5 examples/sec; 0.458 sec/batch)
2016-02-04 03:43:51.876263: step 135540, loss = 0.71 (289.2 examples/sec; 0.443 sec/batch)
2016-02-04 03:43:56.582280: step 135550, loss = 0.83 (270.4 examples/sec; 0.473 sec/batch)
2016-02-04 03:44:01.238703: step 135560, loss = 0.76 (279.6 examples/sec; 0.458 sec/batch)
2016-02-04 03:44:05.918339: step 135570, loss = 0.64 (264.7 examples/sec; 0.483 sec/batch)
2016-02-04 03:44:10.564212: step 135580, loss = 0.75 (276.2 examples/sec; 0.463 sec/batch)
2016-02-04 03:44:15.186234: step 135590, loss = 0.62 (278.8 examples/sec; 0.459 sec/batch)
2016-02-04 03:44:19.854717: step 135600, loss = 0.60 (272.7 examples/sec; 0.469 sec/batch)
2016-02-04 03:44:25.083291: step 135610, loss = 0.54 (284.6 examples/sec; 0.450 sec/batch)
2016-02-04 03:44:29.827118: step 135620, loss = 0.59 (268.5 examples/sec; 0.477 sec/batch)
2016-02-04 03:44:34.532654: step 135630, loss = 0.78 (283.6 examples/sec; 0.451 sec/batch)
2016-02-04 03:44:39.159161: step 135640, loss = 0.87 (254.3 examples/sec; 0.503 sec/batch)
2016-02-04 03:44:43.899550: step 135650, loss = 0.57 (269.4 examples/sec; 0.475 sec/batch)
2016-02-04 03:44:48.609391: step 135660, loss = 0.87 (271.5 examples/sec; 0.471 sec/batch)
2016-02-04 03:44:53.298905: step 135670, loss = 0.58 (267.9 examples/sec; 0.478 sec/batch)
2016-02-04 03:44:57.995552: step 135680, loss = 0.63 (251.9 examples/sec; 0.508 sec/batch)
2016-02-04 03:45:02.646994: step 135690, loss = 0.69 (287.3 examples/sec; 0.445 sec/batch)
2016-02-04 03:45:07.375977: step 135700, loss = 0.58 (266.2 examples/sec; 0.481 sec/batch)
2016-02-04 03:45:12.620270: step 135710, loss = 0.58 (264.6 examples/sec; 0.484 sec/batch)
2016-02-04 03:45:17.286782: step 135720, loss = 0.59 (262.7 examples/sec; 0.487 sec/batch)
2016-02-04 03:45:22.005671: step 135730, loss = 0.52 (264.0 examples/sec; 0.485 sec/batch)
2016-02-04 03:45:26.623173: step 135740, loss = 0.78 (297.8 examples/sec; 0.430 sec/batch)
2016-02-04 03:45:31.263608: step 135750, loss = 0.59 (279.5 examples/sec; 0.458 sec/batch)
2016-02-04 03:45:36.032132: step 135760, loss = 0.67 (282.5 examples/sec; 0.453 sec/batch)
2016-02-04 03:45:40.721081: step 135770, loss = 0.59 (275.0 examples/sec; 0.465 sec/batch)
2016-02-04 03:45:45.484242: step 135780, loss = 0.73 (303.6 examples/sec; 0.422 sec/batch)
2016-02-04 03:45:50.237137: step 135790, loss = 0.67 (290.3 examples/sec; 0.441 sec/batch)
2016-02-04 03:45:54.999417: step 135800, loss = 0.72 (271.5 examples/sec; 0.471 sec/batch)
2016-02-04 03:46:00.179490: step 135810, loss = 0.61 (302.6 examples/sec; 0.423 sec/batch)
2016-02-04 03:46:04.772883: step 135820, loss = 0.65 (269.0 examples/sec; 0.476 sec/batch)
2016-02-04 03:46:09.496595: step 135830, loss = 0.68 (272.1 examples/sec; 0.470 sec/batch)
2016-02-04 03:46:14.184431: step 135840, loss = 0.60 (304.5 examples/sec; 0.420 sec/batch)
2016-02-04 03:46:18.876188: step 135850, loss = 0.66 (308.4 examples/sec; 0.415 sec/batch)
2016-02-04 03:46:23.643377: step 135860, loss = 0.66 (256.2 examples/sec; 0.500 sec/batch)
2016-02-04 03:46:28.302684: step 135870, loss = 0.61 (273.5 examples/sec; 0.468 sec/batch)
2016-02-04 03:46:32.898976: step 135880, loss = 0.57 (319.9 examples/sec; 0.400 sec/batch)
2016-02-04 03:46:37.612147: step 135890, loss = 0.73 (274.2 examples/sec; 0.467 sec/batch)
2016-02-04 03:46:42.317911: step 135900, loss = 0.64 (268.1 examples/sec; 0.477 sec/batch)
2016-02-04 03:46:47.515762: step 135910, loss = 0.57 (268.6 examples/sec; 0.477 sec/batch)
2016-02-04 03:46:52.255815: step 135920, loss = 0.69 (269.0 examples/sec; 0.476 sec/batch)
2016-02-04 03:46:56.953228: step 135930, loss = 0.65 (262.5 examples/sec; 0.488 sec/batch)
2016-02-04 03:47:01.630162: step 135940, loss = 0.62 (261.2 examples/sec; 0.490 sec/batch)
2016-02-04 03:47:06.284300: step 135950, loss = 0.64 (294.5 examples/sec; 0.435 sec/batch)
2016-02-04 03:47:11.005947: step 135960, loss = 0.76 (270.5 examples/sec; 0.473 sec/batch)
2016-02-04 03:47:15.766460: step 135970, loss = 0.72 (259.3 examples/sec; 0.494 sec/batch)
2016-02-04 03:47:20.402160: step 135980, loss = 0.75 (255.2 examples/sec; 0.502 sec/batch)
2016-02-04 03:47:25.078997: step 135990, loss = 0.69 (279.6 examples/sec; 0.458 sec/batch)
2016-02-04 03:47:29.686905: step 136000, loss = 0.60 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 03:47:34.961014: step 136010, loss = 0.78 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 03:47:39.563881: step 136020, loss = 0.73 (280.2 examples/sec; 0.457 sec/batch)
2016-02-04 03:47:44.266417: step 136030, loss = 0.66 (265.2 examples/sec; 0.483 sec/batch)
2016-02-04 03:47:48.925699: step 136040, loss = 0.74 (325.0 examples/sec; 0.394 sec/batch)
2016-02-04 03:47:53.646215: step 136050, loss = 0.68 (287.1 examples/sec; 0.446 sec/batch)
2016-02-04 03:47:58.280668: step 136060, loss = 0.70 (272.1 examples/sec; 0.470 sec/batch)
2016-02-04 03:48:03.039253: step 136070, loss = 0.69 (244.2 examples/sec; 0.524 sec/batch)
2016-02-04 03:48:07.709837: step 136080, loss = 0.66 (280.2 examples/sec; 0.457 sec/batch)
2016-02-04 03:48:12.443832: step 136090, loss = 0.57 (287.4 examples/sec; 0.445 sec/batch)
2016-02-04 03:48:17.219882: step 136100, loss = 0.61 (274.6 examples/sec; 0.466 sec/batch)
2016-02-04 03:48:22.403456: step 136110, loss = 0.62 (276.4 examples/sec; 0.463 sec/batch)
2016-02-04 03:48:27.031145: step 136120, loss = 0.62 (299.1 examples/sec; 0.428 sec/batch)
2016-02-04 03:48:31.721526: step 136130, loss = 0.63 (285.8 examples/sec; 0.448 sec/batch)
2016-02-04 03:48:36.448449: step 136140, loss = 0.65 (256.1 examples/sec; 0.500 sec/batch)
2016-02-04 03:48:41.101287: step 136150, loss = 0.44 (258.6 examples/sec; 0.495 sec/batch)
2016-02-04 03:48:45.832116: step 136160, loss = 0.64 (255.1 examples/sec; 0.502 sec/batch)
2016-02-04 03:48:50.489661: step 136170, loss = 0.69 (300.4 examples/sec; 0.426 sec/batch)
2016-02-04 03:48:55.250496: step 136180, loss = 0.73 (265.9 examples/sec; 0.481 sec/batch)
2016-02-04 03:48:59.938157: step 136190, loss = 0.67 (269.0 examples/sec; 0.476 sec/batch)
2016-02-04 03:49:04.681993: step 136200, loss = 0.66 (266.4 examples/sec; 0.480 sec/batch)
2016-02-04 03:49:09.890101: step 136210, loss = 0.56 (262.6 examples/sec; 0.487 sec/batch)
2016-02-04 03:49:14.615508: step 136220, loss = 0.72 (272.3 examples/sec; 0.470 sec/batch)
2016-02-04 03:49:19.358646: step 136230, loss = 0.74 (272.1 examples/sec; 0.470 sec/batch)
2016-02-04 03:49:24.083217: step 136240, loss = 0.70 (255.2 examples/sec; 0.502 sec/batch)
2016-02-04 03:49:28.815027: step 136250, loss = 0.75 (248.9 examples/sec; 0.514 sec/batch)
2016-02-04 03:49:33.438483: step 136260, loss = 0.59 (300.4 examples/sec; 0.426 sec/batch)
2016-02-04 03:49:38.177539: step 136270, loss = 0.63 (275.7 examples/sec; 0.464 sec/batch)
2016-02-04 03:49:42.833756: step 136280, loss = 0.60 (260.8 examples/sec; 0.491 sec/batch)
2016-02-04 03:49:47.519282: step 136290, loss = 0.63 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 03:49:52.143375: step 136300, loss = 0.64 (276.6 examples/sec; 0.463 sec/batch)
2016-02-04 03:49:57.391202: step 136310, loss = 0.74 (279.4 examples/sec; 0.458 sec/batch)
2016-02-04 03:50:02.169691: step 136320, loss = 0.65 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 03:50:06.875818: step 136330, loss = 0.59 (256.7 examples/sec; 0.499 sec/batch)
2016-02-04 03:50:11.581969: step 136340, loss = 0.80 (268.4 examples/sec; 0.477 sec/batch)
2016-02-04 03:50:16.377053: step 136350, loss = 0.58 (258.8 examples/sec; 0.495 sec/batch)
2016-02-04 03:50:21.035142: step 136360, loss = 0.68 (271.8 examples/sec; 0.471 sec/batch)
2016-02-04 03:50:25.722104: step 136370, loss = 0.73 (275.2 examples/sec; 0.465 sec/batch)
2016-02-04 03:50:30.414973: step 136380, loss = 0.70 (297.0 examples/sec; 0.431 sec/batch)
2016-02-04 03:50:35.122424: step 136390, loss = 0.89 (267.1 examples/sec; 0.479 sec/batch)
2016-02-04 03:50:39.795286: step 136400, loss = 0.62 (283.8 examples/sec; 0.451 sec/batch)
2016-02-04 03:50:45.032908: step 136410, loss = 0.66 (290.7 examples/sec; 0.440 sec/batch)
2016-02-04 03:50:49.768481: step 136420, loss = 0.61 (280.8 examples/sec; 0.456 sec/batch)
2016-02-04 03:50:54.431380: step 136430, loss = 0.73 (288.5 examples/sec; 0.444 sec/batch)
2016-02-04 03:50:59.165179: step 136440, loss = 0.79 (255.0 examples/sec; 0.502 sec/batch)
2016-02-04 03:51:03.759121: step 136450, loss = 0.62 (294.5 examples/sec; 0.435 sec/batch)
2016-02-04 03:51:08.426653: step 136460, loss = 0.62 (297.6 examples/sec; 0.430 sec/batch)
2016-02-04 03:51:13.079303: step 136470, loss = 0.72 (282.5 examples/sec; 0.453 sec/batch)
2016-02-04 03:51:17.682756: step 136480, loss = 0.68 (279.8 examples/sec; 0.457 sec/batch)
2016-02-04 03:51:22.312738: step 136490, loss = 0.62 (266.8 examples/sec; 0.480 sec/batch)
2016-02-04 03:51:26.956999: step 136500, loss = 0.87 (282.6 examples/sec; 0.453 sec/batch)
2016-02-04 03:51:32.139063: step 136510, loss = 0.70 (286.8 examples/sec; 0.446 sec/batch)
2016-02-04 03:51:36.825082: step 136520, loss = 0.67 (278.1 examples/sec; 0.460 sec/batch)
2016-02-04 03:51:41.471809: step 136530, loss = 0.81 (295.6 examples/sec; 0.433 sec/batch)
2016-02-04 03:51:46.233810: step 136540, loss = 0.75 (265.5 examples/sec; 0.482 sec/batch)
2016-02-04 03:51:50.896800: step 136550, loss = 0.70 (272.4 examples/sec; 0.470 sec/batch)
2016-02-04 03:51:55.713581: step 136560, loss = 0.59 (255.4 examples/sec; 0.501 sec/batch)
2016-02-04 03:52:00.409379: step 136570, loss = 0.64 (293.2 examples/sec; 0.437 sec/batch)
2016-02-04 03:52:05.172373: step 136580, loss = 0.73 (265.5 examples/sec; 0.482 sec/batch)
2016-02-04 03:52:09.846632: step 136590, loss = 0.72 (255.4 examples/sec; 0.501 sec/batch)
2016-02-04 03:52:14.605582: step 136600, loss = 0.70 (271.3 examples/sec; 0.472 sec/batch)
2016-02-04 03:52:19.782982: step 136610, loss = 0.71 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 03:52:24.485199: step 136620, loss = 0.64 (294.7 examples/sec; 0.434 sec/batch)
2016-02-04 03:52:29.191845: step 136630, loss = 0.63 (264.7 examples/sec; 0.484 sec/batch)
2016-02-04 03:52:33.936994: step 136640, loss = 0.75 (272.2 examples/sec; 0.470 sec/batch)
2016-02-04 03:52:38.596169: step 136650, loss = 0.74 (281.4 examples/sec; 0.455 sec/batch)
2016-02-04 03:52:43.253933: step 136660, loss = 0.62 (288.2 examples/sec; 0.444 sec/batch)
2016-02-04 03:52:47.927872: step 136670, loss = 0.74 (252.2 examples/sec; 0.508 sec/batch)
2016-02-04 03:52:52.551544: step 136680, loss = 0.70 (285.3 examples/sec; 0.449 sec/batch)
2016-02-04 03:52:57.201697: step 136690, loss = 0.67 (290.9 examples/sec; 0.440 sec/batch)
2016-02-04 03:53:01.913028: step 136700, loss = 0.74 (272.4 examples/sec; 0.470 sec/batch)
2016-02-04 03:53:07.040956: step 136710, loss = 0.62 (272.4 examples/sec; 0.470 sec/batch)
2016-02-04 03:53:11.767137: step 136720, loss = 0.60 (264.8 examples/sec; 0.483 sec/batch)
2016-02-04 03:53:16.421170: step 136730, loss = 0.69 (278.4 examples/sec; 0.460 sec/batch)
2016-02-04 03:53:21.074120: step 136740, loss = 0.57 (285.9 examples/sec; 0.448 sec/batch)
2016-02-04 03:53:25.746573: step 136750, loss = 0.61 (289.8 examples/sec; 0.442 sec/batch)
2016-02-04 03:53:30.436079: step 136760, loss = 0.61 (272.3 examples/sec; 0.470 sec/batch)
2016-02-04 03:53:35.103083: step 136770, loss = 0.61 (276.5 examples/sec; 0.463 sec/batch)
2016-02-04 03:53:39.718214: step 136780, loss = 0.59 (304.3 examples/sec; 0.421 sec/batch)
2016-02-04 03:53:44.411130: step 136790, loss = 0.61 (269.5 examples/sec; 0.475 sec/batch)
2016-02-04 03:53:49.063185: step 136800, loss = 0.59 (283.2 examples/sec; 0.452 sec/batch)
2016-02-04 03:53:54.342221: step 136810, loss = 0.55 (265.3 examples/sec; 0.482 sec/batch)
2016-02-04 03:53:59.056933: step 136820, loss = 0.72 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 03:54:03.701696: step 136830, loss = 0.58 (278.7 examples/sec; 0.459 sec/batch)
2016-02-04 03:54:08.326734: step 136840, loss = 0.60 (286.3 examples/sec; 0.447 sec/batch)
2016-02-04 03:54:13.086891: step 136850, loss = 0.53 (248.3 examples/sec; 0.515 sec/batch)
2016-02-04 03:54:17.665181: step 136860, loss = 0.58 (317.7 examples/sec; 0.403 sec/batch)
2016-02-04 03:54:22.357152: step 136870, loss = 0.64 (274.8 examples/sec; 0.466 sec/batch)
2016-02-04 03:54:27.112911: step 136880, loss = 0.47 (264.6 examples/sec; 0.484 sec/batch)
2016-02-04 03:54:31.749652: step 136890, loss = 0.66 (294.4 examples/sec; 0.435 sec/batch)
2016-02-04 03:54:36.523135: step 136900, loss = 0.58 (272.6 examples/sec; 0.470 sec/batch)
2016-02-04 03:54:41.696690: step 136910, loss = 0.63 (289.2 examples/sec; 0.443 sec/batch)
2016-02-04 03:54:46.364181: step 136920, loss = 0.60 (254.9 examples/sec; 0.502 sec/batch)
2016-02-04 03:54:51.000361: step 136930, loss = 0.51 (278.0 examples/sec; 0.460 sec/batch)
2016-02-04 03:54:55.721063: step 136940, loss = 0.51 (264.7 examples/sec; 0.484 sec/batch)
2016-02-04 03:55:00.451253: step 136950, loss = 0.54 (281.3 examples/sec; 0.455 sec/batch)
2016-02-04 03:55:05.157042: step 136960, loss = 0.48 (258.2 examples/sec; 0.496 sec/batch)
2016-02-04 03:55:09.826810: step 136970, loss = 0.50 (276.6 examples/sec; 0.463 sec/batch)
2016-02-04 03:55:14.575228: step 136980, loss = 0.53 (261.8 examples/sec; 0.489 sec/batch)
2016-02-04 03:55:19.165372: step 136990, loss = 0.57 (283.1 examples/sec; 0.452 sec/batch)
2016-02-04 03:55:23.879517: step 137000, loss = 0.58 (300.0 examples/sec; 0.427 sec/batch)
2016-02-04 03:55:29.127135: step 137010, loss = 0.41 (292.9 examples/sec; 0.437 sec/batch)
2016-02-04 03:55:33.878640: step 137020, loss = 0.55 (253.9 examples/sec; 0.504 sec/batch)
2016-02-04 03:55:38.494033: step 137030, loss = 0.57 (287.5 examples/sec; 0.445 sec/batch)
2016-02-04 03:55:43.227431: step 137040, loss = 0.65 (252.8 examples/sec; 0.506 sec/batch)
2016-02-04 03:55:47.901080: step 137050, loss = 0.65 (274.5 examples/sec; 0.466 sec/batch)
2016-02-04 03:55:52.581355: step 137060, loss = 0.56 (296.8 examples/sec; 0.431 sec/batch)
2016-02-04 03:55:57.331581: step 137070, loss = 0.49 (264.1 examples/sec; 0.485 sec/batch)
2016-02-04 03:56:02.089223: step 137080, loss = 0.62 (247.6 examples/sec; 0.517 sec/batch)
2016-02-04 03:56:06.621550: step 137090, loss = 0.68 (295.3 examples/sec; 0.434 sec/batch)
2016-02-04 03:56:11.330314: step 137100, loss = 0.51 (244.9 examples/sec; 0.523 sec/batch)
2016-02-04 03:56:16.507849: step 137110, loss = 0.56 (272.0 examples/sec; 0.471 sec/batch)
2016-02-04 03:56:21.190229: step 137120, loss = 0.48 (287.3 examples/sec; 0.446 sec/batch)
2016-02-04 03:56:25.895266: step 137130, loss = 0.69 (273.1 examples/sec; 0.469 sec/batch)
2016-02-04 03:56:30.579600: step 137140, loss = 0.55 (282.5 examples/sec; 0.453 sec/batch)
2016-02-04 03:56:35.241602: step 137150, loss = 0.53 (287.3 examples/sec; 0.446 sec/batch)
2016-02-04 03:56:39.880504: step 137160, loss = 0.55 (273.9 examples/sec; 0.467 sec/batch)
2016-02-04 03:56:44.520700: step 137170, loss = 0.72 (298.5 examples/sec; 0.429 sec/batch)
2016-02-04 03:56:49.292443: step 137180, loss = 0.58 (250.5 examples/sec; 0.511 sec/batch)
2016-02-04 03:56:54.046667: step 137190, loss = 0.57 (252.3 examples/sec; 0.507 sec/batch)
2016-02-04 03:56:58.753607: step 137200, loss = 0.54 (292.5 examples/sec; 0.438 sec/batch)
2016-02-04 03:57:04.013204: step 137210, loss = 0.55 (268.5 examples/sec; 0.477 sec/batch)
2016-02-04 03:57:08.754619: step 137220, loss = 0.51 (255.8 examples/sec; 0.500 sec/batch)
2016-02-04 03:57:13.471324: step 137230, loss = 0.62 (247.0 examples/sec; 0.518 sec/batch)
2016-02-04 03:57:18.147080: step 137240, loss = 0.54 (288.3 examples/sec; 0.444 sec/batch)
2016-02-04 03:57:22.792397: step 137250, loss = 0.67 (287.0 examples/sec; 0.446 sec/batch)
2016-02-04 03:57:27.422374: step 137260, loss = 0.45 (278.2 examples/sec; 0.460 sec/batch)
2016-02-04 03:57:32.086269: step 137270, loss = 0.61 (304.0 examples/sec; 0.421 sec/batch)
2016-02-04 03:57:36.695698: step 137280, loss = 0.69 (280.9 examples/sec; 0.456 sec/batch)
2016-02-04 03:57:41.408852: step 137290, loss = 0.57 (262.2 examples/sec; 0.488 sec/batch)
2016-02-04 03:57:45.999868: step 137300, loss = 0.53 (274.1 examples/sec; 0.467 sec/batch)
2016-02-04 03:57:51.182475: step 137310, loss = 0.59 (284.4 examples/sec; 0.450 sec/batch)
2016-02-04 03:57:55.862348: step 137320, loss = 0.56 (268.5 examples/sec; 0.477 sec/batch)
2016-02-04 03:58:00.644633: step 137330, loss = 0.52 (292.5 examples/sec; 0.438 sec/batch)
2016-02-04 03:58:05.383371: step 137340, loss = 0.57 (275.5 examples/sec; 0.465 sec/batch)
2016-02-04 03:58:10.088969: step 137350, loss = 0.60 (285.6 examples/sec; 0.448 sec/batch)
2016-02-04 03:58:14.845108: step 137360, loss = 0.50 (280.3 examples/sec; 0.457 sec/batch)
2016-02-04 03:58:19.594153: step 137370, loss = 0.72 (256.0 examples/sec; 0.500 sec/batch)
2016-02-04 03:58:24.277532: step 137380, loss = 0.52 (277.6 examples/sec; 0.461 sec/batch)
2016-02-04 03:58:29.022434: step 137390, loss = 0.60 (271.7 examples/sec; 0.471 sec/batch)
2016-02-04 03:58:33.735439: step 137400, loss = 0.58 (272.0 examples/sec; 0.471 sec/batch)
2016-02-04 03:58:38.913102: step 137410, loss = 0.62 (281.7 examples/sec; 0.454 sec/batch)
2016-02-04 03:58:43.696233: step 137420, loss = 0.50 (266.9 examples/sec; 0.479 sec/batch)
2016-02-04 03:58:48.410538: step 137430, loss = 0.53 (272.1 examples/sec; 0.470 sec/batch)
2016-02-04 03:58:53.088685: step 137440, loss = 0.55 (270.7 examples/sec; 0.473 sec/batch)
2016-02-04 03:58:57.809012: step 137450, loss = 0.58 (286.6 examples/sec; 0.447 sec/batch)
2016-02-04 03:59:02.499517: step 137460, loss = 0.49 (269.3 examples/sec; 0.475 sec/batch)
2016-02-04 03:59:07.156586: step 137470, loss = 0.52 (295.4 examples/sec; 0.433 sec/batch)
2016-02-04 03:59:11.840805: step 137480, loss = 0.42 (262.2 examples/sec; 0.488 sec/batch)
2016-02-04 03:59:16.556095: step 137490, loss = 0.52 (279.3 examples/sec; 0.458 sec/batch)
2016-02-04 03:59:21.368161: step 137500, loss = 0.56 (255.6 examples/sec; 0.501 sec/batch)
2016-02-04 03:59:26.562885: step 137510, loss = 0.51 (280.5 examples/sec; 0.456 sec/batch)
2016-02-04 03:59:31.267948: step 137520, loss = 0.64 (272.9 examples/sec; 0.469 sec/batch)
2016-02-04 03:59:35.983346: step 137530, loss = 0.59 (269.5 examples/sec; 0.475 sec/batch)
2016-02-04 03:59:40.733808: step 137540, loss = 0.59 (272.5 examples/sec; 0.470 sec/batch)
2016-02-04 03:59:45.468494: step 137550, loss = 0.49 (281.5 examples/sec; 0.455 sec/batch)
2016-02-04 03:59:50.159378: step 137560, loss = 0.52 (286.5 examples/sec; 0.447 sec/batch)
2016-02-04 03:59:54.820813: step 137570, loss = 0.48 (272.7 examples/sec; 0.469 sec/batch)
2016-02-04 03:59:59.514696: step 137580, loss = 0.40 (271.4 examples/sec; 0.472 sec/batch)
2016-02-04 04:00:04.226158: step 137590, loss = 0.52 (283.3 examples/sec; 0.452 sec/batch)
2016-02-04 04:00:08.926900: step 137600, loss = 0.71 (285.7 examples/sec; 0.448 sec/batch)
2016-02-04 04:00:14.206409: step 137610, loss = 0.59 (262.8 examples/sec; 0.487 sec/batch)
2016-02-04 04:00:18.906811: step 137620, loss = 0.47 (262.8 examples/sec; 0.487 sec/batch)
2016-02-04 04:00:23.623423: step 137630, loss = 0.54 (283.5 examples/sec; 0.452 sec/batch)
2016-02-04 04:00:28.323759: step 137640, loss = 0.56 (259.8 examples/sec; 0.493 sec/batch)
2016-02-04 04:00:33.027097: step 137650, loss = 0.60 (253.0 examples/sec; 0.506 sec/batch)
2016-02-04 04:00:37.734517: step 137660, loss = 0.58 (275.8 examples/sec; 0.464 sec/batch)
2016-02-04 04:00:42.417716: step 137670, loss = 0.51 (274.4 examples/sec; 0.466 sec/batch)
2016-02-04 04:00:47.041266: step 137680, loss = 0.55 (274.6 examples/sec; 0.466 sec/batch)
2016-02-04 04:00:51.618066: step 137690, loss = 0.48 (265.6 examples/sec; 0.482 sec/batch)
2016-02-04 04:00:56.334664: step 137700, loss = 0.39 (263.0 examples/sec; 0.487 sec/batch)
2016-02-04 04:01:01.523336: step 137710, loss = 0.51 (261.5 examples/sec; 0.489 sec/batch)
2016-02-04 04:01:06.142983: step 137720, loss = 0.55 (306.6 examples/sec; 0.418 sec/batch)
2016-02-04 04:01:10.863932: step 137730, loss = 0.71 (265.8 examples/sec; 0.481 sec/batch)
2016-02-04 04:01:15.465520: step 137740, loss = 0.56 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 04:01:20.097608: step 137750, loss = 0.47 (284.8 examples/sec; 0.449 sec/batch)
2016-02-04 04:01:24.546764: step 137760, loss = 0.61 (298.1 examples/sec; 0.429 sec/batch)
2016-02-04 04:01:29.213506: step 137770, loss = 0.53 (279.3 examples/sec; 0.458 sec/batch)
2016-02-04 04:01:33.925496: step 137780, loss = 0.46 (269.6 examples/sec; 0.475 sec/batch)
2016-02-04 04:01:38.489505: step 137790, loss = 0.66 (288.0 examples/sec; 0.444 sec/batch)
2016-02-04 04:01:43.130824: step 137800, loss = 0.48 (272.6 examples/sec; 0.470 sec/batch)
2016-02-04 04:01:48.342599: step 137810, loss = 0.58 (270.4 examples/sec; 0.473 sec/batch)
2016-02-04 04:01:53.059359: step 137820, loss = 0.43 (277.4 examples/sec; 0.461 sec/batch)
2016-02-04 04:01:57.740081: step 137830, loss = 0.53 (276.2 examples/sec; 0.464 sec/batch)
2016-02-04 04:02:02.450376: step 137840, loss = 0.52 (296.8 examples/sec; 0.431 sec/batch)
2016-02-04 04:02:07.064450: step 137850, loss = 0.44 (288.3 examples/sec; 0.444 sec/batch)
2016-02-04 04:02:11.826307: step 137860, loss = 0.59 (251.0 examples/sec; 0.510 sec/batch)
2016-02-04 04:02:16.497624: step 137870, loss = 0.43 (249.1 examples/sec; 0.514 sec/batch)
2016-02-04 04:02:21.201056: step 137880, loss = 0.48 (255.4 examples/sec; 0.501 sec/batch)
2016-02-04 04:02:25.914345: step 137890, loss = 0.51 (288.2 examples/sec; 0.444 sec/batch)
2016-02-04 04:02:30.653545: step 137900, loss = 0.50 (294.1 examples/sec; 0.435 sec/batch)
2016-02-04 04:02:35.954608: step 137910, loss = 0.52 (259.5 examples/sec; 0.493 sec/batch)
2016-02-04 04:02:40.638217: step 137920, loss = 0.60 (269.6 examples/sec; 0.475 sec/batch)
2016-02-04 04:02:45.324022: step 137930, loss = 0.53 (284.3 examples/sec; 0.450 sec/batch)
2016-02-04 04:02:50.065360: step 137940, loss = 0.55 (242.9 examples/sec; 0.527 sec/batch)
2016-02-04 04:02:54.735320: step 137950, loss = 0.49 (285.4 examples/sec; 0.448 sec/batch)
2016-02-04 04:02:59.446075: step 137960, loss = 0.43 (293.0 examples/sec; 0.437 sec/batch)
2016-02-04 04:03:04.223929: step 137970, loss = 0.57 (265.0 examples/sec; 0.483 sec/batch)
2016-02-04 04:03:08.893381: step 137980, loss = 0.50 (257.8 examples/sec; 0.496 sec/batch)
2016-02-04 04:03:13.552191: step 137990, loss = 0.47 (273.7 examples/sec; 0.468 sec/batch)
2016-02-04 04:03:18.307473: step 138000, loss = 0.56 (267.9 examples/sec; 0.478 sec/batch)
2016-02-04 04:03:23.541983: step 138010, loss = 0.57 (272.2 examples/sec; 0.470 sec/batch)
2016-02-04 04:03:28.286107: step 138020, loss = 0.65 (271.8 examples/sec; 0.471 sec/batch)
2016-02-04 04:03:32.937407: step 138030, loss = 0.56 (286.3 examples/sec; 0.447 sec/batch)
2016-02-04 04:03:37.700532: step 138040, loss = 0.58 (267.2 examples/sec; 0.479 sec/batch)
2016-02-04 04:03:42.330894: step 138050, loss = 0.68 (252.7 examples/sec; 0.506 sec/batch)
2016-02-04 04:03:47.016796: step 138060, loss = 0.51 (293.3 examples/sec; 0.436 sec/batch)
2016-02-04 04:03:51.798511: step 138070, loss = 0.57 (277.0 examples/sec; 0.462 sec/batch)
2016-02-04 04:03:56.424125: step 138080, loss = 0.47 (278.2 examples/sec; 0.460 sec/batch)
2016-02-04 04:04:01.137518: step 138090, loss = 0.43 (280.2 examples/sec; 0.457 sec/batch)
2016-02-04 04:04:05.825815: step 138100, loss = 0.51 (259.6 examples/sec; 0.493 sec/batch)
2016-02-04 04:04:10.982250: step 138110, loss = 0.52 (255.3 examples/sec; 0.501 sec/batch)
2016-02-04 04:04:15.676684: step 138120, loss = 0.59 (295.4 examples/sec; 0.433 sec/batch)
2016-02-04 04:04:20.434893: step 138130, loss = 0.49 (251.4 examples/sec; 0.509 sec/batch)
2016-02-04 04:04:25.166604: step 138140, loss = 0.50 (274.0 examples/sec; 0.467 sec/batch)
2016-02-04 04:04:29.845785: step 138150, loss = 0.51 (264.3 examples/sec; 0.484 sec/batch)
2016-02-04 04:04:34.546809: step 138160, loss = 0.43 (311.5 examples/sec; 0.411 sec/batch)
2016-02-04 04:04:39.305967: step 138170, loss = 0.46 (257.5 examples/sec; 0.497 sec/batch)
2016-02-04 04:04:44.003848: step 138180, loss = 0.49 (256.9 examples/sec; 0.498 sec/batch)
2016-02-04 04:04:48.650743: step 138190, loss = 0.54 (268.3 examples/sec; 0.477 sec/batch)
2016-02-04 04:04:53.450407: step 138200, loss = 0.51 (268.2 examples/sec; 0.477 sec/batch)
2016-02-04 04:04:58.696777: step 138210, loss = 0.46 (270.0 examples/sec; 0.474 sec/batch)
2016-02-04 04:05:03.402773: step 138220, loss = 0.53 (271.7 examples/sec; 0.471 sec/batch)
2016-02-04 04:05:08.072168: step 138230, loss = 0.48 (265.0 examples/sec; 0.483 sec/batch)
2016-02-04 04:05:12.792157: step 138240, loss = 0.63 (242.3 examples/sec; 0.528 sec/batch)
2016-02-04 04:05:17.486539: step 138250, loss = 0.54 (272.2 examples/sec; 0.470 sec/batch)
2016-02-04 04:05:22.233645: step 138260, loss = 0.51 (296.5 examples/sec; 0.432 sec/batch)
2016-02-04 04:05:26.944120: step 138270, loss = 0.63 (281.0 examples/sec; 0.456 sec/batch)
2016-02-04 04:05:31.654216: step 138280, loss = 0.52 (307.5 examples/sec; 0.416 sec/batch)
2016-02-04 04:05:36.402724: step 138290, loss = 0.54 (289.1 examples/sec; 0.443 sec/batch)
2016-02-04 04:05:41.126605: step 138300, loss = 0.44 (272.9 examples/sec; 0.469 sec/batch)
2016-02-04 04:05:46.284881: step 138310, loss = 0.50 (284.6 examples/sec; 0.450 sec/batch)
2016-02-04 04:05:50.907475: step 138320, loss = 0.53 (267.9 examples/sec; 0.478 sec/batch)
2016-02-04 04:05:55.582922: step 138330, loss = 0.53 (280.4 examples/sec; 0.456 sec/batch)
2016-02-04 04:06:00.286023: step 138340, loss = 0.48 (266.9 examples/sec; 0.480 sec/batch)
2016-02-04 04:06:04.913441: step 138350, loss = 0.43 (288.9 examples/sec; 0.443 sec/batch)
2016-02-04 04:06:09.635694: step 138360, loss = 0.53 (258.6 examples/sec; 0.495 sec/batch)
2016-02-04 04:06:14.347182: step 138370, loss = 0.46 (267.4 examples/sec; 0.479 sec/batch)
2016-02-04 04:06:19.088916: step 138380, loss = 0.42 (280.7 examples/sec; 0.456 sec/batch)
2016-02-04 04:06:23.873551: step 138390, loss = 0.50 (260.5 examples/sec; 0.491 sec/batch)
2016-02-04 04:06:28.593788: step 138400, loss = 0.46 (289.4 examples/sec; 0.442 sec/batch)
2016-02-04 04:06:33.908234: step 138410, loss = 0.43 (237.4 examples/sec; 0.539 sec/batch)
2016-02-04 04:06:38.622410: step 138420, loss = 0.52 (277.7 examples/sec; 0.461 sec/batch)
2016-02-04 04:06:43.385741: step 138430, loss = 0.48 (249.6 examples/sec; 0.513 sec/batch)
2016-02-04 04:06:48.077631: step 138440, loss = 0.53 (263.4 examples/sec; 0.486 sec/batch)
2016-02-04 04:06:52.817034: step 138450, loss = 0.47 (279.8 examples/sec; 0.457 sec/batch)
2016-02-04 04:06:57.498864: step 138460, loss = 0.43 (274.4 examples/sec; 0.466 sec/batch)
2016-02-04 04:07:02.219667: step 138470, loss = 0.54 (280.4 examples/sec; 0.456 sec/batch)
2016-02-04 04:07:06.946048: step 138480, loss = 0.43 (270.8 examples/sec; 0.473 sec/batch)
2016-02-04 04:07:11.634803: step 138490, loss = 0.47 (291.1 examples/sec; 0.440 sec/batch)
2016-02-04 04:07:16.277801: step 138500, loss = 0.58 (285.9 examples/sec; 0.448 sec/batch)
2016-02-04 04:07:21.563030: step 138510, loss = 0.56 (271.8 examples/sec; 0.471 sec/batch)
2016-02-04 04:07:26.194855: step 138520, loss = 0.65 (288.7 examples/sec; 0.443 sec/batch)
2016-02-04 04:07:30.856047: step 138530, loss = 0.53 (284.8 examples/sec; 0.449 sec/batch)
2016-02-04 04:07:35.446986: step 138540, loss = 0.47 (302.1 examples/sec; 0.424 sec/batch)
2016-02-04 04:07:40.201992: step 138550, loss = 0.44 (245.2 examples/sec; 0.522 sec/batch)
2016-02-04 04:07:44.921256: step 138560, loss = 0.60 (266.0 examples/sec; 0.481 sec/batch)
2016-02-04 04:07:49.607766: step 138570, loss = 0.50 (269.2 examples/sec; 0.476 sec/batch)
2016-02-04 04:07:54.384409: step 138580, loss = 0.56 (258.4 examples/sec; 0.495 sec/batch)
2016-02-04 04:07:59.039338: step 138590, loss = 0.38 (269.3 examples/sec; 0.475 sec/batch)
2016-02-04 04:08:03.752232: step 138600, loss = 0.55 (271.5 examples/sec; 0.471 sec/batch)
2016-02-04 04:08:08.966894: step 138610, loss = 0.50 (269.8 examples/sec; 0.474 sec/batch)
2016-02-04 04:08:13.557890: step 138620, loss = 0.44 (275.6 examples/sec; 0.464 sec/batch)
2016-02-04 04:08:18.274606: step 138630, loss = 0.41 (276.7 examples/sec; 0.463 sec/batch)
2016-02-04 04:08:22.934948: step 138640, loss = 0.55 (269.6 examples/sec; 0.475 sec/batch)
2016-02-04 04:08:27.640100: step 138650, loss = 0.49 (267.3 examples/sec; 0.479 sec/batch)
2016-02-04 04:08:32.356562: step 138660, loss = 0.52 (259.2 examples/sec; 0.494 sec/batch)
2016-02-04 04:08:37.082731: step 138670, loss = 0.57 (298.2 examples/sec; 0.429 sec/batch)
2016-02-04 04:08:41.759811: step 138680, loss = 0.54 (285.2 examples/sec; 0.449 sec/batch)
2016-02-04 04:08:46.383655: step 138690, loss = 0.40 (278.8 examples/sec; 0.459 sec/batch)
2016-02-04 04:08:51.066058: step 138700, loss = 0.53 (264.9 examples/sec; 0.483 sec/batch)
2016-02-04 04:08:56.348014: step 138710, loss = 0.45 (252.5 examples/sec; 0.507 sec/batch)
2016-02-04 04:09:00.926688: step 138720, loss = 0.52 (280.6 examples/sec; 0.456 sec/batch)
2016-02-04 04:09:05.604808: step 138730, loss = 0.53 (275.4 examples/sec; 0.465 sec/batch)
2016-02-04 04:09:10.251685: step 138740, loss = 0.45 (309.1 examples/sec; 0.414 sec/batch)
2016-02-04 04:09:14.967263: step 138750, loss = 0.52 (294.8 examples/sec; 0.434 sec/batch)
2016-02-04 04:09:19.659644: step 138760, loss = 0.44 (250.0 examples/sec; 0.512 sec/batch)
2016-02-04 04:09:24.331214: step 138770, loss = 0.51 (264.8 examples/sec; 0.483 sec/batch)
2016-02-04 04:09:29.039515: step 138780, loss = 0.53 (298.6 examples/sec; 0.429 sec/batch)
2016-02-04 04:09:33.821215: step 138790, loss = 0.51 (285.8 examples/sec; 0.448 sec/batch)
2016-02-04 04:09:38.545824: step 138800, loss = 0.45 (251.8 examples/sec; 0.508 sec/batch)
2016-02-04 04:09:43.757520: step 138810, loss = 0.49 (270.3 examples/sec; 0.473 sec/batch)
2016-02-04 04:09:48.465659: step 138820, loss = 0.41 (292.1 examples/sec; 0.438 sec/batch)
2016-02-04 04:09:53.176586: step 138830, loss = 0.71 (267.0 examples/sec; 0.479 sec/batch)
2016-02-04 04:09:57.813460: step 138840, loss = 0.55 (268.0 examples/sec; 0.478 sec/batch)
2016-02-04 04:10:02.527736: step 138850, loss = 0.43 (291.5 examples/sec; 0.439 sec/batch)
2016-02-04 04:10:07.238377: step 138860, loss = 0.44 (269.5 examples/sec; 0.475 sec/batch)
2016-02-04 04:10:11.912705: step 138870, loss = 0.59 (280.6 examples/sec; 0.456 sec/batch)
2016-02-04 04:10:16.547807: step 138880, loss = 0.47 (282.0 examples/sec; 0.454 sec/batch)
2016-02-04 04:10:21.262845: step 138890, loss = 0.53 (273.6 examples/sec; 0.468 sec/batch)
2016-02-04 04:10:25.958567: step 138900, loss = 0.54 (269.3 examples/sec; 0.475 sec/batch)
2016-02-04 04:10:31.162794: step 138910, loss = 0.46 (276.5 examples/sec; 0.463 sec/batch)
2016-02-04 04:10:35.956629: step 138920, loss = 0.50 (248.4 examples/sec; 0.515 sec/batch)
2016-02-04 04:10:40.641756: step 138930, loss = 0.46 (272.1 examples/sec; 0.470 sec/batch)
2016-02-04 04:10:45.401797: step 138940, loss = 0.45 (257.6 examples/sec; 0.497 sec/batch)
2016-02-04 04:10:50.069439: step 138950, loss = 0.44 (261.0 examples/sec; 0.490 sec/batch)
2016-02-04 04:10:54.730305: step 138960, loss = 0.41 (275.5 examples/sec; 0.465 sec/batch)
2016-02-04 04:10:59.448692: step 138970, loss = 0.50 (279.9 examples/sec; 0.457 sec/batch)
2016-02-04 04:11:04.184198: step 138980, loss = 0.48 (265.0 examples/sec; 0.483 sec/batch)
2016-02-04 04:11:08.891734: step 138990, loss = 0.43 (275.4 examples/sec; 0.465 sec/batch)
2016-02-04 04:11:13.609800: step 139000, loss = 0.60 (270.4 examples/sec; 0.473 sec/batch)
2016-02-04 04:11:18.796952: step 139010, loss = 0.55 (275.9 examples/sec; 0.464 sec/batch)
2016-02-04 04:11:23.560939: step 139020, loss = 0.55 (272.8 examples/sec; 0.469 sec/batch)
2016-02-04 04:11:28.296474: step 139030, loss = 0.62 (284.5 examples/sec; 0.450 sec/batch)
2016-02-04 04:11:32.977941: step 139040, loss = 0.56 (270.7 examples/sec; 0.473 sec/batch)
2016-02-04 04:11:37.699487: step 139050, loss = 0.51 (272.4 examples/sec; 0.470 sec/batch)
2016-02-04 04:11:42.366307: step 139060, loss = 0.58 (289.8 examples/sec; 0.442 sec/batch)
2016-02-04 04:11:47.061253: step 139070, loss = 0.48 (265.1 examples/sec; 0.483 sec/batch)
2016-02-04 04:11:51.711935: step 139080, loss = 0.43 (283.9 examples/sec; 0.451 sec/batch)
2016-02-04 04:11:56.485700: step 139090, loss = 0.44 (292.7 examples/sec; 0.437 sec/batch)
2016-02-04 04:12:01.243123: step 139100, loss = 0.43 (265.5 examples/sec; 0.482 sec/batch)
2016-02-04 04:12:06.504804: step 139110, loss = 0.54 (279.0 examples/sec; 0.459 sec/batch)
2016-02-04 04:12:11.161342: step 139120, loss = 0.52 (278.4 examples/sec; 0.460 sec/batch)
2016-02-04 04:12:15.902872: step 139130, loss = 0.39 (278.6 examples/sec; 0.459 sec/batch)
2016-02-04 04:12:20.506045: step 139140, loss = 0.41 (291.5 examples/sec; 0.439 sec/batch)
2016-02-04 04:12:25.241628: step 139150, loss = 0.66 (269.6 examples/sec; 0.475 sec/batch)
2016-02-04 04:12:29.892730: step 139160, loss = 0.39 (247.6 examples/sec; 0.517 sec/batch)
2016-02-04 04:12:34.585736: step 139170, loss = 0.54 (268.5 examples/sec; 0.477 sec/batch)
2016-02-04 04:12:39.250473: step 139180, loss = 0.40 (297.6 examples/sec; 0.430 sec/batch)
2016-02-04 04:12:44.045873: step 139190, loss = 0.46 (272.8 examples/sec; 0.469 sec/batch)
2016-02-04 04:12:48.646221: step 139200, loss = 0.48 (317.3 examples/sec; 0.403 sec/batch)
2016-02-04 04:12:53.885425: step 139210, loss = 0.46 (288.2 examples/sec; 0.444 sec/batch)
2016-02-04 04:12:58.575103: step 139220, loss = 0.40 (285.2 examples/sec; 0.449 sec/batch)
2016-02-04 04:13:03.332075: step 139230, loss = 0.45 (249.8 examples/sec; 0.512 sec/batch)
2016-02-04 04:13:08.069171: step 139240, loss = 0.45 (295.6 examples/sec; 0.433 sec/batch)
2016-02-04 04:13:12.771174: step 139250, loss = 0.43 (293.0 examples/sec; 0.437 sec/batch)
2016-02-04 04:13:17.546675: step 139260, loss = 0.47 (277.8 examples/sec; 0.461 sec/batch)
2016-02-04 04:13:22.236136: step 139270, loss = 0.44 (260.4 examples/sec; 0.492 sec/batch)
2016-02-04 04:13:26.970391: step 139280, loss = 0.51 (256.2 examples/sec; 0.500 sec/batch)
2016-02-04 04:13:31.791667: step 139290, loss = 0.44 (264.1 examples/sec; 0.485 sec/batch)
2016-02-04 04:13:36.454957: step 139300, loss = 0.42 (292.0 examples/sec; 0.438 sec/batch)
2016-02-04 04:13:41.701436: step 139310, loss = 0.51 (287.9 examples/sec; 0.445 sec/batch)
2016-02-04 04:13:46.409494: step 139320, loss = 0.48 (273.6 examples/sec; 0.468 sec/batch)
2016-02-04 04:13:51.110163: step 139330, loss = 0.46 (254.0 examples/sec; 0.504 sec/batch)
2016-02-04 04:13:55.820026: step 139340, loss = 0.50 (283.8 examples/sec; 0.451 sec/batch)
2016-02-04 04:14:00.568480: step 139350, loss = 0.48 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 04:14:05.184670: step 139360, loss = 0.48 (270.9 examples/sec; 0.472 sec/batch)
2016-02-04 04:14:09.881711: step 139370, loss = 0.58 (252.9 examples/sec; 0.506 sec/batch)
2016-02-04 04:14:14.501201: step 139380, loss = 0.43 (301.4 examples/sec; 0.425 sec/batch)
2016-02-04 04:14:19.246722: step 139390, loss = 0.47 (281.3 examples/sec; 0.455 sec/batch)
2016-02-04 04:14:23.940875: step 139400, loss = 0.43 (301.6 examples/sec; 0.424 sec/batch)
2016-02-04 04:14:29.120935: step 139410, loss = 0.45 (277.3 examples/sec; 0.462 sec/batch)
2016-02-04 04:14:33.773182: step 139420, loss = 0.42 (284.9 examples/sec; 0.449 sec/batch)
2016-02-04 04:14:38.499356: step 139430, loss = 0.43 (283.0 examples/sec; 0.452 sec/batch)
2016-02-04 04:14:43.256905: step 139440, loss = 0.53 (255.1 examples/sec; 0.502 sec/batch)
2016-02-04 04:14:47.933210: step 139450, loss = 0.43 (267.2 examples/sec; 0.479 sec/batch)
2016-02-04 04:14:52.538384: step 139460, loss = 0.48 (282.9 examples/sec; 0.452 sec/batch)
2016-02-04 04:14:57.248967: step 139470, loss = 0.50 (280.4 examples/sec; 0.457 sec/batch)
2016-02-04 04:15:01.932197: step 139480, loss = 0.42 (260.8 examples/sec; 0.491 sec/batch)
2016-02-04 04:15:06.695276: step 139490, loss = 0.39 (289.2 examples/sec; 0.443 sec/batch)
2016-02-04 04:15:11.456378: step 139500, loss = 0.48 (248.6 examples/sec; 0.515 sec/batch)
2016-02-04 04:15:16.679467: step 139510, loss = 0.43 (276.6 examples/sec; 0.463 sec/batch)
2016-02-04 04:15:21.376968: step 139520, loss = 0.38 (274.2 examples/sec; 0.467 sec/batch)
2016-02-04 04:15:26.082565: step 139530, loss = 0.43 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 04:15:30.748087: step 139540, loss = 0.50 (274.8 examples/sec; 0.466 sec/batch)
2016-02-04 04:15:35.455659: step 139550, loss = 0.47 (256.4 examples/sec; 0.499 sec/batch)
2016-02-04 04:15:40.168498: step 139560, loss = 0.42 (281.2 examples/sec; 0.455 sec/batch)
2016-02-04 04:15:44.857147: step 139570, loss = 0.52 (256.1 examples/sec; 0.500 sec/batch)
2016-02-04 04:15:49.550498: step 139580, loss = 0.50 (286.2 examples/sec; 0.447 sec/batch)
2016-02-04 04:15:54.261432: step 139590, loss = 0.49 (275.4 examples/sec; 0.465 sec/batch)
2016-02-04 04:15:59.015489: step 139600, loss = 0.38 (279.5 examples/sec; 0.458 sec/batch)
2016-02-04 04:16:04.271639: step 139610, loss = 0.36 (268.2 examples/sec; 0.477 sec/batch)
2016-02-04 04:16:09.043467: step 139620, loss = 0.41 (262.9 examples/sec; 0.487 sec/batch)
2016-02-04 04:16:13.770097: step 139630, loss = 0.55 (267.3 examples/sec; 0.479 sec/batch)
2016-02-04 04:16:18.558816: step 139640, loss = 0.43 (253.8 examples/sec; 0.504 sec/batch)
2016-02-04 04:16:23.282814: step 139650, loss = 0.40 (263.1 examples/sec; 0.487 sec/batch)
2016-02-04 04:16:28.066468: step 139660, loss = 0.45 (255.9 examples/sec; 0.500 sec/batch)
2016-02-04 04:16:32.831626: step 139670, loss = 0.51 (255.6 examples/sec; 0.501 sec/batch)
2016-02-04 04:16:37.551811: step 139680, loss = 0.47 (262.6 examples/sec; 0.487 sec/batch)
2016-02-04 04:16:42.334561: step 139690, loss = 0.44 (254.7 examples/sec; 0.502 sec/batch)
2016-02-04 04:16:46.983049: step 139700, loss = 0.41 (254.5 examples/sec; 0.503 sec/batch)
2016-02-04 04:16:52.233794: step 139710, loss = 0.53 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 04:16:56.982365: step 139720, loss = 0.36 (259.7 examples/sec; 0.493 sec/batch)
2016-02-04 04:17:01.662701: step 139730, loss = 0.45 (254.3 examples/sec; 0.503 sec/batch)
2016-02-04 04:17:06.364962: step 139740, loss = 0.57 (286.8 examples/sec; 0.446 sec/batch)
2016-02-04 04:17:11.108822: step 139750, loss = 0.46 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 04:17:15.810240: step 139760, loss = 0.43 (254.8 examples/sec; 0.502 sec/batch)
2016-02-04 04:17:20.504382: step 139770, loss = 0.47 (266.2 examples/sec; 0.481 sec/batch)
2016-02-04 04:17:25.279918: step 139780, loss = 0.48 (254.7 examples/sec; 0.503 sec/batch)
2016-02-04 04:17:29.896181: step 139790, loss = 0.44 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 04:17:34.603182: step 139800, loss = 0.39 (287.0 examples/sec; 0.446 sec/batch)
2016-02-04 04:17:39.788843: step 139810, loss = 0.42 (288.1 examples/sec; 0.444 sec/batch)
2016-02-04 04:17:44.479137: step 139820, loss = 0.39 (281.1 examples/sec; 0.455 sec/batch)
2016-02-04 04:17:49.137822: step 139830, loss = 0.42 (260.4 examples/sec; 0.492 sec/batch)
2016-02-04 04:17:53.837651: step 139840, loss = 0.42 (271.3 examples/sec; 0.472 sec/batch)
2016-02-04 04:17:58.429326: step 139850, loss = 0.42 (262.2 examples/sec; 0.488 sec/batch)
2016-02-04 04:18:03.136646: step 139860, loss = 0.45 (257.9 examples/sec; 0.496 sec/batch)
2016-02-04 04:18:07.769599: step 139870, loss = 0.42 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 04:18:12.580561: step 139880, loss = 0.41 (256.4 examples/sec; 0.499 sec/batch)
2016-02-04 04:18:17.236297: step 139890, loss = 0.40 (265.4 examples/sec; 0.482 sec/batch)
2016-02-04 04:18:21.886750: step 139900, loss = 0.49 (284.6 examples/sec; 0.450 sec/batch)
2016-02-04 04:18:27.122446: step 139910, loss = 0.47 (256.5 examples/sec; 0.499 sec/batch)
2016-02-04 04:18:31.804868: step 139920, loss = 0.48 (281.7 examples/sec; 0.454 sec/batch)
2016-02-04 04:18:36.559432: step 139930, loss = 0.45 (254.1 examples/sec; 0.504 sec/batch)
2016-02-04 04:18:41.328996: step 139940, loss = 0.55 (296.1 examples/sec; 0.432 sec/batch)
2016-02-04 04:18:46.065532: step 139950, loss = 0.51 (254.6 examples/sec; 0.503 sec/batch)
2016-02-04 04:18:50.703547: step 139960, loss = 0.35 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 04:18:55.477096: step 139970, loss = 0.44 (283.0 examples/sec; 0.452 sec/batch)
2016-02-04 04:19:00.166885: step 139980, loss = 0.44 (261.2 examples/sec; 0.490 sec/batch)
2016-02-04 04:19:04.798149: step 139990, loss = 0.44 (273.7 examples/sec; 0.468 sec/batch)
2016-02-04 04:19:09.496855: step 140000, loss = 0.48 (268.4 examples/sec; 0.477 sec/batch)
2016-02-04 04:19:14.664036: step 140010, loss = 0.48 (274.0 examples/sec; 0.467 sec/batch)
2016-02-04 04:19:19.279107: step 140020, loss = 0.53 (267.0 examples/sec; 0.479 sec/batch)
2016-02-04 04:19:24.004089: step 140030, loss = 0.44 (259.9 examples/sec; 0.492 sec/batch)
2016-02-04 04:19:28.667965: step 140040, loss = 0.44 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 04:19:33.376284: step 140050, loss = 0.40 (283.1 examples/sec; 0.452 sec/batch)
2016-02-04 04:19:38.110178: step 140060, loss = 0.41 (257.5 examples/sec; 0.497 sec/batch)
2016-02-04 04:19:42.828628: step 140070, loss = 0.52 (273.4 examples/sec; 0.468 sec/batch)
2016-02-04 04:19:47.499966: step 140080, loss = 0.41 (279.8 examples/sec; 0.457 sec/batch)
2016-02-04 04:19:52.190111: step 140090, loss = 0.43 (283.6 examples/sec; 0.451 sec/batch)
2016-02-04 04:19:56.800344: step 140100, loss = 0.42 (281.0 examples/sec; 0.455 sec/batch)
2016-02-04 04:20:02.000501: step 140110, loss = 0.49 (275.2 examples/sec; 0.465 sec/batch)
2016-02-04 04:20:06.655272: step 140120, loss = 0.52 (270.8 examples/sec; 0.473 sec/batch)
2016-02-04 04:20:11.280801: step 140130, loss = 0.50 (267.4 examples/sec; 0.479 sec/batch)
2016-02-04 04:20:15.894376: step 140140, loss = 0.45 (253.8 examples/sec; 0.504 sec/batch)
2016-02-04 04:20:20.507340: step 140150, loss = 0.49 (289.3 examples/sec; 0.442 sec/batch)
2016-02-04 04:20:25.182232: step 140160, loss = 0.49 (296.1 examples/sec; 0.432 sec/batch)
2016-02-04 04:20:29.893801: step 140170, loss = 0.50 (267.6 examples/sec; 0.478 sec/batch)
2016-02-04 04:20:34.553893: step 140180, loss = 0.54 (279.1 examples/sec; 0.459 sec/batch)
2016-02-04 04:20:39.224011: step 140190, loss = 0.43 (270.9 examples/sec; 0.472 sec/batch)
2016-02-04 04:20:43.902538: step 140200, loss = 0.36 (262.6 examples/sec; 0.487 sec/batch)
2016-02-04 04:20:49.084104: step 140210, loss = 0.41 (261.5 examples/sec; 0.490 sec/batch)
2016-02-04 04:20:53.752274: step 140220, loss = 0.45 (305.3 examples/sec; 0.419 sec/batch)
2016-02-04 04:20:58.489664: step 140230, loss = 0.40 (267.9 examples/sec; 0.478 sec/batch)
2016-02-04 04:21:03.177361: step 140240, loss = 0.40 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 04:21:07.931535: step 140250, loss = 0.50 (268.1 examples/sec; 0.477 sec/batch)
2016-02-04 04:21:12.629035: step 140260, loss = 0.40 (270.9 examples/sec; 0.473 sec/batch)
2016-02-04 04:21:17.334340: step 140270, loss = 0.46 (276.1 examples/sec; 0.464 sec/batch)
2016-02-04 04:21:21.970317: step 140280, loss = 0.46 (281.7 examples/sec; 0.454 sec/batch)
2016-02-04 04:21:26.711210: step 140290, loss = 0.43 (270.1 examples/sec; 0.474 sec/batch)
2016-02-04 04:21:31.358601: step 140300, loss = 0.55 (269.5 examples/sec; 0.475 sec/batch)
2016-02-04 04:21:36.519022: step 140310, loss = 0.59 (266.9 examples/sec; 0.480 sec/batch)
2016-02-04 04:21:41.170268: step 140320, loss = 0.46 (283.2 examples/sec; 0.452 sec/batch)
2016-02-04 04:21:45.882278: step 140330, loss = 0.52 (258.9 examples/sec; 0.494 sec/batch)
2016-02-04 04:21:50.551386: step 140340, loss = 0.48 (280.6 examples/sec; 0.456 sec/batch)
2016-02-04 04:21:55.298664: step 140350, loss = 0.45 (271.7 examples/sec; 0.471 sec/batch)
2016-02-04 04:22:00.047432: step 140360, loss = 0.46 (253.9 examples/sec; 0.504 sec/batch)
2016-02-04 04:22:04.742497: step 140370, loss = 0.41 (264.4 examples/sec; 0.484 sec/batch)
2016-02-04 04:22:09.487800: step 140380, loss = 0.41 (257.0 examples/sec; 0.498 sec/batch)
2016-02-04 04:22:14.209388: step 140390, loss = 0.50 (277.2 examples/sec; 0.462 sec/batch)
2016-02-04 04:22:18.941166: step 140400, loss = 0.44 (279.6 examples/sec; 0.458 sec/batch)
2016-02-04 04:22:24.125644: step 140410, loss = 0.40 (272.7 examples/sec; 0.469 sec/batch)
2016-02-04 04:22:28.825541: step 140420, loss = 0.34 (287.7 examples/sec; 0.445 sec/batch)
2016-02-04 04:22:33.419299: step 140430, loss = 0.55 (295.4 examples/sec; 0.433 sec/batch)
2016-02-04 04:22:38.095956: step 140440, loss = 0.43 (261.6 examples/sec; 0.489 sec/batch)
2016-02-04 04:22:42.789347: step 140450, loss = 0.47 (280.5 examples/sec; 0.456 sec/batch)
2016-02-04 04:22:47.515456: step 140460, loss = 0.49 (280.9 examples/sec; 0.456 sec/batch)
2016-02-04 04:22:52.200407: step 140470, loss = 0.42 (273.4 examples/sec; 0.468 sec/batch)
2016-02-04 04:22:56.843555: step 140480, loss = 0.45 (288.2 examples/sec; 0.444 sec/batch)
2016-02-04 04:23:01.426805: step 140490, loss = 0.44 (277.8 examples/sec; 0.461 sec/batch)
2016-02-04 04:23:06.000813: step 140500, loss = 0.41 (301.8 examples/sec; 0.424 sec/batch)
2016-02-04 04:23:11.186077: step 140510, loss = 0.39 (252.0 examples/sec; 0.508 sec/batch)
2016-02-04 04:23:15.905052: step 140520, loss = 0.37 (267.7 examples/sec; 0.478 sec/batch)
2016-02-04 04:23:20.612364: step 140530, loss = 0.38 (296.1 examples/sec; 0.432 sec/batch)
2016-02-04 04:23:25.325550: step 140540, loss = 0.38 (267.1 examples/sec; 0.479 sec/batch)
2016-02-04 04:23:30.042646: step 140550, loss = 0.39 (241.2 examples/sec; 0.531 sec/batch)
2016-02-04 04:23:34.643593: step 140560, loss = 0.53 (237.2 examples/sec; 0.540 sec/batch)
2016-02-04 04:23:39.386513: step 140570, loss = 0.56 (257.4 examples/sec; 0.497 sec/batch)
2016-02-04 04:23:44.040416: step 140580, loss = 0.50 (295.8 examples/sec; 0.433 sec/batch)
2016-02-04 04:23:48.727185: step 140590, loss = 0.38 (276.2 examples/sec; 0.464 sec/batch)
2016-02-04 04:23:53.484336: step 140600, loss = 0.44 (274.8 examples/sec; 0.466 sec/batch)
2016-02-04 04:23:58.709764: step 140610, loss = 0.38 (247.7 examples/sec; 0.517 sec/batch)
2016-02-04 04:24:03.464629: step 140620, loss = 0.46 (291.0 examples/sec; 0.440 sec/batch)
2016-02-04 04:24:08.160470: step 140630, loss = 0.45 (257.9 examples/sec; 0.496 sec/batch)
2016-02-04 04:24:12.895303: step 140640, loss = 0.42 (261.6 examples/sec; 0.489 sec/batch)
2016-02-04 04:24:17.573165: step 140650, loss = 0.38 (275.2 examples/sec; 0.465 sec/batch)
2016-02-04 04:24:22.352084: step 140660, loss = 0.38 (277.3 examples/sec; 0.462 sec/batch)
2016-02-04 04:24:27.091229: step 140670, loss = 0.51 (268.2 examples/sec; 0.477 sec/batch)
2016-02-04 04:24:31.878209: step 140680, loss = 0.54 (275.2 examples/sec; 0.465 sec/batch)
2016-02-04 04:24:36.540243: step 140690, loss = 0.52 (267.3 examples/sec; 0.479 sec/batch)
2016-02-04 04:24:41.199035: step 140700, loss = 0.42 (278.3 examples/sec; 0.460 sec/batch)
2016-02-04 04:24:46.326919: step 140710, loss = 0.37 (257.1 examples/sec; 0.498 sec/batch)
2016-02-04 04:24:50.943468: step 140720, loss = 0.49 (266.1 examples/sec; 0.481 sec/batch)
2016-02-04 04:24:55.689344: step 140730, loss = 0.48 (259.5 examples/sec; 0.493 sec/batch)
2016-02-04 04:25:00.397372: step 140740, loss = 0.40 (261.0 examples/sec; 0.490 sec/batch)
2016-02-04 04:25:05.114190: step 140750, loss = 0.49 (261.7 examples/sec; 0.489 sec/batch)
2016-02-04 04:25:09.710551: step 140760, loss = 0.45 (282.8 examples/sec; 0.453 sec/batch)
2016-02-04 04:25:14.391623: step 140770, loss = 0.48 (260.5 examples/sec; 0.491 sec/batch)
2016-02-04 04:25:19.052876: step 140780, loss = 0.38 (274.7 examples/sec; 0.466 sec/batch)
2016-02-04 04:25:23.735339: step 140790, loss = 0.51 (288.4 examples/sec; 0.444 sec/batch)
2016-02-04 04:25:28.478249: step 140800, loss = 0.40 (275.2 examples/sec; 0.465 sec/batch)
2016-02-04 04:25:33.785810: step 140810, loss = 0.44 (265.3 examples/sec; 0.482 sec/batch)
2016-02-04 04:25:38.471924: step 140820, loss = 0.47 (308.9 examples/sec; 0.414 sec/batch)
2016-02-04 04:25:43.194385: step 140830, loss = 0.39 (262.7 examples/sec; 0.487 sec/batch)
2016-02-04 04:25:47.851468: step 140840, loss = 0.40 (262.4 examples/sec; 0.488 sec/batch)
2016-02-04 04:25:52.564807: step 140850, loss = 0.35 (265.5 examples/sec; 0.482 sec/batch)
2016-02-04 04:25:57.274282: step 140860, loss = 0.44 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 04:26:01.898678: step 140870, loss = 0.41 (285.6 examples/sec; 0.448 sec/batch)
2016-02-04 04:26:06.572672: step 140880, loss = 0.37 (263.0 examples/sec; 0.487 sec/batch)
2016-02-04 04:26:11.248583: step 140890, loss = 0.39 (273.6 examples/sec; 0.468 sec/batch)
2016-02-04 04:26:15.984215: step 140900, loss = 0.36 (266.2 examples/sec; 0.481 sec/batch)
2016-02-04 04:26:21.308033: step 140910, loss = 0.56 (273.5 examples/sec; 0.468 sec/batch)
2016-02-04 04:26:26.049332: step 140920, loss = 0.44 (291.8 examples/sec; 0.439 sec/batch)
2016-02-04 04:26:30.715496: step 140930, loss = 0.44 (270.5 examples/sec; 0.473 sec/batch)
2016-02-04 04:26:35.454441: step 140940, loss = 0.48 (271.3 examples/sec; 0.472 sec/batch)
2016-02-04 04:26:40.092803: step 140950, loss = 0.46 (274.3 examples/sec; 0.467 sec/batch)
2016-02-04 04:26:44.865627: step 140960, loss = 0.47 (255.9 examples/sec; 0.500 sec/batch)
2016-02-04 04:26:49.573276: step 140970, loss = 0.42 (268.5 examples/sec; 0.477 sec/batch)
2016-02-04 04:26:54.301193: step 140980, loss = 0.39 (268.5 examples/sec; 0.477 sec/batch)
2016-02-04 04:26:59.042672: step 140990, loss = 0.41 (277.0 examples/sec; 0.462 sec/batch)
2016-02-04 04:27:03.867569: step 141000, loss = 0.40 (262.1 examples/sec; 0.488 sec/batch)
2016-02-04 04:27:09.081255: step 141010, loss = 0.48 (264.1 examples/sec; 0.485 sec/batch)
2016-02-04 04:27:13.732842: step 141020, loss = 0.37 (277.2 examples/sec; 0.462 sec/batch)
2016-02-04 04:27:18.443662: step 141030, loss = 0.32 (270.8 examples/sec; 0.473 sec/batch)
2016-02-04 04:27:23.213787: step 141040, loss = 0.43 (289.0 examples/sec; 0.443 sec/batch)
2016-02-04 04:27:27.960637: step 141050, loss = 0.44 (294.1 examples/sec; 0.435 sec/batch)
2016-02-04 04:27:32.766803: step 141060, loss = 0.44 (255.3 examples/sec; 0.501 sec/batch)
2016-02-04 04:27:37.436710: step 141070, loss = 0.37 (267.4 examples/sec; 0.479 sec/batch)
2016-02-04 04:27:42.145249: step 141080, loss = 0.45 (274.3 examples/sec; 0.467 sec/batch)
2016-02-04 04:27:46.869166: step 141090, loss = 0.39 (258.4 examples/sec; 0.495 sec/batch)
2016-02-04 04:27:51.610547: step 141100, loss = 0.51 (276.1 examples/sec; 0.464 sec/batch)
2016-02-04 04:27:56.847114: step 141110, loss = 0.37 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 04:28:01.525635: step 141120, loss = 0.41 (277.9 examples/sec; 0.461 sec/batch)
2016-02-04 04:28:06.224125: step 141130, loss = 0.43 (292.3 examples/sec; 0.438 sec/batch)
2016-02-04 04:28:11.029781: step 141140, loss = 0.32 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 04:28:15.840345: step 141150, loss = 0.46 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 04:28:20.586293: step 141160, loss = 0.43 (277.9 examples/sec; 0.461 sec/batch)
2016-02-04 04:28:25.407514: step 141170, loss = 0.44 (248.3 examples/sec; 0.516 sec/batch)
2016-02-04 04:28:30.114547: step 141180, loss = 0.51 (243.9 examples/sec; 0.525 sec/batch)
2016-02-04 04:28:34.854038: step 141190, loss = 0.36 (270.7 examples/sec; 0.473 sec/batch)
2016-02-04 04:28:39.548291: step 141200, loss = 0.37 (271.3 examples/sec; 0.472 sec/batch)
2016-02-04 04:28:44.792734: step 141210, loss = 0.38 (254.5 examples/sec; 0.503 sec/batch)
2016-02-04 04:28:49.497335: step 141220, loss = 0.47 (293.4 examples/sec; 0.436 sec/batch)
2016-02-04 04:28:54.209378: step 141230, loss = 0.51 (243.2 examples/sec; 0.526 sec/batch)
2016-02-04 04:28:58.854617: step 141240, loss = 0.37 (270.0 examples/sec; 0.474 sec/batch)
2016-02-04 04:29:03.628965: step 141250, loss = 0.38 (277.4 examples/sec; 0.461 sec/batch)
2016-02-04 04:29:08.422031: step 141260, loss = 0.52 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 04:29:13.146409: step 141270, loss = 0.54 (273.6 examples/sec; 0.468 sec/batch)
2016-02-04 04:29:17.866164: step 141280, loss = 0.47 (279.7 examples/sec; 0.458 sec/batch)
2016-02-04 04:29:22.619333: step 141290, loss = 0.46 (253.0 examples/sec; 0.506 sec/batch)
2016-02-04 04:29:27.358539: step 141300, loss = 0.40 (268.7 examples/sec; 0.476 sec/batch)
2016-02-04 04:29:32.602234: step 141310, loss = 0.42 (276.1 examples/sec; 0.464 sec/batch)
2016-02-04 04:29:37.238084: step 141320, loss = 0.37 (262.3 examples/sec; 0.488 sec/batch)
2016-02-04 04:29:41.913895: step 141330, loss = 0.41 (279.7 examples/sec; 0.458 sec/batch)
2016-02-04 04:29:46.583136: step 141340, loss = 0.38 (271.3 examples/sec; 0.472 sec/batch)
2016-02-04 04:29:51.279975: step 141350, loss = 0.32 (295.7 examples/sec; 0.433 sec/batch)
2016-02-04 04:29:56.000210: step 141360, loss = 0.37 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 04:30:00.622813: step 141370, loss = 0.44 (303.1 examples/sec; 0.422 sec/batch)
2016-02-04 04:30:05.290523: step 141380, loss = 0.52 (272.0 examples/sec; 0.471 sec/batch)
2016-02-04 04:30:09.964514: step 141390, loss = 0.39 (269.5 examples/sec; 0.475 sec/batch)
2016-02-04 04:30:14.673062: step 141400, loss = 0.45 (271.6 examples/sec; 0.471 sec/batch)
2016-02-04 04:30:19.915257: step 141410, loss = 0.35 (267.6 examples/sec; 0.478 sec/batch)
2016-02-04 04:30:24.620286: step 141420, loss = 0.43 (271.5 examples/sec; 0.471 sec/batch)
2016-02-04 04:30:29.355887: step 141430, loss = 0.43 (256.4 examples/sec; 0.499 sec/batch)
2016-02-04 04:30:34.083522: step 141440, loss = 0.57 (253.8 examples/sec; 0.504 sec/batch)
2016-02-04 04:30:38.738759: step 141450, loss = 0.33 (285.5 examples/sec; 0.448 sec/batch)
2016-02-04 04:30:43.442181: step 141460, loss = 0.41 (265.1 examples/sec; 0.483 sec/batch)
2016-02-04 04:30:48.142087: step 141470, loss = 0.45 (270.8 examples/sec; 0.473 sec/batch)
2016-02-04 04:30:52.869346: step 141480, loss = 0.43 (280.6 examples/sec; 0.456 sec/batch)
2016-02-04 04:30:57.545096: step 141490, loss = 0.42 (250.7 examples/sec; 0.511 sec/batch)
2016-02-04 04:31:02.217151: step 141500, loss = 0.42 (280.7 examples/sec; 0.456 sec/batch)
2016-02-04 04:31:07.454161: step 141510, loss = 0.36 (272.1 examples/sec; 0.470 sec/batch)
2016-02-04 04:31:12.219220: step 141520, loss = 0.44 (257.1 examples/sec; 0.498 sec/batch)
2016-02-04 04:31:16.919389: step 141530, loss = 0.55 (275.4 examples/sec; 0.465 sec/batch)
2016-02-04 04:31:21.603585: step 141540, loss = 0.51 (257.5 examples/sec; 0.497 sec/batch)
2016-02-04 04:31:26.274871: step 141550, loss = 0.41 (289.3 examples/sec; 0.442 sec/batch)
2016-02-04 04:31:30.886976: step 141560, loss = 0.39 (305.3 examples/sec; 0.419 sec/batch)
2016-02-04 04:31:35.652668: step 141570, loss = 0.41 (280.7 examples/sec; 0.456 sec/batch)
2016-02-04 04:31:40.340912: step 141580, loss = 0.43 (280.5 examples/sec; 0.456 sec/batch)
2016-02-04 04:31:44.951871: step 141590, loss = 0.41 (279.1 examples/sec; 0.459 sec/batch)
2016-02-04 04:31:49.655350: step 141600, loss = 0.35 (254.0 examples/sec; 0.504 sec/batch)
2016-02-04 04:31:54.843065: step 141610, loss = 0.37 (281.8 examples/sec; 0.454 sec/batch)
2016-02-04 04:31:59.582093: step 141620, loss = 0.45 (274.7 examples/sec; 0.466 sec/batch)
2016-02-04 04:32:04.238554: step 141630, loss = 0.47 (283.9 examples/sec; 0.451 sec/batch)
2016-02-04 04:32:08.893931: step 141640, loss = 0.47 (269.0 examples/sec; 0.476 sec/batch)
2016-02-04 04:32:13.617293: step 141650, loss = 0.40 (258.3 examples/sec; 0.496 sec/batch)
2016-02-04 04:32:18.188593: step 141660, loss = 0.41 (305.4 examples/sec; 0.419 sec/batch)
2016-02-04 04:32:22.854705: step 141670, loss = 0.44 (313.5 examples/sec; 0.408 sec/batch)
2016-02-04 04:32:27.544435: step 141680, loss = 0.44 (278.5 examples/sec; 0.460 sec/batch)
2016-02-04 04:32:32.143300: step 141690, loss = 0.34 (291.9 examples/sec; 0.439 sec/batch)
2016-02-04 04:32:36.891190: step 141700, loss = 0.40 (294.1 examples/sec; 0.435 sec/batch)
2016-02-04 04:32:42.096332: step 141710, loss = 0.52 (267.4 examples/sec; 0.479 sec/batch)
2016-02-04 04:32:46.770268: step 141720, loss = 0.44 (297.4 examples/sec; 0.430 sec/batch)
2016-02-04 04:32:51.498954: step 141730, loss = 0.48 (259.7 examples/sec; 0.493 sec/batch)
2016-02-04 04:32:56.226678: step 141740, loss = 0.44 (273.9 examples/sec; 0.467 sec/batch)
2016-02-04 04:33:00.842591: step 141750, loss = 0.37 (253.8 examples/sec; 0.504 sec/batch)
2016-02-04 04:33:05.560803: step 141760, loss = 0.36 (268.3 examples/sec; 0.477 sec/batch)
2016-02-04 04:33:10.329579: step 141770, loss = 0.41 (270.0 examples/sec; 0.474 sec/batch)
2016-02-04 04:33:15.082609: step 141780, loss = 0.43 (252.2 examples/sec; 0.507 sec/batch)
2016-02-04 04:33:19.785661: step 141790, loss = 0.50 (258.7 examples/sec; 0.495 sec/batch)
2016-02-04 04:33:24.487770: step 141800, loss = 0.45 (289.1 examples/sec; 0.443 sec/batch)
2016-02-04 04:33:29.796388: step 141810, loss = 0.48 (267.8 examples/sec; 0.478 sec/batch)
2016-02-04 04:33:34.434587: step 141820, loss = 0.37 (272.2 examples/sec; 0.470 sec/batch)
2016-02-04 04:33:39.162162: step 141830, loss = 0.42 (276.3 examples/sec; 0.463 sec/batch)
2016-02-04 04:33:43.918584: step 141840, loss = 0.42 (287.0 examples/sec; 0.446 sec/batch)
2016-02-04 04:33:48.693071: step 141850, loss = 0.47 (267.8 examples/sec; 0.478 sec/batch)
2016-02-04 04:33:53.390311: step 141860, loss = 0.32 (280.3 examples/sec; 0.457 sec/batch)
2016-02-04 04:33:58.123411: step 141870, loss = 0.32 (263.4 examples/sec; 0.486 sec/batch)
2016-02-04 04:34:02.806762: step 141880, loss = 0.39 (266.7 examples/sec; 0.480 sec/batch)
2016-02-04 04:34:07.574802: step 141890, loss = 0.40 (248.1 examples/sec; 0.516 sec/batch)
2016-02-04 04:34:12.285246: step 141900, loss = 0.41 (247.1 examples/sec; 0.518 sec/batch)
2016-02-04 04:34:17.370948: step 141910, loss = 0.44 (265.7 examples/sec; 0.482 sec/batch)
2016-02-04 04:34:22.074586: step 141920, loss = 0.58 (282.9 examples/sec; 0.452 sec/batch)
2016-02-04 04:34:26.750393: step 141930, loss = 0.45 (309.4 examples/sec; 0.414 sec/batch)
2016-02-04 04:34:31.538674: step 141940, loss = 0.45 (258.7 examples/sec; 0.495 sec/batch)
2016-02-04 04:34:36.288037: step 141950, loss = 0.47 (245.5 examples/sec; 0.521 sec/batch)
2016-02-04 04:34:41.023054: step 141960, loss = 0.38 (251.0 examples/sec; 0.510 sec/batch)
2016-02-04 04:34:45.707334: step 141970, loss = 0.42 (268.1 examples/sec; 0.477 sec/batch)
2016-02-04 04:34:50.456794: step 141980, loss = 0.38 (270.8 examples/sec; 0.473 sec/batch)
2016-02-04 04:34:55.202252: step 141990, loss = 0.39 (238.1 examples/sec; 0.538 sec/batch)
2016-02-04 04:34:59.896353: step 142000, loss = 0.44 (271.6 examples/sec; 0.471 sec/batch)
2016-02-04 04:35:05.186423: step 142010, loss = 0.37 (249.4 examples/sec; 0.513 sec/batch)
2016-02-04 04:35:09.915599: step 142020, loss = 0.44 (255.2 examples/sec; 0.502 sec/batch)
2016-02-04 04:35:14.647745: step 142030, loss = 0.37 (284.8 examples/sec; 0.449 sec/batch)
2016-02-04 04:35:19.313491: step 142040, loss = 0.34 (287.0 examples/sec; 0.446 sec/batch)
2016-02-04 04:35:24.177595: step 142050, loss = 0.36 (254.1 examples/sec; 0.504 sec/batch)
2016-02-04 04:35:28.895545: step 142060, loss = 0.33 (288.1 examples/sec; 0.444 sec/batch)
2016-02-04 04:35:33.635065: step 142070, loss = 0.37 (275.6 examples/sec; 0.464 sec/batch)
2016-02-04 04:35:38.409881: step 142080, loss = 0.37 (262.3 examples/sec; 0.488 sec/batch)
2016-02-04 04:35:43.046714: step 142090, loss = 0.42 (270.2 examples/sec; 0.474 sec/batch)
2016-02-04 04:35:47.761809: step 142100, loss = 0.40 (270.5 examples/sec; 0.473 sec/batch)
2016-02-04 04:35:53.101469: step 142110, loss = 0.38 (271.5 examples/sec; 0.472 sec/batch)
2016-02-04 04:35:57.804014: step 142120, loss = 0.38 (284.1 examples/sec; 0.450 sec/batch)
2016-02-04 04:36:02.537557: step 142130, loss = 0.42 (257.8 examples/sec; 0.497 sec/batch)
2016-02-04 04:36:07.231830: step 142140, loss = 0.44 (264.8 examples/sec; 0.483 sec/batch)
2016-02-04 04:36:11.992790: step 142150, loss = 0.37 (269.7 examples/sec; 0.475 sec/batch)
2016-02-04 04:36:16.679536: step 142160, loss = 0.52 (289.9 examples/sec; 0.441 sec/batch)
2016-02-04 04:36:21.341643: step 142170, loss = 0.48 (283.5 examples/sec; 0.451 sec/batch)
2016-02-04 04:36:26.146332: step 142180, loss = 0.36 (254.1 examples/sec; 0.504 sec/batch)
2016-02-04 04:36:30.868059: step 142190, loss = 0.39 (279.0 examples/sec; 0.459 sec/batch)
2016-02-04 04:36:35.582334: step 142200, loss = 0.38 (280.5 examples/sec; 0.456 sec/batch)
2016-02-04 04:36:40.823755: step 142210, loss = 0.36 (268.0 examples/sec; 0.478 sec/batch)
2016-02-04 04:36:45.471291: step 142220, loss = 0.48 (283.3 examples/sec; 0.452 sec/batch)
2016-02-04 04:36:50.186392: step 142230, loss = 0.31 (259.3 examples/sec; 0.494 sec/batch)
2016-02-04 04:36:54.862346: step 142240, loss = 0.39 (262.3 examples/sec; 0.488 sec/batch)
2016-02-04 04:36:59.538628: step 142250, loss = 0.36 (260.2 examples/sec; 0.492 sec/batch)
2016-02-04 04:37:04.256068: step 142260, loss = 0.32 (277.0 examples/sec; 0.462 sec/batch)
2016-02-04 04:37:08.964351: step 142270, loss = 0.44 (277.1 examples/sec; 0.462 sec/batch)
2016-02-04 04:37:13.640943: step 142280, loss = 0.49 (260.1 examples/sec; 0.492 sec/batch)
2016-02-04 04:37:18.309158: step 142290, loss = 0.35 (287.1 examples/sec; 0.446 sec/batch)
2016-02-04 04:37:23.057828: step 142300, loss = 0.43 (267.7 examples/sec; 0.478 sec/batch)
2016-02-04 04:37:28.281239: step 142310, loss = 0.33 (301.7 examples/sec; 0.424 sec/batch)
2016-02-04 04:37:33.021006: step 142320, loss = 0.39 (258.3 examples/sec; 0.495 sec/batch)
2016-02-04 04:37:37.720179: step 142330, loss = 0.47 (292.8 examples/sec; 0.437 sec/batch)
2016-02-04 04:37:42.444688: step 142340, loss = 0.34 (266.8 examples/sec; 0.480 sec/batch)
2016-02-04 04:37:47.092216: step 142350, loss = 0.42 (254.4 examples/sec; 0.503 sec/batch)
2016-02-04 04:37:51.746351: step 142360, loss = 0.44 (270.8 examples/sec; 0.473 sec/batch)
2016-02-04 04:37:56.421258: step 142370, loss = 0.38 (295.1 examples/sec; 0.434 sec/batch)
2016-02-04 04:38:01.122680: step 142380, loss = 0.42 (278.4 examples/sec; 0.460 sec/batch)
2016-02-04 04:38:05.864783: step 142390, loss = 0.44 (254.2 examples/sec; 0.503 sec/batch)
2016-02-04 04:38:10.531002: step 142400, loss = 0.31 (295.2 examples/sec; 0.434 sec/batch)
2016-02-04 04:38:15.895704: step 142410, loss = 0.37 (254.9 examples/sec; 0.502 sec/batch)
2016-02-04 04:38:20.559424: step 142420, loss = 0.47 (258.2 examples/sec; 0.496 sec/batch)
2016-02-04 04:38:25.302608: step 142430, loss = 0.53 (253.2 examples/sec; 0.506 sec/batch)
2016-02-04 04:38:29.958624: step 142440, loss = 0.47 (257.9 examples/sec; 0.496 sec/batch)
2016-02-04 04:38:34.618274: step 142450, loss = 0.39 (282.9 examples/sec; 0.452 sec/batch)
2016-02-04 04:38:39.253708: step 142460, loss = 0.37 (270.0 examples/sec; 0.474 sec/batch)
2016-02-04 04:38:44.005192: step 142470, loss = 0.36 (262.2 examples/sec; 0.488 sec/batch)
2016-02-04 04:38:48.755172: step 142480, loss = 0.35 (286.3 examples/sec; 0.447 sec/batch)
2016-02-04 04:38:53.501317: step 142490, loss = 0.40 (271.2 examples/sec; 0.472 sec/batch)
2016-02-04 04:38:58.236031: step 142500, loss = 0.35 (289.0 examples/sec; 0.443 sec/batch)
2016-02-04 04:39:03.485667: step 142510, loss = 0.41 (279.5 examples/sec; 0.458 sec/batch)
2016-02-04 04:39:08.282411: step 142520, loss = 0.42 (251.3 examples/sec; 0.509 sec/batch)
2016-02-04 04:39:12.981938: step 142530, loss = 0.34 (297.2 examples/sec; 0.431 sec/batch)
2016-02-04 04:39:17.698182: step 142540, loss = 0.46 (283.1 examples/sec; 0.452 sec/batch)
2016-02-04 04:39:22.347042: step 142550, loss = 0.34 (292.3 examples/sec; 0.438 sec/batch)
2016-02-04 04:39:27.100647: step 142560, loss = 0.52 (285.7 examples/sec; 0.448 sec/batch)
2016-02-04 04:39:31.783420: step 142570, loss = 0.37 (286.6 examples/sec; 0.447 sec/batch)
2016-02-04 04:39:36.444227: step 142580, loss = 0.39 (282.6 examples/sec; 0.453 sec/batch)
2016-02-04 04:39:41.156592: step 142590, loss = 0.36 (252.5 examples/sec; 0.507 sec/batch)
2016-02-04 04:39:45.918899: step 142600, loss = 0.39 (263.0 examples/sec; 0.487 sec/batch)
2016-02-04 04:39:51.153005: step 142610, loss = 0.42 (270.5 examples/sec; 0.473 sec/batch)
2016-02-04 04:39:55.882019: step 142620, loss = 0.36 (278.1 examples/sec; 0.460 sec/batch)
2016-02-04 04:40:00.573313: step 142630, loss = 0.51 (254.2 examples/sec; 0.504 sec/batch)
2016-02-04 04:40:05.321288: step 142640, loss = 0.44 (279.0 examples/sec; 0.459 sec/batch)
2016-02-04 04:40:10.017876: step 142650, loss = 0.36 (250.0 examples/sec; 0.512 sec/batch)
2016-02-04 04:40:14.657494: step 142660, loss = 0.46 (263.8 examples/sec; 0.485 sec/batch)
2016-02-04 04:40:19.408085: step 142670, loss = 0.39 (245.0 examples/sec; 0.522 sec/batch)
2016-02-04 04:40:24.140721: step 142680, loss = 0.44 (267.1 examples/sec; 0.479 sec/batch)
2016-02-04 04:40:28.909546: step 142690, loss = 0.33 (247.3 examples/sec; 0.518 sec/batch)
2016-02-04 04:40:33.539726: step 142700, loss = 0.42 (283.1 examples/sec; 0.452 sec/batch)
2016-02-04 04:40:38.726068: step 142710, loss = 0.35 (265.0 examples/sec; 0.483 sec/batch)
2016-02-04 04:40:43.413486: step 142720, loss = 0.38 (285.1 examples/sec; 0.449 sec/batch)
2016-02-04 04:40:48.022875: step 142730, loss = 0.44 (261.6 examples/sec; 0.489 sec/batch)
2016-02-04 04:40:52.732820: step 142740, loss = 0.46 (271.1 examples/sec; 0.472 sec/batch)
2016-02-04 04:40:57.392966: step 142750, loss = 0.46 (284.2 examples/sec; 0.450 sec/batch)
2016-02-04 04:41:02.057828: step 142760, loss = 0.35 (277.3 examples/sec; 0.462 sec/batch)
2016-02-04 04:41:06.674517: step 142770, loss = 0.47 (265.9 examples/sec; 0.481 sec/batch)
2016-02-04 04:41:11.433744: step 142780, loss = 0.34 (259.0 examples/sec; 0.494 sec/batch)
2016-02-04 04:41:16.149897: step 142790, loss = 0.37 (258.4 examples/sec; 0.495 sec/batch)
2016-02-04 04:41:20.830209: step 142800, loss = 0.40 (244.6 examples/sec; 0.523 sec/batch)
2016-02-04 04:41:25.945486: step 142810, loss = 0.38 (279.8 examples/sec; 0.457 sec/batch)
2016-02-04 04:41:30.628512: step 142820, loss = 0.45 (276.2 examples/sec; 0.463 sec/batch)
2016-02-04 04:41:35.482127: step 142830, loss = 0.41 (250.8 examples/sec; 0.510 sec/batch)
2016-02-04 04:41:40.210230: step 142840, loss = 0.29 (257.0 examples/sec; 0.498 sec/batch)
2016-02-04 04:41:44.865994: step 142850, loss = 0.39 (302.0 examples/sec; 0.424 sec/batch)
2016-02-04 04:41:49.501102: step 142860, loss = 0.29 (273.9 examples/sec; 0.467 sec/batch)
2016-02-04 04:41:54.269343: step 142870, loss = 0.42 (249.3 examples/sec; 0.513 sec/batch)
2016-02-04 04:41:58.958320: step 142880, loss = 0.40 (275.5 examples/sec; 0.465 sec/batch)
2016-02-04 04:42:03.715128: step 142890, loss = 0.33 (266.9 examples/sec; 0.480 sec/batch)
2016-02-04 04:42:08.415095: step 142900, loss = 0.40 (286.5 examples/sec; 0.447 sec/batch)
2016-02-04 04:42:13.676199: step 142910, loss = 0.50 (263.6 examples/sec; 0.485 sec/batch)
2016-02-04 04:42:18.319740: step 142920, loss = 0.33 (262.2 examples/sec; 0.488 sec/batch)
2016-02-04 04:42:23.037218: step 142930, loss = 0.37 (282.8 examples/sec; 0.453 sec/batch)
2016-02-04 04:42:27.813720: step 142940, loss = 0.43 (275.6 examples/sec; 0.464 sec/batch)
2016-02-04 04:42:32.595354: step 142950, loss = 0.40 (279.4 examples/sec; 0.458 sec/batch)
2016-02-04 04:42:37.340102: step 142960, loss = 0.39 (275.7 examples/sec; 0.464 sec/batch)
2016-02-04 04:42:42.087118: step 142970, loss = 0.33 (275.5 examples/sec; 0.465 sec/batch)
2016-02-04 04:42:46.826736: step 142980, loss = 0.43 (309.3 examples/sec; 0.414 sec/batch)
2016-02-04 04:42:51.415327: step 142990, loss = 0.36 (285.5 examples/sec; 0.448 sec/batch)
2016-02-04 04:42:56.095217: step 143000, loss = 0.40 (274.7 examples/sec; 0.466 sec/batch)
2016-02-04 04:43:01.409299: step 143010, loss = 0.41 (262.6 examples/sec; 0.488 sec/batch)
2016-02-04 04:43:06.100036: step 143020, loss = 0.41 (273.5 examples/sec; 0.468 sec/batch)
2016-02-04 04:43:10.771432: step 143030, loss = 0.29 (295.3 examples/sec; 0.433 sec/batch)
2016-02-04 04:43:15.520679: step 143040, loss = 0.48 (253.8 examples/sec; 0.504 sec/batch)
2016-02-04 04:43:20.263428: step 143050, loss = 0.39 (286.4 examples/sec; 0.447 sec/batch)
2016-02-04 04:43:25.059847: step 143060, loss = 0.58 (289.3 examples/sec; 0.442 sec/batch)
2016-02-04 04:43:29.736584: step 143070, loss = 0.44 (279.0 examples/sec; 0.459 sec/batch)
2016-02-04 04:43:34.412883: step 143080, loss = 0.37 (274.6 examples/sec; 0.466 sec/batch)
2016-02-04 04:43:39.139120: step 143090, loss = 0.39 (287.9 examples/sec; 0.445 sec/batch)
2016-02-04 04:43:43.835860: step 143100, loss = 0.35 (281.7 examples/sec; 0.454 sec/batch)
2016-02-04 04:43:49.038586: step 143110, loss = 0.45 (289.5 examples/sec; 0.442 sec/batch)
2016-02-04 04:43:53.807970: step 143120, loss = 0.38 (267.5 examples/sec; 0.478 sec/batch)
2016-02-04 04:43:58.511013: step 143130, loss = 0.41 (270.7 examples/sec; 0.473 sec/batch)
2016-02-04 04:44:03.246733: step 143140, loss = 0.38 (262.6 examples/sec; 0.487 sec/batch)
2016-02-04 04:44:07.952740: step 143150, loss = 0.39 (265.2 examples/sec; 0.483 sec/batch)
2016-02-04 04:44:12.654033: step 143160, loss = 0.42 (289.7 examples/sec; 0.442 sec/batch)
2016-02-04 04:44:17.319805: step 143170, loss = 0.47 (271.1 examples/sec; 0.472 sec/batch)
2016-02-04 04:44:22.031698: step 143180, loss = 0.30 (280.9 examples/sec; 0.456 sec/batch)
2016-02-04 04:44:26.712806: step 143190, loss = 0.42 (273.8 examples/sec; 0.467 sec/batch)
2016-02-04 04:44:31.504753: step 143200, loss = 0.36 (277.8 examples/sec; 0.461 sec/batch)
2016-02-04 04:44:36.831109: step 143210, loss = 0.33 (265.0 examples/sec; 0.483 sec/batch)
2016-02-04 04:44:41.547469: step 143220, loss = 0.51 (272.6 examples/sec; 0.470 sec/batch)
2016-02-04 04:44:46.256106: step 143230, loss = 0.41 (276.3 examples/sec; 0.463 sec/batch)
2016-02-04 04:44:50.933882: step 143240, loss = 0.44 (268.7 examples/sec; 0.476 sec/batch)
2016-02-04 04:44:55.654714: step 143250, loss = 0.35 (283.3 examples/sec; 0.452 sec/batch)
2016-02-04 04:45:00.426641: step 143260, loss = 0.35 (284.4 examples/sec; 0.450 sec/batch)
2016-02-04 04:45:05.162964: step 143270, loss = 0.44 (244.8 examples/sec; 0.523 sec/batch)
2016-02-04 04:45:09.848638: step 143280, loss = 0.29 (257.4 examples/sec; 0.497 sec/batch)
2016-02-04 04:45:14.531427: step 143290, loss = 0.43 (262.7 examples/sec; 0.487 sec/batch)
2016-02-04 04:45:19.141300: step 143300, loss = 0.36 (286.7 examples/sec; 0.446 sec/batch)
2016-02-04 04:45:24.431713: step 143310, loss = 0.38 (253.2 examples/sec; 0.505 sec/batch)
2016-02-04 04:45:29.210073: step 143320, loss = 0.38 (271.3 examples/sec; 0.472 sec/batch)
2016-02-04 04:45:33.924406: step 143330, loss = 0.32 (281.6 examples/sec; 0.454 sec/batch)
2016-02-04 04:45:38.582334: step 143340, loss = 0.37 (275.4 examples/sec; 0.465 sec/batch)
2016-02-04 04:45:43.294775: step 143350, loss = 0.40 (266.9 examples/sec; 0.480 sec/batch)
2016-02-04 04:45:47.935325: step 143360, loss = 0.42 (275.4 examples/sec; 0.465 sec/batch)
2016-02-04 04:45:52.643241: step 143370, loss = 0.35 (275.2 examples/sec; 0.465 sec/batch)
2016-02-04 04:45:57.335353: step 143380, loss = 0.44 (277.2 examples/sec; 0.462 sec/batch)
2016-02-04 04:46:02.111520: step 143390, loss = 0.34 (275.3 examples/sec; 0.465 sec/batch)
2016-02-04 04:46:06.762705: step 143400, loss = 0.41 (286.2 examples/sec; 0.447 sec/batch)
2016-02-04 04:46:12.007653: step 143410, loss = 0.32 (270.0 examples/sec; 0.474 sec/batch)
2016-02-04 04:46:16.705115: step 143420, loss = 0.36 (259.3 examples/sec; 0.494 sec/batch)
2016-02-04 04:46:21.376439: step 143430, loss = 0.42 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 04:46:26.062739: step 143440, loss = 0.36 (266.5 examples/sec; 0.480 sec/batch)
2016-02-04 04:46:30.720804: step 143450, loss = 0.51 (271.2 examples/sec; 0.472 sec/batch)
2016-02-04 04:46:35.350281: step 143460, loss = 0.42 (277.7 examples/sec; 0.461 sec/batch)
2016-02-04 04:46:39.995883: step 143470, loss = 0.44 (278.4 examples/sec; 0.460 sec/batch)
2016-02-04 04:46:44.697177: step 143480, loss = 0.36 (285.2 examples/sec; 0.449 sec/batch)
2016-02-04 04:46:49.385942: step 143490, loss = 0.48 (296.2 examples/sec; 0.432 sec/batch)
2016-02-04 04:46:54.144488: step 143500, loss = 0.36 (262.1 examples/sec; 0.488 sec/batch)
2016-02-04 04:46:59.353853: step 143510, loss = 0.41 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 04:47:04.058979: step 143520, loss = 0.36 (255.0 examples/sec; 0.502 sec/batch)
2016-02-04 04:47:08.800830: step 143530, loss = 0.37 (263.3 examples/sec; 0.486 sec/batch)
2016-02-04 04:47:13.481820: step 143540, loss = 0.30 (266.0 examples/sec; 0.481 sec/batch)
2016-02-04 04:47:18.236445: step 143550, loss = 0.30 (260.7 examples/sec; 0.491 sec/batch)
2016-02-04 04:47:22.916315: step 143560, loss = 0.32 (283.1 examples/sec; 0.452 sec/batch)
2016-02-04 04:47:27.664080: step 143570, loss = 0.37 (268.4 examples/sec; 0.477 sec/batch)
2016-02-04 04:47:32.306032: step 143580, loss = 0.46 (279.2 examples/sec; 0.459 sec/batch)
2016-02-04 04:47:37.068027: step 143590, loss = 0.36 (275.3 examples/sec; 0.465 sec/batch)
2016-02-04 04:47:41.758059: step 143600, loss = 0.44 (286.8 examples/sec; 0.446 sec/batch)
2016-02-04 04:47:46.991703: step 143610, loss = 0.41 (293.3 examples/sec; 0.436 sec/batch)
2016-02-04 04:47:51.696950: step 143620, loss = 0.37 (256.7 examples/sec; 0.499 sec/batch)
2016-02-04 04:47:56.368536: step 143630, loss = 0.34 (265.0 examples/sec; 0.483 sec/batch)
2016-02-04 04:48:01.112192: step 143640, loss = 0.31 (262.4 examples/sec; 0.488 sec/batch)
2016-02-04 04:48:05.829756: step 143650, loss = 0.32 (293.1 examples/sec; 0.437 sec/batch)
2016-02-04 04:48:10.531993: step 143660, loss = 0.31 (287.9 examples/sec; 0.445 sec/batch)
2016-02-04 04:48:15.211660: step 143670, loss = 0.50 (266.1 examples/sec; 0.481 sec/batch)
2016-02-04 04:48:19.816234: step 143680, loss = 0.27 (255.5 examples/sec; 0.501 sec/batch)
2016-02-04 04:48:24.521758: step 143690, loss = 0.44 (283.4 examples/sec; 0.452 sec/batch)
2016-02-04 04:48:29.261011: step 143700, loss = 0.34 (268.8 examples/sec; 0.476 sec/batch)
2016-02-04 04:48:34.489326: step 143710, loss = 0.31 (272.5 examples/sec; 0.470 sec/batch)
2016-02-04 04:48:39.226325: step 143720, loss = 0.41 (273.4 examples/sec; 0.468 sec/batch)
2016-02-04 04:48:43.986818: step 143730, loss = 0.33 (279.2 examples/sec; 0.458 sec/batch)
2016-02-04 04:48:48.690325: step 143740, loss = 0.32 (278.8 examples/sec; 0.459 sec/batch)
2016-02-04 04:48:53.402892: step 143750, loss = 0.39 (270.4 examples/sec; 0.473 sec/batch)
2016-02-04 04:48:58.124834: step 143760, loss = 0.31 (279.5 examples/sec; 0.458 sec/batch)
2016-02-04 04:49:02.762523: step 143770, loss = 0.35 (277.4 examples/sec; 0.461 sec/batch)
2016-02-04 04:49:07.407456: step 143780, loss = 0.35 (260.9 examples/sec; 0.491 sec/batch)
2016-02-04 04:49:12.108070: step 143790, loss = 0.37 (279.4 examples/sec; 0.458 sec/batch)
2016-02-04 04:49:16.842454: step 143800, loss = 0.35 (302.5 examples/sec; 0.423 sec/batch)
2016-02-04 04:49:22.126528: step 143810, loss = 0.42 (251.0 examples/sec; 0.510 sec/batch)
2016-02-04 04:49:26.719697: step 143820, loss = 0.41 (274.9 examples/sec; 0.466 sec/batch)
2016-02-04 04:49:31.317851: step 143830, loss = 0.34 (268.2 examples/sec; 0.477 sec/batch)
2016-02-04 04:49:35.992806: step 143840, loss = 0.40 (249.4 examples/sec; 0.513 sec/batch)
2016-02-04 04:49:40.712928: step 143850, loss = 0.42 (270.5 examples/sec; 0.473 sec/batch)
2016-02-04 04:49:45.460360: step 143860, loss = 0.45 (264.6 examples/sec; 0.484 sec/batch)
2016-02-04 04:49:50.149719: step 143870, loss = 0.47 (287.2 examples/sec; 0.446 sec/batch)
2016-02-04 04:49:54.844758: step 143880, loss = 0.35 (277.8 examples/sec; 0.461 sec/batch)
2016-02-04 04:49:59.615822: step 143890, loss = 0.37 (275.8 examples/sec; 0.464 sec/batch)
2016-02-04 04:50:04.348339: step 143900, loss = 0.42 (277.4 examples/sec; 0.461 sec/batch)
2016-02-04 04:50:09.523361: step 143910, loss = 0.37 (272.8 examples/sec; 0.469 sec/batch)
2016-02-04 04:50:14.223625: step 143920, loss = 0.50 (282.7 examples/sec; 0.453 sec/batch)
2016-02-04 04:50:18.953520: step 143930, loss = 0.47 (276.2 examples/sec; 0.463 sec/batch)
2016-02-04 04:50:23.654367: step 143940, loss = 0.49 (282.9 examples/sec; 0.452 sec/batch)
2016-02-04 04:50:28.452610: step 143950, loss = 0.34 (273.4 examples/sec; 0.468 sec/batch)
2016-02-04 04:50:33.191090: step 143960, loss = 0.33 (261.5 examples/sec; 0.490 sec/batch)
2016-02-04 04:50:37.810502: step 143970, loss = 0.39 (284.8 examples/sec; 0.450 sec/batch)
2016-02-04 04:50:42.516597: step 143980, loss = 0.40 (297.5 examples/sec; 0.430 sec/batch)
2016-02-04 04:50:47.240067: step 143990, loss = 0.34 (241.3 examples/sec; 0.530 sec/batch)
2016-02-04 04:50:51.925229: step 144000, loss = 0.39 (248.3 examples/sec; 0.516 sec/batch)
2016-02-04 04:50:57.058351: step 144010, loss = 0.31 (291.7 examples/sec; 0.439 sec/batch)
2016-02-04 04:51:01.782729: step 144020, loss = 0.32 (262.6 examples/sec; 0.487 sec/batch)
2016-02-04 04:51:06.434635: step 144030, loss = 0.51 (265.4 examples/sec; 0.482 sec/batch)
2016-02-04 04:51:11.201103: step 144040, loss = 0.46 (272.0 examples/sec; 0.471 sec/batch)
2016-02-04 04:51:15.832689: step 144050, loss = 0.42 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 04:51:20.583230: step 144060, loss = 0.37 (261.2 examples/sec; 0.490 sec/batch)
2016-02-04 04:51:25.297377: step 144070, loss = 0.49 (265.6 examples/sec; 0.482 sec/batch)
2016-02-04 04:51:30.062658: step 144080, loss = 0.40 (246.9 examples/sec; 0.518 sec/batch)
2016-02-04 04:51:34.711661: step 144090, loss = 0.33 (301.1 examples/sec; 0.425 sec/batch)
2016-02-04 04:51:39.349790: step 144100, loss = 0.38 (288.3 examples/sec; 0.444 sec/batch)
2016-02-04 04:51:44.477946: step 144110, loss = 0.35 (266.4 examples/sec; 0.480 sec/batch)
2016-02-04 04:51:49.169460: step 144120, loss = 0.36 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 04:51:53.856929: step 144130, loss = 0.35 (255.2 examples/sec; 0.502 sec/batch)
2016-02-04 04:51:58.432314: step 144140, loss = 0.44 (271.4 examples/sec; 0.472 sec/batch)
2016-02-04 04:52:03.112649: step 144150, loss = 0.35 (279.1 examples/sec; 0.459 sec/batch)
2016-02-04 04:52:07.876935: step 144160, loss = 0.29 (272.8 examples/sec; 0.469 sec/batch)
2016-02-04 04:52:12.572672: step 144170, loss = 0.43 (273.3 examples/sec; 0.468 sec/batch)
2016-02-04 04:52:17.239994: step 144180, loss = 0.37 (287.1 examples/sec; 0.446 sec/batch)
2016-02-04 04:52:21.899797: step 144190, loss = 0.37 (276.3 examples/sec; 0.463 sec/batch)
2016-02-04 04:52:26.523515: step 144200, loss = 0.38 (281.6 examples/sec; 0.455 sec/batch)
2016-02-04 04:52:31.662080: step 144210, loss = 0.33 (290.3 examples/sec; 0.441 sec/batch)
2016-02-04 04:52:36.364259: step 144220, loss = 0.32 (254.3 examples/sec; 0.503 sec/batch)
2016-02-04 04:52:41.042085: step 144230, loss = 0.41 (267.1 examples/sec; 0.479 sec/batch)
2016-02-04 04:52:45.688112: step 144240, loss = 0.39 (274.6 examples/sec; 0.466 sec/batch)
2016-02-04 04:52:50.378525: step 144250, loss = 0.27 (287.8 examples/sec; 0.445 sec/batch)
2016-02-04 04:52:55.049539: step 144260, loss = 0.35 (295.0 examples/sec; 0.434 sec/batch)
2016-02-04 04:52:59.807106: step 144270, loss = 0.35 (254.6 examples/sec; 0.503 sec/batch)
2016-02-04 04:53:04.414162: step 144280, loss = 0.36 (282.8 examples/sec; 0.453 sec/batch)
2016-02-04 04:53:09.135470: step 144290, loss = 0.37 (266.8 examples/sec; 0.480 sec/batch)
2016-02-04 04:53:13.863863: step 144300, loss = 0.30 (284.2 examples/sec; 0.450 sec/batch)
2016-02-04 04:53:18.950885: step 144310, loss = 0.40 (285.1 examples/sec; 0.449 sec/batch)
2016-02-04 04:53:23.562464: step 144320, loss = 0.45 (294.5 examples/sec; 0.435 sec/batch)
2016-02-04 04:53:28.163745: step 144330, loss = 0.36 (265.0 examples/sec; 0.483 sec/batch)
2016-02-04 04:53:32.898175: step 144340, loss = 0.34 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 04:53:37.559860: step 144350, loss = 0.29 (278.8 examples/sec; 0.459 sec/batch)
2016-02-04 04:53:42.249064: step 144360, loss = 0.37 (285.3 examples/sec; 0.449 sec/batch)
2016-02-04 04:53:46.863632: step 144370, loss = 0.31 (262.8 examples/sec; 0.487 sec/batch)
2016-02-04 04:53:51.526249: step 144380, loss = 0.31 (273.4 examples/sec; 0.468 sec/batch)
2016-02-04 04:53:56.163309: step 144390, loss = 0.34 (269.6 examples/sec; 0.475 sec/batch)
2016-02-04 04:54:00.866908: step 144400, loss = 0.36 (267.0 examples/sec; 0.479 sec/batch)
2016-02-04 04:54:06.064678: step 144410, loss = 0.39 (306.1 examples/sec; 0.418 sec/batch)
2016-02-04 04:54:10.761892: step 144420, loss = 0.33 (264.4 examples/sec; 0.484 sec/batch)
2016-02-04 04:54:15.352589: step 144430, loss = 0.35 (283.4 examples/sec; 0.452 sec/batch)
2016-02-04 04:54:20.010252: step 144440, loss = 0.36 (258.0 examples/sec; 0.496 sec/batch)
2016-02-04 04:54:24.585587: step 144450, loss = 0.30 (296.1 examples/sec; 0.432 sec/batch)
2016-02-04 04:54:29.389689: step 144460, loss = 0.31 (254.5 examples/sec; 0.503 sec/batch)
2016-02-04 04:54:34.082292: step 144470, loss = 0.43 (269.7 examples/sec; 0.475 sec/batch)
2016-02-04 04:54:38.789141: step 144480, loss = 0.38 (282.5 examples/sec; 0.453 sec/batch)
2016-02-04 04:54:43.543175: step 144490, loss = 0.37 (269.6 examples/sec; 0.475 sec/batch)
2016-02-04 04:54:48.312546: step 144500, loss = 0.39 (245.2 examples/sec; 0.522 sec/batch)
2016-02-04 04:54:53.542555: step 144510, loss = 0.42 (268.8 examples/sec; 0.476 sec/batch)
2016-02-04 04:54:58.283176: step 144520, loss = 0.37 (302.1 examples/sec; 0.424 sec/batch)
2016-02-04 04:55:03.080859: step 144530, loss = 0.40 (253.9 examples/sec; 0.504 sec/batch)
2016-02-04 04:55:07.782428: step 144540, loss = 0.34 (288.6 examples/sec; 0.444 sec/batch)
2016-02-04 04:55:12.456419: step 144550, loss = 0.49 (266.7 examples/sec; 0.480 sec/batch)
2016-02-04 04:55:17.161737: step 144560, loss = 0.40 (247.5 examples/sec; 0.517 sec/batch)
2016-02-04 04:55:21.792692: step 144570, loss = 0.38 (286.3 examples/sec; 0.447 sec/batch)
2016-02-04 04:55:26.566070: step 144580, loss = 0.40 (278.8 examples/sec; 0.459 sec/batch)
2016-02-04 04:55:31.294593: step 144590, loss = 0.43 (263.0 examples/sec; 0.487 sec/batch)
2016-02-04 04:55:35.999295: step 144600, loss = 0.33 (249.7 examples/sec; 0.513 sec/batch)
2016-02-04 04:55:41.192182: step 144610, loss = 0.34 (274.5 examples/sec; 0.466 sec/batch)
2016-02-04 04:55:45.843791: step 144620, loss = 0.28 (284.3 examples/sec; 0.450 sec/batch)
2016-02-04 04:55:50.531878: step 144630, loss = 0.41 (265.1 examples/sec; 0.483 sec/batch)
2016-02-04 04:55:55.192034: step 144640, loss = 0.44 (289.6 examples/sec; 0.442 sec/batch)
2016-02-04 04:55:59.911958: step 144650, loss = 0.28 (287.0 examples/sec; 0.446 sec/batch)
2016-02-04 04:56:04.593532: step 144660, loss = 0.32 (256.1 examples/sec; 0.500 sec/batch)
2016-02-04 04:56:09.187005: step 144670, loss = 0.43 (265.4 examples/sec; 0.482 sec/batch)
2016-02-04 04:56:13.879411: step 144680, loss = 0.47 (268.6 examples/sec; 0.476 sec/batch)
2016-02-04 04:56:18.443197: step 144690, loss = 0.37 (273.3 examples/sec; 0.468 sec/batch)
2016-02-04 04:56:23.047962: step 144700, loss = 0.40 (300.3 examples/sec; 0.426 sec/batch)
2016-02-04 04:56:28.291088: step 144710, loss = 0.34 (264.3 examples/sec; 0.484 sec/batch)
2016-02-04 04:56:33.083856: step 144720, loss = 0.31 (283.0 examples/sec; 0.452 sec/batch)
2016-02-04 04:56:37.856784: step 144730, loss = 0.38 (292.5 examples/sec; 0.438 sec/batch)
2016-02-04 04:56:42.548004: step 144740, loss = 0.37 (267.5 examples/sec; 0.479 sec/batch)
2016-02-04 04:56:47.243363: step 144750, loss = 0.41 (296.8 examples/sec; 0.431 sec/batch)
2016-02-04 04:56:52.027329: step 144760, loss = 0.38 (259.9 examples/sec; 0.493 sec/batch)
2016-02-04 04:56:56.804920: step 144770, loss = 0.33 (277.7 examples/sec; 0.461 sec/batch)
2016-02-04 04:57:01.550968: step 144780, loss = 0.32 (279.1 examples/sec; 0.459 sec/batch)
2016-02-04 04:57:06.344188: step 144790, loss = 0.35 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 04:57:11.081920: step 144800, loss = 0.31 (243.1 examples/sec; 0.526 sec/batch)
2016-02-04 04:57:16.343433: step 144810, loss = 0.31 (265.2 examples/sec; 0.483 sec/batch)
2016-02-04 04:57:21.033607: step 144820, loss = 0.34 (268.4 examples/sec; 0.477 sec/batch)
2016-02-04 04:57:25.685472: step 144830, loss = 0.35 (272.8 examples/sec; 0.469 sec/batch)
2016-02-04 04:57:30.401253: step 144840, loss = 0.31 (292.1 examples/sec; 0.438 sec/batch)
2016-02-04 04:57:35.165046: step 144850, loss = 0.37 (274.0 examples/sec; 0.467 sec/batch)
2016-02-04 04:57:39.945180: step 144860, loss = 0.39 (258.1 examples/sec; 0.496 sec/batch)
2016-02-04 04:57:44.597362: step 144870, loss = 0.49 (280.9 examples/sec; 0.456 sec/batch)
2016-02-04 04:57:49.317491: step 144880, loss = 0.44 (260.3 examples/sec; 0.492 sec/batch)
2016-02-04 04:57:54.099371: step 144890, loss = 0.37 (255.1 examples/sec; 0.502 sec/batch)
2016-02-04 04:57:58.879636: step 144900, loss = 0.45 (258.5 examples/sec; 0.495 sec/batch)
2016-02-04 04:58:04.171573: step 144910, loss = 0.31 (267.3 examples/sec; 0.479 sec/batch)
2016-02-04 04:58:08.921436: step 144920, loss = 0.32 (274.5 examples/sec; 0.466 sec/batch)
2016-02-04 04:58:13.660132: step 144930, loss = 0.37 (243.4 examples/sec; 0.526 sec/batch)
2016-02-04 04:58:18.332739: step 144940, loss = 0.40 (266.7 examples/sec; 0.480 sec/batch)
2016-02-04 04:58:23.065806: step 144950, loss = 0.31 (275.3 examples/sec; 0.465 sec/batch)
2016-02-04 04:58:27.770046: step 144960, loss = 0.33 (260.7 examples/sec; 0.491 sec/batch)
2016-02-04 04:58:32.431514: step 144970, loss = 0.34 (250.7 examples/sec; 0.511 sec/batch)
2016-02-04 04:58:37.114658: step 144980, loss = 0.38 (269.7 examples/sec; 0.475 sec/batch)
2016-02-04 04:58:41.851390: step 144990, loss = 0.45 (259.6 examples/sec; 0.493 sec/batch)
2016-02-04 04:58:46.504515: step 145000, loss = 0.32 (269.1 examples/sec; 0.476 sec/batch)
2016-02-04 04:58:51.716504: step 145010, loss = 0.37 (267.1 examples/sec; 0.479 sec/batch)
2016-02-04 04:58:56.357764: step 145020, loss = 0.47 (295.9 examples/sec; 0.433 sec/batch)
2016-02-04 04:59:01.135751: step 145030, loss = 0.27 (276.3 examples/sec; 0.463 sec/batch)
2016-02-04 04:59:05.779629: step 145040, loss = 0.36 (273.1 examples/sec; 0.469 sec/batch)
2016-02-04 04:59:10.500651: step 145050, loss = 0.35 (291.5 examples/sec; 0.439 sec/batch)
2016-02-04 04:59:15.164628: step 145060, loss = 0.34 (261.8 examples/sec; 0.489 sec/batch)
2016-02-04 04:59:19.760815: step 145070, loss = 0.35 (262.0 examples/sec; 0.488 sec/batch)
2016-02-04 04:59:24.396950: step 145080, loss = 0.47 (278.6 examples/sec; 0.459 sec/batch)
2016-02-04 04:59:29.079201: step 145090, loss = 0.50 (282.3 examples/sec; 0.453 sec/batch)
2016-02-04 04:59:33.798195: step 145100, loss = 0.39 (272.3 examples/sec; 0.470 sec/batch)
2016-02-04 04:59:39.017559: step 145110, loss = 0.34 (256.7 examples/sec; 0.499 sec/batch)
2016-02-04 04:59:43.724026: step 145120, loss = 0.36 (276.8 examples/sec; 0.462 sec/batch)
2016-02-04 04:59:48.441610: step 145130, loss = 0.41 (310.3 examples/sec; 0.413 sec/batch)
2016-02-04 04:59:53.173396: step 145140, loss = 0.34 (260.3 examples/sec; 0.492 sec/batch)
2016-02-04 04:59:57.782321: step 145150, loss = 0.42 (306.4 examples/sec; 0.418 sec/batch)
2016-02-04 05:00:02.431901: step 145160, loss = 0.37 (273.6 examples/sec; 0.468 sec/batch)
2016-02-04 05:00:07.078921: step 145170, loss = 0.30 (287.9 examples/sec; 0.445 sec/batch)
2016-02-04 05:00:11.796525: step 145180, loss = 0.32 (297.5 examples/sec; 0.430 sec/batch)
2016-02-04 05:00:16.487154: step 145190, loss = 0.42 (282.1 examples/sec; 0.454 sec/batch)
2016-02-04 05:00:21.161718: step 145200, loss = 0.48 (254.9 examples/sec; 0.502 sec/batch)
2016-02-04 05:00:26.338688: step 145210, loss = 0.38 (269.7 examples/sec; 0.475 sec/batch)
2016-02-04 05:00:30.988040: step 145220, loss = 0.35 (276.6 examples/sec; 0.463 sec/batch)
2016-02-04 05:00:35.651813: step 145230, loss = 0.38 (276.9 examples/sec; 0.462 sec/batch)
2016-02-04 05:00:40.352279: step 145240, loss = 0.37 (278.9 examples/sec; 0.459 sec/batch)
2016-02-04 05:00:44.988610: step 145250, loss = 0.30 (278.4 examples/sec; 0.460 sec/batch)
2016-02-04 05:00:49.688256: step 145260, loss = 0.36 (260.6 examples/sec; 0.491 sec/batch)
2016-02-04 05:00:54.369749: step 145270, loss = 0.36 (261.4 examples/sec; 0.490 sec/batch)
2016-02-04 05:00:59.075114: step 145280, loss = 0.35 (274.8 examples/sec; 0.466 sec/batch)
2016-02-04 05:01:03.820904: step 145290, loss = 0.40 (269.0 examples/sec; 0.476 sec/batch)
2016-02-04 05:01:08.576992: step 145300, loss = 0.38 (259.0 examples/sec; 0.494 sec/batch)
2016-02-04 05:01:13.721449: step 145310, loss = 0.33 (289.6 examples/sec; 0.442 sec/batch)
2016-02-04 05:01:18.383081: step 145320, loss = 0.51 (286.6 examples/sec; 0.447 sec/batch)
2016-02-04 05:01:23.145496: step 145330, loss = 0.34 (273.2 examples/sec; 0.469 sec/batch)
2016-02-04 05:01:27.852208: step 145340, loss = 0.33 (266.4 examples/sec; 0.480 sec/batch)
2016-02-04 05:01:32.618595: step 145350, loss = 0.36 (247.6 examples/sec; 0.517 sec/batch)
2016-02-04 05:01:37.317603: step 145360, loss = 0.38 (274.4 examples/sec; 0.466 sec/batch)
2016-02-04 05:01:42.063747: step 145370, loss = 0.42 (267.9 examples/sec; 0.478 sec/batch)
2016-02-04 05:01:46.686181: step 145380, loss = 0.31 (282.0 examples/sec; 0.454 sec/batch)
2016-02-04 05:01:51.376156: step 145390, loss = 0.40 (293.5 examples/sec; 0.436 sec/batch)
2016-02-04 05:01:56.144352: step 145400, loss = 0.38 (272.1 examples/sec; 0.470 sec/batch)
2016-02-04 05:02:01.389932: step 145410, loss = 0.35 (287.1 examples/sec; 0.446 sec/batch)
2016-02-04 05:02:06.130539: step 145420, loss = 0.35 (263.4 examples/sec; 0.486 sec/batch)
2016-02-04 05:02:10.831404: step 145430, loss = 0.43 (299.4 examples/sec; 0.428 sec/batch)
2016-02-04 05:02:15.526616: step 145440, loss = 0.45 (246.6 examples/sec; 0.519 sec/batch)
2016-02-04 05:02:20.176841: step 145450, loss = 0.43 (289.2 examples/sec; 0.443 sec/batch)
2016-02-04 05:02:24.880384: step 145460, loss = 0.34 (258.4 examples/sec; 0.495 sec/batch)
2016-02-04 05:02:29.583367: step 145470, loss = 0.35 (268.7 examples/sec; 0.476 sec/batch)
2016-02-04 05:02:34.273731: step 145480, loss = 0.30 (276.5 examples/sec; 0.463 sec/batch)
2016-02-04 05:02:39.009850: step 145490, loss = 0.38 (275.6 examples/sec; 0.464 sec/batch)
2016-02-04 05:02:43.748019: step 145500, loss = 0.37 (269.0 examples/sec; 0.476 sec/batch)
2016-02-04 05:02:49.039888: step 145510, loss = 0.33 (258.2 examples/sec; 0.496 sec/batch)
2016-02-04 05:02:53.776510: step 145520, loss = 0.32 (270.3 examples/sec; 0.473 sec/batch)
2016-02-04 05:02:58.458341: step 145530, loss = 0.35 (272.5 examples/sec; 0.470 sec/batch)
2016-02-04 05:03:03.138599: step 145540, loss = 0.35 (285.7 examples/sec; 0.448 sec/batch)
2016-02-04 05:03:07.798035: step 145550, loss = 0.38 (266.1 examples/sec; 0.481 sec/batch)
2016-02-04 05:03:12.495058: step 145560, loss = 0.42 (248.3 examples/sec; 0.516 sec/batch)
2016-02-04 05:03:17.117841: step 145570, loss = 0.34 (289.6 examples/sec; 0.442 sec/batch)
2016-02-04 05:03:21.823603: step 145580, loss = 0.45 (296.6 examples/sec; 0.432 sec/batch)
2016-02-04 05:03:26.501498: step 145590, loss = 0.37 (272.2 examples/sec; 0.470 sec/batch)
2016-02-04 05:03:31.228025: step 145600, loss = 0.34 (283.6 examples/sec; 0.451 sec/batch)
2016-02-04 05:03:36.405548: step 145610, loss = 0.39 (266.8 examples/sec; 0.480 sec/batch)
2016-02-04 05:03:41.073164: step 145620, loss = 0.36 (264.9 examples/sec; 0.483 sec/batch)
2016-02-04 05:03:45.733678: step 145630, loss = 0.36 (260.1 examples/sec; 0.492 sec/batch)
2016-02-04 05:03:50.303056: step 145640, loss = 0.41 (287.3 examples/sec; 0.446 sec/batch)
2016-02-04 05:03:54.968443: step 145650, loss = 0.38 (262.3 examples/sec; 0.488 sec/batch)
2016-02-04 05:03:59.598793: step 145660, loss = 0.27 (293.6 examples/sec; 0.436 sec/batch)
2016-02-04 05:04:04.409447: step 145670, loss = 0.35 (257.9 examples/sec; 0.496 sec/batch)
2016-02-04 05:04:09.121736: step 145680, loss = 0.40 (272.6 examples/sec; 0.469 sec/batch)
2016-02-04 05:04:13.858344: step 145690, loss = 0.29 (264.0 examples/sec; 0.485 sec/batch)
2016-02-04 05:04:18.585197: step 145700, loss = 0.36 (287.7 examples/sec; 0.445 sec/batch)
2016-02-04 05:04:23.853973: step 145710, loss = 0.36 (259.0 examples/sec; 0.494 sec/batch)
2016-02-04 05:04:28.584432: step 145720, loss = 0.40 (275.9 examples/sec; 0.464 sec/batch)
2016-02-04 05:04:33.283713: step 145730, loss = 0.32 (260.2 examples/sec; 0.492 sec/batch)
2016-02-04 05:04:37.959014: step 145740, loss = 0.29 (270.8 examples/sec; 0.473 sec/batch)
2016-02-04 05:04:42.661337: step 145750, loss = 0.34 (296.5 examples/sec; 0.432 sec/batch)
2016-02-04 05:04:47.359907: step 145760, loss = 0.34 (272.6 examples/sec; 0.470 sec/batch)
2016-02-04 05:04:52.007829: step 145770, loss = 0.37 (275.2 examples/sec; 0.465 sec/batch)
2016-02-04 05:04:56.785632: step 145780, loss = 0.30 (276.1 examples/sec; 0.464 sec/batch)
2016-02-04 05:05:01.474265: step 145790, loss = 0.31 (268.6 examples/sec; 0.476 sec/batch)
2016-02-04 05:05:06.151190: step 145800, loss = 0.35 (271.2 examples/sec; 0.472 sec/batch)
2016-02-04 05:05:11.409581: step 145810, loss = 0.35 (256.8 examples/sec; 0.498 sec/batch)
2016-02-04 05:05:16.073447: step 145820, loss = 0.32 (268.1 examples/sec; 0.477 sec/batch)
2016-02-04 05:05:20.688928: step 145830, loss = 0.35 (253.9 examples/sec; 0.504 sec/batch)
2016-02-04 05:05:25.371135: step 145840, loss = 0.36 (269.4 examples/sec; 0.475 sec/batch)
2016-02-04 05:05:30.115702: step 145850, loss = 0.32 (260.8 examples/sec; 0.491 sec/batch)
2016-02-04 05:05:34.762100: step 145860, loss = 0.40 (268.3 examples/sec; 0.477 sec/batch)
2016-02-04 05:05:39.503402: step 145870, loss = 0.43 (282.0 examples/sec; 0.454 sec/batch)
2016-02-04 05:05:44.307627: step 145880, loss = 0.32 (265.1 examples/sec; 0.483 sec/batch)
2016-02-04 05:05:49.001434: step 145890, loss = 0.37 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 05:05:53.833779: step 145900, loss = 0.44 (261.5 examples/sec; 0.490 sec/batch)
2016-02-04 05:05:59.007176: step 145910, loss = 0.40 (277.1 examples/sec; 0.462 sec/batch)
2016-02-04 05:06:03.722053: step 145920, loss = 0.29 (284.0 examples/sec; 0.451 sec/batch)
2016-02-04 05:06:08.530294: step 145930, loss = 0.30 (254.6 examples/sec; 0.503 sec/batch)
2016-02-04 05:06:13.188648: step 145940, loss = 0.37 (288.6 examples/sec; 0.444 sec/batch)
2016-02-04 05:06:17.932331: step 145950, loss = 0.38 (261.3 examples/sec; 0.490 sec/batch)
2016-02-04 05:06:22.709713: step 145960, loss = 0.39 (268.7 examples/sec; 0.476 sec/batch)
2016-02-04 05:06:27.461875: step 145970, loss = 0.26 (259.3 examples/sec; 0.494 sec/batch)
2016-02-04 05:06:32.219758: step 145980, loss = 0.43 (263.4 examples/sec; 0.486 sec/batch)
2016-02-04 05:06:36.930290: step 145990, loss = 0.34 (286.6 examples/sec; 0.447 sec/batch)
2016-02-04 05:06:41.669223: step 146000, loss = 0.38 (295.4 examples/sec; 0.433 sec/batch)
2016-02-04 05:06:46.845791: step 146010, loss = 0.35 (275.7 examples/sec; 0.464 sec/batch)
2016-02-04 05:06:51.600348: step 146020, loss = 0.34 (297.9 examples/sec; 0.430 sec/batch)
2016-02-04 05:06:56.327336: step 146030, loss = 0.30 (265.1 examples/sec; 0.483 sec/batch)
2016-02-04 05:07:01.018058: step 146040, loss = 0.36 (289.9 examples/sec; 0.441 sec/batch)
2016-02-04 05:07:05.769937: step 146050, loss = 0.43 (247.8 examples/sec; 0.517 sec/batch)
2016-02-04 05:07:10.500611: step 146060, loss = 0.40 (291.8 examples/sec; 0.439 sec/batch)
2016-02-04 05:07:15.246245: step 146070, loss = 0.31 (273.6 examples/sec; 0.468 sec/batch)
2016-02-04 05:07:19.901449: step 146080, loss = 0.34 (270.0 examples/sec; 0.474 sec/batch)
2016-02-04 05:07:24.635604: step 146090, loss = 0.31 (262.3 examples/sec; 0.488 sec/batch)
2016-02-04 05:07:29.248052: step 146100, loss = 0.30 (267.2 examples/sec; 0.479 sec/batch)
2016-02-04 05:07:34.478302: step 146110, loss = 0.31 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 05:07:39.075839: step 146120, loss = 0.40 (278.6 examples/sec; 0.460 sec/batch)
2016-02-04 05:07:43.694047: step 146130, loss = 0.38 (298.5 examples/sec; 0.429 sec/batch)
2016-02-04 05:07:48.306306: step 146140, loss = 0.41 (250.9 examples/sec; 0.510 sec/batch)
2016-02-04 05:07:52.908868: step 146150, loss = 0.28 (282.8 examples/sec; 0.453 sec/batch)
2016-02-04 05:07:57.670270: step 146160, loss = 0.37 (268.2 examples/sec; 0.477 sec/batch)
2016-02-04 05:08:02.367834: step 146170, loss = 0.39 (273.2 examples/sec; 0.469 sec/batch)
2016-02-04 05:08:07.092739: step 146180, loss = 0.32 (272.6 examples/sec; 0.470 sec/batch)
2016-02-04 05:08:11.841118: step 146190, loss = 0.33 (285.2 examples/sec; 0.449 sec/batch)
2016-02-04 05:08:16.605154: step 146200, loss = 0.37 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 05:08:21.785779: step 146210, loss = 0.29 (281.4 examples/sec; 0.455 sec/batch)
2016-02-04 05:08:26.485780: step 146220, loss = 0.31 (268.7 examples/sec; 0.476 sec/batch)
2016-02-04 05:08:31.247629: step 146230, loss = 0.34 (269.1 examples/sec; 0.476 sec/batch)
2016-02-04 05:08:36.012485: step 146240, loss = 0.32 (260.4 examples/sec; 0.492 sec/batch)
2016-02-04 05:08:40.725373: step 146250, loss = 0.38 (253.2 examples/sec; 0.505 sec/batch)
2016-02-04 05:08:45.506988: step 146260, loss = 0.40 (251.6 examples/sec; 0.509 sec/batch)
2016-02-04 05:08:50.274230: step 146270, loss = 0.30 (258.3 examples/sec; 0.496 sec/batch)
2016-02-04 05:08:54.997748: step 146280, loss = 0.26 (268.9 examples/sec; 0.476 sec/batch)
2016-02-04 05:08:59.713230: step 146290, loss = 0.36 (254.6 examples/sec; 0.503 sec/batch)
2016-02-04 05:09:04.439976: step 146300, loss = 0.33 (284.7 examples/sec; 0.450 sec/batch)
2016-02-04 05:09:09.660774: step 146310, loss = 0.32 (256.2 examples/sec; 0.500 sec/batch)
2016-02-04 05:09:14.480925: step 146320, loss = 0.39 (267.3 examples/sec; 0.479 sec/batch)
2016-02-04 05:09:19.149935: step 146330, loss = 0.36 (265.4 examples/sec; 0.482 sec/batch)
2016-02-04 05:09:23.867372: step 146340, loss = 0.29 (272.2 examples/sec; 0.470 sec/batch)
2016-02-04 05:09:28.590767: step 146350, loss = 0.33 (246.4 examples/sec; 0.520 sec/batch)
2016-02-04 05:09:33.289438: step 146360, loss = 0.30 (315.6 examples/sec; 0.406 sec/batch)
2016-02-04 05:09:38.000148: step 146370, loss = 0.42 (261.6 examples/sec; 0.489 sec/batch)
2016-02-04 05:09:42.654497: step 146380, loss = 0.41 (298.8 examples/sec; 0.428 sec/batch)
2016-02-04 05:09:47.321957: step 146390, loss = 0.29 (277.0 examples/sec; 0.462 sec/batch)
2016-02-04 05:09:52.048720: step 146400, loss = 0.31 (261.9 examples/sec; 0.489 sec/batch)
2016-02-04 05:09:57.283903: step 146410, loss = 0.36 (291.0 examples/sec; 0.440 sec/batch)
2016-02-04 05:10:02.053267: step 146420, loss = 0.37 (261.0 examples/sec; 0.490 sec/batch)
2016-02-04 05:10:06.705269: step 146430, loss = 0.30 (282.2 examples/sec; 0.454 sec/batch)
2016-02-04 05:10:11.424774: step 146440, loss = 0.34 (264.8 examples/sec; 0.483 sec/batch)
2016-02-04 05:10:16.092638: step 146450, loss = 0.36 (291.4 examples/sec; 0.439 sec/batch)
2016-02-04 05:10:20.734645: step 146460, loss = 0.44 (282.9 examples/sec; 0.453 sec/batch)
2016-02-04 05:10:25.446982: step 146470, loss = 0.31 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 05:10:30.183604: step 146480, loss = 0.34 (266.1 examples/sec; 0.481 sec/batch)
2016-02-04 05:10:34.960337: step 146490, loss = 0.41 (269.4 examples/sec; 0.475 sec/batch)
2016-02-04 05:10:39.554748: step 146500, loss = 0.34 (270.5 examples/sec; 0.473 sec/batch)
2016-02-04 05:10:44.783147: step 146510, loss = 0.29 (287.1 examples/sec; 0.446 sec/batch)
2016-02-04 05:10:49.502866: step 146520, loss = 0.37 (253.7 examples/sec; 0.505 sec/batch)
2016-02-04 05:10:54.180712: step 146530, loss = 0.35 (282.8 examples/sec; 0.453 sec/batch)
2016-02-04 05:10:58.869228: step 146540, loss = 0.27 (281.2 examples/sec; 0.455 sec/batch)
2016-02-04 05:11:03.541330: step 146550, loss = 0.32 (300.1 examples/sec; 0.427 sec/batch)
2016-02-04 05:11:08.255349: step 146560, loss = 0.35 (268.3 examples/sec; 0.477 sec/batch)
2016-02-04 05:11:12.972439: step 146570, loss = 0.34 (250.4 examples/sec; 0.511 sec/batch)
2016-02-04 05:11:17.693296: step 146580, loss = 0.26 (264.1 examples/sec; 0.485 sec/batch)
2016-02-04 05:11:22.443412: step 146590, loss = 0.35 (258.4 examples/sec; 0.495 sec/batch)
2016-02-04 05:11:27.154290: step 146600, loss = 0.29 (273.9 examples/sec; 0.467 sec/batch)
2016-02-04 05:11:32.305713: step 146610, loss = 0.41 (281.1 examples/sec; 0.455 sec/batch)
2016-02-04 05:11:37.071733: step 146620, loss = 0.36 (262.4 examples/sec; 0.488 sec/batch)
2016-02-04 05:11:41.773367: step 146630, loss = 0.27 (271.0 examples/sec; 0.472 sec/batch)
2016-02-04 05:11:46.323560: step 146640, loss = 0.36 (287.1 examples/sec; 0.446 sec/batch)
2016-02-04 05:11:51.042648: step 146650, loss = 0.37 (282.7 examples/sec; 0.453 sec/batch)
2016-02-04 05:11:55.721591: step 146660, loss = 0.34 (288.9 examples/sec; 0.443 sec/batch)
2016-02-04 05:12:00.393163: step 146670, loss = 0.40 (257.5 examples/sec; 0.497 sec/batch)
2016-02-04 05:12:05.038496: step 146680, loss = 0.40 (253.4 examples/sec; 0.505 sec/batch)
2016-02-04 05:12:09.669937: step 146690, loss = 0.38 (285.9 examples/sec; 0.448 sec/batch)
2016-02-04 05:12:14.296789: step 146700, loss = 0.29 (295.8 examples/sec; 0.433 sec/batch)
2016-02-04 05:12:19.510500: step 146710, loss = 0.32 (262.8 examples/sec; 0.487 sec/batch)
2016-02-04 05:12:24.137116: step 146720, loss = 0.42 (270.3 examples/sec; 0.474 sec/batch)
2016-02-04 05:12:28.780083: step 146730, loss = 0.42 (287.1 examples/sec; 0.446 sec/batch)
2016-02-04 05:12:33.372911: step 146740, loss = 0.28 (305.2 examples/sec; 0.419 sec/batch)
2016-02-04 05:12:38.009792: step 146750, loss = 0.40 (295.8 examples/sec; 0.433 sec/batch)
2016-02-04 05:12:42.636120: step 146760, loss = 0.36 (309.0 examples/sec; 0.414 sec/batch)
2016-02-04 05:12:47.142276: step 146770, loss = 0.39 (301.2 examples/sec; 0.425 sec/batch)
2016-02-04 05:12:51.756317: step 146780, loss = 0.29 (253.5 examples/sec; 0.505 sec/batch)
2016-02-04 05:12:56.395714: step 146790, loss = 0.32 (258.8 examples/sec; 0.495 sec/batch)
2016-02-04 05:13:01.036918: step 146800, loss = 0.33 (257.5 examples/sec; 0.497 sec/batch)
2016-02-04 05:13:06.268202: step 146810, loss = 0.36 (274.7 examples/sec; 0.466 sec/batch)
2016-02-04 05:13:10.882818: step 146820, loss = 0.32 (292.5 examples/sec; 0.438 sec/batch)
2016-02-04 05:13:15.461669: step 146830, loss = 0.39 (326.3 examples/sec; 0.392 sec/batch)
2016-02-04 05:13:20.142139: step 146840, loss = 0.39 (250.3 examples/sec; 0.511 sec/batch)
2016-02-04 05:13:24.798136: step 146850, loss = 0.31 (274.1 examples/sec; 0.467 sec/batch)
2016-02-04 05:13:29.294789: step 146860, loss = 0.37 (291.6 examples/sec; 0.439 sec/batch)
2016-02-04 05:13:34.012850: step 146870, loss = 0.36 (287.4 examples/sec; 0.445 sec/batch)
2016-02-04 05:13:38.750970: step 146880, loss = 0.30 (272.4 examples/sec; 0.470 sec/batch)
2016-02-04 05:13:43.485361: step 146890, loss = 0.34 (283.2 examples/sec; 0.452 sec/batch)
2016-02-04 05:13:48.140462: step 146900, loss = 0.30 (276.4 examples/sec; 0.463 sec/batch)
2016-02-04 05:13:53.321649: step 146910, loss = 0.43 (281.6 examples/sec; 0.455 sec/batch)
2016-02-04 05:13:58.129011: step 146920, loss = 0.30 (262.9 examples/sec; 0.487 sec/batch)
2016-02-04 05:14:02.964841: step 146930, loss = 0.39 (256.9 examples/sec; 0.498 sec/batch)
2016-02-04 05:14:07.624319: step 146940, loss = 0.35 (241.2 examples/sec; 0.531 sec/batch)
2016-02-04 05:14:12.287180: step 146950, loss = 0.38 (280.8 examples/sec; 0.456 sec/batch)
2016-02-04 05:14:17.031800: step 146960, loss = 0.32 (266.1 examples/sec; 0.481 sec/batch)
2016-02-04 05:14:21.748416: step 146970, loss = 0.35 (274.5 examples/sec; 0.466 sec/batch)
2016-02-04 05:14:26.504880: step 146980, loss = 0.28 (255.0 examples/sec; 0.502 sec/batch)
2016-02-04 05:14:31.159633: step 146990, loss = 0.34 (276.0 examples/sec; 0.464 sec/batch)
2016-02-04 05:14:35.888955: step 147000, loss = 0.32 (265.5 examples/sec; 0.482 sec/batch)
2016-02-04 05:14:41.118962: step 147010, loss = 0.28 (279.1 examples/sec; 0.459 sec/batch)
2016-02-04 05:14:45.891992: step 147020, loss = 0.23 (269.8 examples/sec; 0.474 sec/batch)
2016-02-04 05:14:50.561038: step 147030, loss = 0.33 (279.8 examples/sec; 0.458 sec/batch)
2016-02-04 05:14:55.164908: step 147040, loss = 0.30 (306.4 examples/sec; 0.418 sec/batch)
2016-02-04 05:14:59.840589: step 147050, loss = 0.31 (265.3 examples/sec; 0.483 sec/batch)
2016-02-04 05:15:04.623955: step 147060, loss = 0.39 (249.6 examples/sec; 0.513 sec/batch)
2016-02-04 05:15:09.331548: step 147070, loss = 0.30 (274.3 examples/sec; 0.467 sec/batch)
2016-02-04 05:15:14.022676: step 147080, loss = 0.30 (290.4 examples/sec; 0.441 sec/batch)
2016-02-04 05:15:18.773615: step 147090, loss = 0.44 (284.6 examples/sec; 0.450 sec/batch)
2016-02-04 05:15:23.498917: step 147100, loss = 0.37 (277.5 examples/sec; 0.461 sec/batch)
2016-02-04 05:15:28.883387: step 147110, loss = 0.38 (259.1 examples/sec; 0.494 sec/batch)
2016-02-04 05:15:33.704838: step 147120, loss = 0.38 (272.6 examples/sec; 0.470 sec/batch)
2016-02-04 05:15:38.365815: step 147130, loss = 0.31 (279.4 examples/sec; 0.458 sec/batch)
2016-02-04 05:15:42.992217: step 147140, loss = 0.30 (277.8 examples/sec; 0.461 sec/batch)
2016-02-04 05:15:47.709499: step 147150, loss = 0.34 (269.5 examples/sec; 0.475 sec/batch)
2016-02-04 05:15:52.374268: step 147160, loss = 0.27 (277.8 examples/sec; 0.461 sec/batch)
2016-02-04 05:15:57.121706: step 147170, loss = 0.25 (262.7 examples/sec; 0.487 sec/batch)
2016-02-04 05:16:01.795850: step 147180, loss = 0.31 (279.1 examples/sec; 0.459 sec/batch)
2016-02-04 05:16:06.435641: step 147190, loss = 0.28 (283.9 examples/sec; 0.451 sec/batch)
2016-02-04 05:16:11.071098: step 147200, loss = 0.35 (250.5 examples/sec; 0.511 sec/batch)
2016-02-04 05:16:16.320854: step 147210, loss = 0.37 (286.6 examples/sec; 0.447 sec/batch)
2016-02-04 05:16:21.057102: step 147220, loss = 0.46 (262.9 examples/sec; 0.487 sec/batch)
2016-02-04 05:16:25.727764: step 147230, loss = 0.33 (275.8 examples/sec; 0.464 sec/batch)
2016-02-04 05:16:30.456594: step 147240, loss = 0.30 (257.0 examples/sec; 0.498 sec/batch)
2016-02-04 05:16:35.176997: step 147250, loss = 0.23 (297.2 examples/sec; 0.431 sec/batch)
2016-02-04 05:16:39.905537: step 147260, loss = 0.31 (256.8 examples/sec; 0.498 sec/batch)
2016-02-04 05:16:44.648827: step 147270, loss = 0.34 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 05:16:49.307953: step 147280, loss = 0.34 (275.5 examples/sec; 0.465 sec/batch)
2016-02-04 05:16:54.052286: step 147290, loss = 0.32 (266.2 examples/sec; 0.481 sec/batch)
2016-02-04 05:16:58.675332: step 147300, loss = 0.28 (266.1 examples/sec; 0.481 sec/batch)
2016-02-04 05:17:04.060819: step 147310, loss = 0.29 (257.7 examples/sec; 0.497 sec/batch)
2016-02-04 05:17:08.718695: step 147320, loss = 0.28 (264.3 examples/sec; 0.484 sec/batch)
2016-02-04 05:17:13.487019: step 147330, loss = 0.35 (276.4 examples/sec; 0.463 sec/batch)
2016-02-04 05:17:18.205412: step 147340, loss = 0.35 (267.9 examples/sec; 0.478 sec/batch)
2016-02-04 05:17:22.895472: step 147350, loss = 0.39 (267.8 examples/sec; 0.478 sec/batch)
2016-02-04 05:17:27.645032: step 147360, loss = 0.30 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 05:17:32.439672: step 147370, loss = 0.29 (281.0 examples/sec; 0.456 sec/batch)
2016-02-04 05:17:37.163134: step 147380, loss = 0.34 (269.0 examples/sec; 0.476 sec/batch)
2016-02-04 05:17:41.845533: step 147390, loss = 0.28 (269.4 examples/sec; 0.475 sec/batch)
2016-02-04 05:17:46.606738: step 147400, loss = 0.35 (270.1 examples/sec; 0.474 sec/batch)
2016-02-04 05:17:51.966384: step 147410, loss = 0.25 (262.3 examples/sec; 0.488 sec/batch)
2016-02-04 05:17:56.623564: step 147420, loss = 0.33 (282.0 examples/sec; 0.454 sec/batch)
2016-02-04 05:18:01.392577: step 147430, loss = 0.31 (252.8 examples/sec; 0.506 sec/batch)
2016-02-04 05:18:06.037988: step 147440, loss = 0.30 (291.7 examples/sec; 0.439 sec/batch)
2016-02-04 05:18:10.737587: step 147450, loss = 0.36 (267.7 examples/sec; 0.478 sec/batch)
2016-02-04 05:18:15.514471: step 147460, loss = 0.28 (258.3 examples/sec; 0.496 sec/batch)
2016-02-04 05:18:20.168864: step 147470, loss = 0.29 (268.9 examples/sec; 0.476 sec/batch)
2016-02-04 05:18:24.859802: step 147480, loss = 0.28 (278.2 examples/sec; 0.460 sec/batch)
2016-02-04 05:18:29.614764: step 147490, loss = 0.32 (273.5 examples/sec; 0.468 sec/batch)
2016-02-04 05:18:34.397944: step 147500, loss = 0.34 (267.5 examples/sec; 0.479 sec/batch)
2016-02-04 05:18:39.607898: step 147510, loss = 0.34 (276.3 examples/sec; 0.463 sec/batch)
2016-02-04 05:18:44.394990: step 147520, loss = 0.35 (247.2 examples/sec; 0.518 sec/batch)
2016-02-04 05:18:49.128635: step 147530, loss = 0.26 (267.0 examples/sec; 0.479 sec/batch)
2016-02-04 05:18:53.872167: step 147540, loss = 0.36 (269.6 examples/sec; 0.475 sec/batch)
2016-02-04 05:18:58.595337: step 147550, loss = 0.34 (287.5 examples/sec; 0.445 sec/batch)
2016-02-04 05:19:03.250170: step 147560, loss = 0.40 (264.0 examples/sec; 0.485 sec/batch)
2016-02-04 05:19:07.950883: step 147570, loss = 0.37 (278.6 examples/sec; 0.459 sec/batch)
2016-02-04 05:19:12.612737: step 147580, loss = 0.29 (269.4 examples/sec; 0.475 sec/batch)
2016-02-04 05:19:17.428317: step 147590, loss = 0.44 (250.9 examples/sec; 0.510 sec/batch)
2016-02-04 05:19:22.105259: step 147600, loss = 0.31 (285.9 examples/sec; 0.448 sec/batch)
2016-02-04 05:19:27.375756: step 147610, loss = 0.24 (268.0 examples/sec; 0.478 sec/batch)
2016-02-04 05:19:32.150659: step 147620, loss = 0.30 (255.4 examples/sec; 0.501 sec/batch)
2016-02-04 05:19:36.882174: step 147630, loss = 0.31 (261.5 examples/sec; 0.489 sec/batch)
2016-02-04 05:19:41.645575: step 147640, loss = 0.30 (270.5 examples/sec; 0.473 sec/batch)
2016-02-04 05:19:46.368045: step 147650, loss = 0.33 (272.1 examples/sec; 0.470 sec/batch)
2016-02-04 05:19:51.041074: step 147660, loss = 0.30 (273.3 examples/sec; 0.468 sec/batch)
2016-02-04 05:19:55.838459: step 147670, loss = 0.33 (246.2 examples/sec; 0.520 sec/batch)
2016-02-04 05:20:00.542323: step 147680, loss = 0.31 (280.7 examples/sec; 0.456 sec/batch)
2016-02-04 05:20:05.273343: step 147690, loss = 0.29 (255.2 examples/sec; 0.502 sec/batch)
2016-02-04 05:20:09.992928: step 147700, loss = 0.36 (289.4 examples/sec; 0.442 sec/batch)
2016-02-04 05:20:15.239215: step 147710, loss = 0.35 (301.0 examples/sec; 0.425 sec/batch)
2016-02-04 05:20:20.015914: step 147720, loss = 0.33 (278.2 examples/sec; 0.460 sec/batch)
2016-02-04 05:20:24.697667: step 147730, loss = 0.28 (271.7 examples/sec; 0.471 sec/batch)
2016-02-04 05:20:29.372141: step 147740, loss = 0.34 (266.1 examples/sec; 0.481 sec/batch)
2016-02-04 05:20:34.095866: step 147750, loss = 0.34 (243.9 examples/sec; 0.525 sec/batch)
2016-02-04 05:20:38.762699: step 147760, loss = 0.42 (251.6 examples/sec; 0.509 sec/batch)
2016-02-04 05:20:43.444874: step 147770, loss = 0.30 (273.9 examples/sec; 0.467 sec/batch)
2016-02-04 05:20:48.145431: step 147780, loss = 0.50 (270.3 examples/sec; 0.474 sec/batch)
2016-02-04 05:20:52.894178: step 147790, loss = 0.35 (293.3 examples/sec; 0.436 sec/batch)
2016-02-04 05:20:57.617746: step 147800, loss = 0.36 (284.7 examples/sec; 0.450 sec/batch)
2016-02-04 05:21:02.828774: step 147810, loss = 0.43 (282.8 examples/sec; 0.453 sec/batch)
2016-02-04 05:21:07.551311: step 147820, loss = 0.42 (276.8 examples/sec; 0.462 sec/batch)
2016-02-04 05:21:12.302734: step 147830, loss = 0.35 (271.3 examples/sec; 0.472 sec/batch)
2016-02-04 05:21:16.998366: step 147840, loss = 0.42 (289.3 examples/sec; 0.442 sec/batch)
2016-02-04 05:21:21.730531: step 147850, loss = 0.30 (266.4 examples/sec; 0.480 sec/batch)
2016-02-04 05:21:26.392296: step 147860, loss = 0.32 (306.5 examples/sec; 0.418 sec/batch)
2016-02-04 05:21:31.159907: step 147870, loss = 0.32 (259.0 examples/sec; 0.494 sec/batch)
2016-02-04 05:21:35.877213: step 147880, loss = 0.31 (254.4 examples/sec; 0.503 sec/batch)
2016-02-04 05:21:40.545141: step 147890, loss = 0.32 (293.1 examples/sec; 0.437 sec/batch)
2016-02-04 05:21:45.252300: step 147900, loss = 0.40 (256.6 examples/sec; 0.499 sec/batch)
2016-02-04 05:21:50.528912: step 147910, loss = 0.25 (291.2 examples/sec; 0.440 sec/batch)
2016-02-04 05:21:55.248228: step 147920, loss = 0.28 (256.9 examples/sec; 0.498 sec/batch)
2016-02-04 05:22:00.043513: step 147930, loss = 0.30 (261.8 examples/sec; 0.489 sec/batch)
2016-02-04 05:22:04.795421: step 147940, loss = 0.29 (258.4 examples/sec; 0.495 sec/batch)
2016-02-04 05:22:09.554251: step 147950, loss = 0.33 (272.6 examples/sec; 0.470 sec/batch)
2016-02-04 05:22:14.272889: step 147960, loss = 0.41 (290.4 examples/sec; 0.441 sec/batch)
2016-02-04 05:22:18.968647: step 147970, loss = 0.32 (287.4 examples/sec; 0.445 sec/batch)
2016-02-04 05:22:23.731142: step 147980, loss = 0.38 (266.0 examples/sec; 0.481 sec/batch)
2016-02-04 05:22:28.423447: step 147990, loss = 0.28 (276.3 examples/sec; 0.463 sec/batch)
2016-02-04 05:22:33.139379: step 148000, loss = 0.32 (264.4 examples/sec; 0.484 sec/batch)
2016-02-04 05:22:38.319736: step 148010, loss = 0.27 (305.5 examples/sec; 0.419 sec/batch)
2016-02-04 05:22:43.028881: step 148020, loss = 0.27 (277.1 examples/sec; 0.462 sec/batch)
2016-02-04 05:22:47.704706: step 148030, loss = 0.39 (261.3 examples/sec; 0.490 sec/batch)
2016-02-04 05:22:52.437053: step 148040, loss = 0.35 (261.4 examples/sec; 0.490 sec/batch)
2016-02-04 05:22:57.237549: step 148050, loss = 0.30 (281.5 examples/sec; 0.455 sec/batch)
2016-02-04 05:23:01.961193: step 148060, loss = 0.37 (271.8 examples/sec; 0.471 sec/batch)
2016-02-04 05:23:06.629732: step 148070, loss = 0.41 (264.7 examples/sec; 0.484 sec/batch)
2016-02-04 05:23:11.314229: step 148080, loss = 0.36 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 05:23:16.064236: step 148090, loss = 0.37 (287.4 examples/sec; 0.445 sec/batch)
2016-02-04 05:23:20.821711: step 148100, loss = 0.31 (248.2 examples/sec; 0.516 sec/batch)
2016-02-04 05:23:26.030116: step 148110, loss = 0.32 (252.6 examples/sec; 0.507 sec/batch)
2016-02-04 05:23:30.744978: step 148120, loss = 0.33 (277.7 examples/sec; 0.461 sec/batch)
2016-02-04 05:23:35.311463: step 148130, loss = 0.40 (272.5 examples/sec; 0.470 sec/batch)
2016-02-04 05:23:39.995145: step 148140, loss = 0.32 (255.0 examples/sec; 0.502 sec/batch)
2016-02-04 05:23:44.661585: step 148150, loss = 0.36 (281.5 examples/sec; 0.455 sec/batch)
2016-02-04 05:23:49.286075: step 148160, loss = 0.37 (283.1 examples/sec; 0.452 sec/batch)
2016-02-04 05:23:54.028244: step 148170, loss = 0.35 (262.8 examples/sec; 0.487 sec/batch)
2016-02-04 05:23:58.690046: step 148180, loss = 0.32 (290.9 examples/sec; 0.440 sec/batch)
2016-02-04 05:24:03.453900: step 148190, loss = 0.39 (276.8 examples/sec; 0.462 sec/batch)
2016-02-04 05:24:08.191161: step 148200, loss = 0.30 (276.8 examples/sec; 0.462 sec/batch)
2016-02-04 05:24:13.408384: step 148210, loss = 0.26 (273.8 examples/sec; 0.467 sec/batch)
2016-02-04 05:24:18.127936: step 148220, loss = 0.29 (255.3 examples/sec; 0.501 sec/batch)
2016-02-04 05:24:22.869553: step 148230, loss = 0.37 (245.2 examples/sec; 0.522 sec/batch)
2016-02-04 05:24:27.526418: step 148240, loss = 0.36 (315.2 examples/sec; 0.406 sec/batch)
2016-02-04 05:24:32.268482: step 148250, loss = 0.29 (275.0 examples/sec; 0.466 sec/batch)
2016-02-04 05:24:37.018151: step 148260, loss = 0.31 (272.3 examples/sec; 0.470 sec/batch)
2016-02-04 05:24:41.742392: step 148270, loss = 0.29 (303.2 examples/sec; 0.422 sec/batch)
2016-02-04 05:24:46.454789: step 148280, loss = 0.35 (281.4 examples/sec; 0.455 sec/batch)
2016-02-04 05:24:51.185800: step 148290, loss = 0.31 (280.4 examples/sec; 0.456 sec/batch)
2016-02-04 05:24:55.933229: step 148300, loss = 0.28 (281.5 examples/sec; 0.455 sec/batch)
2016-02-04 05:25:01.212456: step 148310, loss = 0.34 (281.4 examples/sec; 0.455 sec/batch)
2016-02-04 05:25:05.994716: step 148320, loss = 0.34 (238.8 examples/sec; 0.536 sec/batch)
2016-02-04 05:25:10.720771: step 148330, loss = 0.35 (285.8 examples/sec; 0.448 sec/batch)
2016-02-04 05:25:15.461836: step 148340, loss = 0.38 (258.4 examples/sec; 0.495 sec/batch)
2016-02-04 05:25:20.278605: step 148350, loss = 0.40 (304.1 examples/sec; 0.421 sec/batch)
2016-02-04 05:25:25.025493: step 148360, loss = 0.33 (255.0 examples/sec; 0.502 sec/batch)
2016-02-04 05:25:29.782324: step 148370, loss = 0.36 (266.2 examples/sec; 0.481 sec/batch)
2016-02-04 05:25:34.496869: step 148380, loss = 0.39 (265.7 examples/sec; 0.482 sec/batch)
2016-02-04 05:25:39.241510: step 148390, loss = 0.34 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 05:25:43.950497: step 148400, loss = 0.43 (265.5 examples/sec; 0.482 sec/batch)
2016-02-04 05:25:49.213573: step 148410, loss = 0.34 (255.4 examples/sec; 0.501 sec/batch)
2016-02-04 05:25:53.842631: step 148420, loss = 0.35 (299.9 examples/sec; 0.427 sec/batch)
2016-02-04 05:25:58.475468: step 148430, loss = 0.25 (308.7 examples/sec; 0.415 sec/batch)
2016-02-04 05:26:03.180722: step 148440, loss = 0.34 (268.4 examples/sec; 0.477 sec/batch)
2016-02-04 05:26:07.878318: step 148450, loss = 0.37 (302.9 examples/sec; 0.423 sec/batch)
2016-02-04 05:26:12.590003: step 148460, loss = 0.38 (288.4 examples/sec; 0.444 sec/batch)
2016-02-04 05:26:17.337442: step 148470, loss = 0.30 (264.4 examples/sec; 0.484 sec/batch)
2016-02-04 05:26:22.102815: step 148480, loss = 0.35 (260.7 examples/sec; 0.491 sec/batch)
2016-02-04 05:26:26.891771: step 148490, loss = 0.36 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 05:26:31.644240: step 148500, loss = 0.33 (275.6 examples/sec; 0.464 sec/batch)
2016-02-04 05:26:36.829335: step 148510, loss = 0.32 (262.3 examples/sec; 0.488 sec/batch)
2016-02-04 05:26:41.534174: step 148520, loss = 0.40 (262.1 examples/sec; 0.488 sec/batch)
2016-02-04 05:26:46.284248: step 148530, loss = 0.31 (275.8 examples/sec; 0.464 sec/batch)
2016-02-04 05:26:51.035286: step 148540, loss = 0.38 (272.2 examples/sec; 0.470 sec/batch)
2016-02-04 05:26:55.759056: step 148550, loss = 0.29 (252.9 examples/sec; 0.506 sec/batch)
2016-02-04 05:27:00.373554: step 148560, loss = 0.27 (288.8 examples/sec; 0.443 sec/batch)
2016-02-04 05:27:05.067642: step 148570, loss = 0.41 (275.5 examples/sec; 0.465 sec/batch)
2016-02-04 05:27:09.788189: step 148580, loss = 0.36 (243.0 examples/sec; 0.527 sec/batch)
2016-02-04 05:27:14.465037: step 148590, loss = 0.35 (264.8 examples/sec; 0.483 sec/batch)
2016-02-04 05:27:19.173828: step 148600, loss = 0.32 (282.6 examples/sec; 0.453 sec/batch)
2016-02-04 05:27:24.365073: step 148610, loss = 0.32 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 05:27:29.057190: step 148620, loss = 0.35 (273.5 examples/sec; 0.468 sec/batch)
2016-02-04 05:27:33.781885: step 148630, loss = 0.38 (247.4 examples/sec; 0.517 sec/batch)
2016-02-04 05:27:38.418564: step 148640, loss = 0.30 (266.0 examples/sec; 0.481 sec/batch)
2016-02-04 05:27:43.123942: step 148650, loss = 0.43 (249.2 examples/sec; 0.514 sec/batch)
2016-02-04 05:27:47.872958: step 148660, loss = 0.30 (278.4 examples/sec; 0.460 sec/batch)
2016-02-04 05:27:52.540996: step 148670, loss = 0.41 (277.6 examples/sec; 0.461 sec/batch)
2016-02-04 05:27:57.214517: step 148680, loss = 0.35 (289.2 examples/sec; 0.443 sec/batch)
2016-02-04 05:28:01.922957: step 148690, loss = 0.38 (294.7 examples/sec; 0.434 sec/batch)
2016-02-04 05:28:06.657511: step 148700, loss = 0.38 (276.4 examples/sec; 0.463 sec/batch)
2016-02-04 05:28:11.833772: step 148710, loss = 0.39 (275.5 examples/sec; 0.465 sec/batch)
2016-02-04 05:28:16.577243: step 148720, loss = 0.36 (250.5 examples/sec; 0.511 sec/batch)
2016-02-04 05:28:21.324757: step 148730, loss = 0.29 (285.3 examples/sec; 0.449 sec/batch)
2016-02-04 05:28:26.026189: step 148740, loss = 0.28 (272.0 examples/sec; 0.471 sec/batch)
2016-02-04 05:28:30.726177: step 148750, loss = 0.37 (283.1 examples/sec; 0.452 sec/batch)
2016-02-04 05:28:35.378693: step 148760, loss = 0.30 (275.9 examples/sec; 0.464 sec/batch)
2016-02-04 05:28:40.102107: step 148770, loss = 0.29 (282.0 examples/sec; 0.454 sec/batch)
2016-02-04 05:28:44.821888: step 148780, loss = 0.28 (270.7 examples/sec; 0.473 sec/batch)
2016-02-04 05:28:49.541122: step 148790, loss = 0.37 (262.1 examples/sec; 0.488 sec/batch)
2016-02-04 05:28:54.197003: step 148800, loss = 0.36 (292.1 examples/sec; 0.438 sec/batch)
2016-02-04 05:28:59.407306: step 148810, loss = 0.35 (252.2 examples/sec; 0.507 sec/batch)
2016-02-04 05:29:04.072955: step 148820, loss = 0.33 (268.5 examples/sec; 0.477 sec/batch)
2016-02-04 05:29:08.818336: step 148830, loss = 0.32 (254.5 examples/sec; 0.503 sec/batch)
2016-02-04 05:29:13.574687: step 148840, loss = 0.28 (267.6 examples/sec; 0.478 sec/batch)
2016-02-04 05:29:18.275392: step 148850, loss = 0.34 (275.7 examples/sec; 0.464 sec/batch)
2016-02-04 05:29:22.981723: step 148860, loss = 0.48 (294.6 examples/sec; 0.435 sec/batch)
2016-02-04 05:29:27.688936: step 148870, loss = 0.33 (283.0 examples/sec; 0.452 sec/batch)
2016-02-04 05:29:32.371603: step 148880, loss = 0.32 (258.9 examples/sec; 0.494 sec/batch)
2016-02-04 05:29:37.017071: step 148890, loss = 0.30 (268.3 examples/sec; 0.477 sec/batch)
2016-02-04 05:29:41.757432: step 148900, loss = 0.37 (257.0 examples/sec; 0.498 sec/batch)
2016-02-04 05:29:46.994230: step 148910, loss = 0.33 (278.9 examples/sec; 0.459 sec/batch)
2016-02-04 05:29:51.696394: step 148920, loss = 0.28 (256.1 examples/sec; 0.500 sec/batch)
2016-02-04 05:29:56.450614: step 148930, loss = 0.40 (278.8 examples/sec; 0.459 sec/batch)
2016-02-04 05:30:01.140506: step 148940, loss = 0.32 (303.2 examples/sec; 0.422 sec/batch)
2016-02-04 05:30:05.898227: step 148950, loss = 0.44 (260.8 examples/sec; 0.491 sec/batch)
2016-02-04 05:30:10.526715: step 148960, loss = 0.27 (286.8 examples/sec; 0.446 sec/batch)
2016-02-04 05:30:15.163034: step 148970, loss = 0.26 (310.2 examples/sec; 0.413 sec/batch)
2016-02-04 05:30:19.871286: step 148980, loss = 0.33 (270.2 examples/sec; 0.474 sec/batch)
2016-02-04 05:30:24.602686: step 148990, loss = 0.31 (283.9 examples/sec; 0.451 sec/batch)
2016-02-04 05:30:29.328257: step 149000, loss = 0.34 (290.3 examples/sec; 0.441 sec/batch)
2016-02-04 05:30:34.529462: step 149010, loss = 0.43 (291.7 examples/sec; 0.439 sec/batch)
2016-02-04 05:30:39.306822: step 149020, loss = 0.46 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 05:30:44.030789: step 149030, loss = 0.35 (265.7 examples/sec; 0.482 sec/batch)
2016-02-04 05:30:48.837863: step 149040, loss = 0.29 (255.4 examples/sec; 0.501 sec/batch)
2016-02-04 05:30:53.540338: step 149050, loss = 0.35 (276.1 examples/sec; 0.464 sec/batch)
2016-02-04 05:30:58.272024: step 149060, loss = 0.44 (276.8 examples/sec; 0.462 sec/batch)
2016-02-04 05:31:03.015567: step 149070, loss = 0.39 (282.3 examples/sec; 0.453 sec/batch)
2016-02-04 05:31:07.743387: step 149080, loss = 0.40 (285.7 examples/sec; 0.448 sec/batch)
2016-02-04 05:31:12.560946: step 149090, loss = 0.44 (255.8 examples/sec; 0.500 sec/batch)
2016-02-04 05:31:17.245088: step 149100, loss = 0.29 (292.8 examples/sec; 0.437 sec/batch)
2016-02-04 05:31:22.562751: step 149110, loss = 0.38 (262.4 examples/sec; 0.488 sec/batch)
2016-02-04 05:31:27.257415: step 149120, loss = 0.25 (253.7 examples/sec; 0.505 sec/batch)
2016-02-04 05:31:31.976495: step 149130, loss = 0.36 (292.5 examples/sec; 0.438 sec/batch)
2016-02-04 05:31:36.607304: step 149140, loss = 0.32 (272.8 examples/sec; 0.469 sec/batch)
2016-02-04 05:31:41.363912: step 149150, loss = 0.30 (271.7 examples/sec; 0.471 sec/batch)
2016-02-04 05:31:46.072656: step 149160, loss = 0.31 (288.7 examples/sec; 0.443 sec/batch)
2016-02-04 05:31:50.742032: step 149170, loss = 0.32 (293.7 examples/sec; 0.436 sec/batch)
2016-02-04 05:31:55.458011: step 149180, loss = 0.38 (280.5 examples/sec; 0.456 sec/batch)
2016-02-04 05:32:00.255794: step 149190, loss = 0.26 (266.0 examples/sec; 0.481 sec/batch)
2016-02-04 05:32:04.841466: step 149200, loss = 0.33 (301.9 examples/sec; 0.424 sec/batch)
2016-02-04 05:32:10.022243: step 149210, loss = 0.33 (279.6 examples/sec; 0.458 sec/batch)
2016-02-04 05:32:14.686953: step 149220, loss = 0.37 (262.1 examples/sec; 0.488 sec/batch)
2016-02-04 05:32:19.346133: step 149230, loss = 0.32 (296.2 examples/sec; 0.432 sec/batch)
2016-02-04 05:32:24.031826: step 149240, loss = 0.36 (277.2 examples/sec; 0.462 sec/batch)
2016-02-04 05:32:28.678690: step 149250, loss = 0.33 (299.4 examples/sec; 0.428 sec/batch)
2016-02-04 05:32:33.355530: step 149260, loss = 0.24 (294.6 examples/sec; 0.435 sec/batch)
2016-02-04 05:32:38.003808: step 149270, loss = 0.31 (278.1 examples/sec; 0.460 sec/batch)
2016-02-04 05:32:42.612325: step 149280, loss = 0.31 (292.5 examples/sec; 0.438 sec/batch)
2016-02-04 05:32:47.257469: step 149290, loss = 0.25 (265.9 examples/sec; 0.481 sec/batch)
2016-02-04 05:32:51.926019: step 149300, loss = 0.24 (264.6 examples/sec; 0.484 sec/batch)
2016-02-04 05:32:57.120560: step 149310, loss = 0.29 (254.1 examples/sec; 0.504 sec/batch)
2016-02-04 05:33:01.857415: step 149320, loss = 0.28 (252.6 examples/sec; 0.507 sec/batch)
2016-02-04 05:33:06.559324: step 149330, loss = 0.30 (255.1 examples/sec; 0.502 sec/batch)
2016-02-04 05:33:11.270131: step 149340, loss = 0.32 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 05:33:16.010806: step 149350, loss = 0.29 (301.5 examples/sec; 0.425 sec/batch)
2016-02-04 05:33:20.752788: step 149360, loss = 0.32 (293.3 examples/sec; 0.436 sec/batch)
2016-02-04 05:33:25.518692: step 149370, loss = 0.32 (264.7 examples/sec; 0.484 sec/batch)
2016-02-04 05:33:30.201414: step 149380, loss = 0.33 (288.7 examples/sec; 0.443 sec/batch)
2016-02-04 05:33:35.012657: step 149390, loss = 0.44 (255.6 examples/sec; 0.501 sec/batch)
2016-02-04 05:33:39.734262: step 149400, loss = 0.30 (264.0 examples/sec; 0.485 sec/batch)
2016-02-04 05:33:45.106043: step 149410, loss = 0.29 (256.1 examples/sec; 0.500 sec/batch)
2016-02-04 05:33:49.820565: step 149420, loss = 0.26 (307.5 examples/sec; 0.416 sec/batch)
2016-02-04 05:33:54.572740: step 149430, loss = 0.36 (296.2 examples/sec; 0.432 sec/batch)
2016-02-04 05:33:59.290328: step 149440, loss = 0.43 (267.4 examples/sec; 0.479 sec/batch)
2016-02-04 05:34:04.009640: step 149450, loss = 0.32 (270.3 examples/sec; 0.474 sec/batch)
2016-02-04 05:34:08.652814: step 149460, loss = 0.32 (275.8 examples/sec; 0.464 sec/batch)
2016-02-04 05:34:13.301183: step 149470, loss = 0.39 (300.2 examples/sec; 0.426 sec/batch)
2016-02-04 05:34:18.013721: step 149480, loss = 0.33 (261.9 examples/sec; 0.489 sec/batch)
2016-02-04 05:34:22.728493: step 149490, loss = 0.32 (270.8 examples/sec; 0.473 sec/batch)
2016-02-04 05:34:27.369900: step 149500, loss = 0.27 (291.4 examples/sec; 0.439 sec/batch)
2016-02-04 05:34:32.493182: step 149510, loss = 0.36 (278.4 examples/sec; 0.460 sec/batch)
2016-02-04 05:34:37.259560: step 149520, loss = 0.34 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 05:34:42.011349: step 149530, loss = 0.31 (262.9 examples/sec; 0.487 sec/batch)
2016-02-04 05:34:46.617014: step 149540, loss = 0.26 (278.9 examples/sec; 0.459 sec/batch)
2016-02-04 05:34:51.354570: step 149550, loss = 0.31 (267.4 examples/sec; 0.479 sec/batch)
2016-02-04 05:34:56.065826: step 149560, loss = 0.30 (270.1 examples/sec; 0.474 sec/batch)
2016-02-04 05:35:00.745780: step 149570, loss = 0.27 (279.9 examples/sec; 0.457 sec/batch)
2016-02-04 05:35:05.436940: step 149580, loss = 0.40 (281.8 examples/sec; 0.454 sec/batch)
2016-02-04 05:35:10.128145: step 149590, loss = 0.33 (256.5 examples/sec; 0.499 sec/batch)
2016-02-04 05:35:14.807752: step 149600, loss = 0.36 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 05:35:20.044455: step 149610, loss = 0.41 (262.6 examples/sec; 0.488 sec/batch)
2016-02-04 05:35:24.703400: step 149620, loss = 0.23 (277.6 examples/sec; 0.461 sec/batch)
2016-02-04 05:35:29.305392: step 149630, loss = 0.35 (287.9 examples/sec; 0.445 sec/batch)
2016-02-04 05:35:33.962390: step 149640, loss = 0.45 (273.2 examples/sec; 0.469 sec/batch)
2016-02-04 05:35:38.660228: step 149650, loss = 0.34 (304.7 examples/sec; 0.420 sec/batch)
2016-02-04 05:35:43.426793: step 149660, loss = 0.41 (269.5 examples/sec; 0.475 sec/batch)
2016-02-04 05:35:48.178254: step 149670, loss = 0.36 (283.0 examples/sec; 0.452 sec/batch)
2016-02-04 05:35:52.862461: step 149680, loss = 0.27 (266.1 examples/sec; 0.481 sec/batch)
2016-02-04 05:35:57.538901: step 149690, loss = 0.35 (281.2 examples/sec; 0.455 sec/batch)
2016-02-04 05:36:02.208341: step 149700, loss = 0.30 (296.4 examples/sec; 0.432 sec/batch)
2016-02-04 05:36:07.436562: step 149710, loss = 0.34 (277.5 examples/sec; 0.461 sec/batch)
2016-02-04 05:36:12.046907: step 149720, loss = 0.29 (270.8 examples/sec; 0.473 sec/batch)
2016-02-04 05:36:16.750571: step 149730, loss = 0.41 (268.6 examples/sec; 0.476 sec/batch)
2016-02-04 05:36:21.492566: step 149740, loss = 0.39 (273.4 examples/sec; 0.468 sec/batch)
2016-02-04 05:36:26.103718: step 149750, loss = 0.37 (295.0 examples/sec; 0.434 sec/batch)
2016-02-04 05:36:30.790340: step 149760, loss = 0.34 (276.5 examples/sec; 0.463 sec/batch)
2016-02-04 05:36:35.523557: step 149770, loss = 0.27 (262.1 examples/sec; 0.488 sec/batch)
2016-02-04 05:36:40.180797: step 149780, loss = 0.29 (276.8 examples/sec; 0.462 sec/batch)
2016-02-04 05:36:44.968997: step 149790, loss = 0.27 (254.9 examples/sec; 0.502 sec/batch)
2016-02-04 05:36:49.717727: step 149800, loss = 0.43 (261.0 examples/sec; 0.490 sec/batch)
2016-02-04 05:36:55.022822: step 149810, loss = 0.43 (255.2 examples/sec; 0.501 sec/batch)
2016-02-04 05:36:59.763713: step 149820, loss = 0.38 (266.1 examples/sec; 0.481 sec/batch)
2016-02-04 05:37:04.350790: step 149830, loss = 0.30 (298.4 examples/sec; 0.429 sec/batch)
2016-02-04 05:37:09.109272: step 149840, loss = 0.32 (282.0 examples/sec; 0.454 sec/batch)
2016-02-04 05:37:13.908723: step 149850, loss = 0.25 (258.2 examples/sec; 0.496 sec/batch)
2016-02-04 05:37:18.612951: step 149860, loss = 0.43 (283.3 examples/sec; 0.452 sec/batch)
2016-02-04 05:37:23.227697: step 149870, loss = 0.34 (263.8 examples/sec; 0.485 sec/batch)
2016-02-04 05:37:27.919458: step 149880, loss = 0.27 (271.4 examples/sec; 0.472 sec/batch)
2016-02-04 05:37:32.439118: step 149890, loss = 0.40 (263.0 examples/sec; 0.487 sec/batch)
2016-02-04 05:37:37.103019: step 149900, loss = 0.29 (253.1 examples/sec; 0.506 sec/batch)
2016-02-04 05:37:42.217263: step 149910, loss = 0.27 (284.5 examples/sec; 0.450 sec/batch)
2016-02-04 05:37:46.840766: step 149920, loss = 0.27 (311.0 examples/sec; 0.412 sec/batch)
2016-02-04 05:37:51.515731: step 149930, loss = 0.22 (278.8 examples/sec; 0.459 sec/batch)
2016-02-04 05:37:56.225911: step 149940, loss = 0.37 (294.6 examples/sec; 0.434 sec/batch)
2016-02-04 05:38:00.972251: step 149950, loss = 0.26 (256.1 examples/sec; 0.500 sec/batch)
2016-02-04 05:38:05.648894: step 149960, loss = 0.21 (286.3 examples/sec; 0.447 sec/batch)
2016-02-04 05:38:10.259761: step 149970, loss = 0.32 (274.0 examples/sec; 0.467 sec/batch)
2016-02-04 05:38:14.883321: step 149980, loss = 0.35 (294.5 examples/sec; 0.435 sec/batch)
2016-02-04 05:38:19.467944: step 149990, loss = 0.27 (283.9 examples/sec; 0.451 sec/batch)
2016-02-04 05:38:24.136859: step 150000, loss = 0.25 (253.2 examples/sec; 0.506 sec/batch)
2016-02-04 05:38:29.363633: step 150010, loss = 0.34 (259.8 examples/sec; 0.493 sec/batch)
2016-02-04 05:38:34.097178: step 150020, loss = 0.32 (273.4 examples/sec; 0.468 sec/batch)
2016-02-04 05:38:38.851991: step 150030, loss = 0.35 (285.2 examples/sec; 0.449 sec/batch)
2016-02-04 05:38:43.588813: step 150040, loss = 0.29 (277.2 examples/sec; 0.462 sec/batch)
2016-02-04 05:38:48.308242: step 150050, loss = 0.28 (250.2 examples/sec; 0.512 sec/batch)
2016-02-04 05:38:53.020441: step 150060, loss = 0.33 (265.7 examples/sec; 0.482 sec/batch)
2016-02-04 05:38:57.644788: step 150070, loss = 0.23 (276.9 examples/sec; 0.462 sec/batch)
2016-02-04 05:39:02.356068: step 150080, loss = 0.26 (295.4 examples/sec; 0.433 sec/batch)
2016-02-04 05:39:07.071350: step 150090, loss = 0.40 (256.2 examples/sec; 0.500 sec/batch)
2016-02-04 05:39:11.750487: step 150100, loss = 0.38 (282.7 examples/sec; 0.453 sec/batch)
2016-02-04 05:39:16.952730: step 150110, loss = 0.30 (263.9 examples/sec; 0.485 sec/batch)
2016-02-04 05:39:21.615367: step 150120, loss = 0.38 (266.9 examples/sec; 0.480 sec/batch)
2016-02-04 05:39:26.247635: step 150130, loss = 0.30 (255.0 examples/sec; 0.502 sec/batch)
2016-02-04 05:39:30.829748: step 150140, loss = 0.29 (280.3 examples/sec; 0.457 sec/batch)
2016-02-04 05:39:35.412699: step 150150, loss = 0.28 (285.2 examples/sec; 0.449 sec/batch)
2016-02-04 05:39:40.197761: step 150160, loss = 0.40 (256.3 examples/sec; 0.499 sec/batch)
2016-02-04 05:39:44.934028: step 150170, loss = 0.23 (252.3 examples/sec; 0.507 sec/batch)
2016-02-04 05:39:49.669005: step 150180, loss = 0.37 (262.8 examples/sec; 0.487 sec/batch)
2016-02-04 05:39:54.424589: step 150190, loss = 0.38 (272.0 examples/sec; 0.471 sec/batch)
2016-02-04 05:39:59.154511: step 150200, loss = 0.35 (263.1 examples/sec; 0.487 sec/batch)
2016-02-04 05:40:04.387177: step 150210, loss = 0.35 (256.0 examples/sec; 0.500 sec/batch)
2016-02-04 05:40:09.092683: step 150220, loss = 0.31 (261.5 examples/sec; 0.489 sec/batch)
2016-02-04 05:40:13.710205: step 150230, loss = 0.27 (273.4 examples/sec; 0.468 sec/batch)
2016-02-04 05:40:18.512388: step 150240, loss = 0.32 (288.4 examples/sec; 0.444 sec/batch)
2016-02-04 05:40:23.239533: step 150250, loss = 0.23 (254.9 examples/sec; 0.502 sec/batch)
2016-02-04 05:40:27.999985: step 150260, loss = 0.32 (254.8 examples/sec; 0.502 sec/batch)
2016-02-04 05:40:32.758756: step 150270, loss = 0.35 (254.1 examples/sec; 0.504 sec/batch)
2016-02-04 05:40:37.363083: step 150280, loss = 0.33 (282.2 examples/sec; 0.454 sec/batch)
2016-02-04 05:40:42.166357: step 150290, loss = 0.31 (258.7 examples/sec; 0.495 sec/batch)
2016-02-04 05:40:46.901517: step 150300, loss = 0.31 (282.4 examples/sec; 0.453 sec/batch)
2016-02-04 05:40:52.095650: step 150310, loss = 0.37 (337.8 examples/sec; 0.379 sec/batch)
2016-02-04 05:40:56.782442: step 150320, loss = 0.33 (248.7 examples/sec; 0.515 sec/batch)
2016-02-04 05:41:01.517220: step 150330, loss = 0.33 (265.3 examples/sec; 0.482 sec/batch)
2016-02-04 05:41:06.182278: step 150340, loss = 0.23 (259.5 examples/sec; 0.493 sec/batch)
2016-02-04 05:41:10.919574: step 150350, loss = 0.33 (255.0 examples/sec; 0.502 sec/batch)
2016-02-04 05:41:15.580875: step 150360, loss = 0.37 (288.4 examples/sec; 0.444 sec/batch)
2016-02-04 05:41:20.259705: step 150370, loss = 0.32 (298.2 examples/sec; 0.429 sec/batch)
2016-02-04 05:41:25.091862: step 150380, loss = 0.26 (274.1 examples/sec; 0.467 sec/batch)
2016-02-04 05:41:29.776094: step 150390, loss = 0.26 (266.7 examples/sec; 0.480 sec/batch)
2016-02-04 05:41:34.433758: step 150400, loss = 0.28 (277.9 examples/sec; 0.461 sec/batch)
2016-02-04 05:41:39.712179: step 150410, loss = 0.34 (267.6 examples/sec; 0.478 sec/batch)
2016-02-04 05:41:44.385914: step 150420, loss = 0.36 (299.8 examples/sec; 0.427 sec/batch)
2016-02-04 05:41:49.142187: step 150430, loss = 0.41 (247.0 examples/sec; 0.518 sec/batch)
2016-02-04 05:41:53.795280: step 150440, loss = 0.34 (274.5 examples/sec; 0.466 sec/batch)
2016-02-04 05:41:58.477205: step 150450, loss = 0.33 (281.8 examples/sec; 0.454 sec/batch)
2016-02-04 05:42:03.225063: step 150460, loss = 0.42 (264.8 examples/sec; 0.483 sec/batch)
2016-02-04 05:42:07.922882: step 150470, loss = 0.38 (267.0 examples/sec; 0.479 sec/batch)
2016-02-04 05:42:12.729510: step 150480, loss = 0.28 (253.3 examples/sec; 0.505 sec/batch)
2016-02-04 05:42:17.422328: step 150490, loss = 0.29 (289.0 examples/sec; 0.443 sec/batch)
2016-02-04 05:42:22.157960: step 150500, loss = 0.31 (270.3 examples/sec; 0.473 sec/batch)
2016-02-04 05:42:27.423266: step 150510, loss = 0.31 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 05:42:32.115535: step 150520, loss = 0.24 (252.2 examples/sec; 0.508 sec/batch)
2016-02-04 05:42:36.837592: step 150530, loss = 0.30 (283.7 examples/sec; 0.451 sec/batch)
2016-02-04 05:42:41.506991: step 150540, loss = 0.30 (267.4 examples/sec; 0.479 sec/batch)
2016-02-04 05:42:46.247944: step 150550, loss = 0.28 (269.7 examples/sec; 0.475 sec/batch)
2016-02-04 05:42:50.947716: step 150560, loss = 0.35 (252.6 examples/sec; 0.507 sec/batch)
2016-02-04 05:42:55.637057: step 150570, loss = 0.34 (280.5 examples/sec; 0.456 sec/batch)
2016-02-04 05:43:00.316008: step 150580, loss = 0.32 (276.7 examples/sec; 0.463 sec/batch)
2016-02-04 05:43:05.000902: step 150590, loss = 0.36 (266.0 examples/sec; 0.481 sec/batch)
2016-02-04 05:43:09.675531: step 150600, loss = 0.36 (248.3 examples/sec; 0.515 sec/batch)
2016-02-04 05:43:14.840817: step 150610, loss = 0.37 (279.7 examples/sec; 0.458 sec/batch)
2016-02-04 05:43:19.578824: step 150620, loss = 0.37 (278.6 examples/sec; 0.459 sec/batch)
2016-02-04 05:43:24.314622: step 150630, loss = 0.29 (287.9 examples/sec; 0.445 sec/batch)
2016-02-04 05:43:28.986551: step 150640, loss = 0.38 (264.7 examples/sec; 0.484 sec/batch)
2016-02-04 05:43:33.693183: step 150650, loss = 0.22 (266.4 examples/sec; 0.481 sec/batch)
2016-02-04 05:43:38.412935: step 150660, loss = 0.32 (269.6 examples/sec; 0.475 sec/batch)
2016-02-04 05:43:43.151454: step 150670, loss = 0.24 (262.1 examples/sec; 0.488 sec/batch)
2016-02-04 05:43:47.821465: step 150680, loss = 0.29 (252.7 examples/sec; 0.507 sec/batch)
2016-02-04 05:43:52.509711: step 150690, loss = 0.24 (279.0 examples/sec; 0.459 sec/batch)
2016-02-04 05:43:57.134098: step 150700, loss = 0.35 (256.8 examples/sec; 0.498 sec/batch)
2016-02-04 05:44:02.383628: step 150710, loss = 0.24 (292.4 examples/sec; 0.438 sec/batch)
2016-02-04 05:44:07.031589: step 150720, loss = 0.29 (262.7 examples/sec; 0.487 sec/batch)
2016-02-04 05:44:11.764054: step 150730, loss = 0.27 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 05:44:16.444452: step 150740, loss = 0.21 (263.9 examples/sec; 0.485 sec/batch)
2016-02-04 05:44:21.127599: step 150750, loss = 0.31 (272.9 examples/sec; 0.469 sec/batch)
2016-02-04 05:44:25.796031: step 150760, loss = 0.23 (283.1 examples/sec; 0.452 sec/batch)
2016-02-04 05:44:30.549011: step 150770, loss = 0.30 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 05:44:35.280076: step 150780, loss = 0.40 (282.5 examples/sec; 0.453 sec/batch)
2016-02-04 05:44:40.075582: step 150790, loss = 0.38 (259.4 examples/sec; 0.493 sec/batch)
2016-02-04 05:44:44.803395: step 150800, loss = 0.32 (259.1 examples/sec; 0.494 sec/batch)
2016-02-04 05:44:49.954063: step 150810, loss = 0.30 (288.2 examples/sec; 0.444 sec/batch)
2016-02-04 05:44:54.593604: step 150820, loss = 0.37 (273.4 examples/sec; 0.468 sec/batch)
2016-02-04 05:44:59.168079: step 150830, loss = 0.24 (270.3 examples/sec; 0.473 sec/batch)
2016-02-04 05:45:03.816706: step 150840, loss = 0.30 (266.4 examples/sec; 0.481 sec/batch)
2016-02-04 05:45:08.407540: step 150850, loss = 0.29 (279.6 examples/sec; 0.458 sec/batch)
2016-02-04 05:45:13.106915: step 150860, loss = 0.34 (260.5 examples/sec; 0.491 sec/batch)
2016-02-04 05:45:17.788367: step 150870, loss = 0.34 (271.1 examples/sec; 0.472 sec/batch)
2016-02-04 05:45:22.366116: step 150880, loss = 0.36 (325.2 examples/sec; 0.394 sec/batch)
2016-02-04 05:45:27.123993: step 150890, loss = 0.31 (306.3 examples/sec; 0.418 sec/batch)
2016-02-04 05:45:31.846071: step 150900, loss = 0.30 (290.6 examples/sec; 0.440 sec/batch)
2016-02-04 05:45:37.126226: step 150910, loss = 0.33 (266.4 examples/sec; 0.480 sec/batch)
2016-02-04 05:45:41.873343: step 150920, loss = 0.23 (267.1 examples/sec; 0.479 sec/batch)
2016-02-04 05:45:46.576768: step 150930, loss = 0.34 (271.7 examples/sec; 0.471 sec/batch)
2016-02-04 05:45:51.225949: step 150940, loss = 0.31 (276.9 examples/sec; 0.462 sec/batch)
2016-02-04 05:45:55.945669: step 150950, loss = 0.30 (287.4 examples/sec; 0.445 sec/batch)
2016-02-04 05:46:00.716132: step 150960, loss = 0.47 (250.4 examples/sec; 0.511 sec/batch)
2016-02-04 05:46:05.408128: step 150970, loss = 0.35 (282.0 examples/sec; 0.454 sec/batch)
2016-02-04 05:46:10.048461: step 150980, loss = 0.30 (285.3 examples/sec; 0.449 sec/batch)
2016-02-04 05:46:14.778746: step 150990, loss = 0.36 (278.5 examples/sec; 0.460 sec/batch)
2016-02-04 05:46:19.487226: step 151000, loss = 0.34 (265.6 examples/sec; 0.482 sec/batch)
2016-02-04 05:46:24.695138: step 151010, loss = 0.30 (257.3 examples/sec; 0.497 sec/batch)
2016-02-04 05:46:29.349544: step 151020, loss = 0.29 (272.4 examples/sec; 0.470 sec/batch)
2016-02-04 05:46:33.967403: step 151030, loss = 0.33 (289.1 examples/sec; 0.443 sec/batch)
2016-02-04 05:46:38.715072: step 151040, loss = 0.26 (268.6 examples/sec; 0.477 sec/batch)
2016-02-04 05:46:43.428267: step 151050, loss = 0.31 (286.4 examples/sec; 0.447 sec/batch)
2016-02-04 05:46:48.114320: step 151060, loss = 0.31 (241.8 examples/sec; 0.529 sec/batch)
2016-02-04 05:46:52.785954: step 151070, loss = 0.31 (278.6 examples/sec; 0.459 sec/batch)
2016-02-04 05:46:57.478544: step 151080, loss = 0.32 (288.1 examples/sec; 0.444 sec/batch)
2016-02-04 05:47:02.261411: step 151090, loss = 0.32 (271.6 examples/sec; 0.471 sec/batch)
2016-02-04 05:47:06.859604: step 151100, loss = 0.24 (293.6 examples/sec; 0.436 sec/batch)
2016-02-04 05:47:12.048564: step 151110, loss = 0.27 (284.0 examples/sec; 0.451 sec/batch)
2016-02-04 05:47:16.735860: step 151120, loss = 0.30 (278.4 examples/sec; 0.460 sec/batch)
2016-02-04 05:47:21.527664: step 151130, loss = 0.29 (273.7 examples/sec; 0.468 sec/batch)
2016-02-04 05:47:26.220040: step 151140, loss = 0.34 (284.1 examples/sec; 0.451 sec/batch)
2016-02-04 05:47:30.962272: step 151150, loss = 0.35 (265.6 examples/sec; 0.482 sec/batch)
2016-02-04 05:47:35.620848: step 151160, loss = 0.34 (267.9 examples/sec; 0.478 sec/batch)
2016-02-04 05:47:40.299231: step 151170, loss = 0.33 (263.6 examples/sec; 0.486 sec/batch)
2016-02-04 05:47:44.983441: step 151180, loss = 0.32 (261.9 examples/sec; 0.489 sec/batch)
2016-02-04 05:47:49.762171: step 151190, loss = 0.42 (260.4 examples/sec; 0.492 sec/batch)
2016-02-04 05:47:54.484935: step 151200, loss = 0.30 (258.2 examples/sec; 0.496 sec/batch)
2016-02-04 05:47:59.615966: step 151210, loss = 0.31 (291.0 examples/sec; 0.440 sec/batch)
2016-02-04 05:48:04.284490: step 151220, loss = 0.32 (293.5 examples/sec; 0.436 sec/batch)
2016-02-04 05:48:08.993977: step 151230, loss = 0.36 (284.6 examples/sec; 0.450 sec/batch)
2016-02-04 05:48:13.733599: step 151240, loss = 0.31 (254.2 examples/sec; 0.503 sec/batch)
2016-02-04 05:48:18.358149: step 151250, loss = 0.30 (267.9 examples/sec; 0.478 sec/batch)
2016-02-04 05:48:23.064472: step 151260, loss = 0.36 (270.2 examples/sec; 0.474 sec/batch)
2016-02-04 05:48:27.761923: step 151270, loss = 0.31 (272.5 examples/sec; 0.470 sec/batch)
2016-02-04 05:48:32.454429: step 151280, loss = 0.38 (295.2 examples/sec; 0.434 sec/batch)
2016-02-04 05:48:37.245749: step 151290, loss = 0.32 (246.8 examples/sec; 0.519 sec/batch)
2016-02-04 05:48:41.992072: step 151300, loss = 0.31 (277.1 examples/sec; 0.462 sec/batch)
2016-02-04 05:48:47.254770: step 151310, loss = 0.37 (247.5 examples/sec; 0.517 sec/batch)
2016-02-04 05:48:51.987263: step 151320, loss = 0.32 (281.8 examples/sec; 0.454 sec/batch)
2016-02-04 05:48:56.657351: step 151330, loss = 0.28 (278.4 examples/sec; 0.460 sec/batch)
2016-02-04 05:49:01.406286: step 151340, loss = 0.32 (255.6 examples/sec; 0.501 sec/batch)
2016-02-04 05:49:06.088300: step 151350, loss = 0.38 (305.1 examples/sec; 0.419 sec/batch)
2016-02-04 05:49:10.788122: step 151360, loss = 0.31 (273.4 examples/sec; 0.468 sec/batch)
2016-02-04 05:49:15.492933: step 151370, loss = 0.32 (265.8 examples/sec; 0.482 sec/batch)
2016-02-04 05:49:20.253832: step 151380, loss = 0.32 (286.7 examples/sec; 0.447 sec/batch)
2016-02-04 05:49:24.939003: step 151390, loss = 0.36 (285.2 examples/sec; 0.449 sec/batch)
2016-02-04 05:49:29.597469: step 151400, loss = 0.31 (298.2 examples/sec; 0.429 sec/batch)
2016-02-04 05:49:34.822033: step 151410, loss = 0.28 (266.9 examples/sec; 0.480 sec/batch)
2016-02-04 05:49:39.537028: step 151420, loss = 0.30 (296.7 examples/sec; 0.431 sec/batch)
2016-02-04 05:49:44.251192: step 151430, loss = 0.35 (253.9 examples/sec; 0.504 sec/batch)
2016-02-04 05:49:48.983387: step 151440, loss = 0.26 (280.3 examples/sec; 0.457 sec/batch)
2016-02-04 05:49:53.788850: step 151450, loss = 0.33 (266.7 examples/sec; 0.480 sec/batch)
2016-02-04 05:49:58.483801: step 151460, loss = 0.28 (267.1 examples/sec; 0.479 sec/batch)
2016-02-04 05:50:03.219045: step 151470, loss = 0.32 (274.8 examples/sec; 0.466 sec/batch)
2016-02-04 05:50:07.869233: step 151480, loss = 0.27 (284.9 examples/sec; 0.449 sec/batch)
2016-02-04 05:50:12.552268: step 151490, loss = 0.36 (272.4 examples/sec; 0.470 sec/batch)
2016-02-04 05:50:17.250647: step 151500, loss = 0.33 (270.1 examples/sec; 0.474 sec/batch)
2016-02-04 05:50:22.472551: step 151510, loss = 0.28 (289.7 examples/sec; 0.442 sec/batch)
2016-02-04 05:50:27.164703: step 151520, loss = 0.26 (282.3 examples/sec; 0.453 sec/batch)
2016-02-04 05:50:31.882351: step 151530, loss = 0.33 (272.3 examples/sec; 0.470 sec/batch)
2016-02-04 05:50:36.619323: step 151540, loss = 0.43 (269.3 examples/sec; 0.475 sec/batch)
2016-02-04 05:50:41.335099: step 151550, loss = 0.24 (270.9 examples/sec; 0.473 sec/batch)
2016-02-04 05:50:46.134583: step 151560, loss = 0.31 (265.1 examples/sec; 0.483 sec/batch)
2016-02-04 05:50:50.819922: step 151570, loss = 0.24 (283.4 examples/sec; 0.452 sec/batch)
2016-02-04 05:50:55.561527: step 151580, loss = 0.43 (263.5 examples/sec; 0.486 sec/batch)
2016-02-04 05:51:00.306718: step 151590, loss = 0.29 (278.2 examples/sec; 0.460 sec/batch)
2016-02-04 05:51:05.061795: step 151600, loss = 0.30 (283.2 examples/sec; 0.452 sec/batch)
2016-02-04 05:51:10.244563: step 151610, loss = 0.37 (248.9 examples/sec; 0.514 sec/batch)
2016-02-04 05:51:14.996424: step 151620, loss = 0.27 (262.3 examples/sec; 0.488 sec/batch)
2016-02-04 05:51:19.716348: step 151630, loss = 0.24 (239.6 examples/sec; 0.534 sec/batch)
2016-02-04 05:51:24.397975: step 151640, loss = 0.31 (272.6 examples/sec; 0.470 sec/batch)
2016-02-04 05:51:29.095777: step 151650, loss = 0.24 (264.2 examples/sec; 0.484 sec/batch)
2016-02-04 05:51:33.801870: step 151660, loss = 0.31 (276.0 examples/sec; 0.464 sec/batch)
2016-02-04 05:51:38.522890: step 151670, loss = 0.35 (265.8 examples/sec; 0.482 sec/batch)
2016-02-04 05:51:43.118565: step 151680, loss = 0.37 (274.8 examples/sec; 0.466 sec/batch)
2016-02-04 05:51:47.773313: step 151690, loss = 0.28 (284.8 examples/sec; 0.449 sec/batch)
2016-02-04 05:51:52.437107: step 151700, loss = 0.37 (283.9 examples/sec; 0.451 sec/batch)
2016-02-04 05:51:57.680014: step 151710, loss = 0.23 (265.9 examples/sec; 0.481 sec/batch)
2016-02-04 05:52:02.329809: step 151720, loss = 0.42 (254.2 examples/sec; 0.504 sec/batch)
2016-02-04 05:52:07.005688: step 151730, loss = 0.36 (272.8 examples/sec; 0.469 sec/batch)
2016-02-04 05:52:11.701359: step 151740, loss = 0.35 (268.0 examples/sec; 0.478 sec/batch)
2016-02-04 05:52:16.423063: step 151750, loss = 0.30 (261.5 examples/sec; 0.489 sec/batch)
2016-02-04 05:52:21.129711: step 151760, loss = 0.26 (274.3 examples/sec; 0.467 sec/batch)
2016-02-04 05:52:25.818362: step 151770, loss = 0.34 (267.9 examples/sec; 0.478 sec/batch)
2016-02-04 05:52:30.528451: step 151780, loss = 0.28 (269.3 examples/sec; 0.475 sec/batch)
2016-02-04 05:52:35.218565: step 151790, loss = 0.27 (257.4 examples/sec; 0.497 sec/batch)
2016-02-04 05:52:39.971016: step 151800, loss = 0.39 (251.5 examples/sec; 0.509 sec/batch)
2016-02-04 05:52:45.102388: step 151810, loss = 0.36 (281.8 examples/sec; 0.454 sec/batch)
2016-02-04 05:52:49.782446: step 151820, loss = 0.34 (265.3 examples/sec; 0.483 sec/batch)
2016-02-04 05:52:54.464840: step 151830, loss = 0.28 (266.0 examples/sec; 0.481 sec/batch)
2016-02-04 05:52:59.131214: step 151840, loss = 0.33 (285.9 examples/sec; 0.448 sec/batch)
2016-02-04 05:53:03.717577: step 151850, loss = 0.33 (293.8 examples/sec; 0.436 sec/batch)
2016-02-04 05:53:08.444634: step 151860, loss = 0.41 (292.2 examples/sec; 0.438 sec/batch)
2016-02-04 05:53:13.188764: step 151870, loss = 0.33 (264.7 examples/sec; 0.484 sec/batch)
2016-02-04 05:53:17.806579: step 151880, loss = 0.35 (276.0 examples/sec; 0.464 sec/batch)
2016-02-04 05:53:22.571230: step 151890, loss = 0.24 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 05:53:27.293863: step 151900, loss = 0.32 (261.9 examples/sec; 0.489 sec/batch)
2016-02-04 05:53:32.461442: step 151910, loss = 0.43 (293.8 examples/sec; 0.436 sec/batch)
2016-02-04 05:53:37.077979: step 151920, loss = 0.31 (294.1 examples/sec; 0.435 sec/batch)
2016-02-04 05:53:41.847317: step 151930, loss = 0.28 (269.6 examples/sec; 0.475 sec/batch)
2016-02-04 05:53:46.517727: step 151940, loss = 0.27 (269.5 examples/sec; 0.475 sec/batch)
2016-02-04 05:53:51.195153: step 151950, loss = 0.26 (284.1 examples/sec; 0.451 sec/batch)
2016-02-04 05:53:55.895766: step 151960, loss = 0.27 (265.4 examples/sec; 0.482 sec/batch)
2016-02-04 05:54:00.528732: step 151970, loss = 0.26 (282.6 examples/sec; 0.453 sec/batch)
2016-02-04 05:54:05.273964: step 151980, loss = 0.26 (262.9 examples/sec; 0.487 sec/batch)
2016-02-04 05:54:09.966046: step 151990, loss = 0.25 (260.6 examples/sec; 0.491 sec/batch)
2016-02-04 05:54:14.475241: step 152000, loss = 0.31 (297.8 examples/sec; 0.430 sec/batch)
2016-02-04 05:54:19.730853: step 152010, loss = 0.30 (264.0 examples/sec; 0.485 sec/batch)
2016-02-04 05:54:24.409859: step 152020, loss = 0.31 (268.7 examples/sec; 0.476 sec/batch)
2016-02-04 05:54:29.061538: step 152030, loss = 0.34 (304.2 examples/sec; 0.421 sec/batch)
2016-02-04 05:54:33.727249: step 152040, loss = 0.27 (275.5 examples/sec; 0.465 sec/batch)
2016-02-04 05:54:38.405298: step 152050, loss = 0.36 (274.4 examples/sec; 0.467 sec/batch)
2016-02-04 05:54:42.994850: step 152060, loss = 0.31 (269.5 examples/sec; 0.475 sec/batch)
2016-02-04 05:54:47.746197: step 152070, loss = 0.25 (258.8 examples/sec; 0.495 sec/batch)
2016-02-04 05:54:52.341535: step 152080, loss = 0.28 (298.6 examples/sec; 0.429 sec/batch)
2016-02-04 05:54:57.055266: step 152090, loss = 0.31 (260.9 examples/sec; 0.491 sec/batch)
2016-02-04 05:55:01.769968: step 152100, loss = 0.36 (267.7 examples/sec; 0.478 sec/batch)
2016-02-04 05:55:06.906015: step 152110, loss = 0.31 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 05:55:11.615323: step 152120, loss = 0.35 (263.3 examples/sec; 0.486 sec/batch)
2016-02-04 05:55:16.305854: step 152130, loss = 0.35 (269.5 examples/sec; 0.475 sec/batch)
2016-02-04 05:55:21.058508: step 152140, loss = 0.27 (255.0 examples/sec; 0.502 sec/batch)
2016-02-04 05:55:25.639072: step 152150, loss = 0.39 (311.2 examples/sec; 0.411 sec/batch)
2016-02-04 05:55:30.290820: step 152160, loss = 0.31 (266.7 examples/sec; 0.480 sec/batch)
2016-02-04 05:55:35.028716: step 152170, loss = 0.25 (251.2 examples/sec; 0.509 sec/batch)
2016-02-04 05:55:39.704644: step 152180, loss = 0.26 (261.6 examples/sec; 0.489 sec/batch)
2016-02-04 05:55:44.375267: step 152190, loss = 0.37 (280.6 examples/sec; 0.456 sec/batch)
2016-02-04 05:55:49.077523: step 152200, loss = 0.28 (244.4 examples/sec; 0.524 sec/batch)
2016-02-04 05:55:54.252723: step 152210, loss = 0.32 (264.3 examples/sec; 0.484 sec/batch)
2016-02-04 05:55:58.907530: step 152220, loss = 0.26 (286.5 examples/sec; 0.447 sec/batch)
2016-02-04 05:56:03.637831: step 152230, loss = 0.30 (273.7 examples/sec; 0.468 sec/batch)
2016-02-04 05:56:08.392011: step 152240, loss = 0.32 (262.1 examples/sec; 0.488 sec/batch)
2016-02-04 05:56:13.222394: step 152250, loss = 0.31 (250.7 examples/sec; 0.510 sec/batch)
2016-02-04 05:56:18.041500: step 152260, loss = 0.34 (263.8 examples/sec; 0.485 sec/batch)
2016-02-04 05:56:22.696632: step 152270, loss = 0.29 (285.2 examples/sec; 0.449 sec/batch)
2016-02-04 05:56:27.453506: step 152280, loss = 0.36 (263.4 examples/sec; 0.486 sec/batch)
2016-02-04 05:56:32.202911: step 152290, loss = 0.34 (244.8 examples/sec; 0.523 sec/batch)
2016-02-04 05:56:36.828264: step 152300, loss = 0.26 (273.2 examples/sec; 0.469 sec/batch)
2016-02-04 05:56:41.992057: step 152310, loss = 0.30 (260.3 examples/sec; 0.492 sec/batch)
2016-02-04 05:56:46.651346: step 152320, loss = 0.28 (278.2 examples/sec; 0.460 sec/batch)
2016-02-04 05:56:51.294907: step 152330, loss = 0.29 (264.6 examples/sec; 0.484 sec/batch)
2016-02-04 05:56:56.012866: step 152340, loss = 0.26 (273.5 examples/sec; 0.468 sec/batch)
2016-02-04 05:57:00.697764: step 152350, loss = 0.29 (271.6 examples/sec; 0.471 sec/batch)
2016-02-04 05:57:05.554611: step 152360, loss = 0.25 (239.2 examples/sec; 0.535 sec/batch)
2016-02-04 05:57:10.275435: step 152370, loss = 0.32 (267.6 examples/sec; 0.478 sec/batch)
2016-02-04 05:57:15.003015: step 152380, loss = 0.21 (273.4 examples/sec; 0.468 sec/batch)
2016-02-04 05:57:19.699272: step 152390, loss = 0.25 (284.4 examples/sec; 0.450 sec/batch)
2016-02-04 05:57:24.366264: step 152400, loss = 0.34 (268.6 examples/sec; 0.477 sec/batch)
2016-02-04 05:57:29.550490: step 152410, loss = 0.34 (247.6 examples/sec; 0.517 sec/batch)
2016-02-04 05:57:34.191556: step 152420, loss = 0.28 (287.6 examples/sec; 0.445 sec/batch)
2016-02-04 05:57:38.818356: step 152430, loss = 0.30 (258.2 examples/sec; 0.496 sec/batch)
2016-02-04 05:57:43.455133: step 152440, loss = 0.29 (300.5 examples/sec; 0.426 sec/batch)
2016-02-04 05:57:48.169217: step 152450, loss = 0.27 (264.9 examples/sec; 0.483 sec/batch)
2016-02-04 05:57:52.913393: step 152460, loss = 0.27 (240.2 examples/sec; 0.533 sec/batch)
2016-02-04 05:57:57.675683: step 152470, loss = 0.23 (260.2 examples/sec; 0.492 sec/batch)
2016-02-04 05:58:02.340645: step 152480, loss = 0.29 (282.6 examples/sec; 0.453 sec/batch)
2016-02-04 05:58:07.035672: step 152490, loss = 0.29 (274.4 examples/sec; 0.467 sec/batch)
2016-02-04 05:58:11.829709: step 152500, loss = 0.28 (246.9 examples/sec; 0.518 sec/batch)
2016-02-04 05:58:17.092771: step 152510, loss = 0.30 (261.0 examples/sec; 0.490 sec/batch)
2016-02-04 05:58:21.789924: step 152520, loss = 0.23 (268.2 examples/sec; 0.477 sec/batch)
2016-02-04 05:58:26.493724: step 152530, loss = 0.36 (289.9 examples/sec; 0.442 sec/batch)
2016-02-04 05:58:31.185091: step 152540, loss = 0.33 (263.4 examples/sec; 0.486 sec/batch)
2016-02-04 05:58:35.947514: step 152550, loss = 0.23 (267.6 examples/sec; 0.478 sec/batch)
2016-02-04 05:58:40.621714: step 152560, loss = 0.30 (290.3 examples/sec; 0.441 sec/batch)
2016-02-04 05:58:45.321759: step 152570, loss = 0.27 (262.8 examples/sec; 0.487 sec/batch)
2016-02-04 05:58:50.018064: step 152580, loss = 0.24 (282.7 examples/sec; 0.453 sec/batch)
2016-02-04 05:58:54.686788: step 152590, loss = 0.32 (315.4 examples/sec; 0.406 sec/batch)
2016-02-04 05:58:59.406113: step 152600, loss = 0.29 (279.7 examples/sec; 0.458 sec/batch)
2016-02-04 05:59:04.634935: step 152610, loss = 0.28 (281.0 examples/sec; 0.456 sec/batch)
2016-02-04 05:59:09.302545: step 152620, loss = 0.28 (272.4 examples/sec; 0.470 sec/batch)
2016-02-04 05:59:14.035714: step 152630, loss = 0.33 (265.4 examples/sec; 0.482 sec/batch)
2016-02-04 05:59:18.702703: step 152640, loss = 0.38 (270.3 examples/sec; 0.473 sec/batch)
2016-02-04 05:59:23.344566: step 152650, loss = 0.27 (293.8 examples/sec; 0.436 sec/batch)
2016-02-04 05:59:28.089138: step 152660, loss = 0.23 (265.4 examples/sec; 0.482 sec/batch)
2016-02-04 05:59:32.760772: step 152670, loss = 0.31 (277.4 examples/sec; 0.461 sec/batch)
2016-02-04 05:59:37.445998: step 152680, loss = 0.32 (290.7 examples/sec; 0.440 sec/batch)
2016-02-04 05:59:42.100507: step 152690, loss = 0.22 (263.9 examples/sec; 0.485 sec/batch)
2016-02-04 05:59:46.865055: step 152700, loss = 0.41 (271.0 examples/sec; 0.472 sec/batch)
2016-02-04 05:59:52.108404: step 152710, loss = 0.34 (265.6 examples/sec; 0.482 sec/batch)
2016-02-04 05:59:56.825874: step 152720, loss = 0.23 (257.0 examples/sec; 0.498 sec/batch)
2016-02-04 06:00:01.553772: step 152730, loss = 0.32 (289.0 examples/sec; 0.443 sec/batch)
2016-02-04 06:00:06.263966: step 152740, loss = 0.32 (305.9 examples/sec; 0.418 sec/batch)
2016-02-04 06:00:10.948319: step 152750, loss = 0.26 (281.3 examples/sec; 0.455 sec/batch)
2016-02-04 06:00:15.712611: step 152760, loss = 0.39 (263.5 examples/sec; 0.486 sec/batch)
2016-02-04 06:00:20.414028: step 152770, loss = 0.27 (289.3 examples/sec; 0.442 sec/batch)
2016-02-04 06:00:25.105068: step 152780, loss = 0.27 (292.0 examples/sec; 0.438 sec/batch)
2016-02-04 06:00:29.884905: step 152790, loss = 0.31 (274.2 examples/sec; 0.467 sec/batch)
2016-02-04 06:00:34.590212: step 152800, loss = 0.32 (277.5 examples/sec; 0.461 sec/batch)
2016-02-04 06:00:39.890956: step 152810, loss = 0.21 (266.5 examples/sec; 0.480 sec/batch)
2016-02-04 06:00:44.607969: step 152820, loss = 0.33 (275.0 examples/sec; 0.465 sec/batch)
2016-02-04 06:00:49.319122: step 152830, loss = 0.27 (283.9 examples/sec; 0.451 sec/batch)
2016-02-04 06:00:54.104401: step 152840, loss = 0.23 (273.2 examples/sec; 0.469 sec/batch)
2016-02-04 06:00:58.858927: step 152850, loss = 0.34 (269.1 examples/sec; 0.476 sec/batch)
2016-02-04 06:01:03.506302: step 152860, loss = 0.34 (288.3 examples/sec; 0.444 sec/batch)
2016-02-04 06:01:08.295789: step 152870, loss = 0.25 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 06:01:12.932932: step 152880, loss = 0.29 (279.9 examples/sec; 0.457 sec/batch)
2016-02-04 06:01:17.726808: step 152890, loss = 0.28 (249.1 examples/sec; 0.514 sec/batch)
2016-02-04 06:01:22.487941: step 152900, loss = 0.31 (267.2 examples/sec; 0.479 sec/batch)
2016-02-04 06:01:27.622562: step 152910, loss = 0.36 (268.2 examples/sec; 0.477 sec/batch)
2016-02-04 06:01:32.387970: step 152920, loss = 0.27 (253.5 examples/sec; 0.505 sec/batch)
2016-02-04 06:01:36.997583: step 152930, loss = 0.33 (275.7 examples/sec; 0.464 sec/batch)
2016-02-04 06:01:41.663133: step 152940, loss = 0.33 (269.8 examples/sec; 0.475 sec/batch)
2016-02-04 06:01:46.348658: step 152950, loss = 0.34 (269.6 examples/sec; 0.475 sec/batch)
2016-02-04 06:01:51.036824: step 152960, loss = 0.38 (283.1 examples/sec; 0.452 sec/batch)
2016-02-04 06:01:55.727951: step 152970, loss = 0.26 (264.9 examples/sec; 0.483 sec/batch)
2016-02-04 06:02:00.422798: step 152980, loss = 0.30 (247.6 examples/sec; 0.517 sec/batch)
2016-02-04 06:02:05.152062: step 152990, loss = 0.26 (252.5 examples/sec; 0.507 sec/batch)
2016-02-04 06:02:09.861757: step 153000, loss = 0.33 (264.0 examples/sec; 0.485 sec/batch)
2016-02-04 06:02:15.065755: step 153010, loss = 0.36 (269.4 examples/sec; 0.475 sec/batch)
2016-02-04 06:02:19.746449: step 153020, loss = 0.35 (311.9 examples/sec; 0.410 sec/batch)
2016-02-04 06:02:24.418372: step 153030, loss = 0.33 (278.7 examples/sec; 0.459 sec/batch)
2016-02-04 06:02:29.178385: step 153040, loss = 0.30 (252.7 examples/sec; 0.507 sec/batch)
2016-02-04 06:02:33.861039: step 153050, loss = 0.27 (275.3 examples/sec; 0.465 sec/batch)
2016-02-04 06:02:38.427179: step 153060, loss = 0.21 (287.7 examples/sec; 0.445 sec/batch)
2016-02-04 06:02:43.143735: step 153070, loss = 0.24 (272.9 examples/sec; 0.469 sec/batch)
2016-02-04 06:02:47.788185: step 153080, loss = 0.30 (276.0 examples/sec; 0.464 sec/batch)
2016-02-04 06:02:52.435615: step 153090, loss = 0.36 (262.7 examples/sec; 0.487 sec/batch)
2016-02-04 06:02:57.165777: step 153100, loss = 0.33 (282.9 examples/sec; 0.452 sec/batch)
2016-02-04 06:03:02.380087: step 153110, loss = 0.33 (272.7 examples/sec; 0.469 sec/batch)
2016-02-04 06:03:07.185058: step 153120, loss = 0.28 (269.6 examples/sec; 0.475 sec/batch)
2016-02-04 06:03:11.887933: step 153130, loss = 0.25 (271.2 examples/sec; 0.472 sec/batch)
2016-02-04 06:03:16.589236: step 153140, loss = 0.37 (291.3 examples/sec; 0.439 sec/batch)
2016-02-04 06:03:21.293947: step 153150, loss = 0.26 (267.3 examples/sec; 0.479 sec/batch)
2016-02-04 06:03:25.997401: step 153160, loss = 0.23 (275.8 examples/sec; 0.464 sec/batch)
2016-02-04 06:03:30.664379: step 153170, loss = 0.37 (266.4 examples/sec; 0.481 sec/batch)
2016-02-04 06:03:35.396673: step 153180, loss = 0.35 (276.3 examples/sec; 0.463 sec/batch)
2016-02-04 06:03:40.149829: step 153190, loss = 0.29 (272.9 examples/sec; 0.469 sec/batch)
2016-02-04 06:03:44.834188: step 153200, loss = 0.33 (261.5 examples/sec; 0.489 sec/batch)
2016-02-04 06:03:50.068334: step 153210, loss = 0.22 (246.9 examples/sec; 0.519 sec/batch)
2016-02-04 06:03:54.720925: step 153220, loss = 0.33 (285.6 examples/sec; 0.448 sec/batch)
2016-02-04 06:03:59.380146: step 153230, loss = 0.34 (294.2 examples/sec; 0.435 sec/batch)
2016-02-04 06:04:04.023777: step 153240, loss = 0.32 (273.8 examples/sec; 0.467 sec/batch)
2016-02-04 06:04:08.643011: step 153250, loss = 0.25 (289.8 examples/sec; 0.442 sec/batch)
2016-02-04 06:04:13.437412: step 153260, loss = 0.42 (298.6 examples/sec; 0.429 sec/batch)
2016-02-04 06:04:18.064391: step 153270, loss = 0.29 (266.1 examples/sec; 0.481 sec/batch)
2016-02-04 06:04:22.704418: step 153280, loss = 0.30 (286.9 examples/sec; 0.446 sec/batch)
2016-02-04 06:04:27.432757: step 153290, loss = 0.27 (279.3 examples/sec; 0.458 sec/batch)
2016-02-04 06:04:32.135053: step 153300, loss = 0.23 (271.5 examples/sec; 0.471 sec/batch)
2016-02-04 06:04:37.353547: step 153310, loss = 0.28 (289.1 examples/sec; 0.443 sec/batch)
2016-02-04 06:04:42.030730: step 153320, loss = 0.33 (273.7 examples/sec; 0.468 sec/batch)
2016-02-04 06:04:46.574900: step 153330, loss = 0.27 (306.9 examples/sec; 0.417 sec/batch)
2016-02-04 06:04:51.284834: step 153340, loss = 0.26 (252.6 examples/sec; 0.507 sec/batch)
2016-02-04 06:04:55.887859: step 153350, loss = 0.24 (267.3 examples/sec; 0.479 sec/batch)
2016-02-04 06:05:00.518641: step 153360, loss = 0.34 (291.1 examples/sec; 0.440 sec/batch)
2016-02-04 06:05:05.166940: step 153370, loss = 0.25 (286.8 examples/sec; 0.446 sec/batch)
2016-02-04 06:05:09.792327: step 153380, loss = 0.32 (287.6 examples/sec; 0.445 sec/batch)
2016-02-04 06:05:14.384874: step 153390, loss = 0.32 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 06:05:18.833935: step 153400, loss = 0.33 (283.0 examples/sec; 0.452 sec/batch)
2016-02-04 06:05:23.991177: step 153410, loss = 0.23 (266.9 examples/sec; 0.480 sec/batch)
2016-02-04 06:05:28.670863: step 153420, loss = 0.31 (281.0 examples/sec; 0.456 sec/batch)
2016-02-04 06:05:33.388003: step 153430, loss = 0.27 (254.3 examples/sec; 0.503 sec/batch)
2016-02-04 06:05:38.118353: step 153440, loss = 0.38 (268.9 examples/sec; 0.476 sec/batch)
2016-02-04 06:05:42.805967: step 153450, loss = 0.25 (311.7 examples/sec; 0.411 sec/batch)
2016-02-04 06:05:47.536828: step 153460, loss = 0.29 (268.3 examples/sec; 0.477 sec/batch)
2016-02-04 06:05:52.321738: step 153470, loss = 0.23 (251.3 examples/sec; 0.509 sec/batch)
2016-02-04 06:05:57.005998: step 153480, loss = 0.33 (269.3 examples/sec; 0.475 sec/batch)
2016-02-04 06:06:01.741224: step 153490, loss = 0.31 (253.5 examples/sec; 0.505 sec/batch)
2016-02-04 06:06:06.470144: step 153500, loss = 0.36 (264.2 examples/sec; 0.484 sec/batch)
2016-02-04 06:06:11.707381: step 153510, loss = 0.27 (265.5 examples/sec; 0.482 sec/batch)
2016-02-04 06:06:16.458279: step 153520, loss = 0.38 (271.3 examples/sec; 0.472 sec/batch)
2016-02-04 06:06:21.240328: step 153530, loss = 0.24 (266.1 examples/sec; 0.481 sec/batch)
2016-02-04 06:06:25.957835: step 153540, loss = 0.31 (263.9 examples/sec; 0.485 sec/batch)
2016-02-04 06:06:30.631136: step 153550, loss = 0.28 (257.0 examples/sec; 0.498 sec/batch)
2016-02-04 06:06:35.376713: step 153560, loss = 0.25 (241.3 examples/sec; 0.531 sec/batch)
2016-02-04 06:06:40.106686: step 153570, loss = 0.42 (267.0 examples/sec; 0.479 sec/batch)
2016-02-04 06:06:44.878503: step 153580, loss = 0.37 (265.8 examples/sec; 0.482 sec/batch)
2016-02-04 06:06:49.536955: step 153590, loss = 0.32 (290.7 examples/sec; 0.440 sec/batch)
2016-02-04 06:06:54.237329: step 153600, loss = 0.30 (272.0 examples/sec; 0.471 sec/batch)
2016-02-04 06:06:59.528670: step 153610, loss = 0.23 (271.2 examples/sec; 0.472 sec/batch)
2016-02-04 06:07:04.212214: step 153620, loss = 0.38 (260.7 examples/sec; 0.491 sec/batch)
2016-02-04 06:07:08.874696: step 153630, loss = 0.28 (255.5 examples/sec; 0.501 sec/batch)
2016-02-04 06:07:13.472685: step 153640, loss = 0.33 (293.1 examples/sec; 0.437 sec/batch)
2016-02-04 06:07:18.120437: step 153650, loss = 0.25 (288.5 examples/sec; 0.444 sec/batch)
2016-02-04 06:07:22.832803: step 153660, loss = 0.26 (244.5 examples/sec; 0.523 sec/batch)
2016-02-04 06:07:27.582595: step 153670, loss = 0.32 (273.2 examples/sec; 0.468 sec/batch)
2016-02-04 06:07:32.366295: step 153680, loss = 0.23 (265.6 examples/sec; 0.482 sec/batch)
2016-02-04 06:07:37.149197: step 153690, loss = 0.33 (253.8 examples/sec; 0.504 sec/batch)
2016-02-04 06:07:41.843645: step 153700, loss = 0.23 (275.3 examples/sec; 0.465 sec/batch)
2016-02-04 06:07:47.062450: step 153710, loss = 0.26 (268.0 examples/sec; 0.478 sec/batch)
2016-02-04 06:07:51.750776: step 153720, loss = 0.29 (293.9 examples/sec; 0.436 sec/batch)
2016-02-04 06:07:56.496322: step 153730, loss = 0.33 (249.4 examples/sec; 0.513 sec/batch)
2016-02-04 06:08:01.157924: step 153740, loss = 0.23 (269.4 examples/sec; 0.475 sec/batch)
2016-02-04 06:08:05.888561: step 153750, loss = 0.25 (273.7 examples/sec; 0.468 sec/batch)
2016-02-04 06:08:10.560434: step 153760, loss = 0.32 (258.6 examples/sec; 0.495 sec/batch)
2016-02-04 06:08:15.299355: step 153770, loss = 0.29 (256.6 examples/sec; 0.499 sec/batch)
2016-02-04 06:08:19.934402: step 153780, loss = 0.36 (310.6 examples/sec; 0.412 sec/batch)
2016-02-04 06:08:24.668728: step 153790, loss = 0.30 (258.3 examples/sec; 0.496 sec/batch)
2016-02-04 06:08:29.315463: step 153800, loss = 0.32 (286.0 examples/sec; 0.448 sec/batch)
2016-02-04 06:08:34.424187: step 153810, loss = 0.30 (278.0 examples/sec; 0.460 sec/batch)
2016-02-04 06:08:39.099036: step 153820, loss = 0.27 (285.3 examples/sec; 0.449 sec/batch)
2016-02-04 06:08:43.882195: step 153830, loss = 0.31 (249.7 examples/sec; 0.513 sec/batch)
2016-02-04 06:08:48.601868: step 153840, loss = 0.30 (271.2 examples/sec; 0.472 sec/batch)
2016-02-04 06:08:53.338686: step 153850, loss = 0.31 (247.9 examples/sec; 0.516 sec/batch)
2016-02-04 06:08:57.980104: step 153860, loss = 0.28 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 06:09:02.708566: step 153870, loss = 0.31 (294.0 examples/sec; 0.435 sec/batch)
2016-02-04 06:09:07.428819: step 153880, loss = 0.30 (274.0 examples/sec; 0.467 sec/batch)
2016-02-04 06:09:12.115736: step 153890, loss = 0.28 (282.0 examples/sec; 0.454 sec/batch)
2016-02-04 06:09:16.748730: step 153900, loss = 0.33 (287.0 examples/sec; 0.446 sec/batch)
2016-02-04 06:09:21.989783: step 153910, loss = 0.33 (270.7 examples/sec; 0.473 sec/batch)
2016-02-04 06:09:26.679930: step 153920, loss = 0.31 (273.2 examples/sec; 0.468 sec/batch)
2016-02-04 06:09:31.364926: step 153930, loss = 0.25 (254.3 examples/sec; 0.503 sec/batch)
2016-02-04 06:09:36.070965: step 153940, loss = 0.27 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 06:09:40.750011: step 153950, loss = 0.20 (246.1 examples/sec; 0.520 sec/batch)
2016-02-04 06:09:45.328161: step 153960, loss = 0.24 (276.2 examples/sec; 0.463 sec/batch)
2016-02-04 06:09:50.064097: step 153970, loss = 0.28 (294.2 examples/sec; 0.435 sec/batch)
2016-02-04 06:09:54.693184: step 153980, loss = 0.36 (268.6 examples/sec; 0.476 sec/batch)
2016-02-04 06:09:59.391571: step 153990, loss = 0.28 (256.3 examples/sec; 0.499 sec/batch)
2016-02-04 06:10:04.073284: step 154000, loss = 0.28 (275.4 examples/sec; 0.465 sec/batch)
2016-02-04 06:10:09.366963: step 154010, loss = 0.22 (280.6 examples/sec; 0.456 sec/batch)
2016-02-04 06:10:14.127761: step 154020, loss = 0.28 (264.1 examples/sec; 0.485 sec/batch)
2016-02-04 06:10:18.838584: step 154030, loss = 0.24 (260.0 examples/sec; 0.492 sec/batch)
2016-02-04 06:10:23.489939: step 154040, loss = 0.33 (290.5 examples/sec; 0.441 sec/batch)
2016-02-04 06:10:28.182245: step 154050, loss = 0.38 (250.8 examples/sec; 0.510 sec/batch)
2016-02-04 06:10:32.807467: step 154060, loss = 0.28 (262.8 examples/sec; 0.487 sec/batch)
2016-02-04 06:10:37.426338: step 154070, loss = 0.37 (278.0 examples/sec; 0.460 sec/batch)
2016-02-04 06:10:42.104347: step 154080, loss = 0.28 (266.9 examples/sec; 0.480 sec/batch)
2016-02-04 06:10:46.861853: step 154090, loss = 0.26 (271.4 examples/sec; 0.472 sec/batch)
2016-02-04 06:10:51.548668: step 154100, loss = 0.30 (261.9 examples/sec; 0.489 sec/batch)
2016-02-04 06:10:56.850765: step 154110, loss = 0.32 (280.5 examples/sec; 0.456 sec/batch)
2016-02-04 06:11:01.634800: step 154120, loss = 0.30 (245.2 examples/sec; 0.522 sec/batch)
2016-02-04 06:11:06.374903: step 154130, loss = 0.35 (291.8 examples/sec; 0.439 sec/batch)
2016-02-04 06:11:11.087968: step 154140, loss = 0.27 (280.6 examples/sec; 0.456 sec/batch)
2016-02-04 06:11:15.864043: step 154150, loss = 0.29 (266.5 examples/sec; 0.480 sec/batch)
2016-02-04 06:11:20.605403: step 154160, loss = 0.34 (282.7 examples/sec; 0.453 sec/batch)
2016-02-04 06:11:25.253947: step 154170, loss = 0.24 (265.1 examples/sec; 0.483 sec/batch)
2016-02-04 06:11:29.962045: step 154180, loss = 0.26 (258.3 examples/sec; 0.495 sec/batch)
2016-02-04 06:11:34.682043: step 154190, loss = 0.27 (262.8 examples/sec; 0.487 sec/batch)
2016-02-04 06:11:39.355809: step 154200, loss = 0.31 (275.4 examples/sec; 0.465 sec/batch)
2016-02-04 06:11:44.549952: step 154210, loss = 0.28 (294.3 examples/sec; 0.435 sec/batch)
2016-02-04 06:11:49.211775: step 154220, loss = 0.32 (267.9 examples/sec; 0.478 sec/batch)
2016-02-04 06:11:53.811086: step 154230, loss = 0.40 (271.3 examples/sec; 0.472 sec/batch)
2016-02-04 06:11:58.425541: step 154240, loss = 0.29 (266.5 examples/sec; 0.480 sec/batch)
2016-02-04 06:12:03.065386: step 154250, loss = 0.30 (273.5 examples/sec; 0.468 sec/batch)
2016-02-04 06:12:07.676922: step 154260, loss = 0.35 (274.5 examples/sec; 0.466 sec/batch)
2016-02-04 06:12:12.321783: step 154270, loss = 0.25 (261.9 examples/sec; 0.489 sec/batch)
2016-02-04 06:12:16.872779: step 154280, loss = 0.24 (290.6 examples/sec; 0.441 sec/batch)
2016-02-04 06:12:21.579968: step 154290, loss = 0.30 (299.9 examples/sec; 0.427 sec/batch)
2016-02-04 06:12:26.186703: step 154300, loss = 0.27 (288.4 examples/sec; 0.444 sec/batch)
2016-02-04 06:12:31.439430: step 154310, loss = 0.32 (248.9 examples/sec; 0.514 sec/batch)
2016-02-04 06:12:36.056330: step 154320, loss = 0.29 (299.2 examples/sec; 0.428 sec/batch)
2016-02-04 06:12:40.768685: step 154330, loss = 0.28 (279.0 examples/sec; 0.459 sec/batch)
2016-02-04 06:12:45.236505: step 154340, loss = 0.32 (307.6 examples/sec; 0.416 sec/batch)
2016-02-04 06:12:49.811242: step 154350, loss = 0.29 (284.0 examples/sec; 0.451 sec/batch)
2016-02-04 06:12:54.405075: step 154360, loss = 0.32 (292.3 examples/sec; 0.438 sec/batch)
2016-02-04 06:12:58.979443: step 154370, loss = 0.28 (281.7 examples/sec; 0.454 sec/batch)
2016-02-04 06:13:03.575591: step 154380, loss = 0.31 (283.1 examples/sec; 0.452 sec/batch)
2016-02-04 06:13:08.099910: step 154390, loss = 0.25 (284.8 examples/sec; 0.449 sec/batch)
2016-02-04 06:13:12.705966: step 154400, loss = 0.35 (275.9 examples/sec; 0.464 sec/batch)
2016-02-04 06:13:17.820804: step 154410, loss = 0.29 (251.5 examples/sec; 0.509 sec/batch)
2016-02-04 06:13:22.378325: step 154420, loss = 0.30 (260.1 examples/sec; 0.492 sec/batch)
2016-02-04 06:13:26.932907: step 154430, loss = 0.37 (268.1 examples/sec; 0.477 sec/batch)
2016-02-04 06:13:31.465098: step 154440, loss = 0.28 (259.6 examples/sec; 0.493 sec/batch)
2016-02-04 06:13:36.127004: step 154450, loss = 0.33 (290.6 examples/sec; 0.441 sec/batch)
2016-02-04 06:13:40.699704: step 154460, loss = 0.29 (276.4 examples/sec; 0.463 sec/batch)
2016-02-04 06:13:45.271920: step 154470, loss = 0.26 (273.8 examples/sec; 0.468 sec/batch)
2016-02-04 06:13:49.861619: step 154480, loss = 0.34 (290.3 examples/sec; 0.441 sec/batch)
2016-02-04 06:13:54.513830: step 154490, loss = 0.30 (253.4 examples/sec; 0.505 sec/batch)
2016-02-04 06:13:59.129934: step 154500, loss = 0.28 (299.6 examples/sec; 0.427 sec/batch)
2016-02-04 06:14:04.193207: step 154510, loss = 0.26 (258.8 examples/sec; 0.495 sec/batch)
2016-02-04 06:14:08.857504: step 154520, loss = 0.33 (256.0 examples/sec; 0.500 sec/batch)
2016-02-04 06:14:13.460315: step 154530, loss = 0.37 (274.5 examples/sec; 0.466 sec/batch)
2016-02-04 06:14:17.990486: step 154540, loss = 0.34 (274.9 examples/sec; 0.466 sec/batch)
2016-02-04 06:14:22.684058: step 154550, loss = 0.26 (286.2 examples/sec; 0.447 sec/batch)
2016-02-04 06:14:27.210365: step 154560, loss = 0.35 (279.1 examples/sec; 0.459 sec/batch)
2016-02-04 06:14:31.792598: step 154570, loss = 0.29 (296.4 examples/sec; 0.432 sec/batch)
2016-02-04 06:14:36.336796: step 154580, loss = 0.25 (269.2 examples/sec; 0.475 sec/batch)
2016-02-04 06:14:40.958565: step 154590, loss = 0.27 (273.9 examples/sec; 0.467 sec/batch)
2016-02-04 06:14:45.599819: step 154600, loss = 0.24 (283.9 examples/sec; 0.451 sec/batch)
2016-02-04 06:14:50.817238: step 154610, loss = 0.27 (283.2 examples/sec; 0.452 sec/batch)
2016-02-04 06:14:55.355019: step 154620, loss = 0.30 (259.5 examples/sec; 0.493 sec/batch)
2016-02-04 06:15:00.051160: step 154630, loss = 0.32 (299.4 examples/sec; 0.428 sec/batch)
2016-02-04 06:15:04.634211: step 154640, loss = 0.30 (263.9 examples/sec; 0.485 sec/batch)
2016-02-04 06:15:09.118885: step 154650, loss = 0.23 (262.8 examples/sec; 0.487 sec/batch)
2016-02-04 06:15:13.788805: step 154660, loss = 0.27 (263.0 examples/sec; 0.487 sec/batch)
2016-02-04 06:15:18.483645: step 154670, loss = 0.25 (270.8 examples/sec; 0.473 sec/batch)
2016-02-04 06:15:23.099733: step 154680, loss = 0.26 (287.9 examples/sec; 0.445 sec/batch)
2016-02-04 06:15:27.798754: step 154690, loss = 0.30 (273.4 examples/sec; 0.468 sec/batch)
2016-02-04 06:15:32.517577: step 154700, loss = 0.31 (252.9 examples/sec; 0.506 sec/batch)
2016-02-04 06:15:37.672366: step 154710, loss = 0.33 (265.0 examples/sec; 0.483 sec/batch)
2016-02-04 06:15:42.233888: step 154720, loss = 0.24 (284.7 examples/sec; 0.450 sec/batch)
2016-02-04 06:15:46.730169: step 154730, loss = 0.25 (303.0 examples/sec; 0.422 sec/batch)
2016-02-04 06:15:51.404675: step 154740, loss = 0.28 (256.5 examples/sec; 0.499 sec/batch)
2016-02-04 06:15:56.136920: step 154750, loss = 0.29 (279.6 examples/sec; 0.458 sec/batch)
2016-02-04 06:16:00.796662: step 154760, loss = 0.32 (304.5 examples/sec; 0.420 sec/batch)
2016-02-04 06:16:05.459869: step 154770, loss = 0.24 (285.6 examples/sec; 0.448 sec/batch)
2016-02-04 06:16:10.123303: step 154780, loss = 0.40 (269.6 examples/sec; 0.475 sec/batch)
2016-02-04 06:16:14.707415: step 154790, loss = 0.29 (275.2 examples/sec; 0.465 sec/batch)
2016-02-04 06:16:19.283452: step 154800, loss = 0.28 (290.0 examples/sec; 0.441 sec/batch)
2016-02-04 06:16:24.440019: step 154810, loss = 0.33 (264.1 examples/sec; 0.485 sec/batch)
2016-02-04 06:16:29.159938: step 154820, loss = 0.28 (265.6 examples/sec; 0.482 sec/batch)
2016-02-04 06:16:33.802339: step 154830, loss = 0.36 (275.8 examples/sec; 0.464 sec/batch)
2016-02-04 06:16:38.443869: step 154840, loss = 0.30 (280.6 examples/sec; 0.456 sec/batch)
2016-02-04 06:16:42.999650: step 154850, loss = 0.24 (264.8 examples/sec; 0.483 sec/batch)
2016-02-04 06:16:47.536480: step 154860, loss = 0.29 (279.1 examples/sec; 0.459 sec/batch)
2016-02-04 06:16:52.130244: step 154870, loss = 0.33 (248.1 examples/sec; 0.516 sec/batch)
2016-02-04 06:16:56.816170: step 154880, loss = 0.39 (283.7 examples/sec; 0.451 sec/batch)
2016-02-04 06:17:01.365210: step 154890, loss = 0.35 (288.7 examples/sec; 0.443 sec/batch)
2016-02-04 06:17:05.983720: step 154900, loss = 0.28 (293.9 examples/sec; 0.436 sec/batch)
2016-02-04 06:17:11.129076: step 154910, loss = 0.34 (255.0 examples/sec; 0.502 sec/batch)
2016-02-04 06:17:15.835401: step 154920, loss = 0.31 (268.5 examples/sec; 0.477 sec/batch)
2016-02-04 06:17:20.480930: step 154930, loss = 0.26 (294.6 examples/sec; 0.434 sec/batch)
2016-02-04 06:17:25.181026: step 154940, loss = 0.30 (256.3 examples/sec; 0.499 sec/batch)
2016-02-04 06:17:29.923347: step 154950, loss = 0.29 (249.1 examples/sec; 0.514 sec/batch)
2016-02-04 06:17:34.697206: step 154960, loss = 0.30 (269.0 examples/sec; 0.476 sec/batch)
2016-02-04 06:17:39.393777: step 154970, loss = 0.36 (277.4 examples/sec; 0.461 sec/batch)
2016-02-04 06:17:44.188273: step 154980, loss = 0.29 (271.7 examples/sec; 0.471 sec/batch)
2016-02-04 06:17:48.842481: step 154990, loss = 0.27 (287.2 examples/sec; 0.446 sec/batch)
2016-02-04 06:17:53.568477: step 155000, loss = 0.32 (280.0 examples/sec; 0.457 sec/batch)
2016-02-04 06:17:58.754745: step 155010, loss = 0.28 (297.1 examples/sec; 0.431 sec/batch)
2016-02-04 06:18:03.496384: step 155020, loss = 0.28 (249.4 examples/sec; 0.513 sec/batch)
2016-02-04 06:18:08.201152: step 155030, loss = 0.24 (282.0 examples/sec; 0.454 sec/batch)
2016-02-04 06:18:12.863775: step 155040, loss = 0.29 (273.6 examples/sec; 0.468 sec/batch)
2016-02-04 06:18:17.521709: step 155050, loss = 0.21 (259.9 examples/sec; 0.492 sec/batch)
2016-02-04 06:18:22.171776: step 155060, loss = 0.34 (265.6 examples/sec; 0.482 sec/batch)
2016-02-04 06:18:26.869867: step 155070, loss = 0.36 (296.1 examples/sec; 0.432 sec/batch)
2016-02-04 06:18:31.568162: step 155080, loss = 0.28 (252.4 examples/sec; 0.507 sec/batch)
2016-02-04 06:18:36.277946: step 155090, loss = 0.27 (257.2 examples/sec; 0.498 sec/batch)
2016-02-04 06:18:41.041817: step 155100, loss = 0.24 (283.6 examples/sec; 0.451 sec/batch)
2016-02-04 06:18:46.304236: step 155110, loss = 0.33 (257.0 examples/sec; 0.498 sec/batch)
2016-02-04 06:18:51.060413: step 155120, loss = 0.25 (259.4 examples/sec; 0.493 sec/batch)
2016-02-04 06:18:55.792659: step 155130, loss = 0.26 (281.7 examples/sec; 0.454 sec/batch)
2016-02-04 06:19:00.452530: step 155140, loss = 0.30 (267.6 examples/sec; 0.478 sec/batch)
2016-02-04 06:19:05.228733: step 155150, loss = 0.26 (254.1 examples/sec; 0.504 sec/batch)
2016-02-04 06:19:09.929515: step 155160, loss = 0.22 (267.7 examples/sec; 0.478 sec/batch)
2016-02-04 06:19:14.620623: step 155170, loss = 0.34 (291.0 examples/sec; 0.440 sec/batch)
2016-02-04 06:19:19.411173: step 155180, loss = 0.28 (274.9 examples/sec; 0.466 sec/batch)
2016-02-04 06:19:24.138605: step 155190, loss = 0.28 (254.6 examples/sec; 0.503 sec/batch)
2016-02-04 06:19:28.770751: step 155200, loss = 0.30 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 06:19:33.965845: step 155210, loss = 0.26 (276.8 examples/sec; 0.462 sec/batch)
2016-02-04 06:19:38.700145: step 155220, loss = 0.32 (273.9 examples/sec; 0.467 sec/batch)
2016-02-04 06:19:43.427338: step 155230, loss = 0.33 (273.5 examples/sec; 0.468 sec/batch)
2016-02-04 06:19:48.172729: step 155240, loss = 0.31 (276.8 examples/sec; 0.462 sec/batch)
2016-02-04 06:19:52.923485: step 155250, loss = 0.27 (257.2 examples/sec; 0.498 sec/batch)
2016-02-04 06:19:57.709786: step 155260, loss = 0.28 (271.3 examples/sec; 0.472 sec/batch)
2016-02-04 06:20:02.432392: step 155270, loss = 0.34 (262.5 examples/sec; 0.488 sec/batch)
2016-02-04 06:20:07.067331: step 155280, loss = 0.25 (247.7 examples/sec; 0.517 sec/batch)
2016-02-04 06:20:11.711442: step 155290, loss = 0.31 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 06:20:16.480411: step 155300, loss = 0.26 (252.8 examples/sec; 0.506 sec/batch)
2016-02-04 06:20:21.708674: step 155310, loss = 0.39 (251.9 examples/sec; 0.508 sec/batch)
2016-02-04 06:20:26.485147: step 155320, loss = 0.24 (260.6 examples/sec; 0.491 sec/batch)
2016-02-04 06:20:31.120219: step 155330, loss = 0.24 (284.5 examples/sec; 0.450 sec/batch)
2016-02-04 06:20:35.848803: step 155340, loss = 0.30 (266.4 examples/sec; 0.481 sec/batch)
2016-02-04 06:20:40.580461: step 155350, loss = 0.31 (258.2 examples/sec; 0.496 sec/batch)
2016-02-04 06:20:45.329377: step 155360, loss = 0.27 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 06:20:50.084812: step 155370, loss = 0.25 (276.8 examples/sec; 0.462 sec/batch)
2016-02-04 06:20:54.762784: step 155380, loss = 0.23 (275.2 examples/sec; 0.465 sec/batch)
2016-02-04 06:20:59.484988: step 155390, loss = 0.36 (262.6 examples/sec; 0.487 sec/batch)
2016-02-04 06:21:04.211702: step 155400, loss = 0.34 (269.3 examples/sec; 0.475 sec/batch)
2016-02-04 06:21:09.437694: step 155410, loss = 0.20 (276.1 examples/sec; 0.464 sec/batch)
2016-02-04 06:21:14.142034: step 155420, loss = 0.27 (264.7 examples/sec; 0.484 sec/batch)
2016-02-04 06:21:18.818496: step 155430, loss = 0.26 (271.5 examples/sec; 0.471 sec/batch)
2016-02-04 06:21:23.534056: step 155440, loss = 0.33 (274.8 examples/sec; 0.466 sec/batch)
2016-02-04 06:21:28.123405: step 155450, loss = 0.28 (287.4 examples/sec; 0.445 sec/batch)
2016-02-04 06:21:32.903014: step 155460, loss = 0.34 (260.4 examples/sec; 0.492 sec/batch)
2016-02-04 06:21:37.656339: step 155470, loss = 0.45 (270.7 examples/sec; 0.473 sec/batch)
2016-02-04 06:21:42.397769: step 155480, loss = 0.34 (272.3 examples/sec; 0.470 sec/batch)
2016-02-04 06:21:47.038936: step 155490, loss = 0.28 (275.3 examples/sec; 0.465 sec/batch)
2016-02-04 06:21:51.814012: step 155500, loss = 0.31 (272.4 examples/sec; 0.470 sec/batch)
2016-02-04 06:21:57.045340: step 155510, loss = 0.32 (273.1 examples/sec; 0.469 sec/batch)
2016-02-04 06:22:01.742691: step 155520, loss = 0.27 (295.2 examples/sec; 0.434 sec/batch)
2016-02-04 06:22:06.479384: step 155530, loss = 0.30 (265.6 examples/sec; 0.482 sec/batch)
2016-02-04 06:22:11.139100: step 155540, loss = 0.26 (261.5 examples/sec; 0.489 sec/batch)
2016-02-04 06:22:15.845147: step 155550, loss = 0.32 (257.8 examples/sec; 0.496 sec/batch)
2016-02-04 06:22:20.589242: step 155560, loss = 0.28 (268.1 examples/sec; 0.478 sec/batch)
2016-02-04 06:22:25.327936: step 155570, loss = 0.32 (271.8 examples/sec; 0.471 sec/batch)
2016-02-04 06:22:30.053656: step 155580, loss = 0.30 (286.0 examples/sec; 0.447 sec/batch)
2016-02-04 06:22:34.869281: step 155590, loss = 0.31 (278.0 examples/sec; 0.460 sec/batch)
2016-02-04 06:22:39.611476: step 155600, loss = 0.26 (256.3 examples/sec; 0.499 sec/batch)
2016-02-04 06:22:44.919709: step 155610, loss = 0.23 (261.6 examples/sec; 0.489 sec/batch)
2016-02-04 06:22:49.699097: step 155620, loss = 0.29 (269.4 examples/sec; 0.475 sec/batch)
2016-02-04 06:22:54.383841: step 155630, loss = 0.36 (281.1 examples/sec; 0.455 sec/batch)
2016-02-04 06:22:59.124297: step 155640, loss = 0.29 (272.3 examples/sec; 0.470 sec/batch)
2016-02-04 06:23:03.846490: step 155650, loss = 0.29 (263.3 examples/sec; 0.486 sec/batch)
2016-02-04 06:23:08.523619: step 155660, loss = 0.34 (267.2 examples/sec; 0.479 sec/batch)
2016-02-04 06:23:13.294815: step 155670, loss = 0.27 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 06:23:18.045005: step 155680, loss = 0.38 (255.6 examples/sec; 0.501 sec/batch)
2016-02-04 06:23:22.692480: step 155690, loss = 0.33 (300.5 examples/sec; 0.426 sec/batch)
2016-02-04 06:23:27.404381: step 155700, loss = 0.25 (273.3 examples/sec; 0.468 sec/batch)
2016-02-04 06:23:32.587123: step 155710, loss = 0.22 (258.5 examples/sec; 0.495 sec/batch)
2016-02-04 06:23:37.291599: step 155720, loss = 0.32 (260.1 examples/sec; 0.492 sec/batch)
2016-02-04 06:23:42.005462: step 155730, loss = 0.30 (284.6 examples/sec; 0.450 sec/batch)
2016-02-04 06:23:46.702825: step 155740, loss = 0.27 (280.4 examples/sec; 0.456 sec/batch)
2016-02-04 06:23:51.439995: step 155750, loss = 0.30 (270.7 examples/sec; 0.473 sec/batch)
2016-02-04 06:23:56.216570: step 155760, loss = 0.22 (262.1 examples/sec; 0.488 sec/batch)
2016-02-04 06:24:00.911804: step 155770, loss = 0.30 (269.1 examples/sec; 0.476 sec/batch)
2016-02-04 06:24:05.616397: step 155780, loss = 0.28 (275.6 examples/sec; 0.464 sec/batch)
2016-02-04 06:24:10.257842: step 155790, loss = 0.32 (288.6 examples/sec; 0.444 sec/batch)
2016-02-04 06:24:14.974130: step 155800, loss = 0.25 (279.4 examples/sec; 0.458 sec/batch)
2016-02-04 06:24:20.167560: step 155810, loss = 0.29 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 06:24:24.841521: step 155820, loss = 0.27 (271.6 examples/sec; 0.471 sec/batch)
2016-02-04 06:24:29.463685: step 155830, loss = 0.36 (306.7 examples/sec; 0.417 sec/batch)
2016-02-04 06:24:34.204372: step 155840, loss = 0.31 (278.0 examples/sec; 0.460 sec/batch)
2016-02-04 06:24:38.909307: step 155850, loss = 0.27 (265.9 examples/sec; 0.481 sec/batch)
2016-02-04 06:24:43.582045: step 155860, loss = 0.25 (288.8 examples/sec; 0.443 sec/batch)
2016-02-04 06:24:48.306339: step 155870, loss = 0.31 (288.2 examples/sec; 0.444 sec/batch)
2016-02-04 06:24:53.045315: step 155880, loss = 0.28 (274.0 examples/sec; 0.467 sec/batch)
2016-02-04 06:24:57.750887: step 155890, loss = 0.32 (259.3 examples/sec; 0.494 sec/batch)
2016-02-04 06:25:02.462199: step 155900, loss = 0.28 (298.6 examples/sec; 0.429 sec/batch)
2016-02-04 06:25:07.751892: step 155910, loss = 0.26 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 06:25:12.518421: step 155920, loss = 0.35 (287.5 examples/sec; 0.445 sec/batch)
2016-02-04 06:25:17.275505: step 155930, loss = 0.40 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 06:25:21.962159: step 155940, loss = 0.24 (257.6 examples/sec; 0.497 sec/batch)
2016-02-04 06:25:26.646711: step 155950, loss = 0.33 (285.0 examples/sec; 0.449 sec/batch)
2016-02-04 06:25:31.353852: step 155960, loss = 0.37 (272.0 examples/sec; 0.471 sec/batch)
2016-02-04 06:25:36.055393: step 155970, loss = 0.30 (299.8 examples/sec; 0.427 sec/batch)
2016-02-04 06:25:40.752959: step 155980, loss = 0.41 (259.2 examples/sec; 0.494 sec/batch)
2016-02-04 06:25:45.425579: step 155990, loss = 0.34 (290.7 examples/sec; 0.440 sec/batch)
2016-02-04 06:25:50.124865: step 156000, loss = 0.29 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 06:25:55.368302: step 156010, loss = 0.36 (265.9 examples/sec; 0.481 sec/batch)
2016-02-04 06:26:00.017740: step 156020, loss = 0.24 (294.0 examples/sec; 0.435 sec/batch)
2016-02-04 06:26:04.750387: step 156030, loss = 0.33 (282.8 examples/sec; 0.453 sec/batch)
2016-02-04 06:26:09.487890: step 156040, loss = 0.40 (260.1 examples/sec; 0.492 sec/batch)
2016-02-04 06:26:14.261060: step 156050, loss = 0.24 (261.7 examples/sec; 0.489 sec/batch)
2016-02-04 06:26:18.887008: step 156060, loss = 0.27 (292.6 examples/sec; 0.437 sec/batch)
2016-02-04 06:26:23.572926: step 156070, loss = 0.27 (265.9 examples/sec; 0.481 sec/batch)
2016-02-04 06:26:28.350061: step 156080, loss = 0.23 (257.9 examples/sec; 0.496 sec/batch)
2016-02-04 06:26:33.123301: step 156090, loss = 0.29 (249.8 examples/sec; 0.512 sec/batch)
2016-02-04 06:26:37.750637: step 156100, loss = 0.33 (276.1 examples/sec; 0.464 sec/batch)
2016-02-04 06:26:42.927891: step 156110, loss = 0.23 (309.8 examples/sec; 0.413 sec/batch)
2016-02-04 06:26:47.626416: step 156120, loss = 0.34 (257.9 examples/sec; 0.496 sec/batch)
2016-02-04 06:26:52.424789: step 156130, loss = 0.30 (258.4 examples/sec; 0.495 sec/batch)
2016-02-04 06:26:57.150846: step 156140, loss = 0.26 (282.7 examples/sec; 0.453 sec/batch)
2016-02-04 06:27:01.926403: step 156150, loss = 0.28 (258.5 examples/sec; 0.495 sec/batch)
2016-02-04 06:27:06.662871: step 156160, loss = 0.25 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 06:27:11.335407: step 156170, loss = 0.32 (276.2 examples/sec; 0.463 sec/batch)
2016-02-04 06:27:16.004091: step 156180, loss = 0.26 (271.0 examples/sec; 0.472 sec/batch)
2016-02-04 06:27:20.739033: step 156190, loss = 0.27 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 06:27:25.430566: step 156200, loss = 0.29 (288.8 examples/sec; 0.443 sec/batch)
2016-02-04 06:27:30.665354: step 156210, loss = 0.32 (290.6 examples/sec; 0.440 sec/batch)
2016-02-04 06:27:35.383952: step 156220, loss = 0.25 (275.3 examples/sec; 0.465 sec/batch)
2016-02-04 06:27:40.127550: step 156230, loss = 0.26 (270.9 examples/sec; 0.473 sec/batch)
2016-02-04 06:27:44.880571: step 156240, loss = 0.33 (297.3 examples/sec; 0.431 sec/batch)
2016-02-04 06:27:49.557803: step 156250, loss = 0.30 (275.4 examples/sec; 0.465 sec/batch)
2016-02-04 06:27:54.290800: step 156260, loss = 0.26 (259.3 examples/sec; 0.494 sec/batch)
2016-02-04 06:27:58.959945: step 156270, loss = 0.25 (271.5 examples/sec; 0.471 sec/batch)
2016-02-04 06:28:03.683173: step 156280, loss = 0.24 (276.6 examples/sec; 0.463 sec/batch)
2016-02-04 06:28:08.418901: step 156290, loss = 0.29 (257.1 examples/sec; 0.498 sec/batch)
2016-02-04 06:28:13.107901: step 156300, loss = 0.26 (283.9 examples/sec; 0.451 sec/batch)
2016-02-04 06:28:18.411066: step 156310, loss = 0.27 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 06:28:23.116657: step 156320, loss = 0.38 (250.4 examples/sec; 0.511 sec/batch)
2016-02-04 06:28:27.784228: step 156330, loss = 0.26 (247.3 examples/sec; 0.517 sec/batch)
2016-02-04 06:28:32.521020: step 156340, loss = 0.34 (261.1 examples/sec; 0.490 sec/batch)
2016-02-04 06:28:37.087744: step 156350, loss = 0.31 (281.8 examples/sec; 0.454 sec/batch)
2016-02-04 06:28:41.832362: step 156360, loss = 0.30 (272.6 examples/sec; 0.469 sec/batch)
2016-02-04 06:28:46.394699: step 156370, loss = 0.33 (302.6 examples/sec; 0.423 sec/batch)
2016-02-04 06:28:51.007361: step 156380, loss = 0.34 (267.4 examples/sec; 0.479 sec/batch)
2016-02-04 06:28:55.564512: step 156390, loss = 0.28 (267.0 examples/sec; 0.479 sec/batch)
2016-02-04 06:29:00.205231: step 156400, loss = 0.28 (254.4 examples/sec; 0.503 sec/batch)
2016-02-04 06:29:05.443704: step 156410, loss = 0.30 (276.5 examples/sec; 0.463 sec/batch)
2016-02-04 06:29:10.153787: step 156420, loss = 0.27 (268.0 examples/sec; 0.478 sec/batch)
2016-02-04 06:29:14.830711: step 156430, loss = 0.25 (271.8 examples/sec; 0.471 sec/batch)
2016-02-04 06:29:19.513661: step 156440, loss = 0.39 (275.5 examples/sec; 0.465 sec/batch)
2016-02-04 06:29:24.233273: step 156450, loss = 0.32 (267.1 examples/sec; 0.479 sec/batch)
2016-02-04 06:29:28.859969: step 156460, loss = 0.25 (251.2 examples/sec; 0.509 sec/batch)
2016-02-04 06:29:33.596065: step 156470, loss = 0.31 (252.9 examples/sec; 0.506 sec/batch)
2016-02-04 06:29:38.349845: step 156480, loss = 0.26 (280.8 examples/sec; 0.456 sec/batch)
2016-02-04 06:29:43.046100: step 156490, loss = 0.30 (263.8 examples/sec; 0.485 sec/batch)
2016-02-04 06:29:47.710144: step 156500, loss = 0.31 (281.4 examples/sec; 0.455 sec/batch)
2016-02-04 06:29:52.896228: step 156510, loss = 0.41 (300.9 examples/sec; 0.425 sec/batch)
2016-02-04 06:29:57.681705: step 156520, loss = 0.34 (244.1 examples/sec; 0.524 sec/batch)
2016-02-04 06:30:02.316656: step 156530, loss = 0.34 (307.5 examples/sec; 0.416 sec/batch)
2016-02-04 06:30:06.951608: step 156540, loss = 0.29 (259.9 examples/sec; 0.492 sec/batch)
2016-02-04 06:30:11.558599: step 156550, loss = 0.21 (299.2 examples/sec; 0.428 sec/batch)
2016-02-04 06:30:16.290479: step 156560, loss = 0.24 (271.0 examples/sec; 0.472 sec/batch)
2016-02-04 06:30:21.002683: step 156570, loss = 0.20 (279.1 examples/sec; 0.459 sec/batch)
2016-02-04 06:30:25.770867: step 156580, loss = 0.25 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 06:30:30.457224: step 156590, loss = 0.32 (285.5 examples/sec; 0.448 sec/batch)
2016-02-04 06:30:35.134599: step 156600, loss = 0.26 (290.8 examples/sec; 0.440 sec/batch)
2016-02-04 06:30:40.347782: step 156610, loss = 0.30 (281.8 examples/sec; 0.454 sec/batch)
2016-02-04 06:30:45.020578: step 156620, loss = 0.34 (265.5 examples/sec; 0.482 sec/batch)
2016-02-04 06:30:49.790315: step 156630, loss = 0.28 (258.1 examples/sec; 0.496 sec/batch)
2016-02-04 06:30:54.521371: step 156640, loss = 0.35 (248.9 examples/sec; 0.514 sec/batch)
2016-02-04 06:30:59.129587: step 156650, loss = 0.32 (288.2 examples/sec; 0.444 sec/batch)
2016-02-04 06:31:03.782802: step 156660, loss = 0.31 (257.1 examples/sec; 0.498 sec/batch)
2016-02-04 06:31:08.424009: step 156670, loss = 0.25 (266.9 examples/sec; 0.480 sec/batch)
2016-02-04 06:31:13.115993: step 156680, loss = 0.32 (275.6 examples/sec; 0.465 sec/batch)
2016-02-04 06:31:17.877780: step 156690, loss = 0.28 (276.5 examples/sec; 0.463 sec/batch)
2016-02-04 06:31:22.566471: step 156700, loss = 0.27 (285.2 examples/sec; 0.449 sec/batch)
2016-02-04 06:31:27.790574: step 156710, loss = 0.25 (293.6 examples/sec; 0.436 sec/batch)
2016-02-04 06:31:32.540952: step 156720, loss = 0.25 (281.2 examples/sec; 0.455 sec/batch)
2016-02-04 06:31:37.295503: step 156730, loss = 0.24 (266.1 examples/sec; 0.481 sec/batch)
2016-02-04 06:31:42.038625: step 156740, loss = 0.36 (286.0 examples/sec; 0.448 sec/batch)
2016-02-04 06:31:46.705249: step 156750, loss = 0.23 (287.9 examples/sec; 0.445 sec/batch)
2016-02-04 06:31:51.477261: step 156760, loss = 0.31 (267.9 examples/sec; 0.478 sec/batch)
2016-02-04 06:31:56.075924: step 156770, loss = 0.26 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 06:32:00.775160: step 156780, loss = 0.28 (258.7 examples/sec; 0.495 sec/batch)
2016-02-04 06:32:05.495774: step 156790, loss = 0.26 (289.0 examples/sec; 0.443 sec/batch)
2016-02-04 06:32:10.275784: step 156800, loss = 0.28 (260.7 examples/sec; 0.491 sec/batch)
2016-02-04 06:32:15.550406: step 156810, loss = 0.33 (261.7 examples/sec; 0.489 sec/batch)
2016-02-04 06:32:20.257154: step 156820, loss = 0.21 (264.7 examples/sec; 0.484 sec/batch)
2016-02-04 06:32:24.942312: step 156830, loss = 0.36 (292.9 examples/sec; 0.437 sec/batch)
2016-02-04 06:32:29.677366: step 156840, loss = 0.27 (262.4 examples/sec; 0.488 sec/batch)
2016-02-04 06:32:34.403674: step 156850, loss = 0.26 (285.0 examples/sec; 0.449 sec/batch)
2016-02-04 06:32:39.177271: step 156860, loss = 0.27 (244.2 examples/sec; 0.524 sec/batch)
2016-02-04 06:32:43.878718: step 156870, loss = 0.24 (255.6 examples/sec; 0.501 sec/batch)
2016-02-04 06:32:48.654042: step 156880, loss = 0.30 (262.8 examples/sec; 0.487 sec/batch)
2016-02-04 06:32:53.303397: step 156890, loss = 0.29 (285.8 examples/sec; 0.448 sec/batch)
2016-02-04 06:32:58.112034: step 156900, loss = 0.29 (288.1 examples/sec; 0.444 sec/batch)
2016-02-04 06:33:03.335451: step 156910, loss = 0.35 (280.0 examples/sec; 0.457 sec/batch)
2016-02-04 06:33:08.059883: step 156920, loss = 0.29 (259.6 examples/sec; 0.493 sec/batch)
2016-02-04 06:33:12.824840: step 156930, loss = 0.25 (272.1 examples/sec; 0.470 sec/batch)
2016-02-04 06:33:17.564868: step 156940, loss = 0.31 (266.5 examples/sec; 0.480 sec/batch)
2016-02-04 06:33:22.229228: step 156950, loss = 0.28 (277.9 examples/sec; 0.461 sec/batch)
2016-02-04 06:33:26.917759: step 156960, loss = 0.28 (287.3 examples/sec; 0.446 sec/batch)
2016-02-04 06:33:31.640149: step 156970, loss = 0.31 (286.2 examples/sec; 0.447 sec/batch)
2016-02-04 06:33:36.387636: step 156980, loss = 0.23 (279.3 examples/sec; 0.458 sec/batch)
2016-02-04 06:33:41.119638: step 156990, loss = 0.27 (273.6 examples/sec; 0.468 sec/batch)
2016-02-04 06:33:45.874293: step 157000, loss = 0.23 (281.0 examples/sec; 0.455 sec/batch)
2016-02-04 06:33:51.117551: step 157010, loss = 0.33 (273.6 examples/sec; 0.468 sec/batch)
2016-02-04 06:33:55.829460: step 157020, loss = 0.29 (279.3 examples/sec; 0.458 sec/batch)
2016-02-04 06:34:00.568836: step 157030, loss = 0.31 (282.2 examples/sec; 0.454 sec/batch)
2016-02-04 06:34:05.250504: step 157040, loss = 0.38 (258.1 examples/sec; 0.496 sec/batch)
2016-02-04 06:34:09.965678: step 157050, loss = 0.28 (244.7 examples/sec; 0.523 sec/batch)
2016-02-04 06:34:14.691884: step 157060, loss = 0.30 (283.9 examples/sec; 0.451 sec/batch)
2016-02-04 06:34:19.396648: step 157070, loss = 0.30 (262.4 examples/sec; 0.488 sec/batch)
2016-02-04 06:34:24.065844: step 157080, loss = 0.32 (290.7 examples/sec; 0.440 sec/batch)
2016-02-04 06:34:28.778287: step 157090, loss = 0.25 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 06:34:33.490862: step 157100, loss = 0.28 (270.9 examples/sec; 0.473 sec/batch)
2016-02-04 06:34:38.633885: step 157110, loss = 0.27 (277.0 examples/sec; 0.462 sec/batch)
2016-02-04 06:34:43.412864: step 157120, loss = 0.32 (275.3 examples/sec; 0.465 sec/batch)
2016-02-04 06:34:48.110472: step 157130, loss = 0.27 (266.9 examples/sec; 0.480 sec/batch)
2016-02-04 06:34:52.843963: step 157140, loss = 0.28 (272.7 examples/sec; 0.469 sec/batch)
2016-02-04 06:34:57.572543: step 157150, loss = 0.25 (278.8 examples/sec; 0.459 sec/batch)
2016-02-04 06:35:02.210948: step 157160, loss = 0.22 (283.8 examples/sec; 0.451 sec/batch)
2016-02-04 06:35:07.026799: step 157170, loss = 0.27 (263.5 examples/sec; 0.486 sec/batch)
2016-02-04 06:35:11.648774: step 157180, loss = 0.32 (272.6 examples/sec; 0.470 sec/batch)
2016-02-04 06:35:16.384002: step 157190, loss = 0.31 (269.1 examples/sec; 0.476 sec/batch)
2016-02-04 06:35:21.162640: step 157200, loss = 0.36 (274.8 examples/sec; 0.466 sec/batch)
2016-02-04 06:35:26.419482: step 157210, loss = 0.26 (279.5 examples/sec; 0.458 sec/batch)
2016-02-04 06:35:31.063303: step 157220, loss = 0.29 (270.0 examples/sec; 0.474 sec/batch)
2016-02-04 06:35:35.784922: step 157230, loss = 0.28 (285.6 examples/sec; 0.448 sec/batch)
2016-02-04 06:35:40.499347: step 157240, loss = 0.26 (274.9 examples/sec; 0.466 sec/batch)
2016-02-04 06:35:45.196009: step 157250, loss = 0.32 (280.4 examples/sec; 0.456 sec/batch)
2016-02-04 06:35:49.904443: step 157260, loss = 0.33 (252.3 examples/sec; 0.507 sec/batch)
2016-02-04 06:35:54.572228: step 157270, loss = 0.32 (285.4 examples/sec; 0.448 sec/batch)
2016-02-04 06:35:59.323450: step 157280, loss = 0.28 (259.3 examples/sec; 0.494 sec/batch)
2016-02-04 06:36:03.956019: step 157290, loss = 0.33 (279.4 examples/sec; 0.458 sec/batch)
2016-02-04 06:36:08.667009: step 157300, loss = 0.20 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 06:36:13.915389: step 157310, loss = 0.27 (285.6 examples/sec; 0.448 sec/batch)
2016-02-04 06:36:18.609022: step 157320, loss = 0.23 (279.3 examples/sec; 0.458 sec/batch)
2016-02-04 06:36:23.332629: step 157330, loss = 0.32 (271.6 examples/sec; 0.471 sec/batch)
2016-02-04 06:36:28.087020: step 157340, loss = 0.28 (271.4 examples/sec; 0.472 sec/batch)
2016-02-04 06:36:32.834675: step 157350, loss = 0.24 (261.1 examples/sec; 0.490 sec/batch)
2016-02-04 06:36:37.533809: step 157360, loss = 0.30 (261.7 examples/sec; 0.489 sec/batch)
2016-02-04 06:36:42.291142: step 157370, loss = 0.23 (264.4 examples/sec; 0.484 sec/batch)
2016-02-04 06:36:46.985246: step 157380, loss = 0.23 (261.0 examples/sec; 0.490 sec/batch)
2016-02-04 06:36:51.591922: step 157390, loss = 0.24 (282.7 examples/sec; 0.453 sec/batch)
2016-02-04 06:36:56.302775: step 157400, loss = 0.28 (268.6 examples/sec; 0.477 sec/batch)
2016-02-04 06:37:01.587479: step 157410, loss = 0.26 (282.7 examples/sec; 0.453 sec/batch)
2016-02-04 06:37:06.262266: step 157420, loss = 0.42 (291.2 examples/sec; 0.440 sec/batch)
2016-02-04 06:37:11.056324: step 157430, loss = 0.33 (264.2 examples/sec; 0.484 sec/batch)
2016-02-04 06:37:15.813233: step 157440, loss = 0.28 (261.8 examples/sec; 0.489 sec/batch)
2016-02-04 06:37:20.599813: step 157450, loss = 0.24 (275.0 examples/sec; 0.465 sec/batch)
2016-02-04 06:37:25.276066: step 157460, loss = 0.39 (272.6 examples/sec; 0.470 sec/batch)
2016-02-04 06:37:30.113549: step 157470, loss = 0.30 (249.6 examples/sec; 0.513 sec/batch)
2016-02-04 06:37:34.780040: step 157480, loss = 0.28 (271.0 examples/sec; 0.472 sec/batch)
2016-02-04 06:37:39.541112: step 157490, loss = 0.24 (276.6 examples/sec; 0.463 sec/batch)
2016-02-04 06:37:44.272485: step 157500, loss = 0.26 (271.4 examples/sec; 0.472 sec/batch)
2016-02-04 06:37:49.475235: step 157510, loss = 0.31 (303.1 examples/sec; 0.422 sec/batch)
2016-02-04 06:37:54.260553: step 157520, loss = 0.21 (300.4 examples/sec; 0.426 sec/batch)
2016-02-04 06:37:59.026024: step 157530, loss = 0.23 (271.6 examples/sec; 0.471 sec/batch)
2016-02-04 06:38:03.678407: step 157540, loss = 0.28 (275.8 examples/sec; 0.464 sec/batch)
2016-02-04 06:38:08.352153: step 157550, loss = 0.30 (277.2 examples/sec; 0.462 sec/batch)
2016-02-04 06:38:13.050291: step 157560, loss = 0.33 (287.9 examples/sec; 0.445 sec/batch)
2016-02-04 06:38:17.839655: step 157570, loss = 0.35 (271.5 examples/sec; 0.471 sec/batch)
2016-02-04 06:38:22.479756: step 157580, loss = 0.30 (292.4 examples/sec; 0.438 sec/batch)
2016-02-04 06:38:27.285024: step 157590, loss = 0.31 (273.5 examples/sec; 0.468 sec/batch)
2016-02-04 06:38:32.018652: step 157600, loss = 0.26 (269.2 examples/sec; 0.475 sec/batch)
2016-02-04 06:38:37.228720: step 157610, loss = 0.34 (265.7 examples/sec; 0.482 sec/batch)
2016-02-04 06:38:41.958958: step 157620, loss = 0.29 (270.5 examples/sec; 0.473 sec/batch)
2016-02-04 06:38:46.747008: step 157630, loss = 0.27 (274.9 examples/sec; 0.466 sec/batch)
2016-02-04 06:38:51.544383: step 157640, loss = 0.29 (272.3 examples/sec; 0.470 sec/batch)
2016-02-04 06:38:56.267779: step 157650, loss = 0.29 (253.5 examples/sec; 0.505 sec/batch)
2016-02-04 06:39:00.996474: step 157660, loss = 0.25 (259.6 examples/sec; 0.493 sec/batch)
2016-02-04 06:39:05.708662: step 157670, loss = 0.26 (260.6 examples/sec; 0.491 sec/batch)
2016-02-04 06:39:10.434211: step 157680, loss = 0.37 (274.5 examples/sec; 0.466 sec/batch)
2016-02-04 06:39:15.162532: step 157690, loss = 0.29 (256.4 examples/sec; 0.499 sec/batch)
2016-02-04 06:39:19.868015: step 157700, loss = 0.26 (279.3 examples/sec; 0.458 sec/batch)
2016-02-04 06:39:25.117425: step 157710, loss = 0.31 (290.1 examples/sec; 0.441 sec/batch)
2016-02-04 06:39:29.821374: step 157720, loss = 0.27 (265.0 examples/sec; 0.483 sec/batch)
2016-02-04 06:39:34.519336: step 157730, loss = 0.36 (282.2 examples/sec; 0.454 sec/batch)
2016-02-04 06:39:39.243984: step 157740, loss = 0.34 (263.8 examples/sec; 0.485 sec/batch)
2016-02-04 06:39:43.916023: step 157750, loss = 0.34 (262.0 examples/sec; 0.489 sec/batch)
2016-02-04 06:39:48.725395: step 157760, loss = 0.27 (267.8 examples/sec; 0.478 sec/batch)
2016-02-04 06:39:53.481699: step 157770, loss = 0.35 (254.7 examples/sec; 0.503 sec/batch)
2016-02-04 06:39:58.258324: step 157780, loss = 0.37 (274.3 examples/sec; 0.467 sec/batch)
2016-02-04 06:40:03.018741: step 157790, loss = 0.30 (269.2 examples/sec; 0.476 sec/batch)
2016-02-04 06:40:07.779516: step 157800, loss = 0.27 (260.7 examples/sec; 0.491 sec/batch)
2016-02-04 06:40:13.085929: step 157810, loss = 0.34 (265.4 examples/sec; 0.482 sec/batch)
2016-02-04 06:40:17.754583: step 157820, loss = 0.32 (274.4 examples/sec; 0.467 sec/batch)
2016-02-04 06:40:22.388908: step 157830, loss = 0.31 (293.8 examples/sec; 0.436 sec/batch)
2016-02-04 06:40:27.096150: step 157840, loss = 0.29 (265.3 examples/sec; 0.482 sec/batch)
2016-02-04 06:40:31.838883: step 157850, loss = 0.29 (263.9 examples/sec; 0.485 sec/batch)
2016-02-04 06:40:36.635425: step 157860, loss = 0.28 (268.5 examples/sec; 0.477 sec/batch)
2016-02-04 06:40:41.354796: step 157870, loss = 0.30 (263.6 examples/sec; 0.486 sec/batch)
2016-02-04 06:40:46.132828: step 157880, loss = 0.32 (259.6 examples/sec; 0.493 sec/batch)
2016-02-04 06:40:50.884077: step 157890, loss = 0.25 (272.1 examples/sec; 0.470 sec/batch)
2016-02-04 06:40:55.583099: step 157900, loss = 0.25 (268.0 examples/sec; 0.478 sec/batch)
2016-02-04 06:41:00.902439: step 157910, loss = 0.28 (246.3 examples/sec; 0.520 sec/batch)
2016-02-04 06:41:05.582604: step 157920, loss = 0.29 (274.8 examples/sec; 0.466 sec/batch)
2016-02-04 06:41:10.196392: step 157930, loss = 0.32 (280.2 examples/sec; 0.457 sec/batch)
2016-02-04 06:41:14.723515: step 157940, loss = 0.26 (260.0 examples/sec; 0.492 sec/batch)
2016-02-04 06:41:19.459699: step 157950, loss = 0.29 (282.2 examples/sec; 0.454 sec/batch)
2016-02-04 06:41:24.115489: step 157960, loss = 0.34 (287.6 examples/sec; 0.445 sec/batch)
2016-02-04 06:41:28.863062: step 157970, loss = 0.21 (272.6 examples/sec; 0.470 sec/batch)
2016-02-04 06:41:33.597296: step 157980, loss = 0.40 (273.1 examples/sec; 0.469 sec/batch)
2016-02-04 06:41:38.299038: step 157990, loss = 0.26 (254.2 examples/sec; 0.504 sec/batch)
2016-02-04 06:41:42.985216: step 158000, loss = 0.28 (255.1 examples/sec; 0.502 sec/batch)
2016-02-04 06:41:48.181684: step 158010, loss = 0.30 (257.6 examples/sec; 0.497 sec/batch)
2016-02-04 06:41:52.786881: step 158020, loss = 0.27 (287.6 examples/sec; 0.445 sec/batch)
2016-02-04 06:41:57.552801: step 158030, loss = 0.30 (264.8 examples/sec; 0.483 sec/batch)
2016-02-04 06:42:02.251670: step 158040, loss = 0.33 (285.1 examples/sec; 0.449 sec/batch)
2016-02-04 06:42:06.987907: step 158050, loss = 0.32 (265.7 examples/sec; 0.482 sec/batch)
2016-02-04 06:42:11.694158: step 158060, loss = 0.22 (262.4 examples/sec; 0.488 sec/batch)
2016-02-04 06:42:16.405292: step 158070, loss = 0.29 (296.8 examples/sec; 0.431 sec/batch)
2016-02-04 06:42:21.175179: step 158080, loss = 0.30 (255.4 examples/sec; 0.501 sec/batch)
2016-02-04 06:42:25.902610: step 158090, loss = 0.23 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 06:42:30.653425: step 158100, loss = 0.33 (280.0 examples/sec; 0.457 sec/batch)
2016-02-04 06:42:35.851366: step 158110, loss = 0.36 (270.8 examples/sec; 0.473 sec/batch)
2016-02-04 06:42:40.582598: step 158120, loss = 0.26 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 06:42:45.249589: step 158130, loss = 0.28 (239.0 examples/sec; 0.536 sec/batch)
2016-02-04 06:42:49.896754: step 158140, loss = 0.35 (260.1 examples/sec; 0.492 sec/batch)
2016-02-04 06:42:54.538523: step 158150, loss = 0.30 (299.0 examples/sec; 0.428 sec/batch)
2016-02-04 06:42:59.270660: step 158160, loss = 0.26 (276.2 examples/sec; 0.463 sec/batch)
2016-02-04 06:43:03.993136: step 158170, loss = 0.37 (302.8 examples/sec; 0.423 sec/batch)
2016-02-04 06:43:08.811271: step 158180, loss = 0.27 (286.4 examples/sec; 0.447 sec/batch)
2016-02-04 06:43:13.549618: step 158190, loss = 0.27 (277.5 examples/sec; 0.461 sec/batch)
2016-02-04 06:43:18.332723: step 158200, loss = 0.29 (263.3 examples/sec; 0.486 sec/batch)
2016-02-04 06:43:23.595321: step 158210, loss = 0.34 (258.3 examples/sec; 0.496 sec/batch)
2016-02-04 06:43:28.322237: step 158220, loss = 0.42 (278.5 examples/sec; 0.460 sec/batch)
2016-02-04 06:43:33.127085: step 158230, loss = 0.27 (250.0 examples/sec; 0.512 sec/batch)
2016-02-04 06:43:37.882257: step 158240, loss = 0.32 (258.3 examples/sec; 0.495 sec/batch)
2016-02-04 06:43:42.624968: step 158250, loss = 0.28 (280.5 examples/sec; 0.456 sec/batch)
2016-02-04 06:43:47.261172: step 158260, loss = 0.26 (272.4 examples/sec; 0.470 sec/batch)
2016-02-04 06:43:51.913810: step 158270, loss = 0.32 (274.0 examples/sec; 0.467 sec/batch)
2016-02-04 06:43:56.657599: step 158280, loss = 0.28 (256.3 examples/sec; 0.499 sec/batch)
2016-02-04 06:44:01.382166: step 158290, loss = 0.27 (265.0 examples/sec; 0.483 sec/batch)
2016-02-04 06:44:06.212340: step 158300, loss = 0.33 (266.2 examples/sec; 0.481 sec/batch)
2016-02-04 06:44:11.467183: step 158310, loss = 0.26 (282.8 examples/sec; 0.453 sec/batch)
2016-02-04 06:44:16.211553: step 158320, loss = 0.27 (279.7 examples/sec; 0.458 sec/batch)
2016-02-04 06:44:20.953463: step 158330, loss = 0.24 (269.7 examples/sec; 0.475 sec/batch)
2016-02-04 06:44:25.627870: step 158340, loss = 0.31 (266.8 examples/sec; 0.480 sec/batch)
2016-02-04 06:44:30.380018: step 158350, loss = 0.39 (265.8 examples/sec; 0.482 sec/batch)
2016-02-04 06:44:35.031900: step 158360, loss = 0.31 (261.9 examples/sec; 0.489 sec/batch)
2016-02-04 06:44:39.758287: step 158370, loss = 0.27 (260.0 examples/sec; 0.492 sec/batch)
2016-02-04 06:44:44.513368: step 158380, loss = 0.36 (249.5 examples/sec; 0.513 sec/batch)
2016-02-04 06:44:49.401365: step 158390, loss = 0.21 (272.4 examples/sec; 0.470 sec/batch)
2016-02-04 06:44:54.082282: step 158400, loss = 0.26 (276.6 examples/sec; 0.463 sec/batch)
2016-02-04 06:44:59.414458: step 158410, loss = 0.22 (265.5 examples/sec; 0.482 sec/batch)
2016-02-04 06:45:04.213200: step 158420, loss = 0.29 (257.6 examples/sec; 0.497 sec/batch)
2016-02-04 06:45:08.941217: step 158430, loss = 0.33 (272.7 examples/sec; 0.469 sec/batch)
2016-02-04 06:45:13.675416: step 158440, loss = 0.24 (268.9 examples/sec; 0.476 sec/batch)
2016-02-04 06:45:18.383309: step 158450, loss = 0.37 (267.5 examples/sec; 0.478 sec/batch)
2016-02-04 06:45:23.134006: step 158460, loss = 0.34 (273.6 examples/sec; 0.468 sec/batch)
2016-02-04 06:45:27.804247: step 158470, loss = 0.17 (265.2 examples/sec; 0.483 sec/batch)
2016-02-04 06:45:32.570781: step 158480, loss = 0.28 (263.5 examples/sec; 0.486 sec/batch)
2016-02-04 06:45:37.254826: step 158490, loss = 0.27 (284.4 examples/sec; 0.450 sec/batch)
2016-02-04 06:45:42.007739: step 158500, loss = 0.27 (280.2 examples/sec; 0.457 sec/batch)
2016-02-04 06:45:47.222915: step 158510, loss = 0.31 (281.2 examples/sec; 0.455 sec/batch)
2016-02-04 06:45:51.937701: step 158520, loss = 0.27 (260.4 examples/sec; 0.492 sec/batch)
2016-02-04 06:45:56.616286: step 158530, loss = 0.28 (251.7 examples/sec; 0.509 sec/batch)
2016-02-04 06:46:01.293890: step 158540, loss = 0.27 (268.7 examples/sec; 0.476 sec/batch)
2016-02-04 06:46:06.016777: step 158550, loss = 0.24 (269.7 examples/sec; 0.475 sec/batch)
2016-02-04 06:46:10.618898: step 158560, loss = 0.35 (298.8 examples/sec; 0.428 sec/batch)
2016-02-04 06:46:15.345981: step 158570, loss = 0.27 (254.4 examples/sec; 0.503 sec/batch)
2016-02-04 06:46:20.150324: step 158580, loss = 0.30 (251.0 examples/sec; 0.510 sec/batch)
2016-02-04 06:46:24.763459: step 158590, loss = 0.29 (279.3 examples/sec; 0.458 sec/batch)
2016-02-04 06:46:29.441052: step 158600, loss = 0.33 (256.8 examples/sec; 0.499 sec/batch)
2016-02-04 06:46:34.607228: step 158610, loss = 0.31 (288.4 examples/sec; 0.444 sec/batch)
2016-02-04 06:46:39.384063: step 158620, loss = 0.24 (254.4 examples/sec; 0.503 sec/batch)
2016-02-04 06:46:44.160785: step 158630, loss = 0.25 (282.8 examples/sec; 0.453 sec/batch)
2016-02-04 06:46:48.916140: step 158640, loss = 0.25 (260.9 examples/sec; 0.491 sec/batch)
2016-02-04 06:46:53.688832: step 158650, loss = 0.24 (244.7 examples/sec; 0.523 sec/batch)
2016-02-04 06:46:58.467923: step 158660, loss = 0.24 (299.3 examples/sec; 0.428 sec/batch)
2016-02-04 06:47:03.176236: step 158670, loss = 0.26 (277.6 examples/sec; 0.461 sec/batch)
2016-02-04 06:47:07.815289: step 158680, loss = 0.30 (272.0 examples/sec; 0.471 sec/batch)
2016-02-04 06:47:12.481462: step 158690, loss = 0.31 (256.8 examples/sec; 0.498 sec/batch)
2016-02-04 06:47:17.141196: step 158700, loss = 0.30 (261.4 examples/sec; 0.490 sec/batch)
2016-02-04 06:47:22.402435: step 158710, loss = 0.31 (283.0 examples/sec; 0.452 sec/batch)
2016-02-04 06:47:27.081790: step 158720, loss = 0.21 (268.3 examples/sec; 0.477 sec/batch)
2016-02-04 06:47:31.869479: step 158730, loss = 0.23 (236.5 examples/sec; 0.541 sec/batch)
2016-02-04 06:47:36.526533: step 158740, loss = 0.25 (260.6 examples/sec; 0.491 sec/batch)
2016-02-04 06:47:41.225686: step 158750, loss = 0.27 (282.8 examples/sec; 0.453 sec/batch)
2016-02-04 06:47:45.966232: step 158760, loss = 0.32 (277.9 examples/sec; 0.461 sec/batch)
2016-02-04 06:47:50.598698: step 158770, loss = 0.27 (286.1 examples/sec; 0.447 sec/batch)
2016-02-04 06:47:55.396082: step 158780, loss = 0.29 (259.7 examples/sec; 0.493 sec/batch)
2016-02-04 06:48:00.062309: step 158790, loss = 0.30 (260.6 examples/sec; 0.491 sec/batch)
2016-02-04 06:48:04.804419: step 158800, loss = 0.33 (263.9 examples/sec; 0.485 sec/batch)
2016-02-04 06:48:10.076366: step 158810, loss = 0.24 (285.2 examples/sec; 0.449 sec/batch)
2016-02-04 06:48:14.811258: step 158820, loss = 0.26 (270.9 examples/sec; 0.472 sec/batch)
2016-02-04 06:48:19.493458: step 158830, loss = 0.27 (253.5 examples/sec; 0.505 sec/batch)
2016-02-04 06:48:24.181654: step 158840, loss = 0.31 (258.1 examples/sec; 0.496 sec/batch)
2016-02-04 06:48:28.873854: step 158850, loss = 0.30 (280.8 examples/sec; 0.456 sec/batch)
2016-02-04 06:48:33.611362: step 158860, loss = 0.32 (259.7 examples/sec; 0.493 sec/batch)
2016-02-04 06:48:38.294204: step 158870, loss = 0.28 (276.3 examples/sec; 0.463 sec/batch)
2016-02-04 06:48:43.014289: step 158880, loss = 0.27 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 06:48:47.821169: step 158890, loss = 0.36 (241.0 examples/sec; 0.531 sec/batch)
2016-02-04 06:48:52.357786: step 158900, loss = 0.38 (281.0 examples/sec; 0.456 sec/batch)
2016-02-04 06:48:57.511363: step 158910, loss = 0.29 (271.1 examples/sec; 0.472 sec/batch)
2016-02-04 06:49:02.233427: step 158920, loss = 0.37 (289.9 examples/sec; 0.442 sec/batch)
2016-02-04 06:49:06.951502: step 158930, loss = 0.33 (271.3 examples/sec; 0.472 sec/batch)
2016-02-04 06:49:11.644535: step 158940, loss = 0.28 (279.6 examples/sec; 0.458 sec/batch)
2016-02-04 06:49:16.295492: step 158950, loss = 0.25 (282.4 examples/sec; 0.453 sec/batch)
2016-02-04 06:49:21.021049: step 158960, loss = 0.25 (272.5 examples/sec; 0.470 sec/batch)
2016-02-04 06:49:25.718691: step 158970, loss = 0.30 (274.2 examples/sec; 0.467 sec/batch)
2016-02-04 06:49:30.369373: step 158980, loss = 0.39 (289.5 examples/sec; 0.442 sec/batch)
2016-02-04 06:49:35.016451: step 158990, loss = 0.21 (286.4 examples/sec; 0.447 sec/batch)
2016-02-04 06:49:39.852030: step 159000, loss = 0.26 (245.9 examples/sec; 0.521 sec/batch)
2016-02-04 06:49:45.029767: step 159010, loss = 0.21 (274.2 examples/sec; 0.467 sec/batch)
2016-02-04 06:49:49.699110: step 159020, loss = 0.33 (254.3 examples/sec; 0.503 sec/batch)
2016-02-04 06:49:54.351921: step 159030, loss = 0.20 (294.6 examples/sec; 0.435 sec/batch)
2016-02-04 06:49:59.049626: step 159040, loss = 0.24 (304.4 examples/sec; 0.421 sec/batch)
2016-02-04 06:50:03.744089: step 159050, loss = 0.30 (281.5 examples/sec; 0.455 sec/batch)
2016-02-04 06:50:08.349034: step 159060, loss = 0.28 (283.7 examples/sec; 0.451 sec/batch)
2016-02-04 06:50:13.093099: step 159070, loss = 0.39 (282.9 examples/sec; 0.452 sec/batch)
2016-02-04 06:50:17.842242: step 159080, loss = 0.30 (273.1 examples/sec; 0.469 sec/batch)
2016-02-04 06:50:22.562438: step 159090, loss = 0.28 (275.6 examples/sec; 0.464 sec/batch)
2016-02-04 06:50:27.284952: step 159100, loss = 0.29 (295.8 examples/sec; 0.433 sec/batch)
2016-02-04 06:50:32.511772: step 159110, loss = 0.30 (282.4 examples/sec; 0.453 sec/batch)
2016-02-04 06:50:37.249505: step 159120, loss = 0.25 (271.8 examples/sec; 0.471 sec/batch)
2016-02-04 06:50:42.019782: step 159130, loss = 0.35 (258.3 examples/sec; 0.496 sec/batch)
2016-02-04 06:50:46.706800: step 159140, loss = 0.21 (292.7 examples/sec; 0.437 sec/batch)
2016-02-04 06:50:51.487995: step 159150, loss = 0.34 (264.2 examples/sec; 0.484 sec/batch)
2016-02-04 06:50:56.153570: step 159160, loss = 0.28 (259.2 examples/sec; 0.494 sec/batch)
2016-02-04 06:51:00.965833: step 159170, loss = 0.28 (242.7 examples/sec; 0.527 sec/batch)
2016-02-04 06:51:05.686914: step 159180, loss = 0.26 (278.8 examples/sec; 0.459 sec/batch)
2016-02-04 06:51:10.391330: step 159190, loss = 0.28 (282.8 examples/sec; 0.453 sec/batch)
2016-02-04 06:51:15.174410: step 159200, loss = 0.31 (257.2 examples/sec; 0.498 sec/batch)
2016-02-04 06:51:20.444744: step 159210, loss = 0.27 (334.0 examples/sec; 0.383 sec/batch)
2016-02-04 06:51:25.160227: step 159220, loss = 0.37 (276.3 examples/sec; 0.463 sec/batch)
2016-02-04 06:51:29.842651: step 159230, loss = 0.32 (272.0 examples/sec; 0.471 sec/batch)
2016-02-04 06:51:34.586771: step 159240, loss = 0.34 (279.0 examples/sec; 0.459 sec/batch)
2016-02-04 06:51:39.288992: step 159250, loss = 0.27 (274.3 examples/sec; 0.467 sec/batch)
2016-02-04 06:51:44.041271: step 159260, loss = 0.26 (258.6 examples/sec; 0.495 sec/batch)
2016-02-04 06:51:48.770419: step 159270, loss = 0.31 (256.1 examples/sec; 0.500 sec/batch)
2016-02-04 06:51:53.400632: step 159280, loss = 0.26 (312.2 examples/sec; 0.410 sec/batch)
2016-02-04 06:51:58.142963: step 159290, loss = 0.22 (270.9 examples/sec; 0.473 sec/batch)
2016-02-04 06:52:02.773927: step 159300, loss = 0.33 (282.3 examples/sec; 0.453 sec/batch)
2016-02-04 06:52:07.969412: step 159310, loss = 0.28 (302.5 examples/sec; 0.423 sec/batch)
2016-02-04 06:52:12.784182: step 159320, loss = 0.24 (275.5 examples/sec; 0.465 sec/batch)
2016-02-04 06:52:17.461529: step 159330, loss = 0.28 (284.3 examples/sec; 0.450 sec/batch)
2016-02-04 06:52:22.219005: step 159340, loss = 0.28 (271.7 examples/sec; 0.471 sec/batch)
2016-02-04 06:52:26.972421: step 159350, loss = 0.27 (274.8 examples/sec; 0.466 sec/batch)
2016-02-04 06:52:31.727844: step 159360, loss = 0.31 (240.2 examples/sec; 0.533 sec/batch)
2016-02-04 06:52:36.414645: step 159370, loss = 0.27 (284.3 examples/sec; 0.450 sec/batch)
2016-02-04 06:52:41.172142: step 159380, loss = 0.23 (261.1 examples/sec; 0.490 sec/batch)
2016-02-04 06:52:45.905890: step 159390, loss = 0.23 (274.6 examples/sec; 0.466 sec/batch)
2016-02-04 06:52:50.577596: step 159400, loss = 0.29 (274.4 examples/sec; 0.467 sec/batch)
2016-02-04 06:52:55.804319: step 159410, loss = 0.35 (265.7 examples/sec; 0.482 sec/batch)
2016-02-04 06:53:00.549123: step 159420, loss = 0.32 (263.7 examples/sec; 0.485 sec/batch)
2016-02-04 06:53:05.257150: step 159430, loss = 0.29 (283.7 examples/sec; 0.451 sec/batch)
2016-02-04 06:53:10.013789: step 159440, loss = 0.22 (257.8 examples/sec; 0.496 sec/batch)
2016-02-04 06:53:14.684774: step 159450, loss = 0.26 (277.5 examples/sec; 0.461 sec/batch)
2016-02-04 06:53:19.406917: step 159460, loss = 0.27 (246.9 examples/sec; 0.518 sec/batch)
2016-02-04 06:53:24.174095: step 159470, loss = 0.27 (273.1 examples/sec; 0.469 sec/batch)
2016-02-04 06:53:28.948372: step 159480, loss = 0.25 (287.6 examples/sec; 0.445 sec/batch)
2016-02-04 06:53:33.726587: step 159490, loss = 0.24 (273.3 examples/sec; 0.468 sec/batch)
2016-02-04 06:53:38.378262: step 159500, loss = 0.24 (300.5 examples/sec; 0.426 sec/batch)
2016-02-04 06:53:43.699461: step 159510, loss = 0.30 (278.8 examples/sec; 0.459 sec/batch)
2016-02-04 06:53:48.452570: step 159520, loss = 0.23 (259.7 examples/sec; 0.493 sec/batch)
2016-02-04 06:53:53.122270: step 159530, loss = 0.23 (281.0 examples/sec; 0.455 sec/batch)
2016-02-04 06:53:57.809153: step 159540, loss = 0.30 (280.3 examples/sec; 0.457 sec/batch)
2016-02-04 06:54:02.446357: step 159550, loss = 0.27 (295.9 examples/sec; 0.433 sec/batch)
2016-02-04 06:54:07.233059: step 159560, loss = 0.31 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 06:54:11.902258: step 159570, loss = 0.33 (260.2 examples/sec; 0.492 sec/batch)
2016-02-04 06:54:16.632367: step 159580, loss = 0.33 (256.0 examples/sec; 0.500 sec/batch)
2016-02-04 06:54:21.382175: step 159590, loss = 0.33 (253.2 examples/sec; 0.505 sec/batch)
2016-02-04 06:54:26.037560: step 159600, loss = 0.25 (283.3 examples/sec; 0.452 sec/batch)
2016-02-04 06:54:31.236003: step 159610, loss = 0.37 (281.5 examples/sec; 0.455 sec/batch)
2016-02-04 06:54:36.057617: step 159620, loss = 0.30 (260.8 examples/sec; 0.491 sec/batch)
2016-02-04 06:54:40.712815: step 159630, loss = 0.30 (302.4 examples/sec; 0.423 sec/batch)
2016-02-04 06:54:45.485550: step 159640, loss = 0.30 (276.9 examples/sec; 0.462 sec/batch)
2016-02-04 06:54:50.208576: step 159650, loss = 0.38 (264.0 examples/sec; 0.485 sec/batch)
2016-02-04 06:54:54.912997: step 159660, loss = 0.24 (270.7 examples/sec; 0.473 sec/batch)
2016-02-04 06:54:59.596344: step 159670, loss = 0.28 (259.6 examples/sec; 0.493 sec/batch)
2016-02-04 06:55:04.369432: step 159680, loss = 0.29 (276.9 examples/sec; 0.462 sec/batch)
2016-02-04 06:55:09.105663: step 159690, loss = 0.26 (282.7 examples/sec; 0.453 sec/batch)
2016-02-04 06:55:13.872366: step 159700, loss = 0.25 (292.2 examples/sec; 0.438 sec/batch)
2016-02-04 06:55:19.149444: step 159710, loss = 0.26 (263.8 examples/sec; 0.485 sec/batch)
2016-02-04 06:55:23.940426: step 159720, loss = 0.29 (265.1 examples/sec; 0.483 sec/batch)
2016-02-04 06:55:28.637259: step 159730, loss = 0.38 (263.8 examples/sec; 0.485 sec/batch)
2016-02-04 06:55:33.347676: step 159740, loss = 0.30 (296.7 examples/sec; 0.431 sec/batch)
2016-02-04 06:55:38.073622: step 159750, loss = 0.30 (273.9 examples/sec; 0.467 sec/batch)
2016-02-04 06:55:42.794351: step 159760, loss = 0.25 (257.3 examples/sec; 0.497 sec/batch)
2016-02-04 06:55:47.419296: step 159770, loss = 0.30 (268.7 examples/sec; 0.476 sec/batch)
2016-02-04 06:55:52.124956: step 159780, loss = 0.26 (285.4 examples/sec; 0.448 sec/batch)
2016-02-04 06:55:56.818183: step 159790, loss = 0.22 (282.1 examples/sec; 0.454 sec/batch)
2016-02-04 06:56:01.586431: step 159800, loss = 0.31 (253.0 examples/sec; 0.506 sec/batch)
2016-02-04 06:56:06.751348: step 159810, loss = 0.27 (271.1 examples/sec; 0.472 sec/batch)
2016-02-04 06:56:11.396445: step 159820, loss = 0.29 (281.8 examples/sec; 0.454 sec/batch)
2016-02-04 06:56:16.078399: step 159830, loss = 0.31 (249.7 examples/sec; 0.513 sec/batch)
2016-02-04 06:56:20.863258: step 159840, loss = 0.31 (243.3 examples/sec; 0.526 sec/batch)
2016-02-04 06:56:25.582394: step 159850, loss = 0.30 (251.0 examples/sec; 0.510 sec/batch)
2016-02-04 06:56:30.318913: step 159860, loss = 0.24 (265.1 examples/sec; 0.483 sec/batch)
2016-02-04 06:56:34.970822: step 159870, loss = 0.28 (278.2 examples/sec; 0.460 sec/batch)
2016-02-04 06:56:39.593702: step 159880, loss = 0.28 (277.0 examples/sec; 0.462 sec/batch)
2016-02-04 06:56:44.250248: step 159890, loss = 0.23 (264.8 examples/sec; 0.483 sec/batch)
2016-02-04 06:56:48.986876: step 159900, loss = 0.38 (250.4 examples/sec; 0.511 sec/batch)
2016-02-04 06:56:54.218017: step 159910, loss = 0.25 (270.0 examples/sec; 0.474 sec/batch)
2016-02-04 06:56:58.970339: step 159920, loss = 0.25 (289.7 examples/sec; 0.442 sec/batch)
2016-02-04 06:57:03.710877: step 159930, loss = 0.29 (269.8 examples/sec; 0.474 sec/batch)
2016-02-04 06:57:08.367439: step 159940, loss = 0.23 (275.9 examples/sec; 0.464 sec/batch)
2016-02-04 06:57:13.038035: step 159950, loss = 0.23 (259.3 examples/sec; 0.494 sec/batch)
2016-02-04 06:57:17.778385: step 159960, loss = 0.29 (297.3 examples/sec; 0.431 sec/batch)
2016-02-04 06:57:22.520129: step 159970, loss = 0.22 (279.6 examples/sec; 0.458 sec/batch)
2016-02-04 06:57:27.169783: step 159980, loss = 0.30 (270.0 examples/sec; 0.474 sec/batch)
2016-02-04 06:57:31.930750: step 159990, loss = 0.24 (267.5 examples/sec; 0.478 sec/batch)
2016-02-04 06:57:36.610123: step 160000, loss = 0.22 (275.0 examples/sec; 0.466 sec/batch)
2016-02-04 06:57:41.865915: step 160010, loss = 0.27 (269.3 examples/sec; 0.475 sec/batch)
2016-02-04 06:57:46.610161: step 160020, loss = 0.29 (272.7 examples/sec; 0.469 sec/batch)
2016-02-04 06:57:51.283513: step 160030, loss = 0.26 (283.9 examples/sec; 0.451 sec/batch)
2016-02-04 06:57:56.044236: step 160040, loss = 0.34 (276.1 examples/sec; 0.464 sec/batch)
2016-02-04 06:58:00.757984: step 160050, loss = 0.27 (270.4 examples/sec; 0.473 sec/batch)
2016-02-04 06:58:05.420208: step 160060, loss = 0.38 (294.6 examples/sec; 0.434 sec/batch)
2016-02-04 06:58:10.153288: step 160070, loss = 0.37 (286.8 examples/sec; 0.446 sec/batch)
2016-02-04 06:58:14.997305: step 160080, loss = 0.29 (263.6 examples/sec; 0.486 sec/batch)
2016-02-04 06:58:19.764699: step 160090, loss = 0.28 (250.0 examples/sec; 0.512 sec/batch)
2016-02-04 06:58:24.407977: step 160100, loss = 0.24 (293.7 examples/sec; 0.436 sec/batch)
2016-02-04 06:58:29.655090: step 160110, loss = 0.39 (271.1 examples/sec; 0.472 sec/batch)
2016-02-04 06:58:34.283416: step 160120, loss = 0.28 (288.9 examples/sec; 0.443 sec/batch)
2016-02-04 06:58:39.037927: step 160130, loss = 0.31 (250.2 examples/sec; 0.512 sec/batch)
2016-02-04 06:58:43.726787: step 160140, loss = 0.33 (277.7 examples/sec; 0.461 sec/batch)
2016-02-04 06:58:48.391317: step 160150, loss = 0.31 (275.7 examples/sec; 0.464 sec/batch)
2016-02-04 06:58:53.081709: step 160160, loss = 0.23 (297.0 examples/sec; 0.431 sec/batch)
2016-02-04 06:58:57.801460: step 160170, loss = 0.33 (285.2 examples/sec; 0.449 sec/batch)
2016-02-04 06:59:02.529642: step 160180, loss = 0.32 (270.3 examples/sec; 0.474 sec/batch)
2016-02-04 06:59:07.225196: step 160190, loss = 0.31 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 06:59:11.898447: step 160200, loss = 0.27 (254.3 examples/sec; 0.503 sec/batch)
2016-02-04 06:59:17.020003: step 160210, loss = 0.26 (293.9 examples/sec; 0.436 sec/batch)
2016-02-04 06:59:21.738357: step 160220, loss = 0.24 (280.9 examples/sec; 0.456 sec/batch)
2016-02-04 06:59:26.343125: step 160230, loss = 0.21 (288.9 examples/sec; 0.443 sec/batch)
2016-02-04 06:59:31.071949: step 160240, loss = 0.23 (260.3 examples/sec; 0.492 sec/batch)
2016-02-04 06:59:35.744856: step 160250, loss = 0.25 (285.5 examples/sec; 0.448 sec/batch)
2016-02-04 06:59:40.320148: step 160260, loss = 0.22 (299.3 examples/sec; 0.428 sec/batch)
2016-02-04 06:59:44.939774: step 160270, loss = 0.21 (302.4 examples/sec; 0.423 sec/batch)
2016-02-04 06:59:49.563711: step 160280, loss = 0.28 (290.7 examples/sec; 0.440 sec/batch)
2016-02-04 06:59:54.336222: step 160290, loss = 0.39 (293.1 examples/sec; 0.437 sec/batch)
2016-02-04 06:59:59.044863: step 160300, loss = 0.24 (259.9 examples/sec; 0.492 sec/batch)
2016-02-04 07:00:04.279069: step 160310, loss = 0.29 (266.8 examples/sec; 0.480 sec/batch)
2016-02-04 07:00:08.917468: step 160320, loss = 0.25 (295.7 examples/sec; 0.433 sec/batch)
2016-02-04 07:00:13.618903: step 160330, loss = 0.26 (271.5 examples/sec; 0.472 sec/batch)
2016-02-04 07:00:18.344539: step 160340, loss = 0.21 (272.6 examples/sec; 0.469 sec/batch)
2016-02-04 07:00:22.897287: step 160350, loss = 0.23 (286.9 examples/sec; 0.446 sec/batch)
2016-02-04 07:00:27.554392: step 160360, loss = 0.33 (288.5 examples/sec; 0.444 sec/batch)
2016-02-04 07:00:32.281028: step 160370, loss = 0.27 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 07:00:36.988983: step 160380, loss = 0.39 (279.8 examples/sec; 0.457 sec/batch)
2016-02-04 07:00:41.577915: step 160390, loss = 0.34 (322.2 examples/sec; 0.397 sec/batch)
2016-02-04 07:00:46.264283: step 160400, loss = 0.24 (287.2 examples/sec; 0.446 sec/batch)
2016-02-04 07:00:51.441450: step 160410, loss = 0.31 (262.8 examples/sec; 0.487 sec/batch)
2016-02-04 07:00:56.141534: step 160420, loss = 0.26 (284.1 examples/sec; 0.451 sec/batch)
2016-02-04 07:01:00.721145: step 160430, loss = 0.24 (280.9 examples/sec; 0.456 sec/batch)
2016-02-04 07:01:05.514421: step 160440, loss = 0.33 (253.0 examples/sec; 0.506 sec/batch)
2016-02-04 07:01:10.098727: step 160450, loss = 0.35 (272.1 examples/sec; 0.470 sec/batch)
2016-02-04 07:01:14.680527: step 160460, loss = 0.31 (261.6 examples/sec; 0.489 sec/batch)
2016-02-04 07:01:19.325793: step 160470, loss = 0.32 (265.0 examples/sec; 0.483 sec/batch)
2016-02-04 07:01:23.861012: step 160480, loss = 0.23 (273.7 examples/sec; 0.468 sec/batch)
2016-02-04 07:01:28.566222: step 160490, loss = 0.32 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 07:01:33.264614: step 160500, loss = 0.24 (252.5 examples/sec; 0.507 sec/batch)
2016-02-04 07:01:38.433282: step 160510, loss = 0.27 (286.2 examples/sec; 0.447 sec/batch)
2016-02-04 07:01:43.146854: step 160520, loss = 0.27 (264.0 examples/sec; 0.485 sec/batch)
2016-02-04 07:01:47.801806: step 160530, loss = 0.31 (283.0 examples/sec; 0.452 sec/batch)
2016-02-04 07:01:52.508076: step 160540, loss = 0.29 (269.7 examples/sec; 0.475 sec/batch)
2016-02-04 07:01:57.252803: step 160550, loss = 0.27 (249.2 examples/sec; 0.514 sec/batch)
2016-02-04 07:02:01.991356: step 160560, loss = 0.42 (284.1 examples/sec; 0.451 sec/batch)
2016-02-04 07:02:06.673275: step 160570, loss = 0.27 (268.1 examples/sec; 0.477 sec/batch)
2016-02-04 07:02:11.407017: step 160580, loss = 0.26 (267.7 examples/sec; 0.478 sec/batch)
2016-02-04 07:02:16.123690: step 160590, loss = 0.24 (265.6 examples/sec; 0.482 sec/batch)
2016-02-04 07:02:20.760198: step 160600, loss = 0.42 (310.8 examples/sec; 0.412 sec/batch)
2016-02-04 07:02:26.034780: step 160610, loss = 0.31 (283.8 examples/sec; 0.451 sec/batch)
2016-02-04 07:02:30.744365: step 160620, loss = 0.27 (267.8 examples/sec; 0.478 sec/batch)
2016-02-04 07:02:35.458189: step 160630, loss = 0.30 (280.3 examples/sec; 0.457 sec/batch)
2016-02-04 07:02:40.173945: step 160640, loss = 0.30 (267.7 examples/sec; 0.478 sec/batch)
2016-02-04 07:02:44.959131: step 160650, loss = 0.30 (263.6 examples/sec; 0.486 sec/batch)
2016-02-04 07:02:49.677780: step 160660, loss = 0.40 (257.6 examples/sec; 0.497 sec/batch)
2016-02-04 07:02:54.307219: step 160670, loss = 0.25 (273.7 examples/sec; 0.468 sec/batch)
2016-02-04 07:02:59.033499: step 160680, loss = 0.35 (289.1 examples/sec; 0.443 sec/batch)
2016-02-04 07:03:03.793968: step 160690, loss = 0.36 (253.7 examples/sec; 0.504 sec/batch)
2016-02-04 07:03:08.559519: step 160700, loss = 0.26 (254.7 examples/sec; 0.503 sec/batch)
2016-02-04 07:03:13.744321: step 160710, loss = 0.33 (271.8 examples/sec; 0.471 sec/batch)
2016-02-04 07:03:18.440965: step 160720, loss = 0.32 (258.4 examples/sec; 0.495 sec/batch)
2016-02-04 07:03:23.078042: step 160730, loss = 0.37 (274.8 examples/sec; 0.466 sec/batch)
2016-02-04 07:03:27.780844: step 160740, loss = 0.21 (297.8 examples/sec; 0.430 sec/batch)
2016-02-04 07:03:32.567620: step 160750, loss = 0.35 (263.5 examples/sec; 0.486 sec/batch)
2016-02-04 07:03:37.311005: step 160760, loss = 0.27 (252.4 examples/sec; 0.507 sec/batch)
2016-02-04 07:03:42.025191: step 160770, loss = 0.26 (267.8 examples/sec; 0.478 sec/batch)
2016-02-04 07:03:46.724597: step 160780, loss = 0.30 (278.8 examples/sec; 0.459 sec/batch)
2016-02-04 07:03:51.443755: step 160790, loss = 0.20 (265.6 examples/sec; 0.482 sec/batch)
2016-02-04 07:03:56.106746: step 160800, loss = 0.33 (268.0 examples/sec; 0.478 sec/batch)
2016-02-04 07:04:01.365502: step 160810, loss = 0.25 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 07:04:06.089141: step 160820, loss = 0.33 (269.3 examples/sec; 0.475 sec/batch)
2016-02-04 07:04:10.805907: step 160830, loss = 0.30 (266.4 examples/sec; 0.480 sec/batch)
2016-02-04 07:04:15.554490: step 160840, loss = 0.25 (269.3 examples/sec; 0.475 sec/batch)
2016-02-04 07:04:20.326269: step 160850, loss = 0.20 (279.9 examples/sec; 0.457 sec/batch)
2016-02-04 07:04:24.986378: step 160860, loss = 0.33 (276.5 examples/sec; 0.463 sec/batch)
2016-02-04 07:04:29.663315: step 160870, loss = 0.29 (275.6 examples/sec; 0.464 sec/batch)
2016-02-04 07:04:34.317449: step 160880, loss = 0.31 (277.2 examples/sec; 0.462 sec/batch)
2016-02-04 07:04:39.064535: step 160890, loss = 0.27 (289.0 examples/sec; 0.443 sec/batch)
2016-02-04 07:04:43.795198: step 160900, loss = 0.28 (263.2 examples/sec; 0.486 sec/batch)
2016-02-04 07:04:48.982182: step 160910, loss = 0.24 (275.4 examples/sec; 0.465 sec/batch)
2016-02-04 07:04:53.683748: step 160920, loss = 0.35 (266.5 examples/sec; 0.480 sec/batch)
2016-02-04 07:04:58.433588: step 160930, loss = 0.28 (273.3 examples/sec; 0.468 sec/batch)
2016-02-04 07:05:03.158398: step 160940, loss = 0.31 (254.0 examples/sec; 0.504 sec/batch)
2016-02-04 07:05:07.914027: step 160950, loss = 0.23 (255.5 examples/sec; 0.501 sec/batch)
2016-02-04 07:05:12.624648: step 160960, loss = 0.20 (257.3 examples/sec; 0.498 sec/batch)
2016-02-04 07:05:17.398061: step 160970, loss = 0.23 (262.8 examples/sec; 0.487 sec/batch)
2016-02-04 07:05:22.194478: step 160980, loss = 0.26 (261.7 examples/sec; 0.489 sec/batch)
2016-02-04 07:05:27.016808: step 160990, loss = 0.29 (243.6 examples/sec; 0.525 sec/batch)
2016-02-04 07:05:31.719550: step 161000, loss = 0.27 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 07:05:37.045766: step 161010, loss = 0.26 (278.2 examples/sec; 0.460 sec/batch)
2016-02-04 07:05:41.846326: step 161020, loss = 0.34 (253.1 examples/sec; 0.506 sec/batch)
2016-02-04 07:05:46.536478: step 161030, loss = 0.40 (278.6 examples/sec; 0.459 sec/batch)
2016-02-04 07:05:51.320647: step 161040, loss = 0.29 (243.2 examples/sec; 0.526 sec/batch)
2016-02-04 07:05:56.057356: step 161050, loss = 0.32 (280.0 examples/sec; 0.457 sec/batch)
2016-02-04 07:06:00.790280: step 161060, loss = 0.24 (258.1 examples/sec; 0.496 sec/batch)
2016-02-04 07:06:05.524109: step 161070, loss = 0.27 (279.5 examples/sec; 0.458 sec/batch)
2016-02-04 07:06:10.315558: step 161080, loss = 0.29 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 07:06:15.086410: step 161090, loss = 0.33 (269.4 examples/sec; 0.475 sec/batch)
2016-02-04 07:06:19.922660: step 161100, loss = 0.28 (253.4 examples/sec; 0.505 sec/batch)
2016-02-04 07:06:25.138844: step 161110, loss = 0.27 (290.3 examples/sec; 0.441 sec/batch)
2016-02-04 07:06:29.898454: step 161120, loss = 0.27 (271.5 examples/sec; 0.472 sec/batch)
2016-02-04 07:06:34.718739: step 161130, loss = 0.30 (272.5 examples/sec; 0.470 sec/batch)
2016-02-04 07:06:39.407920: step 161140, loss = 0.31 (295.8 examples/sec; 0.433 sec/batch)
2016-02-04 07:06:44.198969: step 161150, loss = 0.29 (266.9 examples/sec; 0.480 sec/batch)
2016-02-04 07:06:48.986794: step 161160, loss = 0.26 (247.6 examples/sec; 0.517 sec/batch)
2016-02-04 07:06:53.762670: step 161170, loss = 0.23 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 07:06:58.553729: step 161180, loss = 0.26 (265.5 examples/sec; 0.482 sec/batch)
2016-02-04 07:07:03.280495: step 161190, loss = 0.26 (260.3 examples/sec; 0.492 sec/batch)
2016-02-04 07:07:08.063313: step 161200, loss = 0.25 (253.4 examples/sec; 0.505 sec/batch)
2016-02-04 07:07:13.298734: step 161210, loss = 0.31 (273.6 examples/sec; 0.468 sec/batch)
2016-02-04 07:07:18.070042: step 161220, loss = 0.25 (250.1 examples/sec; 0.512 sec/batch)
2016-02-04 07:07:22.649568: step 161230, loss = 0.25 (247.5 examples/sec; 0.517 sec/batch)
2016-02-04 07:07:27.314255: step 161240, loss = 0.22 (285.1 examples/sec; 0.449 sec/batch)
2016-02-04 07:07:32.108838: step 161250, loss = 0.22 (266.5 examples/sec; 0.480 sec/batch)
2016-02-04 07:07:36.879932: step 161260, loss = 0.35 (308.7 examples/sec; 0.415 sec/batch)
2016-02-04 07:07:41.580196: step 161270, loss = 0.32 (281.8 examples/sec; 0.454 sec/batch)
2016-02-04 07:07:46.314719: step 161280, loss = 0.25 (252.9 examples/sec; 0.506 sec/batch)
2016-02-04 07:07:51.020700: step 161290, loss = 0.31 (272.2 examples/sec; 0.470 sec/batch)
2016-02-04 07:07:55.755123: step 161300, loss = 0.34 (263.1 examples/sec; 0.486 sec/batch)
2016-02-04 07:08:00.928548: step 161310, loss = 0.30 (293.3 examples/sec; 0.436 sec/batch)
2016-02-04 07:08:05.667959: step 161320, loss = 0.33 (265.4 examples/sec; 0.482 sec/batch)
2016-02-04 07:08:10.377664: step 161330, loss = 0.20 (296.7 examples/sec; 0.431 sec/batch)
2016-02-04 07:08:15.106595: step 161340, loss = 0.35 (257.9 examples/sec; 0.496 sec/batch)
2016-02-04 07:08:19.773038: step 161350, loss = 0.32 (299.4 examples/sec; 0.427 sec/batch)
2016-02-04 07:08:24.523806: step 161360, loss = 0.25 (269.7 examples/sec; 0.475 sec/batch)
2016-02-04 07:08:29.087016: step 161370, loss = 0.24 (275.7 examples/sec; 0.464 sec/batch)
2016-02-04 07:08:33.828205: step 161380, loss = 0.26 (239.5 examples/sec; 0.534 sec/batch)
2016-02-04 07:08:38.546507: step 161390, loss = 0.24 (277.2 examples/sec; 0.462 sec/batch)
2016-02-04 07:08:43.309243: step 161400, loss = 0.25 (280.2 examples/sec; 0.457 sec/batch)
2016-02-04 07:08:48.486171: step 161410, loss = 0.34 (279.2 examples/sec; 0.458 sec/batch)
2016-02-04 07:08:53.114670: step 161420, loss = 0.29 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 07:08:57.882906: step 161430, loss = 0.26 (289.2 examples/sec; 0.443 sec/batch)
2016-02-04 07:09:02.625885: step 161440, loss = 0.31 (273.1 examples/sec; 0.469 sec/batch)
2016-02-04 07:09:07.376941: step 161450, loss = 0.26 (284.6 examples/sec; 0.450 sec/batch)
2016-02-04 07:09:12.051265: step 161460, loss = 0.27 (267.9 examples/sec; 0.478 sec/batch)
2016-02-04 07:09:16.819525: step 161470, loss = 0.30 (279.4 examples/sec; 0.458 sec/batch)
2016-02-04 07:09:21.572327: step 161480, loss = 0.37 (262.1 examples/sec; 0.488 sec/batch)
2016-02-04 07:09:26.250373: step 161490, loss = 0.27 (243.2 examples/sec; 0.526 sec/batch)
2016-02-04 07:09:30.963757: step 161500, loss = 0.36 (285.7 examples/sec; 0.448 sec/batch)
2016-02-04 07:09:36.202088: step 161510, loss = 0.20 (280.9 examples/sec; 0.456 sec/batch)
2016-02-04 07:09:40.942944: step 161520, loss = 0.28 (275.6 examples/sec; 0.464 sec/batch)
2016-02-04 07:09:45.728785: step 161530, loss = 0.30 (245.1 examples/sec; 0.522 sec/batch)
2016-02-04 07:09:50.375272: step 161540, loss = 0.28 (285.6 examples/sec; 0.448 sec/batch)
2016-02-04 07:09:55.106451: step 161550, loss = 0.27 (266.0 examples/sec; 0.481 sec/batch)
2016-02-04 07:09:59.831127: step 161560, loss = 0.25 (271.5 examples/sec; 0.471 sec/batch)
2016-02-04 07:10:04.578871: step 161570, loss = 0.34 (281.5 examples/sec; 0.455 sec/batch)
2016-02-04 07:10:09.370019: step 161580, loss = 0.29 (268.5 examples/sec; 0.477 sec/batch)
2016-02-04 07:10:14.112823: step 161590, loss = 0.33 (289.0 examples/sec; 0.443 sec/batch)
2016-02-04 07:10:18.825044: step 161600, loss = 0.31 (285.1 examples/sec; 0.449 sec/batch)
2016-02-04 07:10:24.043347: step 161610, loss = 0.23 (286.1 examples/sec; 0.447 sec/batch)
2016-02-04 07:10:28.771437: step 161620, loss = 0.27 (278.0 examples/sec; 0.460 sec/batch)
2016-02-04 07:10:33.509540: step 161630, loss = 0.35 (266.4 examples/sec; 0.480 sec/batch)
2016-02-04 07:10:38.242035: step 161640, loss = 0.26 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 07:10:42.963462: step 161650, loss = 0.30 (264.0 examples/sec; 0.485 sec/batch)
2016-02-04 07:10:47.730214: step 161660, loss = 0.31 (272.5 examples/sec; 0.470 sec/batch)
2016-02-04 07:10:52.487077: step 161670, loss = 0.27 (262.1 examples/sec; 0.488 sec/batch)
2016-02-04 07:10:57.169280: step 161680, loss = 0.29 (266.7 examples/sec; 0.480 sec/batch)
2016-02-04 07:11:01.913424: step 161690, loss = 0.28 (258.2 examples/sec; 0.496 sec/batch)
2016-02-04 07:11:06.642528: step 161700, loss = 0.23 (272.7 examples/sec; 0.469 sec/batch)
2016-02-04 07:11:11.846825: step 161710, loss = 0.24 (287.8 examples/sec; 0.445 sec/batch)
2016-02-04 07:11:16.598515: step 161720, loss = 0.30 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 07:11:21.374948: step 161730, loss = 0.30 (267.2 examples/sec; 0.479 sec/batch)
2016-02-04 07:11:26.076084: step 161740, loss = 0.22 (283.4 examples/sec; 0.452 sec/batch)
2016-02-04 07:11:30.812871: step 161750, loss = 0.25 (262.6 examples/sec; 0.487 sec/batch)
2016-02-04 07:11:35.516031: step 161760, loss = 0.25 (254.8 examples/sec; 0.502 sec/batch)
2016-02-04 07:11:40.232423: step 161770, loss = 0.31 (287.8 examples/sec; 0.445 sec/batch)
2016-02-04 07:11:44.874462: step 161780, loss = 0.21 (283.2 examples/sec; 0.452 sec/batch)
2016-02-04 07:11:49.654459: step 161790, loss = 0.27 (264.0 examples/sec; 0.485 sec/batch)
2016-02-04 07:11:54.401893: step 161800, loss = 0.34 (262.4 examples/sec; 0.488 sec/batch)
2016-02-04 07:11:59.680569: step 161810, loss = 0.33 (268.1 examples/sec; 0.477 sec/batch)
2016-02-04 07:12:04.464806: step 161820, loss = 0.22 (271.8 examples/sec; 0.471 sec/batch)
2016-02-04 07:12:09.164016: step 161830, loss = 0.31 (276.0 examples/sec; 0.464 sec/batch)
2016-02-04 07:12:13.850194: step 161840, loss = 0.27 (283.3 examples/sec; 0.452 sec/batch)
2016-02-04 07:12:18.462566: step 161850, loss = 0.25 (295.4 examples/sec; 0.433 sec/batch)
2016-02-04 07:12:23.093975: step 161860, loss = 0.26 (260.3 examples/sec; 0.492 sec/batch)
2016-02-04 07:12:27.695789: step 161870, loss = 0.22 (277.9 examples/sec; 0.461 sec/batch)
2016-02-04 07:12:32.402078: step 161880, loss = 0.29 (279.6 examples/sec; 0.458 sec/batch)
2016-02-04 07:12:37.188040: step 161890, loss = 0.28 (255.2 examples/sec; 0.502 sec/batch)
2016-02-04 07:12:41.878446: step 161900, loss = 0.26 (280.7 examples/sec; 0.456 sec/batch)
2016-02-04 07:12:47.018045: step 161910, loss = 0.29 (260.2 examples/sec; 0.492 sec/batch)
2016-02-04 07:12:51.721207: step 161920, loss = 0.23 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 07:12:56.560387: step 161930, loss = 0.29 (244.0 examples/sec; 0.525 sec/batch)
2016-02-04 07:13:01.264643: step 161940, loss = 0.32 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 07:13:06.019621: step 161950, loss = 0.25 (266.7 examples/sec; 0.480 sec/batch)
2016-02-04 07:13:10.696184: step 161960, loss = 0.25 (278.8 examples/sec; 0.459 sec/batch)
2016-02-04 07:13:15.336037: step 161970, loss = 0.25 (283.8 examples/sec; 0.451 sec/batch)
2016-02-04 07:13:20.049827: step 161980, loss = 0.30 (286.8 examples/sec; 0.446 sec/batch)
2016-02-04 07:13:24.729890: step 161990, loss = 0.26 (287.1 examples/sec; 0.446 sec/batch)
2016-02-04 07:13:29.483136: step 162000, loss = 0.31 (279.2 examples/sec; 0.458 sec/batch)
2016-02-04 07:13:34.727499: step 162010, loss = 0.22 (272.3 examples/sec; 0.470 sec/batch)
2016-02-04 07:13:39.460167: step 162020, loss = 0.25 (252.4 examples/sec; 0.507 sec/batch)
2016-02-04 07:13:44.190536: step 162030, loss = 0.30 (270.5 examples/sec; 0.473 sec/batch)
2016-02-04 07:13:48.883329: step 162040, loss = 0.31 (297.0 examples/sec; 0.431 sec/batch)
2016-02-04 07:13:53.648688: step 162050, loss = 0.31 (251.6 examples/sec; 0.509 sec/batch)
2016-02-04 07:13:58.404063: step 162060, loss = 0.29 (271.1 examples/sec; 0.472 sec/batch)
2016-02-04 07:14:03.126472: step 162070, loss = 0.28 (284.0 examples/sec; 0.451 sec/batch)
2016-02-04 07:14:07.842481: step 162080, loss = 0.28 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 07:14:12.540738: step 162090, loss = 0.27 (275.7 examples/sec; 0.464 sec/batch)
2016-02-04 07:14:17.196040: step 162100, loss = 0.31 (278.8 examples/sec; 0.459 sec/batch)
2016-02-04 07:14:22.385700: step 162110, loss = 0.22 (271.7 examples/sec; 0.471 sec/batch)
2016-02-04 07:14:27.143757: step 162120, loss = 0.29 (293.7 examples/sec; 0.436 sec/batch)
2016-02-04 07:14:31.833344: step 162130, loss = 0.27 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 07:14:36.485043: step 162140, loss = 0.27 (250.8 examples/sec; 0.510 sec/batch)
2016-02-04 07:14:41.173042: step 162150, loss = 0.35 (265.6 examples/sec; 0.482 sec/batch)
2016-02-04 07:14:45.899091: step 162160, loss = 0.25 (288.2 examples/sec; 0.444 sec/batch)
2016-02-04 07:14:50.699542: step 162170, loss = 0.32 (244.2 examples/sec; 0.524 sec/batch)
2016-02-04 07:14:55.403495: step 162180, loss = 0.25 (253.9 examples/sec; 0.504 sec/batch)
2016-02-04 07:15:00.174891: step 162190, loss = 0.22 (272.7 examples/sec; 0.469 sec/batch)
2016-02-04 07:15:04.937316: step 162200, loss = 0.23 (265.5 examples/sec; 0.482 sec/batch)
2016-02-04 07:15:10.235101: step 162210, loss = 0.25 (284.0 examples/sec; 0.451 sec/batch)
2016-02-04 07:15:14.989475: step 162220, loss = 0.25 (307.1 examples/sec; 0.417 sec/batch)
2016-02-04 07:15:19.733998: step 162230, loss = 0.21 (286.8 examples/sec; 0.446 sec/batch)
2016-02-04 07:15:24.495199: step 162240, loss = 0.25 (251.7 examples/sec; 0.508 sec/batch)
2016-02-04 07:15:29.151785: step 162250, loss = 0.33 (280.2 examples/sec; 0.457 sec/batch)
2016-02-04 07:15:33.952957: step 162260, loss = 0.26 (255.5 examples/sec; 0.501 sec/batch)
2016-02-04 07:15:38.654256: step 162270, loss = 0.26 (278.9 examples/sec; 0.459 sec/batch)
2016-02-04 07:15:43.442941: step 162280, loss = 0.33 (251.9 examples/sec; 0.508 sec/batch)
2016-02-04 07:15:48.128418: step 162290, loss = 0.29 (276.7 examples/sec; 0.463 sec/batch)
2016-02-04 07:15:52.790155: step 162300, loss = 0.35 (261.5 examples/sec; 0.489 sec/batch)
2016-02-04 07:15:58.010900: step 162310, loss = 0.23 (270.7 examples/sec; 0.473 sec/batch)
2016-02-04 07:16:02.670748: step 162320, loss = 0.22 (282.0 examples/sec; 0.454 sec/batch)
2016-02-04 07:16:07.424593: step 162330, loss = 0.28 (248.2 examples/sec; 0.516 sec/batch)
2016-02-04 07:16:12.184819: step 162340, loss = 0.29 (289.3 examples/sec; 0.442 sec/batch)
2016-02-04 07:16:16.910000: step 162350, loss = 0.27 (259.7 examples/sec; 0.493 sec/batch)
2016-02-04 07:16:21.691473: step 162360, loss = 0.36 (269.5 examples/sec; 0.475 sec/batch)
2016-02-04 07:16:26.392913: step 162370, loss = 0.30 (259.2 examples/sec; 0.494 sec/batch)
2016-02-04 07:16:31.129358: step 162380, loss = 0.35 (268.1 examples/sec; 0.477 sec/batch)
2016-02-04 07:16:35.855259: step 162390, loss = 0.25 (256.7 examples/sec; 0.499 sec/batch)
2016-02-04 07:16:40.511802: step 162400, loss = 0.22 (277.9 examples/sec; 0.461 sec/batch)
2016-02-04 07:16:45.814124: step 162410, loss = 0.23 (250.8 examples/sec; 0.510 sec/batch)
2016-02-04 07:16:50.391027: step 162420, loss = 0.25 (309.7 examples/sec; 0.413 sec/batch)
2016-02-04 07:16:55.175026: step 162430, loss = 0.25 (283.0 examples/sec; 0.452 sec/batch)
2016-02-04 07:16:59.941544: step 162440, loss = 0.32 (255.9 examples/sec; 0.500 sec/batch)
2016-02-04 07:17:04.642354: step 162450, loss = 0.25 (284.2 examples/sec; 0.450 sec/batch)
2016-02-04 07:17:09.341370: step 162460, loss = 0.24 (290.7 examples/sec; 0.440 sec/batch)
2016-02-04 07:17:14.079826: step 162470, loss = 0.33 (245.8 examples/sec; 0.521 sec/batch)
2016-02-04 07:17:18.766391: step 162480, loss = 0.26 (282.0 examples/sec; 0.454 sec/batch)
2016-02-04 07:17:23.516628: step 162490, loss = 0.23 (282.3 examples/sec; 0.453 sec/batch)
2016-02-04 07:17:28.258873: step 162500, loss = 0.31 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 07:17:33.525090: step 162510, loss = 0.27 (256.2 examples/sec; 0.500 sec/batch)
2016-02-04 07:17:38.208758: step 162520, loss = 0.26 (274.6 examples/sec; 0.466 sec/batch)
2016-02-04 07:17:42.884300: step 162530, loss = 0.35 (277.1 examples/sec; 0.462 sec/batch)
2016-02-04 07:17:47.601380: step 162540, loss = 0.25 (272.0 examples/sec; 0.471 sec/batch)
2016-02-04 07:17:52.329208: step 162550, loss = 0.25 (264.6 examples/sec; 0.484 sec/batch)
2016-02-04 07:17:57.045388: step 162560, loss = 0.21 (277.1 examples/sec; 0.462 sec/batch)
2016-02-04 07:18:01.810876: step 162570, loss = 0.24 (279.5 examples/sec; 0.458 sec/batch)
2016-02-04 07:18:06.464215: step 162580, loss = 0.35 (290.7 examples/sec; 0.440 sec/batch)
2016-02-04 07:18:11.203367: step 162590, loss = 0.41 (264.2 examples/sec; 0.484 sec/batch)
2016-02-04 07:18:15.884613: step 162600, loss = 0.20 (287.4 examples/sec; 0.445 sec/batch)
2016-02-04 07:18:21.151479: step 162610, loss = 0.26 (272.6 examples/sec; 0.469 sec/batch)
2016-02-04 07:18:25.818146: step 162620, loss = 0.26 (270.7 examples/sec; 0.473 sec/batch)
2016-02-04 07:18:30.565543: step 162630, loss = 0.30 (257.0 examples/sec; 0.498 sec/batch)
2016-02-04 07:18:35.282747: step 162640, loss = 0.31 (257.8 examples/sec; 0.497 sec/batch)
2016-02-04 07:18:40.035369: step 162650, loss = 0.26 (288.5 examples/sec; 0.444 sec/batch)
2016-02-04 07:18:44.746371: step 162660, loss = 0.32 (276.7 examples/sec; 0.463 sec/batch)
2016-02-04 07:18:49.508153: step 162670, loss = 0.32 (240.6 examples/sec; 0.532 sec/batch)
2016-02-04 07:18:54.082302: step 162680, loss = 0.23 (272.5 examples/sec; 0.470 sec/batch)
2016-02-04 07:18:58.801258: step 162690, loss = 0.28 (251.0 examples/sec; 0.510 sec/batch)
2016-02-04 07:19:03.436092: step 162700, loss = 0.32 (259.8 examples/sec; 0.493 sec/batch)
2016-02-04 07:19:08.571112: step 162710, loss = 0.32 (273.5 examples/sec; 0.468 sec/batch)
2016-02-04 07:19:13.267179: step 162720, loss = 0.27 (257.4 examples/sec; 0.497 sec/batch)
2016-02-04 07:19:17.979693: step 162730, loss = 0.22 (288.7 examples/sec; 0.443 sec/batch)
2016-02-04 07:19:22.616037: step 162740, loss = 0.33 (299.3 examples/sec; 0.428 sec/batch)
2016-02-04 07:19:27.249659: step 162750, loss = 0.25 (287.7 examples/sec; 0.445 sec/batch)
2016-02-04 07:19:32.033740: step 162760, loss = 0.22 (235.4 examples/sec; 0.544 sec/batch)
2016-02-04 07:19:36.680034: step 162770, loss = 0.31 (281.9 examples/sec; 0.454 sec/batch)
2016-02-04 07:19:41.307569: step 162780, loss = 0.24 (267.1 examples/sec; 0.479 sec/batch)
2016-02-04 07:19:46.056686: step 162790, loss = 0.24 (278.6 examples/sec; 0.459 sec/batch)
2016-02-04 07:19:50.627834: step 162800, loss = 0.22 (280.3 examples/sec; 0.457 sec/batch)
2016-02-04 07:19:55.863951: step 162810, loss = 0.29 (290.9 examples/sec; 0.440 sec/batch)
2016-02-04 07:20:00.606126: step 162820, loss = 0.23 (263.8 examples/sec; 0.485 sec/batch)
2016-02-04 07:20:05.360947: step 162830, loss = 0.25 (271.7 examples/sec; 0.471 sec/batch)
2016-02-04 07:20:10.022933: step 162840, loss = 0.39 (274.4 examples/sec; 0.466 sec/batch)
2016-02-04 07:20:14.695251: step 162850, loss = 0.28 (278.3 examples/sec; 0.460 sec/batch)
2016-02-04 07:20:19.425802: step 162860, loss = 0.26 (256.8 examples/sec; 0.498 sec/batch)
2016-02-04 07:20:24.113008: step 162870, loss = 0.29 (305.6 examples/sec; 0.419 sec/batch)
2016-02-04 07:20:28.688278: step 162880, loss = 0.32 (252.6 examples/sec; 0.507 sec/batch)
2016-02-04 07:20:33.334230: step 162890, loss = 0.24 (250.7 examples/sec; 0.511 sec/batch)
2016-02-04 07:20:37.990789: step 162900, loss = 0.30 (298.2 examples/sec; 0.429 sec/batch)
2016-02-04 07:20:43.124603: step 162910, loss = 0.31 (266.4 examples/sec; 0.481 sec/batch)
2016-02-04 07:20:47.876481: step 162920, loss = 0.28 (263.5 examples/sec; 0.486 sec/batch)
2016-02-04 07:20:52.609044: step 162930, loss = 0.28 (276.1 examples/sec; 0.464 sec/batch)
2016-02-04 07:20:57.293150: step 162940, loss = 0.35 (260.6 examples/sec; 0.491 sec/batch)
2016-02-04 07:21:02.088262: step 162950, loss = 0.34 (263.2 examples/sec; 0.486 sec/batch)
2016-02-04 07:21:06.892687: step 162960, loss = 0.42 (267.9 examples/sec; 0.478 sec/batch)
2016-02-04 07:21:11.558902: step 162970, loss = 0.29 (294.3 examples/sec; 0.435 sec/batch)
2016-02-04 07:21:16.278346: step 162980, loss = 0.30 (264.7 examples/sec; 0.484 sec/batch)
2016-02-04 07:21:21.045454: step 162990, loss = 0.30 (273.7 examples/sec; 0.468 sec/batch)
2016-02-04 07:21:25.848466: step 163000, loss = 0.31 (273.4 examples/sec; 0.468 sec/batch)
2016-02-04 07:21:31.107363: step 163010, loss = 0.25 (299.2 examples/sec; 0.428 sec/batch)
2016-02-04 07:21:35.890427: step 163020, loss = 0.22 (259.7 examples/sec; 0.493 sec/batch)
2016-02-04 07:21:40.615535: step 163030, loss = 0.36 (280.9 examples/sec; 0.456 sec/batch)
2016-02-04 07:21:45.354181: step 163040, loss = 0.36 (276.0 examples/sec; 0.464 sec/batch)
2016-02-04 07:21:50.010496: step 163050, loss = 0.22 (279.4 examples/sec; 0.458 sec/batch)
2016-02-04 07:21:54.781653: step 163060, loss = 0.22 (280.6 examples/sec; 0.456 sec/batch)
2016-02-04 07:21:59.498101: step 163070, loss = 0.35 (284.5 examples/sec; 0.450 sec/batch)
2016-02-04 07:22:04.219969: step 163080, loss = 0.24 (259.8 examples/sec; 0.493 sec/batch)
2016-02-04 07:22:08.984234: step 163090, loss = 0.29 (257.6 examples/sec; 0.497 sec/batch)
2016-02-04 07:22:13.638197: step 163100, loss = 0.28 (269.2 examples/sec; 0.476 sec/batch)
2016-02-04 07:22:18.962047: step 163110, loss = 0.33 (261.2 examples/sec; 0.490 sec/batch)
2016-02-04 07:22:23.747245: step 163120, loss = 0.28 (248.7 examples/sec; 0.515 sec/batch)
2016-02-04 07:22:28.409005: step 163130, loss = 0.27 (290.2 examples/sec; 0.441 sec/batch)
2016-02-04 07:22:33.027516: step 163140, loss = 0.32 (294.7 examples/sec; 0.434 sec/batch)
2016-02-04 07:22:37.807566: step 163150, loss = 0.35 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 07:22:42.501390: step 163160, loss = 0.31 (275.3 examples/sec; 0.465 sec/batch)
2016-02-04 07:22:47.175950: step 163170, loss = 0.24 (265.6 examples/sec; 0.482 sec/batch)
2016-02-04 07:22:51.833461: step 163180, loss = 0.36 (256.4 examples/sec; 0.499 sec/batch)
2016-02-04 07:22:56.450014: step 163190, loss = 0.22 (281.5 examples/sec; 0.455 sec/batch)
2016-02-04 07:23:01.077694: step 163200, loss = 0.26 (282.0 examples/sec; 0.454 sec/batch)
2016-02-04 07:23:06.165618: step 163210, loss = 0.33 (265.7 examples/sec; 0.482 sec/batch)
2016-02-04 07:23:10.853926: step 163220, loss = 0.28 (273.8 examples/sec; 0.468 sec/batch)
2016-02-04 07:23:15.407328: step 163230, loss = 0.25 (290.7 examples/sec; 0.440 sec/batch)
2016-02-04 07:23:20.123252: step 163240, loss = 0.30 (298.2 examples/sec; 0.429 sec/batch)
2016-02-04 07:23:24.808963: step 163250, loss = 0.29 (300.1 examples/sec; 0.427 sec/batch)
2016-02-04 07:23:29.520744: step 163260, loss = 0.37 (287.0 examples/sec; 0.446 sec/batch)
2016-02-04 07:23:34.104542: step 163270, loss = 0.25 (288.7 examples/sec; 0.443 sec/batch)
2016-02-04 07:23:38.739553: step 163280, loss = 0.29 (294.1 examples/sec; 0.435 sec/batch)
2016-02-04 07:23:43.329318: step 163290, loss = 0.23 (277.1 examples/sec; 0.462 sec/batch)
2016-02-04 07:23:47.829019: step 163300, loss = 0.27 (300.7 examples/sec; 0.426 sec/batch)
2016-02-04 07:23:52.955893: step 163310, loss = 0.27 (271.6 examples/sec; 0.471 sec/batch)
2016-02-04 07:23:57.517405: step 163320, loss = 0.26 (281.9 examples/sec; 0.454 sec/batch)
2016-02-04 07:24:02.217944: step 163330, loss = 0.23 (257.5 examples/sec; 0.497 sec/batch)
2016-02-04 07:24:06.846871: step 163340, loss = 0.28 (287.6 examples/sec; 0.445 sec/batch)
2016-02-04 07:24:11.636921: step 163350, loss = 0.33 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 07:24:16.288193: step 163360, loss = 0.22 (284.9 examples/sec; 0.449 sec/batch)
2016-02-04 07:24:20.938512: step 163370, loss = 0.33 (313.4 examples/sec; 0.408 sec/batch)
2016-02-04 07:24:25.624771: step 163380, loss = 0.36 (269.5 examples/sec; 0.475 sec/batch)
2016-02-04 07:24:30.244516: step 163390, loss = 0.32 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 07:24:34.905082: step 163400, loss = 0.30 (267.7 examples/sec; 0.478 sec/batch)
2016-02-04 07:24:40.205082: step 163410, loss = 0.30 (295.5 examples/sec; 0.433 sec/batch)
2016-02-04 07:24:44.761279: step 163420, loss = 0.28 (259.2 examples/sec; 0.494 sec/batch)
2016-02-04 07:24:49.308598: step 163430, loss = 0.32 (299.3 examples/sec; 0.428 sec/batch)
2016-02-04 07:24:53.816365: step 163440, loss = 0.30 (274.4 examples/sec; 0.467 sec/batch)
2016-02-04 07:24:58.307902: step 163450, loss = 0.26 (281.0 examples/sec; 0.456 sec/batch)
2016-02-04 07:25:02.926069: step 163460, loss = 0.21 (281.1 examples/sec; 0.455 sec/batch)
2016-02-04 07:25:07.573012: step 163470, loss = 0.34 (253.8 examples/sec; 0.504 sec/batch)
2016-02-04 07:25:12.321896: step 163480, loss = 0.30 (265.2 examples/sec; 0.483 sec/batch)
2016-02-04 07:25:16.975779: step 163490, loss = 0.31 (301.1 examples/sec; 0.425 sec/batch)
2016-02-04 07:25:21.364658: step 163500, loss = 0.35 (309.8 examples/sec; 0.413 sec/batch)
2016-02-04 07:25:26.348328: step 163510, loss = 0.29 (281.4 examples/sec; 0.455 sec/batch)
2016-02-04 07:25:30.832992: step 163520, loss = 0.27 (290.6 examples/sec; 0.441 sec/batch)
2016-02-04 07:25:35.455014: step 163530, loss = 0.27 (282.0 examples/sec; 0.454 sec/batch)
2016-02-04 07:25:40.092291: step 163540, loss = 0.28 (271.6 examples/sec; 0.471 sec/batch)
2016-02-04 07:25:44.829623: step 163550, loss = 0.30 (265.7 examples/sec; 0.482 sec/batch)
2016-02-04 07:25:49.531275: step 163560, loss = 0.24 (259.9 examples/sec; 0.492 sec/batch)
2016-02-04 07:25:54.019789: step 163570, loss = 0.31 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 07:25:58.787304: step 163580, loss = 0.26 (260.9 examples/sec; 0.491 sec/batch)
2016-02-04 07:26:03.377198: step 163590, loss = 0.25 (288.6 examples/sec; 0.444 sec/batch)
2016-02-04 07:26:07.961481: step 163600, loss = 0.28 (273.4 examples/sec; 0.468 sec/batch)
2016-02-04 07:26:12.976451: step 163610, loss = 0.19 (296.4 examples/sec; 0.432 sec/batch)
2016-02-04 07:26:17.531783: step 163620, loss = 0.31 (286.2 examples/sec; 0.447 sec/batch)
2016-02-04 07:26:22.194733: step 163630, loss = 0.36 (269.0 examples/sec; 0.476 sec/batch)
2016-02-04 07:26:26.574905: step 163640, loss = 0.26 (286.9 examples/sec; 0.446 sec/batch)
2016-02-04 07:26:31.113583: step 163650, loss = 0.25 (267.2 examples/sec; 0.479 sec/batch)
2016-02-04 07:26:35.576151: step 163660, loss = 0.24 (312.0 examples/sec; 0.410 sec/batch)
2016-02-04 07:26:40.117187: step 163670, loss = 0.23 (275.8 examples/sec; 0.464 sec/batch)
2016-02-04 07:26:44.490648: step 163680, loss = 0.27 (308.0 examples/sec; 0.416 sec/batch)
2016-02-04 07:26:48.926676: step 163690, loss = 0.25 (300.7 examples/sec; 0.426 sec/batch)
2016-02-04 07:26:53.327609: step 163700, loss = 0.37 (257.9 examples/sec; 0.496 sec/batch)
2016-02-04 07:26:58.354644: step 163710, loss = 0.38 (250.2 examples/sec; 0.512 sec/batch)
2016-02-04 07:27:02.899330: step 163720, loss = 0.32 (277.1 examples/sec; 0.462 sec/batch)
2016-02-04 07:27:07.362190: step 163730, loss = 0.27 (312.8 examples/sec; 0.409 sec/batch)
2016-02-04 07:27:11.615315: step 163740, loss = 0.21 (327.4 examples/sec; 0.391 sec/batch)
2016-02-04 07:27:16.163681: step 163750, loss = 0.33 (280.9 examples/sec; 0.456 sec/batch)
2016-02-04 07:27:20.571284: step 163760, loss = 0.21 (318.0 examples/sec; 0.403 sec/batch)
2016-02-04 07:27:25.053526: step 163770, loss = 0.20 (292.7 examples/sec; 0.437 sec/batch)
2016-02-04 07:27:29.461292: step 163780, loss = 0.33 (286.0 examples/sec; 0.448 sec/batch)
2016-02-04 07:27:33.869762: step 163790, loss = 0.32 (305.4 examples/sec; 0.419 sec/batch)
2016-02-04 07:27:38.420266: step 163800, loss = 0.28 (272.9 examples/sec; 0.469 sec/batch)
2016-02-04 07:27:43.407187: step 163810, loss = 0.23 (279.7 examples/sec; 0.458 sec/batch)
2016-02-04 07:27:47.847540: step 163820, loss = 0.30 (307.8 examples/sec; 0.416 sec/batch)
2016-02-04 07:27:52.397829: step 163830, loss = 0.25 (271.2 examples/sec; 0.472 sec/batch)
2016-02-04 07:27:56.800920: step 163840, loss = 0.31 (301.8 examples/sec; 0.424 sec/batch)
2016-02-04 07:28:01.295686: step 163850, loss = 0.32 (267.0 examples/sec; 0.479 sec/batch)
2016-02-04 07:28:05.730023: step 163860, loss = 0.41 (282.9 examples/sec; 0.453 sec/batch)
2016-02-04 07:28:10.029215: step 163870, loss = 0.33 (327.9 examples/sec; 0.390 sec/batch)
2016-02-04 07:28:14.471273: step 163880, loss = 0.27 (280.2 examples/sec; 0.457 sec/batch)
2016-02-04 07:28:18.846389: step 163890, loss = 0.25 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 07:28:23.005279: step 163900, loss = 0.28 (299.2 examples/sec; 0.428 sec/batch)
2016-02-04 07:28:27.348985: step 163910, loss = 0.24 (319.0 examples/sec; 0.401 sec/batch)
2016-02-04 07:28:31.217504: step 163920, loss = 0.21 (290.8 examples/sec; 0.440 sec/batch)
2016-02-04 07:28:35.785123: step 163930, loss = 0.22 (263.2 examples/sec; 0.486 sec/batch)
2016-02-04 07:28:40.180300: step 163940, loss = 0.20 (289.3 examples/sec; 0.442 sec/batch)
2016-02-04 07:28:44.555222: step 163950, loss = 0.33 (269.0 examples/sec; 0.476 sec/batch)
2016-02-04 07:28:49.176860: step 163960, loss = 0.25 (273.5 examples/sec; 0.468 sec/batch)
2016-02-04 07:28:53.677192: step 163970, loss = 0.30 (294.4 examples/sec; 0.435 sec/batch)
2016-02-04 07:28:58.234393: step 163980, loss = 0.30 (269.5 examples/sec; 0.475 sec/batch)
2016-02-04 07:29:02.623213: step 163990, loss = 0.28 (309.0 examples/sec; 0.414 sec/batch)
2016-02-04 07:29:06.929101: step 164000, loss = 0.31 (272.7 examples/sec; 0.469 sec/batch)
2016-02-04 07:29:11.641340: step 164010, loss = 0.29 (255.9 examples/sec; 0.500 sec/batch)
2016-02-04 07:29:15.827114: step 164020, loss = 0.31 (337.8 examples/sec; 0.379 sec/batch)
2016-02-04 07:29:20.051570: step 164030, loss = 0.25 (301.5 examples/sec; 0.425 sec/batch)
2016-02-04 07:29:24.304056: step 164040, loss = 0.29 (338.9 examples/sec; 0.378 sec/batch)
2016-02-04 07:29:28.606369: step 164050, loss = 0.31 (293.3 examples/sec; 0.436 sec/batch)
2016-02-04 07:29:32.829929: step 164060, loss = 0.31 (291.3 examples/sec; 0.439 sec/batch)
2016-02-04 07:29:37.120775: step 164070, loss = 0.31 (283.7 examples/sec; 0.451 sec/batch)
2016-02-04 07:29:41.568764: step 164080, loss = 0.32 (274.6 examples/sec; 0.466 sec/batch)
2016-02-04 07:29:45.885148: step 164090, loss = 0.26 (313.0 examples/sec; 0.409 sec/batch)
2016-02-04 07:29:50.257201: step 164100, loss = 0.26 (307.1 examples/sec; 0.417 sec/batch)
2016-02-04 07:29:55.028105: step 164110, loss = 0.31 (325.5 examples/sec; 0.393 sec/batch)
2016-02-04 07:29:59.501630: step 164120, loss = 0.25 (324.8 examples/sec; 0.394 sec/batch)
2016-02-04 07:30:03.848900: step 164130, loss = 0.26 (330.5 examples/sec; 0.387 sec/batch)
2016-02-04 07:30:08.275683: step 164140, loss = 0.25 (335.2 examples/sec; 0.382 sec/batch)
2016-02-04 07:30:12.647300: step 164150, loss = 0.30 (279.8 examples/sec; 0.457 sec/batch)
2016-02-04 07:30:17.032027: step 164160, loss = 0.22 (323.0 examples/sec; 0.396 sec/batch)
2016-02-04 07:30:21.277276: step 164170, loss = 0.31 (285.4 examples/sec; 0.449 sec/batch)
2016-02-04 07:30:25.627825: step 164180, loss = 0.36 (259.5 examples/sec; 0.493 sec/batch)
2016-02-04 07:30:29.987455: step 164190, loss = 0.27 (286.6 examples/sec; 0.447 sec/batch)
2016-02-04 07:30:34.392712: step 164200, loss = 0.19 (296.0 examples/sec; 0.432 sec/batch)
2016-02-04 07:30:39.492036: step 164210, loss = 0.28 (303.6 examples/sec; 0.422 sec/batch)
2016-02-04 07:30:44.048711: step 164220, loss = 0.23 (311.0 examples/sec; 0.412 sec/batch)
2016-02-04 07:30:48.517573: step 164230, loss = 0.22 (252.3 examples/sec; 0.507 sec/batch)
2016-02-04 07:30:52.927183: step 164240, loss = 0.35 (299.1 examples/sec; 0.428 sec/batch)
2016-02-04 07:30:57.176795: step 164250, loss = 0.32 (333.2 examples/sec; 0.384 sec/batch)
2016-02-04 07:31:01.579957: step 164260, loss = 0.35 (289.0 examples/sec; 0.443 sec/batch)
2016-02-04 07:31:06.092359: step 164270, loss = 0.40 (319.6 examples/sec; 0.401 sec/batch)
2016-02-04 07:31:10.442184: step 164280, loss = 0.26 (281.2 examples/sec; 0.455 sec/batch)
2016-02-04 07:31:14.976834: step 164290, loss = 0.31 (262.2 examples/sec; 0.488 sec/batch)
2016-02-04 07:31:19.445430: step 164300, loss = 0.27 (289.0 examples/sec; 0.443 sec/batch)
2016-02-04 07:31:24.329185: step 164310, loss = 0.25 (295.2 examples/sec; 0.434 sec/batch)
2016-02-04 07:31:28.858685: step 164320, loss = 0.38 (276.5 examples/sec; 0.463 sec/batch)
2016-02-04 07:31:33.481792: step 164330, loss = 0.28 (274.4 examples/sec; 0.466 sec/batch)
2016-02-04 07:31:37.693719: step 164340, loss = 0.38 (290.0 examples/sec; 0.441 sec/batch)
2016-02-04 07:31:42.050938: step 164350, loss = 0.31 (291.2 examples/sec; 0.440 sec/batch)
2016-02-04 07:31:46.546206: step 164360, loss = 0.22 (277.6 examples/sec; 0.461 sec/batch)
2016-02-04 07:31:50.934826: step 164370, loss = 0.24 (263.9 examples/sec; 0.485 sec/batch)
2016-02-04 07:31:55.281513: step 164380, loss = 0.23 (278.1 examples/sec; 0.460 sec/batch)
2016-02-04 07:31:59.509166: step 164390, loss = 0.31 (316.2 examples/sec; 0.405 sec/batch)
2016-02-04 07:32:03.849652: step 164400, loss = 0.25 (269.4 examples/sec; 0.475 sec/batch)
2016-02-04 07:32:08.786950: step 164410, loss = 0.38 (278.1 examples/sec; 0.460 sec/batch)
2016-02-04 07:32:13.084468: step 164420, loss = 0.35 (287.0 examples/sec; 0.446 sec/batch)
2016-02-04 07:32:17.422610: step 164430, loss = 0.24 (318.6 examples/sec; 0.402 sec/batch)
2016-02-04 07:32:21.750144: step 164440, loss = 0.31 (277.2 examples/sec; 0.462 sec/batch)
2016-02-04 07:32:26.130468: step 164450, loss = 0.29 (280.7 examples/sec; 0.456 sec/batch)
2016-02-04 07:32:30.502744: step 164460, loss = 0.32 (273.9 examples/sec; 0.467 sec/batch)
2016-02-04 07:32:34.891987: step 164470, loss = 0.24 (283.8 examples/sec; 0.451 sec/batch)
2016-02-04 07:32:39.343886: step 164480, loss = 0.21 (329.4 examples/sec; 0.389 sec/batch)
2016-02-04 07:32:43.744235: step 164490, loss = 0.26 (304.3 examples/sec; 0.421 sec/batch)
2016-02-04 07:32:48.142417: step 164500, loss = 0.27 (270.8 examples/sec; 0.473 sec/batch)
2016-02-04 07:32:52.978666: step 164510, loss = 0.27 (301.4 examples/sec; 0.425 sec/batch)
2016-02-04 07:32:57.408262: step 164520, loss = 0.27 (294.4 examples/sec; 0.435 sec/batch)
2016-02-04 07:33:01.735486: step 164530, loss = 0.32 (289.7 examples/sec; 0.442 sec/batch)
2016-02-04 07:33:06.069013: step 164540, loss = 0.29 (312.1 examples/sec; 0.410 sec/batch)
2016-02-04 07:33:10.496276: step 164550, loss = 0.26 (315.6 examples/sec; 0.406 sec/batch)
2016-02-04 07:33:14.813533: step 164560, loss = 0.35 (299.8 examples/sec; 0.427 sec/batch)
2016-02-04 07:33:19.341062: step 164570, loss = 0.41 (320.2 examples/sec; 0.400 sec/batch)
2016-02-04 07:33:23.860825: step 164580, loss = 0.27 (286.8 examples/sec; 0.446 sec/batch)
2016-02-04 07:33:28.120898: step 164590, loss = 0.37 (340.0 examples/sec; 0.376 sec/batch)
2016-02-04 07:33:32.381061: step 164600, loss = 0.27 (301.4 examples/sec; 0.425 sec/batch)
2016-02-04 07:33:37.166221: step 164610, loss = 0.40 (300.2 examples/sec; 0.426 sec/batch)
2016-02-04 07:33:41.555710: step 164620, loss = 0.23 (313.2 examples/sec; 0.409 sec/batch)
2016-02-04 07:33:45.868884: step 164630, loss = 0.26 (305.9 examples/sec; 0.418 sec/batch)
2016-02-04 07:33:50.424385: step 164640, loss = 0.36 (333.1 examples/sec; 0.384 sec/batch)
2016-02-04 07:33:54.821316: step 164650, loss = 0.29 (282.0 examples/sec; 0.454 sec/batch)
2016-02-04 07:33:59.090277: step 164660, loss = 0.23 (294.7 examples/sec; 0.434 sec/batch)
2016-02-04 07:34:03.443826: step 164670, loss = 0.26 (290.9 examples/sec; 0.440 sec/batch)
2016-02-04 07:34:07.826150: step 164680, loss = 0.28 (259.8 examples/sec; 0.493 sec/batch)
2016-02-04 07:34:12.164450: step 164690, loss = 0.32 (297.4 examples/sec; 0.430 sec/batch)
2016-02-04 07:34:16.649471: step 164700, loss = 0.35 (304.5 examples/sec; 0.420 sec/batch)
2016-02-04 07:34:21.349876: step 164710, loss = 0.30 (273.7 examples/sec; 0.468 sec/batch)
2016-02-04 07:34:25.409069: step 164720, loss = 0.25 (334.4 examples/sec; 0.383 sec/batch)
2016-02-04 07:34:29.745046: step 164730, loss = 0.32 (258.0 examples/sec; 0.496 sec/batch)
2016-02-04 07:34:34.183540: step 164740, loss = 0.25 (289.9 examples/sec; 0.442 sec/batch)
2016-02-04 07:34:38.568452: step 164750, loss = 0.29 (272.2 examples/sec; 0.470 sec/batch)
2016-02-04 07:34:42.889439: step 164760, loss = 0.28 (318.5 examples/sec; 0.402 sec/batch)
2016-02-04 07:34:47.369557: step 164770, loss = 0.27 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 07:34:51.770462: step 164780, loss = 0.26 (291.3 examples/sec; 0.439 sec/batch)
2016-02-04 07:34:56.195978: step 164790, loss = 0.24 (280.7 examples/sec; 0.456 sec/batch)
2016-02-04 07:35:00.615549: step 164800, loss = 0.28 (271.0 examples/sec; 0.472 sec/batch)
2016-02-04 07:35:05.469217: step 164810, loss = 0.26 (258.6 examples/sec; 0.495 sec/batch)
2016-02-04 07:35:10.028115: step 164820, loss = 0.25 (275.5 examples/sec; 0.465 sec/batch)
2016-02-04 07:35:14.280160: step 164830, loss = 0.23 (282.4 examples/sec; 0.453 sec/batch)
2016-02-04 07:35:18.697021: step 164840, loss = 0.28 (310.3 examples/sec; 0.412 sec/batch)
2016-02-04 07:35:23.293134: step 164850, loss = 0.29 (257.6 examples/sec; 0.497 sec/batch)
2016-02-04 07:35:27.590160: step 164860, loss = 0.21 (301.5 examples/sec; 0.425 sec/batch)
2016-02-04 07:35:31.959328: step 164870, loss = 0.29 (273.9 examples/sec; 0.467 sec/batch)
2016-02-04 07:35:36.191890: step 164880, loss = 0.30 (350.6 examples/sec; 0.365 sec/batch)
2016-02-04 07:35:40.610386: step 164890, loss = 0.33 (320.2 examples/sec; 0.400 sec/batch)
2016-02-04 07:35:44.984139: step 164900, loss = 0.31 (313.4 examples/sec; 0.408 sec/batch)
2016-02-04 07:35:49.842260: step 164910, loss = 0.31 (321.5 examples/sec; 0.398 sec/batch)
2016-02-04 07:35:54.077681: step 164920, loss = 0.33 (271.7 examples/sec; 0.471 sec/batch)
2016-02-04 07:35:58.542794: step 164930, loss = 0.30 (295.5 examples/sec; 0.433 sec/batch)
2016-02-04 07:36:03.064098: step 164940, loss = 0.23 (261.5 examples/sec; 0.490 sec/batch)
2016-02-04 07:36:07.547946: step 164950, loss = 0.38 (301.9 examples/sec; 0.424 sec/batch)
2016-02-04 07:36:11.967422: step 164960, loss = 0.31 (272.2 examples/sec; 0.470 sec/batch)
2016-02-04 07:36:16.358955: step 164970, loss = 0.26 (277.5 examples/sec; 0.461 sec/batch)
2016-02-04 07:36:20.647416: step 164980, loss = 0.26 (299.9 examples/sec; 0.427 sec/batch)
2016-02-04 07:36:25.019187: step 164990, loss = 0.26 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 07:36:29.584739: step 165000, loss = 0.29 (293.4 examples/sec; 0.436 sec/batch)
2016-02-04 07:36:34.382417: step 165010, loss = 0.29 (280.3 examples/sec; 0.457 sec/batch)
2016-02-04 07:36:38.579580: step 165020, loss = 0.28 (272.0 examples/sec; 0.471 sec/batch)
2016-02-04 07:36:43.071086: step 165030, loss = 0.28 (271.5 examples/sec; 0.472 sec/batch)
2016-02-04 07:36:47.547140: step 165040, loss = 0.32 (274.1 examples/sec; 0.467 sec/batch)
2016-02-04 07:36:51.948695: step 165050, loss = 0.30 (286.0 examples/sec; 0.448 sec/batch)
2016-02-04 07:36:56.359598: step 165060, loss = 0.23 (281.9 examples/sec; 0.454 sec/batch)
2016-02-04 07:37:00.662540: step 165070, loss = 0.28 (273.3 examples/sec; 0.468 sec/batch)
2016-02-04 07:37:04.952176: step 165080, loss = 0.33 (307.7 examples/sec; 0.416 sec/batch)
2016-02-04 07:37:09.582745: step 165090, loss = 0.28 (288.1 examples/sec; 0.444 sec/batch)
2016-02-04 07:37:13.870978: step 165100, loss = 0.26 (297.8 examples/sec; 0.430 sec/batch)
2016-02-04 07:37:18.901384: step 165110, loss = 0.22 (272.3 examples/sec; 0.470 sec/batch)
2016-02-04 07:37:23.198685: step 165120, loss = 0.20 (299.9 examples/sec; 0.427 sec/batch)
2016-02-04 07:37:27.571495: step 165130, loss = 0.19 (300.4 examples/sec; 0.426 sec/batch)
2016-02-04 07:37:31.946583: step 165140, loss = 0.25 (285.0 examples/sec; 0.449 sec/batch)
2016-02-04 07:37:36.334408: step 165150, loss = 0.27 (318.1 examples/sec; 0.402 sec/batch)
2016-02-04 07:37:40.865012: step 165160, loss = 0.19 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 07:37:45.267121: step 165170, loss = 0.27 (287.8 examples/sec; 0.445 sec/batch)
2016-02-04 07:37:49.670208: step 165180, loss = 0.33 (306.4 examples/sec; 0.418 sec/batch)
2016-02-04 07:37:54.143389: step 165190, loss = 0.24 (318.8 examples/sec; 0.402 sec/batch)
2016-02-04 07:37:58.549032: step 165200, loss = 0.29 (287.2 examples/sec; 0.446 sec/batch)
2016-02-04 07:38:03.378979: step 165210, loss = 0.28 (282.5 examples/sec; 0.453 sec/batch)
2016-02-04 07:38:07.449852: step 165220, loss = 0.22 (305.8 examples/sec; 0.419 sec/batch)
2016-02-04 07:38:11.960836: step 165230, loss = 0.32 (269.7 examples/sec; 0.475 sec/batch)
2016-02-04 07:38:16.169246: step 165240, loss = 0.20 (296.0 examples/sec; 0.432 sec/batch)
2016-02-04 07:38:20.381822: step 165250, loss = 0.30 (265.1 examples/sec; 0.483 sec/batch)
2016-02-04 07:38:24.897350: step 165260, loss = 0.24 (284.1 examples/sec; 0.451 sec/batch)
2016-02-04 07:38:29.325674: step 165270, loss = 0.29 (333.6 examples/sec; 0.384 sec/batch)
2016-02-04 07:38:33.579618: step 165280, loss = 0.27 (298.4 examples/sec; 0.429 sec/batch)
2016-02-04 07:38:37.996538: step 165290, loss = 0.24 (296.4 examples/sec; 0.432 sec/batch)
2016-02-04 07:38:42.453705: step 165300, loss = 0.24 (281.4 examples/sec; 0.455 sec/batch)
2016-02-04 07:38:47.355911: step 165310, loss = 0.27 (258.7 examples/sec; 0.495 sec/batch)
2016-02-04 07:38:51.803012: step 165320, loss = 0.31 (270.9 examples/sec; 0.472 sec/batch)
2016-02-04 07:38:56.245440: step 165330, loss = 0.30 (291.2 examples/sec; 0.439 sec/batch)
2016-02-04 07:39:00.620630: step 165340, loss = 0.32 (307.9 examples/sec; 0.416 sec/batch)
2016-02-04 07:39:05.037800: step 165350, loss = 0.28 (274.7 examples/sec; 0.466 sec/batch)
2016-02-04 07:39:09.406524: step 165360, loss = 0.29 (300.2 examples/sec; 0.426 sec/batch)
2016-02-04 07:39:13.880403: step 165370, loss = 0.28 (283.0 examples/sec; 0.452 sec/batch)
2016-02-04 07:39:18.300193: step 165380, loss = 0.29 (258.3 examples/sec; 0.495 sec/batch)
2016-02-04 07:39:22.681509: step 165390, loss = 0.28 (286.1 examples/sec; 0.447 sec/batch)
2016-02-04 07:39:27.098028: step 165400, loss = 0.24 (271.2 examples/sec; 0.472 sec/batch)
2016-02-04 07:39:32.008104: step 165410, loss = 0.21 (267.8 examples/sec; 0.478 sec/batch)
2016-02-04 07:39:36.279472: step 165420, loss = 0.31 (287.8 examples/sec; 0.445 sec/batch)
2016-02-04 07:39:40.667549: step 165430, loss = 0.23 (275.7 examples/sec; 0.464 sec/batch)
2016-02-04 07:39:44.881762: step 165440, loss = 0.25 (316.8 examples/sec; 0.404 sec/batch)
2016-02-04 07:39:49.324904: step 165450, loss = 0.26 (289.8 examples/sec; 0.442 sec/batch)
2016-02-04 07:39:53.881207: step 165460, loss = 0.28 (273.0 examples/sec; 0.469 sec/batch)
2016-02-04 07:39:58.121570: step 165470, loss = 0.24 (301.0 examples/sec; 0.425 sec/batch)
2016-02-04 07:40:02.433486: step 165480, loss = 0.23 (310.0 examples/sec; 0.413 sec/batch)
2016-02-04 07:40:06.790209: step 165490, loss = 0.39 (327.5 examples/sec; 0.391 sec/batch)
2016-02-04 07:40:11.346559: step 165500, loss = 0.29 (270.3 examples/sec; 0.473 sec/batch)
2016-02-04 07:40:16.448540: step 165510, loss = 0.23 (301.8 examples/sec; 0.424 sec/batch)
2016-02-04 07:40:21.058723: step 165520, loss = 0.30 (261.5 examples/sec; 0.490 sec/batch)
2016-02-04 07:40:25.411272: step 165530, loss = 0.31 (297.3 examples/sec; 0.431 sec/batch)
2016-02-04 07:40:29.670449: step 165540, loss = 0.32 (295.3 examples/sec; 0.434 sec/batch)
2016-02-04 07:40:34.131434: step 165550, loss = 0.32 (269.0 examples/sec; 0.476 sec/batch)
2016-02-04 07:40:38.467296: step 165560, loss = 0.24 (291.2 examples/sec; 0.440 sec/batch)
2016-02-04 07:40:42.762238: step 165570, loss = 0.35 (336.2 examples/sec; 0.381 sec/batch)
2016-02-04 07:40:47.082518: step 165580, loss = 0.28 (290.2 examples/sec; 0.441 sec/batch)
2016-02-04 07:40:51.535755: step 165590, loss = 0.26 (297.4 examples/sec; 0.430 sec/batch)
2016-02-04 07:40:55.859693: step 165600, loss = 0.24 (286.3 examples/sec; 0.447 sec/batch)
2016-02-04 07:41:00.469018: step 165610, loss = 0.28 (332.2 examples/sec; 0.385 sec/batch)
2016-02-04 07:41:04.829447: step 165620, loss = 0.21 (296.4 examples/sec; 0.432 sec/batch)
2016-02-04 07:41:09.191209: step 165630, loss = 0.30 (290.2 examples/sec; 0.441 sec/batch)
2016-02-04 07:41:13.671348: step 165640, loss = 0.25 (294.2 examples/sec; 0.435 sec/batch)
2016-02-04 07:41:18.077466: step 165650, loss = 0.22 (325.2 examples/sec; 0.394 sec/batch)
2016-02-04 07:41:22.458263: step 165660, loss = 0.31 (275.0 examples/sec; 0.465 sec/batch)
2016-02-04 07:41:26.960235: step 165670, loss = 0.32 (307.3 examples/sec; 0.416 sec/batch)
2016-02-04 07:41:31.450219: step 165680, loss = 0.21 (279.6 examples/sec; 0.458 sec/batch)
2016-02-04 07:41:35.865728: step 165690, loss = 0.24 (269.7 examples/sec; 0.475 sec/batch)
2016-02-04 07:41:40.397014: step 165700, loss = 0.29 (258.7 examples/sec; 0.495 sec/batch)
2016-02-04 07:41:45.287332: step 165710, loss = 0.28 (282.2 examples/sec; 0.454 sec/batch)
2016-02-04 07:41:49.759386: step 165720, loss = 0.19 (279.9 examples/sec; 0.457 sec/batch)
2016-02-04 07:41:54.142930: step 165730, loss = 0.31 (281.8 examples/sec; 0.454 sec/batch)
2016-02-04 07:41:58.468647: step 165740, loss = 0.30 (291.8 examples/sec; 0.439 sec/batch)
2016-02-04 07:42:03.090537: step 165750, loss = 0.24 (313.6 examples/sec; 0.408 sec/batch)
2016-02-04 07:42:07.401074: step 165760, loss = 0.26 (297.6 examples/sec; 0.430 sec/batch)
2016-02-04 07:42:11.763748: step 165770, loss = 0.23 (345.4 examples/sec; 0.371 sec/batch)
2016-02-04 07:42:16.155743: step 165780, loss = 0.18 (296.9 examples/sec; 0.431 sec/batch)
2016-02-04 07:42:20.517550: step 165790, loss = 0.28 (277.1 examples/sec; 0.462 sec/batch)
2016-02-04 07:42:24.841365: step 165800, loss = 0.34 (253.7 examples/sec; 0.505 sec/batch)
2016-02-04 07:42:29.754608: step 165810, loss = 0.33 (267.5 examples/sec; 0.478 sec/batch)
2016-02-04 07:42:33.946475: step 165820, loss = 0.28 (290.0 examples/sec; 0.441 sec/batch)
2016-02-04 07:42:38.448239: step 165830, loss = 0.30 (309.4 examples/sec; 0.414 sec/batch)
2016-02-04 07:42:42.774649: step 165840, loss = 0.26 (290.8 examples/sec; 0.440 sec/batch)
2016-02-04 07:42:47.265073: step 165850, loss = 0.23 (260.7 examples/sec; 0.491 sec/batch)
2016-02-04 07:42:51.488653: step 165860, loss = 0.25 (299.3 examples/sec; 0.428 sec/batch)
2016-02-04 07:42:56.034398: step 165870, loss = 0.28 (318.5 examples/sec; 0.402 sec/batch)
2016-02-04 07:43:00.355791: step 165880, loss = 0.26 (282.0 examples/sec; 0.454 sec/batch)
2016-02-04 07:43:04.697226: step 165890, loss = 0.35 (323.0 examples/sec; 0.396 sec/batch)
2016-02-04 07:43:09.144077: step 165900, loss = 0.34 (283.4 examples/sec; 0.452 sec/batch)
2016-02-04 07:43:14.003917: step 165910, loss = 0.30 (315.2 examples/sec; 0.406 sec/batch)
2016-02-04 07:43:18.186240: step 165920, loss = 0.25 (329.1 examples/sec; 0.389 sec/batch)
2016-02-04 07:43:22.583280: step 165930, loss = 0.26 (279.3 examples/sec; 0.458 sec/batch)
2016-02-04 07:43:26.974779: step 165940, loss = 0.26 (340.1 examples/sec; 0.376 sec/batch)
2016-02-04 07:43:31.545029: step 165950, loss = 0.26 (267.9 examples/sec; 0.478 sec/batch)
2016-02-04 07:43:35.853496: step 165960, loss = 0.28 (290.3 examples/sec; 0.441 sec/batch)
2016-02-04 07:43:40.372717: step 165970, loss = 0.24 (244.3 examples/sec; 0.524 sec/batch)
2016-02-04 07:43:44.715662: step 165980, loss = 0.31 (313.2 examples/sec; 0.409 sec/batch)
2016-02-04 07:43:49.145705: step 165990, loss = 0.28 (299.3 examples/sec; 0.428 sec/batch)
2016-02-04 07:43:53.682423: step 166000, loss = 0.21 (271.4 examples/sec; 0.472 sec/batch)
2016-02-04 07:43:58.539580: step 166010, loss = 0.26 (300.2 examples/sec; 0.426 sec/batch)
2016-02-04 07:44:02.942708: step 166020, loss = 0.26 (290.1 examples/sec; 0.441 sec/batch)
2016-02-04 07:44:07.272718: step 166030, loss = 0.32 (298.6 examples/sec; 0.429 sec/batch)
2016-02-04 07:44:11.874820: step 166040, loss = 0.28 (278.5 examples/sec; 0.460 sec/batch)
2016-02-04 07:44:16.271905: step 166050, loss = 0.25 (312.8 examples/sec; 0.409 sec/batch)
2016-02-04 07:44:20.665920: step 166060, loss = 0.42 (287.0 examples/sec; 0.446 sec/batch)
2016-02-04 07:44:25.247278: step 166070, loss = 0.29 (273.4 examples/sec; 0.468 sec/batch)
2016-02-04 07:44:29.900672: step 166080, loss = 0.27 (261.8 examples/sec; 0.489 sec/batch)
2016-02-04 07:44:34.295358: step 166090, loss = 0.19 (292.0 examples/sec; 0.438 sec/batch)
2016-02-04 07:44:38.639372: step 166100, loss = 0.29 (269.0 examples/sec; 0.476 sec/batch)
2016-02-04 07:44:43.479457: step 166110, loss = 0.26 (277.7 examples/sec; 0.461 sec/batch)
2016-02-04 07:44:47.953397: step 166120, loss = 0.31 (285.7 examples/sec; 0.448 sec/batch)
2016-02-04 07:44:52.420085: step 166130, loss = 0.26 (270.4 examples/sec; 0.473 sec/batch)
2016-02-04 07:44:56.792332: step 166140, loss = 0.28 (252.4 examples/sec; 0.507 sec/batch)
2016-02-04 07:45:01.418750: step 166150, loss = 0.31 (266.7 examples/sec; 0.480 sec/batch)
2016-02-04 07:45:05.949162: step 166160, loss = 0.32 (261.7 examples/sec; 0.489 sec/batch)
2016-02-04 07:45:10.430806: step 166170, loss = 0.23 (321.1 examples/sec; 0.399 sec/batch)
2016-02-04 07:45:14.879702: step 166180, loss = 0.21 (286.8 examples/sec; 0.446 sec/batch)
2016-02-04 07:45:19.299306: step 166190, loss = 0.31 (287.3 examples/sec; 0.446 sec/batch)
2016-02-04 07:45:23.851169: step 166200, loss = 0.26 (287.4 examples/sec; 0.445 sec/batch)
2016-02-04 07:45:28.747394: step 166210, loss = 0.39 (288.8 examples/sec; 0.443 sec/batch)
2016-02-04 07:45:33.090763: step 166220, loss = 0.27 (273.8 examples/sec; 0.468 sec/batch)
2016-02-04 07:45:37.483936: step 166230, loss = 0.29 (298.9 examples/sec; 0.428 sec/batch)
2016-02-04 07:45:41.764599: step 166240, loss = 0.26 (315.9 examples/sec; 0.405 sec/batch)
2016-02-04 07:45:46.144096: step 166250, loss = 0.27 (299.4 examples/sec; 0.427 sec/batch)
2016-02-04 07:45:50.602309: step 166260, loss = 0.29 (290.8 examples/sec; 0.440 sec/batch)
2016-02-04 07:45:55.075914: step 166270, loss = 0.27 (286.1 examples/sec; 0.447 sec/batch)
2016-02-04 07:45:59.485860: step 166280, loss = 0.22 (284.6 examples/sec; 0.450 sec/batch)
2016-02-04 07:46:03.795481: step 166290, loss = 0.24 (300.1 examples/sec; 0.427 sec/batch)
2016-02-04 07:46:08.194959: step 166300, loss = 0.30 (265.0 examples/sec; 0.483 sec/batch)
2016-02-04 07:46:13.143113: step 166310, loss = 0.36 (278.8 examples/sec; 0.459 sec/batch)
2016-02-04 07:46:17.435327: step 166320, loss = 0.26 (273.7 examples/sec; 0.468 sec/batch)
2016-02-04 07:46:21.690013: step 166330, loss = 0.37 (263.2 examples/sec; 0.486 sec/batch)
2016-02-04 07:46:26.135355: step 166340, loss = 0.31 (303.5 examples/sec; 0.422 sec/batch)
2016-02-04 07:46:30.388466: step 166350, loss = 0.30 (323.0 examples/sec; 0.396 sec/batch)
2016-02-04 07:46:34.853508: step 166360, loss = 0.31 (279.8 examples/sec; 0.457 sec/batch)
2016-02-04 07:46:39.119246: step 166370, loss = 0.25 (349.3 examples/sec; 0.366 sec/batch)
2016-02-04 07:46:43.567407: step 166380, loss = 0.23 (347.2 examples/sec; 0.369 sec/batch)
2016-02-04 07:46:47.770870: step 166390, loss = 0.27 (301.9 examples/sec; 0.424 sec/batch)
2016-02-04 07:46:52.159406: step 166400, loss = 0.28 (292.1 examples/sec; 0.438 sec/batch)
2016-02-04 07:46:56.794592: step 166410, loss = 0.21 (297.7 examples/sec; 0.430 sec/batch)
2016-02-04 07:47:01.426292: step 166420, loss = 0.25 (253.6 examples/sec; 0.505 sec/batch)
2016-02-04 07:47:05.768927: step 166430, loss = 0.27 (300.5 examples/sec; 0.426 sec/batch)
2016-02-04 07:47:10.024996: step 166440, loss = 0.25 (261.2 examples/sec; 0.490 sec/batch)
2016-02-04 07:47:14.476058: step 166450, loss = 0.28 (318.7 examples/sec; 0.402 sec/batch)
2016-02-04 07:47:18.915674: step 166460, loss = 0.29 (293.0 examples/sec; 0.437 sec/batch)
2016-02-04 07:47:23.180211: step 166470, loss = 0.26 (339.8 examples/sec; 0.377 sec/batch)
2016-02-04 07:47:27.513183: step 166480, loss = 0.32 (279.5 examples/sec; 0.458 sec/batch)
2016-02-04 07:47:31.898258: step 166490, loss = 0.32 (291.2 examples/sec; 0.440 sec/batch)
2016-02-04 07:47:36.225095: step 166500, loss = 0.25 (267.2 examples/sec; 0.479 sec/batch)
2016-02-04 07:47:40.859811: step 166510, loss = 0.32 (295.2 examples/sec; 0.434 sec/batch)
2016-02-04 07:47:45.117488: step 166520, loss = 0.26 (308.5 examples/sec; 0.415 sec/batch)
2016-02-04 07:47:49.400909: step 166530, loss = 0.31 (294.5 examples/sec; 0.435 sec/batch)
2016-02-04 07:47:53.750945: step 166540, loss = 0.37 (297.1 examples/sec; 0.431 sec/batch)
2016-02-04 07:47:57.994246: step 166550, loss = 0.26 (315.5 examples/sec; 0.406 sec/batch)
2016-02-04 07:48:02.385382: step 166560, loss = 0.29 (319.9 examples/sec; 0.400 sec/batch)
2016-02-04 07:48:06.718060: step 166570, loss = 0.29 (318.2 examples/sec; 0.402 sec/batch)
2016-02-04 07:48:11.119690: step 166580, loss = 0.22 (285.0 examples/sec; 0.449 sec/batch)
2016-02-04 07:48:15.641052: step 166590, loss = 0.23 (321.2 examples/sec; 0.398 sec/batch)
2016-02-04 07:48:20.046513: step 166600, loss = 0.24 (286.9 examples/sec; 0.446 sec/batch)
2016-02-04 07:48:24.699830: step 166610, loss = 0.30 (293.6 examples/sec; 0.436 sec/batch)
2016-02-04 07:48:29.230599: step 166620, loss = 0.25 (271.4 examples/sec; 0.472 sec/batch)
2016-02-04 07:48:33.706232: step 166630, loss = 0.22 (277.7 examples/sec; 0.461 sec/batch)
2016-02-04 07:48:38.064573: step 166640, loss = 0.23 (271.1 examples/sec; 0.472 sec/batch)
2016-02-04 07:48:42.476690: step 166650, loss = 0.24 (279.1 examples/sec; 0.459 sec/batch)
2016-02-04 07:48:46.763302: step 166660, loss = 0.27 (293.8 examples/sec; 0.436 sec/batch)
2016-02-04 07:48:51.186173: step 166670, loss = 0.26 (283.9 examples/sec; 0.451 sec/batch)
2016-02-04 07:48:55.506922: step 166680, loss = 0.28 (274.0 examples/sec; 0.467 sec/batch)
2016-02-04 07:48:59.918303: step 166690, loss = 0.28 (328.4 examples/sec; 0.390 sec/batch)
2016-02-04 07:49:04.265093: step 166700, loss = 0.26 (298.9 examples/sec; 0.428 sec/batch)
2016-02-04 07:49:09.166622: step 166710, loss = 0.24 (287.3 examples/sec; 0.445 sec/batch)
2016-02-04 07:49:13.538185: step 166720, loss = 0.24 (266.2 examples/sec; 0.481 sec/batch)
2016-02-04 07:49:17.896386: step 166730, loss = 0.25 (303.6 examples/sec; 0.422 sec/batch)
2016-02-04 07:49:22.373691: step 166740, loss = 0.36 (257.5 examples/sec; 0.497 sec/batch)
2016-02-04 07:49:26.495040: step 166750, loss = 0.25 (322.7 examples/sec; 0.397 sec/batch)
2016-02-04 07:49:31.003604: step 166760, loss = 0.21 (294.1 examples/sec; 0.435 sec/batch)
2016-02-04 07:49:35.215678: step 166770, loss = 0.24 (290.0 examples/sec; 0.441 sec/batch)
2016-02-04 07:49:39.485570: step 166780, loss = 0.35 (288.8 examples/sec; 0.443 sec/batch)
2016-02-04 07:49:43.816125: step 166790, loss = 0.26 (263.0 examples/sec; 0.487 sec/batch)
2016-02-04 07:49:48.137954: step 166800, loss = 0.31 (259.8 examples/sec; 0.493 sec/batch)
2016-02-04 07:49:52.826691: step 166810, loss = 0.24 (282.7 examples/sec; 0.453 sec/batch)
2016-02-04 07:49:57.206920: step 166820, loss = 0.23 (295.8 examples/sec; 0.433 sec/batch)
2016-02-04 07:50:01.681826: step 166830, loss = 0.29 (306.2 examples/sec; 0.418 sec/batch)
2016-02-04 07:50:06.103298: step 166840, loss = 0.30 (271.3 examples/sec; 0.472 sec/batch)
2016-02-04 07:50:10.513394: step 166850, loss = 0.21 (328.1 examples/sec; 0.390 sec/batch)
2016-02-04 07:50:15.005435: step 166860, loss = 0.28 (277.5 examples/sec; 0.461 sec/batch)
2016-02-04 07:50:19.581788: step 166870, loss = 0.34 (281.3 examples/sec; 0.455 sec/batch)
2016-02-04 07:50:24.115574: step 166880, loss = 0.29 (303.4 examples/sec; 0.422 sec/batch)
2016-02-04 07:50:28.472421: step 166890, loss = 0.31 (289.3 examples/sec; 0.442 sec/batch)
2016-02-04 07:50:32.933098: step 166900, loss = 0.29 (295.6 examples/sec; 0.433 sec/batch)
2016-02-04 07:50:37.763697: step 166910, loss = 0.22 (319.0 examples/sec; 0.401 sec/batch)
2016-02-04 07:50:42.177456: step 166920, loss = 0.29 (319.4 examples/sec; 0.401 sec/batch)
2016-02-04 07:50:46.678383: step 166930, loss = 0.28 (289.9 examples/sec; 0.442 sec/batch)
2016-02-04 07:50:51.241462: step 166940, loss = 0.20 (323.8 examples/sec; 0.395 sec/batch)
2016-02-04 07:50:55.536317: step 166950, loss = 0.29 (303.2 examples/sec; 0.422 sec/batch)
2016-02-04 07:50:59.921635: step 166960, loss = 0.29 (306.9 examples/sec; 0.417 sec/batch)
2016-02-04 07:51:04.523896: step 166970, loss = 0.24 (257.7 examples/sec; 0.497 sec/batch)
2016-02-04 07:51:08.929112: step 166980, loss = 0.27 (276.0 examples/sec; 0.464 sec/batch)
2016-02-04 07:51:13.466822: step 166990, loss = 0.22 (290.4 examples/sec; 0.441 sec/batch)
2016-02-04 07:51:17.843735: step 167000, loss = 0.21 (276.6 examples/sec; 0.463 sec/batch)
2016-02-04 07:51:22.784466: step 167010, loss = 0.32 (271.2 examples/sec; 0.472 sec/batch)
2016-02-04 07:51:27.043177: step 167020, loss = 0.28 (309.0 examples/sec; 0.414 sec/batch)
2016-02-04 07:51:31.391839: step 167030, loss = 0.29 (260.0 examples/sec; 0.492 sec/batch)
2016-02-04 07:51:35.884072: step 167040, loss = 0.23 (303.6 examples/sec; 0.422 sec/batch)
2016-02-04 07:51:40.445258: step 167050, loss = 0.20 (279.6 examples/sec; 0.458 sec/batch)
2016-02-04 07:51:44.939649: step 167060, loss = 0.22 (271.1 examples/sec; 0.472 sec/batch)
2016-02-04 07:51:49.290333: step 167070, loss = 0.24 (294.7 examples/sec; 0.434 sec/batch)
2016-02-04 07:51:53.618454: step 167080, loss = 0.32 (279.6 examples/sec; 0.458 sec/batch)
2016-02-04 07:51:58.227742: step 167090, loss = 0.33 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 07:52:02.606581: step 167100, loss = 0.26 (298.3 examples/sec; 0.429 sec/batch)
2016-02-04 07:52:07.589184: step 167110, loss = 0.28 (287.4 examples/sec; 0.445 sec/batch)
2016-02-04 07:52:11.818080: step 167120, loss = 0.26 (294.7 examples/sec; 0.434 sec/batch)
2016-02-04 07:52:16.136617: step 167130, loss = 0.41 (278.1 examples/sec; 0.460 sec/batch)
2016-02-04 07:52:20.551626: step 167140, loss = 0.25 (272.6 examples/sec; 0.470 sec/batch)
2016-02-04 07:52:25.118532: step 167150, loss = 0.30 (269.3 examples/sec; 0.475 sec/batch)
2016-02-04 07:52:29.496349: step 167160, loss = 0.25 (326.5 examples/sec; 0.392 sec/batch)
2016-02-04 07:52:33.967300: step 167170, loss = 0.23 (282.3 examples/sec; 0.453 sec/batch)
2016-02-04 07:52:38.319900: step 167180, loss = 0.27 (278.9 examples/sec; 0.459 sec/batch)
2016-02-04 07:52:42.683602: step 167190, loss = 0.28 (316.4 examples/sec; 0.404 sec/batch)
2016-02-04 07:52:47.143341: step 167200, loss = 0.30 (305.7 examples/sec; 0.419 sec/batch)
2016-02-04 07:52:52.053642: step 167210, loss = 0.29 (310.1 examples/sec; 0.413 sec/batch)
2016-02-04 07:52:56.582691: step 167220, loss = 0.24 (285.5 examples/sec; 0.448 sec/batch)
2016-02-04 07:53:00.908690: step 167230, loss = 0.25 (313.1 examples/sec; 0.409 sec/batch)
2016-02-04 07:53:05.358173: step 167240, loss = 0.27 (274.5 examples/sec; 0.466 sec/batch)
2016-02-04 07:53:09.636198: step 167250, loss = 0.27 (328.1 examples/sec; 0.390 sec/batch)
2016-02-04 07:53:13.958881: step 167260, loss = 0.25 (297.8 examples/sec; 0.430 sec/batch)
2016-02-04 07:53:18.580636: step 167270, loss = 0.27 (284.7 examples/sec; 0.450 sec/batch)
2016-02-04 07:53:23.014514: step 167280, loss = 0.34 (295.6 examples/sec; 0.433 sec/batch)
2016-02-04 07:53:27.426480: step 167290, loss = 0.28 (285.3 examples/sec; 0.449 sec/batch)
2016-02-04 07:53:31.965909: step 167300, loss = 0.28 (271.1 examples/sec; 0.472 sec/batch)
2016-02-04 07:53:36.844772: step 167310, loss = 0.23 (299.9 examples/sec; 0.427 sec/batch)
2016-02-04 07:53:41.227002: step 167320, loss = 0.23 (306.9 examples/sec; 0.417 sec/batch)
2016-02-04 07:53:45.481730: step 167330, loss = 0.34 (277.0 examples/sec; 0.462 sec/batch)
2016-02-04 07:53:49.966891: step 167340, loss = 0.27 (278.5 examples/sec; 0.460 sec/batch)
2016-02-04 07:53:54.331068: step 167350, loss = 0.31 (262.9 examples/sec; 0.487 sec/batch)
2016-02-04 07:53:58.776809: step 167360, loss = 0.21 (296.5 examples/sec; 0.432 sec/batch)
2016-02-04 07:54:03.331657: step 167370, loss = 0.33 (266.8 examples/sec; 0.480 sec/batch)
2016-02-04 07:54:07.831453: step 167380, loss = 0.34 (289.9 examples/sec; 0.442 sec/batch)
2016-02-04 07:54:12.243675: step 167390, loss = 0.27 (301.2 examples/sec; 0.425 sec/batch)
2016-02-04 07:54:16.743931: step 167400, loss = 0.24 (286.9 examples/sec; 0.446 sec/batch)
2016-02-04 07:54:21.829230: step 167410, loss = 0.26 (270.5 examples/sec; 0.473 sec/batch)
2016-02-04 07:54:26.218271: step 167420, loss = 0.26 (304.7 examples/sec; 0.420 sec/batch)
2016-02-04 07:54:30.748460: step 167430, loss = 0.29 (301.1 examples/sec; 0.425 sec/batch)
2016-02-04 07:54:35.134662: step 167440, loss = 0.22 (292.4 examples/sec; 0.438 sec/batch)
2016-02-04 07:54:39.790336: step 167450, loss = 0.32 (256.7 examples/sec; 0.499 sec/batch)
2016-02-04 07:54:44.294475: step 167460, loss = 0.23 (324.2 examples/sec; 0.395 sec/batch)
2016-02-04 07:54:48.599717: step 167470, loss = 0.27 (318.0 examples/sec; 0.403 sec/batch)
2016-02-04 07:54:52.935267: step 167480, loss = 0.28 (283.3 examples/sec; 0.452 sec/batch)
2016-02-04 07:54:57.452868: step 167490, loss = 0.31 (280.0 examples/sec; 0.457 sec/batch)
2016-02-04 07:55:01.933605: step 167500, loss = 0.40 (295.4 examples/sec; 0.433 sec/batch)
2016-02-04 07:55:06.743052: step 167510, loss = 0.35 (322.7 examples/sec; 0.397 sec/batch)
2016-02-04 07:55:11.127041: step 167520, loss = 0.38 (279.6 examples/sec; 0.458 sec/batch)
2016-02-04 07:55:15.595266: step 167530, loss = 0.28 (296.1 examples/sec; 0.432 sec/batch)
2016-02-04 07:55:20.101822: step 167540, loss = 0.30 (303.8 examples/sec; 0.421 sec/batch)
2016-02-04 07:55:24.549232: step 167550, loss = 0.28 (257.1 examples/sec; 0.498 sec/batch)
2016-02-04 07:55:28.869220: step 167560, loss = 0.30 (259.2 examples/sec; 0.494 sec/batch)
2016-02-04 07:55:33.305855: step 167570, loss = 0.28 (297.0 examples/sec; 0.431 sec/batch)
2016-02-04 07:55:37.864399: step 167580, loss = 0.39 (260.2 examples/sec; 0.492 sec/batch)
2016-02-04 07:55:42.142185: step 167590, loss = 0.34 (324.9 examples/sec; 0.394 sec/batch)
2016-02-04 07:55:46.643221: step 167600, loss = 0.23 (286.0 examples/sec; 0.448 sec/batch)
2016-02-04 07:55:51.572174: step 167610, loss = 0.25 (305.8 examples/sec; 0.419 sec/batch)
2016-02-04 07:55:56.084521: step 167620, loss = 0.21 (266.8 examples/sec; 0.480 sec/batch)
2016-02-04 07:56:00.542996: step 167630, loss = 0.28 (296.9 examples/sec; 0.431 sec/batch)
2016-02-04 07:56:04.930301: step 167640, loss = 0.28 (263.9 examples/sec; 0.485 sec/batch)
2016-02-04 07:56:09.331070: step 167650, loss = 0.21 (296.6 examples/sec; 0.432 sec/batch)
2016-02-04 07:56:13.785190: step 167660, loss = 0.23 (327.8 examples/sec; 0.390 sec/batch)
2016-02-04 07:56:18.312220: step 167670, loss = 0.36 (274.6 examples/sec; 0.466 sec/batch)
2016-02-04 07:56:22.486870: step 167680, loss = 0.33 (311.3 examples/sec; 0.411 sec/batch)
2016-02-04 07:56:26.987979: step 167690, loss = 0.22 (288.0 examples/sec; 0.445 sec/batch)
2016-02-04 07:56:31.335841: step 167700, loss = 0.26 (293.1 examples/sec; 0.437 sec/batch)
2016-02-04 07:56:36.159578: step 167710, loss = 0.22 (306.0 examples/sec; 0.418 sec/batch)
2016-02-04 07:56:40.526602: step 167720, loss = 0.24 (329.3 examples/sec; 0.389 sec/batch)
2016-02-04 07:56:45.058370: step 167730, loss = 0.29 (273.6 examples/sec; 0.468 sec/batch)
2016-02-04 07:56:49.527268: step 167740, loss = 0.31 (259.0 examples/sec; 0.494 sec/batch)
2016-02-04 07:56:54.105915: step 167750, loss = 0.30 (269.5 examples/sec; 0.475 sec/batch)
2016-02-04 07:56:58.436598: step 167760, loss = 0.33 (286.0 examples/sec; 0.448 sec/batch)
2016-02-04 07:57:02.805551: step 167770, loss = 0.28 (275.7 examples/sec; 0.464 sec/batch)
2016-02-04 07:57:07.332381: step 167780, loss = 0.32 (281.0 examples/sec; 0.455 sec/batch)
2016-02-04 07:57:11.768959: step 167790, loss = 0.30 (332.7 examples/sec; 0.385 sec/batch)
2016-02-04 07:57:16.169406: step 167800, loss = 0.32 (312.8 examples/sec; 0.409 sec/batch)
2016-02-04 07:57:21.152645: step 167810, loss = 0.27 (290.8 examples/sec; 0.440 sec/batch)
2016-02-04 07:57:25.784386: step 167820, loss = 0.24 (263.3 examples/sec; 0.486 sec/batch)
2016-02-04 07:57:30.286095: step 167830, loss = 0.21 (279.7 examples/sec; 0.458 sec/batch)
2016-02-04 07:57:34.660067: step 167840, loss = 0.26 (329.3 examples/sec; 0.389 sec/batch)
2016-02-04 07:57:39.054116: step 167850, loss = 0.24 (287.9 examples/sec; 0.445 sec/batch)
2016-02-04 07:57:43.482361: step 167860, loss = 0.32 (313.3 examples/sec; 0.409 sec/batch)
2016-02-04 07:57:47.805826: step 167870, loss = 0.27 (296.6 examples/sec; 0.432 sec/batch)
2016-02-04 07:57:52.318939: step 167880, loss = 0.22 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 07:57:56.620836: step 167890, loss = 0.21 (291.9 examples/sec; 0.438 sec/batch)
2016-02-04 07:58:01.052849: step 167900, loss = 0.29 (267.1 examples/sec; 0.479 sec/batch)
2016-02-04 07:58:05.983272: step 167910, loss = 0.29 (300.8 examples/sec; 0.426 sec/batch)
2016-02-04 07:58:10.457554: step 167920, loss = 0.24 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 07:58:14.920697: step 167930, loss = 0.29 (305.8 examples/sec; 0.419 sec/batch)
2016-02-04 07:58:19.431826: step 167940, loss = 0.36 (275.8 examples/sec; 0.464 sec/batch)
2016-02-04 07:58:23.927690: step 167950, loss = 0.33 (278.0 examples/sec; 0.461 sec/batch)
2016-02-04 07:58:28.417752: step 167960, loss = 0.27 (288.4 examples/sec; 0.444 sec/batch)
2016-02-04 07:58:32.816095: step 167970, loss = 0.29 (282.0 examples/sec; 0.454 sec/batch)
2016-02-04 07:58:37.299375: step 167980, loss = 0.30 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 07:58:41.733267: step 167990, loss = 0.31 (281.3 examples/sec; 0.455 sec/batch)
2016-02-04 07:58:46.141104: step 168000, loss = 0.30 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 07:58:50.823369: step 168010, loss = 0.27 (274.2 examples/sec; 0.467 sec/batch)
2016-02-04 07:58:55.387006: step 168020, loss = 0.23 (290.7 examples/sec; 0.440 sec/batch)
2016-02-04 07:58:59.682869: step 168030, loss = 0.41 (296.6 examples/sec; 0.432 sec/batch)
2016-02-04 07:59:04.058733: step 168040, loss = 0.26 (316.6 examples/sec; 0.404 sec/batch)
2016-02-04 07:59:08.663510: step 168050, loss = 0.30 (265.0 examples/sec; 0.483 sec/batch)
2016-02-04 07:59:13.140335: step 168060, loss = 0.25 (276.1 examples/sec; 0.464 sec/batch)
2016-02-04 07:59:17.491724: step 168070, loss = 0.34 (310.9 examples/sec; 0.412 sec/batch)
2016-02-04 07:59:21.907944: step 168080, loss = 0.21 (309.6 examples/sec; 0.413 sec/batch)
2016-02-04 07:59:26.313801: step 168090, loss = 0.25 (297.8 examples/sec; 0.430 sec/batch)
2016-02-04 07:59:30.816555: step 168100, loss = 0.28 (299.2 examples/sec; 0.428 sec/batch)
2016-02-04 07:59:35.648558: step 168110, loss = 0.26 (324.1 examples/sec; 0.395 sec/batch)
2016-02-04 07:59:40.175799: step 168120, loss = 0.41 (303.0 examples/sec; 0.422 sec/batch)
2016-02-04 07:59:44.558339: step 168130, loss = 0.25 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 07:59:49.032846: step 168140, loss = 0.36 (297.2 examples/sec; 0.431 sec/batch)
2016-02-04 07:59:53.359276: step 168150, loss = 0.29 (327.1 examples/sec; 0.391 sec/batch)
2016-02-04 07:59:57.787586: step 168160, loss = 0.28 (281.1 examples/sec; 0.455 sec/batch)
2016-02-04 08:00:02.157697: step 168170, loss = 0.23 (297.8 examples/sec; 0.430 sec/batch)
2016-02-04 08:00:06.620434: step 168180, loss = 0.23 (283.1 examples/sec; 0.452 sec/batch)
2016-02-04 08:00:11.184918: step 168190, loss = 0.25 (306.1 examples/sec; 0.418 sec/batch)
2016-02-04 08:00:15.471632: step 168200, loss = 0.28 (340.8 examples/sec; 0.376 sec/batch)
2016-02-04 08:00:20.362739: step 168210, loss = 0.30 (280.6 examples/sec; 0.456 sec/batch)
2016-02-04 08:00:24.912939: step 168220, loss = 0.27 (291.0 examples/sec; 0.440 sec/batch)
2016-02-04 08:00:29.479351: step 168230, loss = 0.34 (291.2 examples/sec; 0.440 sec/batch)
2016-02-04 08:00:34.023766: step 168240, loss = 0.28 (297.1 examples/sec; 0.431 sec/batch)
2016-02-04 08:00:38.421370: step 168250, loss = 0.24 (283.7 examples/sec; 0.451 sec/batch)
2016-02-04 08:00:42.792418: step 168260, loss = 0.28 (344.3 examples/sec; 0.372 sec/batch)
2016-02-04 08:00:47.195972: step 168270, loss = 0.29 (295.6 examples/sec; 0.433 sec/batch)
2016-02-04 08:00:51.663480: step 168280, loss = 0.23 (299.5 examples/sec; 0.427 sec/batch)
2016-02-04 08:00:56.094923: step 168290, loss = 0.26 (296.5 examples/sec; 0.432 sec/batch)
2016-02-04 08:01:00.430708: step 168300, loss = 0.32 (301.8 examples/sec; 0.424 sec/batch)
2016-02-04 08:01:05.390478: step 168310, loss = 0.28 (287.0 examples/sec; 0.446 sec/batch)
2016-02-04 08:01:09.982303: step 168320, loss = 0.31 (264.3 examples/sec; 0.484 sec/batch)
2016-02-04 08:01:14.461386: step 168330, loss = 0.24 (256.7 examples/sec; 0.499 sec/batch)
2016-02-04 08:01:18.874198: step 168340, loss = 0.30 (274.4 examples/sec; 0.466 sec/batch)
2016-02-04 08:01:23.421329: step 168350, loss = 0.30 (264.1 examples/sec; 0.485 sec/batch)
2016-02-04 08:01:27.788351: step 168360, loss = 0.25 (275.8 examples/sec; 0.464 sec/batch)
2016-02-04 08:01:32.328860: step 168370, loss = 0.28 (251.1 examples/sec; 0.510 sec/batch)
2016-02-04 08:01:36.719558: step 168380, loss = 0.25 (296.2 examples/sec; 0.432 sec/batch)
2016-02-04 08:01:41.098985: step 168390, loss = 0.27 (350.2 examples/sec; 0.366 sec/batch)
2016-02-04 08:01:45.638109: step 168400, loss = 0.31 (265.2 examples/sec; 0.483 sec/batch)
2016-02-04 08:01:50.573607: step 168410, loss = 0.29 (279.4 examples/sec; 0.458 sec/batch)
2016-02-04 08:01:54.997507: step 168420, loss = 0.27 (292.4 examples/sec; 0.438 sec/batch)
2016-02-04 08:01:59.431057: step 168430, loss = 0.29 (296.6 examples/sec; 0.432 sec/batch)
2016-02-04 08:02:03.821607: step 168440, loss = 0.35 (256.9 examples/sec; 0.498 sec/batch)
2016-02-04 08:02:08.149366: step 168450, loss = 0.19 (305.0 examples/sec; 0.420 sec/batch)
2016-02-04 08:02:12.537893: step 168460, loss = 0.22 (265.4 examples/sec; 0.482 sec/batch)
2016-02-04 08:02:16.771144: step 168470, loss = 0.24 (309.9 examples/sec; 0.413 sec/batch)
2016-02-04 08:02:21.168516: step 168480, loss = 0.25 (294.7 examples/sec; 0.434 sec/batch)
2016-02-04 08:02:25.693026: step 168490, loss = 0.32 (294.0 examples/sec; 0.435 sec/batch)
2016-02-04 08:02:30.232172: step 168500, loss = 0.42 (313.7 examples/sec; 0.408 sec/batch)
2016-02-04 08:02:35.173222: step 168510, loss = 0.28 (294.9 examples/sec; 0.434 sec/batch)
2016-02-04 08:02:39.597162: step 168520, loss = 0.30 (259.1 examples/sec; 0.494 sec/batch)
2016-02-04 08:02:44.105655: step 168530, loss = 0.23 (258.0 examples/sec; 0.496 sec/batch)
2016-02-04 08:02:48.683815: step 168540, loss = 0.31 (271.1 examples/sec; 0.472 sec/batch)
2016-02-04 08:02:53.282827: step 168550, loss = 0.28 (292.5 examples/sec; 0.438 sec/batch)
2016-02-04 08:02:57.874599: step 168560, loss = 0.32 (277.6 examples/sec; 0.461 sec/batch)
2016-02-04 08:03:02.452085: step 168570, loss = 0.29 (264.3 examples/sec; 0.484 sec/batch)
2016-02-04 08:03:07.023488: step 168580, loss = 0.28 (303.1 examples/sec; 0.422 sec/batch)
2016-02-04 08:03:11.530559: step 168590, loss = 0.32 (278.9 examples/sec; 0.459 sec/batch)
2016-02-04 08:03:16.156842: step 168600, loss = 0.29 (294.7 examples/sec; 0.434 sec/batch)
2016-02-04 08:03:21.220571: step 168610, loss = 0.21 (292.6 examples/sec; 0.437 sec/batch)
2016-02-04 08:03:25.712866: step 168620, loss = 0.22 (299.5 examples/sec; 0.427 sec/batch)
2016-02-04 08:03:30.275108: step 168630, loss = 0.23 (298.5 examples/sec; 0.429 sec/batch)
2016-02-04 08:03:34.765901: step 168640, loss = 0.27 (288.7 examples/sec; 0.443 sec/batch)
2016-02-04 08:03:39.303003: step 168650, loss = 0.23 (263.3 examples/sec; 0.486 sec/batch)
2016-02-04 08:03:43.735262: step 168660, loss = 0.33 (311.0 examples/sec; 0.412 sec/batch)
2016-02-04 08:03:48.243049: step 168670, loss = 0.24 (280.4 examples/sec; 0.456 sec/batch)
2016-02-04 08:03:52.787981: step 168680, loss = 0.21 (294.7 examples/sec; 0.434 sec/batch)
2016-02-04 08:03:57.225207: step 168690, loss = 0.25 (296.9 examples/sec; 0.431 sec/batch)
2016-02-04 08:04:01.448859: step 168700, loss = 0.21 (328.7 examples/sec; 0.389 sec/batch)
2016-02-04 08:04:06.316472: step 168710, loss = 0.28 (294.1 examples/sec; 0.435 sec/batch)
2016-02-04 08:04:10.686229: step 168720, loss = 0.23 (282.9 examples/sec; 0.452 sec/batch)
2016-02-04 08:04:14.996877: step 168730, loss = 0.28 (283.0 examples/sec; 0.452 sec/batch)
2016-02-04 08:04:19.381768: step 168740, loss = 0.27 (261.5 examples/sec; 0.489 sec/batch)
2016-02-04 08:04:23.960619: step 168750, loss = 0.27 (267.3 examples/sec; 0.479 sec/batch)
2016-02-04 08:04:28.506789: step 168760, loss = 0.27 (285.1 examples/sec; 0.449 sec/batch)
2016-02-04 08:04:32.920105: step 168770, loss = 0.21 (274.6 examples/sec; 0.466 sec/batch)
2016-02-04 08:04:37.365517: step 168780, loss = 0.28 (290.2 examples/sec; 0.441 sec/batch)
2016-02-04 08:04:41.954548: step 168790, loss = 0.27 (287.1 examples/sec; 0.446 sec/batch)
2016-02-04 08:04:46.439194: step 168800, loss = 0.37 (294.0 examples/sec; 0.435 sec/batch)
2016-02-04 08:04:51.406184: step 168810, loss = 0.26 (268.7 examples/sec; 0.476 sec/batch)
2016-02-04 08:04:56.062279: step 168820, loss = 0.37 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 08:05:00.654739: step 168830, loss = 0.25 (260.7 examples/sec; 0.491 sec/batch)
2016-02-04 08:05:05.210810: step 168840, loss = 0.26 (249.7 examples/sec; 0.513 sec/batch)
2016-02-04 08:05:09.578057: step 168850, loss = 0.27 (314.9 examples/sec; 0.406 sec/batch)
2016-02-04 08:05:14.035760: step 168860, loss = 0.27 (268.2 examples/sec; 0.477 sec/batch)
2016-02-04 08:05:18.557525: step 168870, loss = 0.25 (285.7 examples/sec; 0.448 sec/batch)
2016-02-04 08:05:23.037180: step 168880, loss = 0.24 (283.5 examples/sec; 0.451 sec/batch)
2016-02-04 08:05:27.419300: step 168890, loss = 0.18 (298.0 examples/sec; 0.429 sec/batch)
2016-02-04 08:05:31.837503: step 168900, loss = 0.23 (307.5 examples/sec; 0.416 sec/batch)
2016-02-04 08:05:36.952295: step 168910, loss = 0.26 (273.2 examples/sec; 0.469 sec/batch)
2016-02-04 08:05:41.425779: step 168920, loss = 0.25 (275.6 examples/sec; 0.464 sec/batch)
2016-02-04 08:05:45.849539: step 168930, loss = 0.25 (307.6 examples/sec; 0.416 sec/batch)
2016-02-04 08:05:50.321373: step 168940, loss = 0.25 (285.7 examples/sec; 0.448 sec/batch)
2016-02-04 08:05:54.779984: step 168950, loss = 0.24 (308.7 examples/sec; 0.415 sec/batch)
2016-02-04 08:05:59.322165: step 168960, loss = 0.25 (326.3 examples/sec; 0.392 sec/batch)
2016-02-04 08:06:03.837274: step 168970, loss = 0.35 (279.4 examples/sec; 0.458 sec/batch)
2016-02-04 08:06:08.296580: step 168980, loss = 0.25 (286.4 examples/sec; 0.447 sec/batch)
2016-02-04 08:06:12.719947: step 168990, loss = 0.33 (293.1 examples/sec; 0.437 sec/batch)
2016-02-04 08:06:17.170950: step 169000, loss = 0.32 (307.8 examples/sec; 0.416 sec/batch)
2016-02-04 08:06:22.204755: step 169010, loss = 0.31 (262.7 examples/sec; 0.487 sec/batch)
2016-02-04 08:06:26.621251: step 169020, loss = 0.36 (291.2 examples/sec; 0.439 sec/batch)
2016-02-04 08:06:31.277441: step 169030, loss = 0.26 (285.3 examples/sec; 0.449 sec/batch)
2016-02-04 08:06:35.935309: step 169040, loss = 0.29 (260.6 examples/sec; 0.491 sec/batch)
2016-02-04 08:06:40.540561: step 169050, loss = 0.36 (264.4 examples/sec; 0.484 sec/batch)
2016-02-04 08:06:45.221895: step 169060, loss = 0.30 (248.3 examples/sec; 0.515 sec/batch)
2016-02-04 08:06:49.786592: step 169070, loss = 0.41 (292.2 examples/sec; 0.438 sec/batch)
2016-02-04 08:06:54.302221: step 169080, loss = 0.28 (256.3 examples/sec; 0.499 sec/batch)
2016-02-04 08:06:58.750015: step 169090, loss = 0.32 (304.5 examples/sec; 0.420 sec/batch)
2016-02-04 08:07:03.284276: step 169100, loss = 0.32 (260.3 examples/sec; 0.492 sec/batch)
2016-02-04 08:07:08.354365: step 169110, loss = 0.28 (279.6 examples/sec; 0.458 sec/batch)
2016-02-04 08:07:12.793000: step 169120, loss = 0.22 (303.9 examples/sec; 0.421 sec/batch)
2016-02-04 08:07:17.313584: step 169130, loss = 0.31 (286.8 examples/sec; 0.446 sec/batch)
2016-02-04 08:07:21.839840: step 169140, loss = 0.23 (267.9 examples/sec; 0.478 sec/batch)
2016-02-04 08:07:26.368313: step 169150, loss = 0.27 (260.2 examples/sec; 0.492 sec/batch)
2016-02-04 08:07:30.942362: step 169160, loss = 0.23 (272.0 examples/sec; 0.471 sec/batch)
2016-02-04 08:07:35.382381: step 169170, loss = 0.35 (281.0 examples/sec; 0.456 sec/batch)
2016-02-04 08:07:40.009327: step 169180, loss = 0.35 (263.6 examples/sec; 0.486 sec/batch)
2016-02-04 08:07:44.517689: step 169190, loss = 0.31 (307.9 examples/sec; 0.416 sec/batch)
2016-02-04 08:07:49.076022: step 169200, loss = 0.23 (304.3 examples/sec; 0.421 sec/batch)
2016-02-04 08:07:54.125890: step 169210, loss = 0.24 (260.1 examples/sec; 0.492 sec/batch)
2016-02-04 08:07:58.457619: step 169220, loss = 0.30 (296.2 examples/sec; 0.432 sec/batch)
2016-02-04 08:08:02.728513: step 169230, loss = 0.28 (301.7 examples/sec; 0.424 sec/batch)
2016-02-04 08:08:07.152223: step 169240, loss = 0.33 (257.5 examples/sec; 0.497 sec/batch)
2016-02-04 08:08:11.591267: step 169250, loss = 0.24 (279.9 examples/sec; 0.457 sec/batch)
2016-02-04 08:08:16.153693: step 169260, loss = 0.24 (302.7 examples/sec; 0.423 sec/batch)
2016-02-04 08:08:20.631359: step 169270, loss = 0.28 (284.9 examples/sec; 0.449 sec/batch)
2016-02-04 08:08:24.935970: step 169280, loss = 0.30 (286.3 examples/sec; 0.447 sec/batch)
2016-02-04 08:08:29.233803: step 169290, loss = 0.25 (296.1 examples/sec; 0.432 sec/batch)
2016-02-04 08:08:33.836938: step 169300, loss = 0.33 (305.2 examples/sec; 0.419 sec/batch)
2016-02-04 08:08:38.804776: step 169310, loss = 0.22 (267.2 examples/sec; 0.479 sec/batch)
2016-02-04 08:08:43.307229: step 169320, loss = 0.30 (307.3 examples/sec; 0.417 sec/batch)
2016-02-04 08:08:47.763565: step 169330, loss = 0.21 (249.5 examples/sec; 0.513 sec/batch)
2016-02-04 08:08:52.097614: step 169340, loss = 0.22 (298.2 examples/sec; 0.429 sec/batch)
2016-02-04 08:08:56.574690: step 169350, loss = 0.28 (279.7 examples/sec; 0.458 sec/batch)
2016-02-04 08:09:01.144741: step 169360, loss = 0.28 (266.9 examples/sec; 0.479 sec/batch)
2016-02-04 08:09:05.621895: step 169370, loss = 0.26 (269.8 examples/sec; 0.474 sec/batch)
2016-02-04 08:09:10.065514: step 169380, loss = 0.32 (299.9 examples/sec; 0.427 sec/batch)
2016-02-04 08:09:14.636528: step 169390, loss = 0.25 (309.4 examples/sec; 0.414 sec/batch)
2016-02-04 08:09:19.115600: step 169400, loss = 0.30 (258.1 examples/sec; 0.496 sec/batch)
2016-02-04 08:09:24.009385: step 169410, loss = 0.29 (279.3 examples/sec; 0.458 sec/batch)
2016-02-04 08:09:28.470546: step 169420, loss = 0.35 (296.5 examples/sec; 0.432 sec/batch)
2016-02-04 08:09:32.898768: step 169430, loss = 0.26 (298.4 examples/sec; 0.429 sec/batch)
2016-02-04 08:09:37.322349: step 169440, loss = 0.26 (290.3 examples/sec; 0.441 sec/batch)
2016-02-04 08:09:41.728599: step 169450, loss = 0.30 (313.1 examples/sec; 0.409 sec/batch)
2016-02-04 08:09:46.193136: step 169460, loss = 0.21 (289.8 examples/sec; 0.442 sec/batch)
2016-02-04 08:09:50.683249: step 169470, loss = 0.23 (285.0 examples/sec; 0.449 sec/batch)
2016-02-04 08:09:55.287022: step 169480, loss = 0.35 (271.5 examples/sec; 0.471 sec/batch)
2016-02-04 08:09:59.743398: step 169490, loss = 0.18 (293.8 examples/sec; 0.436 sec/batch)
2016-02-04 08:10:03.952874: step 169500, loss = 0.23 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 08:10:08.951911: step 169510, loss = 0.21 (275.9 examples/sec; 0.464 sec/batch)
2016-02-04 08:10:13.541759: step 169520, loss = 0.28 (272.5 examples/sec; 0.470 sec/batch)
2016-02-04 08:10:17.948839: step 169530, loss = 0.28 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 08:10:22.457268: step 169540, loss = 0.31 (291.9 examples/sec; 0.438 sec/batch)
2016-02-04 08:10:26.801735: step 169550, loss = 0.30 (258.1 examples/sec; 0.496 sec/batch)
2016-02-04 08:10:31.283711: step 169560, loss = 0.23 (259.8 examples/sec; 0.493 sec/batch)
2016-02-04 08:10:35.754929: step 169570, loss = 0.28 (263.9 examples/sec; 0.485 sec/batch)
2016-02-04 08:10:40.166521: step 169580, loss = 0.30 (276.7 examples/sec; 0.463 sec/batch)
2016-02-04 08:10:44.853560: step 169590, loss = 0.28 (273.2 examples/sec; 0.469 sec/batch)
2016-02-04 08:10:49.356938: step 169600, loss = 0.24 (277.5 examples/sec; 0.461 sec/batch)
2016-02-04 08:10:54.197098: step 169610, loss = 0.27 (260.0 examples/sec; 0.492 sec/batch)
2016-02-04 08:10:58.567414: step 169620, loss = 0.36 (288.8 examples/sec; 0.443 sec/batch)
2016-02-04 08:11:02.995770: step 169630, loss = 0.32 (275.9 examples/sec; 0.464 sec/batch)
2016-02-04 08:11:07.668773: step 169640, loss = 0.22 (251.0 examples/sec; 0.510 sec/batch)
2016-02-04 08:11:12.061278: step 169650, loss = 0.26 (314.6 examples/sec; 0.407 sec/batch)
2016-02-04 08:11:16.542509: step 169660, loss = 0.29 (260.1 examples/sec; 0.492 sec/batch)
2016-02-04 08:11:21.194971: step 169670, loss = 0.24 (267.8 examples/sec; 0.478 sec/batch)
2016-02-04 08:11:25.792124: step 169680, loss = 0.22 (282.0 examples/sec; 0.454 sec/batch)
2016-02-04 08:11:30.407239: step 169690, loss = 0.38 (269.8 examples/sec; 0.474 sec/batch)
2016-02-04 08:11:34.851064: step 169700, loss = 0.23 (274.1 examples/sec; 0.467 sec/batch)
2016-02-04 08:11:39.883639: step 169710, loss = 0.41 (278.5 examples/sec; 0.460 sec/batch)
2016-02-04 08:11:44.372469: step 169720, loss = 0.42 (281.0 examples/sec; 0.456 sec/batch)
2016-02-04 08:11:48.789079: step 169730, loss = 0.20 (319.0 examples/sec; 0.401 sec/batch)
2016-02-04 08:11:53.334576: step 169740, loss = 0.27 (324.3 examples/sec; 0.395 sec/batch)
2016-02-04 08:11:57.936768: step 169750, loss = 0.22 (282.9 examples/sec; 0.452 sec/batch)
2016-02-04 08:12:02.387001: step 169760, loss = 0.33 (304.0 examples/sec; 0.421 sec/batch)
2016-02-04 08:12:06.841513: step 169770, loss = 0.26 (304.3 examples/sec; 0.421 sec/batch)
2016-02-04 08:12:11.121209: step 169780, loss = 0.22 (361.8 examples/sec; 0.354 sec/batch)
2016-02-04 08:12:15.844979: step 169790, loss = 0.32 (319.6 examples/sec; 0.401 sec/batch)
2016-02-04 08:12:20.473667: step 169800, loss = 0.27 (274.1 examples/sec; 0.467 sec/batch)
2016-02-04 08:12:25.166764: step 169810, loss = 0.34 (284.7 examples/sec; 0.450 sec/batch)
2016-02-04 08:12:29.576081: step 169820, loss = 0.20 (296.8 examples/sec; 0.431 sec/batch)
2016-02-04 08:12:34.224521: step 169830, loss = 0.19 (259.9 examples/sec; 0.493 sec/batch)
2016-02-04 08:12:38.837405: step 169840, loss = 0.22 (278.4 examples/sec; 0.460 sec/batch)
2016-02-04 08:12:43.278663: step 169850, loss = 0.26 (304.5 examples/sec; 0.420 sec/batch)
2016-02-04 08:12:47.929293: step 169860, loss = 0.29 (255.5 examples/sec; 0.501 sec/batch)
2016-02-04 08:12:52.545814: step 169870, loss = 0.27 (249.4 examples/sec; 0.513 sec/batch)
2016-02-04 08:12:56.958077: step 169880, loss = 0.20 (317.5 examples/sec; 0.403 sec/batch)
2016-02-04 08:13:01.303807: step 169890, loss = 0.22 (280.5 examples/sec; 0.456 sec/batch)
2016-02-04 08:13:05.710619: step 169900, loss = 0.23 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 08:13:10.807244: step 169910, loss = 0.24 (255.8 examples/sec; 0.500 sec/batch)
2016-02-04 08:13:15.471481: step 169920, loss = 0.23 (261.3 examples/sec; 0.490 sec/batch)
2016-02-04 08:13:19.952071: step 169930, loss = 0.26 (295.2 examples/sec; 0.434 sec/batch)
2016-02-04 08:13:24.514304: step 169940, loss = 0.28 (283.4 examples/sec; 0.452 sec/batch)
2016-02-04 08:13:29.086915: step 169950, loss = 0.29 (293.5 examples/sec; 0.436 sec/batch)
2016-02-04 08:13:33.650460: step 169960, loss = 0.24 (277.6 examples/sec; 0.461 sec/batch)
2016-02-04 08:13:38.219487: step 169970, loss = 0.20 (271.3 examples/sec; 0.472 sec/batch)
2016-02-04 08:13:42.616284: step 169980, loss = 0.30 (292.4 examples/sec; 0.438 sec/batch)
2016-02-04 08:13:47.180753: step 169990, loss = 0.35 (295.1 examples/sec; 0.434 sec/batch)
2016-02-04 08:13:51.682982: step 170000, loss = 0.29 (272.9 examples/sec; 0.469 sec/batch)
2016-02-04 08:13:56.587649: step 170010, loss = 0.17 (306.4 examples/sec; 0.418 sec/batch)
2016-02-04 08:14:01.116445: step 170020, loss = 0.24 (300.7 examples/sec; 0.426 sec/batch)
2016-02-04 08:14:05.541268: step 170030, loss = 0.30 (294.7 examples/sec; 0.434 sec/batch)
2016-02-04 08:14:09.934910: step 170040, loss = 0.26 (272.7 examples/sec; 0.469 sec/batch)
2016-02-04 08:14:14.343477: step 170050, loss = 0.28 (283.4 examples/sec; 0.452 sec/batch)
2016-02-04 08:14:18.837825: step 170060, loss = 0.30 (286.1 examples/sec; 0.447 sec/batch)
2016-02-04 08:14:23.565456: step 170070, loss = 0.37 (258.9 examples/sec; 0.494 sec/batch)
2016-02-04 08:14:28.082772: step 170080, loss = 0.27 (317.5 examples/sec; 0.403 sec/batch)
2016-02-04 08:14:32.669307: step 170090, loss = 0.19 (307.5 examples/sec; 0.416 sec/batch)
2016-02-04 08:14:37.273043: step 170100, loss = 0.24 (286.7 examples/sec; 0.446 sec/batch)
2016-02-04 08:14:42.314584: step 170110, loss = 0.26 (274.2 examples/sec; 0.467 sec/batch)
2016-02-04 08:14:46.705044: step 170120, loss = 0.24 (297.0 examples/sec; 0.431 sec/batch)
2016-02-04 08:14:51.069968: step 170130, loss = 0.19 (335.1 examples/sec; 0.382 sec/batch)
2016-02-04 08:14:55.638284: step 170140, loss = 0.36 (283.7 examples/sec; 0.451 sec/batch)
2016-02-04 08:15:00.250812: step 170150, loss = 0.33 (265.3 examples/sec; 0.483 sec/batch)
2016-02-04 08:15:04.696312: step 170160, loss = 0.28 (324.9 examples/sec; 0.394 sec/batch)
2016-02-04 08:15:09.387471: step 170170, loss = 0.27 (279.8 examples/sec; 0.457 sec/batch)
2016-02-04 08:15:13.953265: step 170180, loss = 0.20 (258.3 examples/sec; 0.496 sec/batch)
2016-02-04 08:15:18.497418: step 170190, loss = 0.31 (287.2 examples/sec; 0.446 sec/batch)
2016-02-04 08:15:22.965860: step 170200, loss = 0.25 (295.3 examples/sec; 0.433 sec/batch)
2016-02-04 08:15:27.877596: step 170210, loss = 0.31 (282.1 examples/sec; 0.454 sec/batch)
2016-02-04 08:15:32.299698: step 170220, loss = 0.19 (290.0 examples/sec; 0.441 sec/batch)
2016-02-04 08:15:36.759358: step 170230, loss = 0.28 (323.8 examples/sec; 0.395 sec/batch)
2016-02-04 08:15:40.966027: step 170240, loss = 0.26 (304.9 examples/sec; 0.420 sec/batch)
2016-02-04 08:15:45.390264: step 170250, loss = 0.22 (284.5 examples/sec; 0.450 sec/batch)
2016-02-04 08:15:50.032607: step 170260, loss = 0.25 (279.0 examples/sec; 0.459 sec/batch)
2016-02-04 08:15:54.441684: step 170270, loss = 0.21 (307.7 examples/sec; 0.416 sec/batch)
2016-02-04 08:15:58.981072: step 170280, loss = 0.22 (261.2 examples/sec; 0.490 sec/batch)
2016-02-04 08:16:03.468503: step 170290, loss = 0.24 (297.3 examples/sec; 0.431 sec/batch)
2016-02-04 08:16:07.942123: step 170300, loss = 0.21 (251.5 examples/sec; 0.509 sec/batch)
2016-02-04 08:16:13.078996: step 170310, loss = 0.34 (257.7 examples/sec; 0.497 sec/batch)
2016-02-04 08:16:17.656608: step 170320, loss = 0.27 (281.4 examples/sec; 0.455 sec/batch)
2016-02-04 08:16:22.172265: step 170330, loss = 0.24 (265.5 examples/sec; 0.482 sec/batch)
2016-02-04 08:16:26.693054: step 170340, loss = 0.24 (284.2 examples/sec; 0.450 sec/batch)
2016-02-04 08:16:31.082311: step 170350, loss = 0.22 (304.0 examples/sec; 0.421 sec/batch)
2016-02-04 08:16:35.740937: step 170360, loss = 0.27 (306.7 examples/sec; 0.417 sec/batch)
2016-02-04 08:16:40.269043: step 170370, loss = 0.34 (272.0 examples/sec; 0.471 sec/batch)
2016-02-04 08:16:44.731382: step 170380, loss = 0.31 (270.2 examples/sec; 0.474 sec/batch)
2016-02-04 08:16:49.186631: step 170390, loss = 0.25 (293.2 examples/sec; 0.437 sec/batch)
2016-02-04 08:16:53.727320: step 170400, loss = 0.24 (301.3 examples/sec; 0.425 sec/batch)
2016-02-04 08:16:58.722920: step 170410, loss = 0.30 (320.4 examples/sec; 0.400 sec/batch)
2016-02-04 08:17:03.255642: step 170420, loss = 0.30 (270.4 examples/sec; 0.473 sec/batch)
2016-02-04 08:17:07.750040: step 170430, loss = 0.45 (272.9 examples/sec; 0.469 sec/batch)
2016-02-04 08:17:12.264002: step 170440, loss = 0.29 (255.0 examples/sec; 0.502 sec/batch)
2016-02-04 08:17:16.688115: step 170450, loss = 0.22 (319.3 examples/sec; 0.401 sec/batch)
2016-02-04 08:17:21.247202: step 170460, loss = 0.31 (256.1 examples/sec; 0.500 sec/batch)
2016-02-04 08:17:25.858188: step 170470, loss = 0.22 (315.2 examples/sec; 0.406 sec/batch)
2016-02-04 08:17:30.270192: step 170480, loss = 0.31 (287.8 examples/sec; 0.445 sec/batch)
2016-02-04 08:17:34.855021: step 170490, loss = 0.18 (276.4 examples/sec; 0.463 sec/batch)
2016-02-04 08:17:39.332153: step 170500, loss = 0.31 (263.5 examples/sec; 0.486 sec/batch)
2016-02-04 08:17:44.343448: step 170510, loss = 0.25 (287.8 examples/sec; 0.445 sec/batch)
2016-02-04 08:17:48.938314: step 170520, loss = 0.20 (304.7 examples/sec; 0.420 sec/batch)
2016-02-04 08:17:53.543894: step 170530, loss = 0.25 (286.7 examples/sec; 0.447 sec/batch)
2016-02-04 08:17:57.958823: step 170540, loss = 0.29 (326.4 examples/sec; 0.392 sec/batch)
2016-02-04 08:18:02.487460: step 170550, loss = 0.25 (281.2 examples/sec; 0.455 sec/batch)
2016-02-04 08:18:06.848465: step 170560, loss = 0.18 (282.1 examples/sec; 0.454 sec/batch)
2016-02-04 08:18:11.527231: step 170570, loss = 0.25 (294.7 examples/sec; 0.434 sec/batch)
2016-02-04 08:18:16.023585: step 170580, loss = 0.22 (306.7 examples/sec; 0.417 sec/batch)
2016-02-04 08:18:20.444051: step 170590, loss = 0.32 (281.4 examples/sec; 0.455 sec/batch)
2016-02-04 08:18:25.015915: step 170600, loss = 0.26 (258.1 examples/sec; 0.496 sec/batch)
2016-02-04 08:18:30.072383: step 170610, loss = 0.26 (306.4 examples/sec; 0.418 sec/batch)
2016-02-04 08:18:34.564657: step 170620, loss = 0.33 (278.8 examples/sec; 0.459 sec/batch)
2016-02-04 08:18:39.077448: step 170630, loss = 0.27 (281.1 examples/sec; 0.455 sec/batch)
2016-02-04 08:18:43.516245: step 170640, loss = 0.26 (318.4 examples/sec; 0.402 sec/batch)
2016-02-04 08:18:48.156346: step 170650, loss = 0.30 (254.1 examples/sec; 0.504 sec/batch)
2016-02-04 08:18:52.654242: step 170660, loss = 0.32 (277.3 examples/sec; 0.462 sec/batch)
2016-02-04 08:18:57.228071: step 170670, loss = 0.32 (285.1 examples/sec; 0.449 sec/batch)
2016-02-04 08:19:01.626674: step 170680, loss = 0.24 (329.8 examples/sec; 0.388 sec/batch)
2016-02-04 08:19:06.209079: step 170690, loss = 0.31 (264.4 examples/sec; 0.484 sec/batch)
2016-02-04 08:19:10.681808: step 170700, loss = 0.28 (258.9 examples/sec; 0.494 sec/batch)
2016-02-04 08:19:15.574451: step 170710, loss = 0.29 (280.3 examples/sec; 0.457 sec/batch)
2016-02-04 08:19:19.926348: step 170720, loss = 0.26 (319.0 examples/sec; 0.401 sec/batch)
2016-02-04 08:19:24.435817: step 170730, loss = 0.25 (277.8 examples/sec; 0.461 sec/batch)
2016-02-04 08:19:28.928399: step 170740, loss = 0.28 (284.2 examples/sec; 0.450 sec/batch)
2016-02-04 08:19:33.466936: step 170750, loss = 0.22 (262.2 examples/sec; 0.488 sec/batch)
2016-02-04 08:19:37.991499: step 170760, loss = 0.27 (269.0 examples/sec; 0.476 sec/batch)
2016-02-04 08:19:42.610657: step 170770, loss = 0.26 (277.7 examples/sec; 0.461 sec/batch)
2016-02-04 08:19:47.102048: step 170780, loss = 0.26 (334.1 examples/sec; 0.383 sec/batch)
2016-02-04 08:19:51.635833: step 170790, loss = 0.23 (292.8 examples/sec; 0.437 sec/batch)
2016-02-04 08:19:56.012396: step 170800, loss = 0.26 (313.6 examples/sec; 0.408 sec/batch)
2016-02-04 08:20:00.799416: step 170810, loss = 0.32 (271.2 examples/sec; 0.472 sec/batch)
2016-02-04 08:20:05.148866: step 170820, loss = 0.30 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 08:20:09.572413: step 170830, loss = 0.23 (267.5 examples/sec; 0.478 sec/batch)
2016-02-04 08:20:14.049915: step 170840, loss = 0.27 (304.9 examples/sec; 0.420 sec/batch)
2016-02-04 08:20:18.603077: step 170850, loss = 0.29 (286.1 examples/sec; 0.447 sec/batch)
2016-02-04 08:20:23.222856: step 170860, loss = 0.27 (277.3 examples/sec; 0.462 sec/batch)
2016-02-04 08:20:27.910937: step 170870, loss = 0.30 (282.7 examples/sec; 0.453 sec/batch)
2016-02-04 08:20:32.484452: step 170880, loss = 0.29 (275.5 examples/sec; 0.465 sec/batch)
2016-02-04 08:20:37.041257: step 170890, loss = 0.27 (307.6 examples/sec; 0.416 sec/batch)
2016-02-04 08:20:41.501294: step 170900, loss = 0.26 (294.2 examples/sec; 0.435 sec/batch)
2016-02-04 08:20:46.352524: step 170910, loss = 0.29 (285.1 examples/sec; 0.449 sec/batch)
2016-02-04 08:20:50.724697: step 170920, loss = 0.30 (311.3 examples/sec; 0.411 sec/batch)
2016-02-04 08:20:55.168547: step 170930, loss = 0.27 (295.0 examples/sec; 0.434 sec/batch)
2016-02-04 08:20:59.688832: step 170940, loss = 0.29 (294.7 examples/sec; 0.434 sec/batch)
2016-02-04 08:21:04.214329: step 170950, loss = 0.26 (302.8 examples/sec; 0.423 sec/batch)
2016-02-04 08:21:08.781210: step 170960, loss = 0.22 (288.0 examples/sec; 0.444 sec/batch)
2016-02-04 08:21:13.205082: step 170970, loss = 0.23 (305.5 examples/sec; 0.419 sec/batch)
2016-02-04 08:21:17.754662: step 170980, loss = 0.30 (338.7 examples/sec; 0.378 sec/batch)
2016-02-04 08:21:22.255942: step 170990, loss = 0.26 (304.1 examples/sec; 0.421 sec/batch)
2016-02-04 08:21:26.460628: step 171000, loss = 0.25 (295.6 examples/sec; 0.433 sec/batch)
2016-02-04 08:21:31.412606: step 171010, loss = 0.19 (272.8 examples/sec; 0.469 sec/batch)
2016-02-04 08:21:36.007512: step 171020, loss = 0.25 (307.4 examples/sec; 0.416 sec/batch)
2016-02-04 08:21:40.413995: step 171030, loss = 0.27 (282.8 examples/sec; 0.453 sec/batch)
2016-02-04 08:21:44.933730: step 171040, loss = 0.28 (272.8 examples/sec; 0.469 sec/batch)
2016-02-04 08:21:49.339234: step 171050, loss = 0.24 (290.6 examples/sec; 0.441 sec/batch)
2016-02-04 08:21:53.859802: step 171060, loss = 0.24 (294.7 examples/sec; 0.434 sec/batch)
2016-02-04 08:21:58.479506: step 171070, loss = 0.29 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 08:22:03.042302: step 171080, loss = 0.32 (256.4 examples/sec; 0.499 sec/batch)
2016-02-04 08:22:07.553120: step 171090, loss = 0.24 (243.9 examples/sec; 0.525 sec/batch)
2016-02-04 08:22:11.974134: step 171100, loss = 0.32 (303.4 examples/sec; 0.422 sec/batch)
2016-02-04 08:22:17.302417: step 171110, loss = 0.26 (244.2 examples/sec; 0.524 sec/batch)
2016-02-04 08:22:21.861642: step 171120, loss = 0.28 (283.3 examples/sec; 0.452 sec/batch)
2016-02-04 08:22:26.279166: step 171130, loss = 0.20 (270.8 examples/sec; 0.473 sec/batch)
2016-02-04 08:22:30.762476: step 171140, loss = 0.22 (300.5 examples/sec; 0.426 sec/batch)
2016-02-04 08:22:35.354019: step 171150, loss = 0.35 (282.6 examples/sec; 0.453 sec/batch)
2016-02-04 08:22:39.741355: step 171160, loss = 0.24 (313.9 examples/sec; 0.408 sec/batch)
2016-02-04 08:22:44.414514: step 171170, loss = 0.26 (277.8 examples/sec; 0.461 sec/batch)
2016-02-04 08:22:49.032055: step 171180, loss = 0.29 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 08:22:53.597978: step 171190, loss = 0.26 (276.0 examples/sec; 0.464 sec/batch)
2016-02-04 08:22:58.246153: step 171200, loss = 0.24 (312.1 examples/sec; 0.410 sec/batch)
2016-02-04 08:23:03.066050: step 171210, loss = 0.28 (334.4 examples/sec; 0.383 sec/batch)
2016-02-04 08:23:07.603278: step 171220, loss = 0.22 (283.6 examples/sec; 0.451 sec/batch)
2016-02-04 08:23:12.159543: step 171230, loss = 0.32 (273.1 examples/sec; 0.469 sec/batch)
2016-02-04 08:23:16.727798: step 171240, loss = 0.26 (266.1 examples/sec; 0.481 sec/batch)
2016-02-04 08:23:21.278321: step 171250, loss = 0.31 (277.0 examples/sec; 0.462 sec/batch)
2016-02-04 08:23:25.796341: step 171260, loss = 0.37 (299.9 examples/sec; 0.427 sec/batch)
2016-02-04 08:23:30.428733: step 171270, loss = 0.20 (240.2 examples/sec; 0.533 sec/batch)
2016-02-04 08:23:35.024225: step 171280, loss = 0.26 (329.9 examples/sec; 0.388 sec/batch)
2016-02-04 08:23:39.536546: step 171290, loss = 0.23 (307.6 examples/sec; 0.416 sec/batch)
2016-02-04 08:23:43.909443: step 171300, loss = 0.24 (321.6 examples/sec; 0.398 sec/batch)
2016-02-04 08:23:48.948211: step 171310, loss = 0.30 (243.1 examples/sec; 0.527 sec/batch)
2016-02-04 08:23:53.497394: step 171320, loss = 0.25 (277.8 examples/sec; 0.461 sec/batch)
2016-02-04 08:23:58.119416: step 171330, loss = 0.22 (267.0 examples/sec; 0.479 sec/batch)
2016-02-04 08:24:02.843019: step 171340, loss = 0.31 (267.4 examples/sec; 0.479 sec/batch)
2016-02-04 08:24:07.302306: step 171350, loss = 0.20 (284.6 examples/sec; 0.450 sec/batch)
2016-02-04 08:24:11.715276: step 171360, loss = 0.26 (315.8 examples/sec; 0.405 sec/batch)
2016-02-04 08:24:16.230681: step 171370, loss = 0.23 (268.2 examples/sec; 0.477 sec/batch)
2016-02-04 08:24:20.583516: step 171380, loss = 0.23 (295.0 examples/sec; 0.434 sec/batch)
2016-02-04 08:24:25.176138: step 171390, loss = 0.27 (295.3 examples/sec; 0.433 sec/batch)
2016-02-04 08:24:29.713720: step 171400, loss = 0.25 (274.9 examples/sec; 0.466 sec/batch)
2016-02-04 08:24:34.619320: step 171410, loss = 0.23 (310.6 examples/sec; 0.412 sec/batch)
2016-02-04 08:24:38.869842: step 171420, loss = 0.27 (315.8 examples/sec; 0.405 sec/batch)
2016-02-04 08:24:43.432809: step 171430, loss = 0.24 (289.8 examples/sec; 0.442 sec/batch)
2016-02-04 08:24:47.788286: step 171440, loss = 0.22 (299.9 examples/sec; 0.427 sec/batch)
2016-02-04 08:24:52.359269: step 171450, loss = 0.32 (271.7 examples/sec; 0.471 sec/batch)
2016-02-04 08:24:56.863213: step 171460, loss = 0.25 (283.9 examples/sec; 0.451 sec/batch)
2016-02-04 08:25:01.400956: step 171470, loss = 0.26 (316.4 examples/sec; 0.405 sec/batch)
2016-02-04 08:25:05.983844: step 171480, loss = 0.25 (298.5 examples/sec; 0.429 sec/batch)
2016-02-04 08:25:10.425618: step 171490, loss = 0.25 (289.6 examples/sec; 0.442 sec/batch)
2016-02-04 08:25:14.848478: step 171500, loss = 0.37 (285.8 examples/sec; 0.448 sec/batch)
2016-02-04 08:25:19.885270: step 171510, loss = 0.28 (267.9 examples/sec; 0.478 sec/batch)
2016-02-04 08:25:24.361330: step 171520, loss = 0.23 (299.0 examples/sec; 0.428 sec/batch)
2016-02-04 08:25:28.841213: step 171530, loss = 0.23 (292.3 examples/sec; 0.438 sec/batch)
2016-02-04 08:25:33.397476: step 171540, loss = 0.25 (253.1 examples/sec; 0.506 sec/batch)
2016-02-04 08:25:38.053928: step 171550, loss = 0.29 (278.5 examples/sec; 0.460 sec/batch)
2016-02-04 08:25:42.603407: step 171560, loss = 0.33 (254.6 examples/sec; 0.503 sec/batch)
2016-02-04 08:25:47.032630: step 171570, loss = 0.27 (289.7 examples/sec; 0.442 sec/batch)
2016-02-04 08:25:51.482755: step 171580, loss = 0.20 (272.7 examples/sec; 0.469 sec/batch)
2016-02-04 08:25:56.085645: step 171590, loss = 0.25 (271.7 examples/sec; 0.471 sec/batch)
2016-02-04 08:26:00.515200: step 171600, loss = 0.24 (313.4 examples/sec; 0.408 sec/batch)
2016-02-04 08:26:05.420495: step 171610, loss = 0.28 (288.5 examples/sec; 0.444 sec/batch)
2016-02-04 08:26:09.945671: step 171620, loss = 0.30 (282.5 examples/sec; 0.453 sec/batch)
2016-02-04 08:26:14.644108: step 171630, loss = 0.27 (262.5 examples/sec; 0.488 sec/batch)
2016-02-04 08:26:19.203250: step 171640, loss = 0.26 (247.8 examples/sec; 0.517 sec/batch)
2016-02-04 08:26:23.832711: step 171650, loss = 0.33 (261.5 examples/sec; 0.489 sec/batch)
2016-02-04 08:26:28.282140: step 171660, loss = 0.23 (287.5 examples/sec; 0.445 sec/batch)
2016-02-04 08:26:32.776356: step 171670, loss = 0.35 (279.0 examples/sec; 0.459 sec/batch)
2016-02-04 08:26:37.191308: step 171680, loss = 0.22 (256.8 examples/sec; 0.498 sec/batch)
2016-02-04 08:26:41.686912: step 171690, loss = 0.34 (292.2 examples/sec; 0.438 sec/batch)
2016-02-04 08:26:46.105634: step 171700, loss = 0.23 (264.2 examples/sec; 0.484 sec/batch)
2016-02-04 08:26:51.087549: step 171710, loss = 0.34 (275.2 examples/sec; 0.465 sec/batch)
2016-02-04 08:26:55.661274: step 171720, loss = 0.34 (271.7 examples/sec; 0.471 sec/batch)
2016-02-04 08:27:00.026615: step 171730, loss = 0.26 (284.8 examples/sec; 0.449 sec/batch)
2016-02-04 08:27:04.469232: step 171740, loss = 0.26 (299.0 examples/sec; 0.428 sec/batch)
2016-02-04 08:27:09.017901: step 171750, loss = 0.19 (282.6 examples/sec; 0.453 sec/batch)
2016-02-04 08:27:13.252949: step 171760, loss = 0.26 (273.3 examples/sec; 0.468 sec/batch)
2016-02-04 08:27:17.709210: step 171770, loss = 0.26 (304.0 examples/sec; 0.421 sec/batch)
2016-02-04 08:27:22.212067: step 171780, loss = 0.34 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 08:27:26.654803: step 171790, loss = 0.24 (332.3 examples/sec; 0.385 sec/batch)
2016-02-04 08:27:31.162565: step 171800, loss = 0.29 (285.6 examples/sec; 0.448 sec/batch)
2016-02-04 08:27:36.125470: step 171810, loss = 0.29 (285.0 examples/sec; 0.449 sec/batch)
2016-02-04 08:27:40.584649: step 171820, loss = 0.31 (271.9 examples/sec; 0.471 sec/batch)
2016-02-04 08:27:44.904436: step 171830, loss = 0.30 (285.9 examples/sec; 0.448 sec/batch)
2016-02-04 08:27:49.376759: step 171840, loss = 0.29 (291.5 examples/sec; 0.439 sec/batch)
2016-02-04 08:27:53.851802: step 171850, loss = 0.27 (295.3 examples/sec; 0.433 sec/batch)
2016-02-04 08:27:58.187420: step 171860, loss = 0.31 (319.2 examples/sec; 0.401 sec/batch)
2016-02-04 08:28:02.700605: step 171870, loss = 0.21 (285.4 examples/sec; 0.449 sec/batch)
2016-02-04 08:28:07.166668: step 171880, loss = 0.22 (278.5 examples/sec; 0.460 sec/batch)
2016-02-04 08:28:11.715426: step 171890, loss = 0.24 (278.7 examples/sec; 0.459 sec/batch)
2016-02-04 08:28:16.263142: step 171900, loss = 0.31 (266.1 examples/sec; 0.481 sec/batch)
2016-02-04 08:28:21.110979: step 171910, loss = 0.28 (300.1 examples/sec; 0.426 sec/batch)
2016-02-04 08:28:25.705785: step 171920, loss = 0.28 (302.7 examples/sec; 0.423 sec/batch)
2016-02-04 08:28:30.380359: step 171930, loss = 0.22 (251.8 examples/sec; 0.508 sec/batch)
2016-02-04 08:28:34.909608: step 171940, loss = 0.35 (265.1 examples/sec; 0.483 sec/batch)
2016-02-04 08:28:39.500078: step 171950, loss = 0.25 (309.9 examples/sec; 0.413 sec/batch)
2016-02-04 08:28:43.999456: step 171960, loss = 0.30 (267.2 examples/sec; 0.479 sec/batch)
2016-02-04 08:28:48.549895: step 171970, loss = 0.20 (280.9 examples/sec; 0.456 sec/batch)
2016-02-04 08:28:53.184986: step 171980, loss = 0.34 (275.9 examples/sec; 0.464 sec/batch)
2016-02-04 08:28:57.700602: step 171990, loss = 0.33 (279.3 examples/sec; 0.458 sec/batch)
2016-02-04 08:29:02.261382: step 172000, loss = 0.27 (304.3 examples/sec; 0.421 sec/batch)
2016-02-04 08:29:07.101727: step 172010, loss = 0.21 (274.5 examples/sec; 0.466 sec/batch)
2016-02-04 08:29:11.618012: step 172020, loss = 0.21 (337.9 examples/sec; 0.379 sec/batch)
2016-02-04 08:29:16.155714: step 172030, loss = 0.31 (277.7 examples/sec; 0.461 sec/batch)
2016-02-04 08:29:20.742496: step 172040, loss = 0.23 (279.6 examples/sec; 0.458 sec/batch)
2016-02-04 08:29:25.364783: step 172050, loss = 0.27 (257.9 examples/sec; 0.496 sec/batch)
2016-02-04 08:29:29.777226: step 172060, loss = 0.19 (317.2 examples/sec; 0.404 sec/batch)
2016-02-04 08:29:34.257768: step 172070, loss = 0.21 (289.8 examples/sec; 0.442 sec/batch)
2016-02-04 08:29:38.824035: step 172080, loss = 0.30 (293.5 examples/sec; 0.436 sec/batch)
2016-02-04 08:29:43.405446: step 172090, loss = 0.31 (253.0 examples/sec; 0.506 sec/batch)
2016-02-04 08:29:47.915768: step 172100, loss = 0.30 (292.3 examples/sec; 0.438 sec/batch)
2016-02-04 08:29:52.921897: step 172110, loss = 0.31 (289.3 examples/sec; 0.443 sec/batch)
2016-02-04 08:29:57.401259: step 172120, loss = 0.27 (276.2 examples/sec; 0.463 sec/batch)
2016-02-04 08:30:02.065726: step 172130, loss = 0.26 (274.5 examples/sec; 0.466 sec/batch)
2016-02-04 08:30:06.575128: step 172140, loss = 0.33 (307.1 examples/sec; 0.417 sec/batch)
2016-02-04 08:30:11.155348: step 172150, loss = 0.26 (252.8 examples/sec; 0.506 sec/batch)
2016-02-04 08:30:15.810440: step 172160, loss = 0.26 (271.4 examples/sec; 0.472 sec/batch)
2016-02-04 08:30:20.427885: step 172170, loss = 0.27 (268.3 examples/sec; 0.477 sec/batch)
2016-02-04 08:30:25.117467: step 172180, loss = 0.28 (293.7 examples/sec; 0.436 sec/batch)
2016-02-04 08:30:29.655096: step 172190, loss = 0.23 (262.1 examples/sec; 0.488 sec/batch)
2016-02-04 08:30:34.211405: step 172200, loss = 0.26 (275.9 examples/sec; 0.464 sec/batch)
2016-02-04 08:30:39.185144: step 172210, loss = 0.33 (297.2 examples/sec; 0.431 sec/batch)
2016-02-04 08:30:43.678889: step 172220, loss = 0.28 (297.2 examples/sec; 0.431 sec/batch)
2016-02-04 08:30:48.062057: step 172230, loss = 0.22 (307.2 examples/sec; 0.417 sec/batch)
2016-02-04 08:30:52.564186: step 172240, loss = 0.38 (259.8 examples/sec; 0.493 sec/batch)
2016-02-04 08:30:57.004867: step 172250, loss = 0.25 (274.8 examples/sec; 0.466 sec/batch)
2016-02-04 08:31:01.698236: step 172260, loss = 0.31 (279.7 examples/sec; 0.458 sec/batch)
2016-02-04 08:31:06.203750: step 172270, loss = 0.30 (298.8 examples/sec; 0.428 sec/batch)
2016-02-04 08:31:10.689031: step 172280, loss = 0.25 (289.9 examples/sec; 0.442 sec/batch)
2016-02-04 08:31:15.179859: step 172290, loss = 0.26 (310.5 examples/sec; 0.412 sec/batch)
2016-02-04 08:31:19.800295: step 172300, loss = 0.32 (254.5 examples/sec; 0.503 sec/batch)
2016-02-04 08:31:24.810074: step 172310, loss = 0.30 (259.8 examples/sec; 0.493 sec/batch)
2016-02-04 08:31:29.469847: step 172320, loss = 0.28 (257.2 examples/sec; 0.498 sec/batch)
2016-02-04 08:31:34.060534: step 172330, loss = 0.29 (311.9 examples/sec; 0.410 sec/batch)
2016-02-04 08:31:38.599086: step 172340, loss = 0.22 (292.2 examples/sec; 0.438 sec/batch)
2016-02-04 08:31:43.303436: step 172350, loss = 0.32 (273.7 examples/sec; 0.468 sec/batch)
2016-02-04 08:31:47.865336: step 172360, loss = 0.29 (267.0 examples/sec; 0.479 sec/batch)
2016-02-04 08:31:52.308521: step 172370, loss = 0.35 (288.1 examples/sec; 0.444 sec/batch)
2016-02-04 08:31:56.997308: step 172380, loss = 0.31 (271.4 examples/sec; 0.472 sec/batch)
2016-02-04 08:32:01.427411: step 172390, loss = 0.21 (274.6 examples/sec; 0.466 sec/batch)
2016-02-04 08:32:05.890036: step 172400, loss = 0.25 (276.6 examples/sec; 0.463 sec/batch)
2016-02-04 08:32:11.002228: step 172410, loss = 0.24 (288.2 examples/sec; 0.444 sec/batch)
2016-02-04 08:32:15.465128: step 172420, loss = 0.20 (299.3 examples/sec; 0.428 sec/batch)
2016-02-04 08:32:20.013388: step 172430, loss = 0.22 (273.1 examples/sec; 0.469 sec/batch)
2016-02-04 08:32:24.569349: step 172440, loss = 0.30 (265.0 examples/sec; 0.483 sec/batch)
2016-02-04 08:32:29.010279: step 172450, loss = 0.37 (289.1 examples/sec; 0.443 sec/batch)
2016-02-04 08:32:33.550677: step 172460, loss = 0.30 (298.3 examples/sec; 0.429 sec/batch)
2016-02-04 08:32:37.947208: step 172470, loss = 0.21 (239.3 examples/sec; 0.535 sec/batch)
2016-02-04 08:32:42.450235: step 172480, loss = 0.25 (301.6 examples/sec; 0.424 sec/batch)
2016-02-04 08:32:46.779300: step 172490, loss = 0.31 (302.0 examples/sec; 0.424 sec/batch)
2016-02-04 08:32:50.983293: step 172500, loss = 0.29 (309.4 examples/sec; 0.414 sec/batch)
2016-02-04 08:32:55.930227: step 172510, loss = 0.29 (301.5 examples/sec; 0.425 sec/batch)
2016-02-04 08:33:00.574050: step 172520, loss = 0.20 (280.3 examples/sec; 0.457 sec/batch)
2016-02-04 08:33:05.118584: step 172530, loss = 0.27 (272.3 examples/sec; 0.470 sec/batch)
2016-02-04 08:33:09.590032: step 172540, loss = 0.30 (325.5 examples/sec; 0.393 sec/batch)
2016-02-04 08:33:14.132790: step 172550, loss = 0.33 (305.1 examples/sec; 0.420 sec/batch)
2016-02-04 08:33:18.535962: step 172560, loss = 0.29 (297.5 examples/sec; 0.430 sec/batch)
2016-02-04 08:33:23.094971: step 172570, loss = 0.24 (251.7 examples/sec; 0.509 sec/batch)
2016-02-04 08:33:27.777198: step 172580, loss = 0.19 (276.3 examples/sec; 0.463 sec/batch)
2016-02-04 08:33:32.181258: step 172590, loss = 0.25 (293.9 examples/sec; 0.436 sec/batch)
2016-02-04 08:33:36.760598: step 172600, loss = 0.30 (289.7 examples/sec; 0.442 sec/batch)
2016-02-04 08:33:41.558749: step 172610, loss = 0.43 (276.1 examples/sec; 0.464 sec/batch)
2016-02-04 08:33:46.204390: step 172620, loss = 0.28 (310.7 examples/sec; 0.412 sec/batch)
2016-02-04 08:33:50.792738: step 172630, loss = 0.28 (257.6 examples/sec; 0.497 sec/batch)
2016-02-04 08:33:55.316024: step 172640, loss = 0.26 (280.0 examples/sec; 0.457 sec/batch)
2016-02-04 08:33:59.782673: step 172650, loss = 0.22 (265.1 examples/sec; 0.483 sec/batch)
2016-02-04 08:34:04.216703: step 172660, loss = 0.31 (253.7 examples/sec; 0.504 sec/batch)
2016-02-04 08:34:08.746171: step 172670, loss = 0.28 (272.2 examples/sec; 0.470 sec/batch)
2016-02-04 08:34:13.451016: step 172680, loss = 0.28 (278.4 examples/sec; 0.460 sec/batch)
2016-02-04 08:34:18.063788: step 172690, loss = 0.25 (283.5 examples/sec; 0.452 sec/batch)
2016-02-04 08:34:22.557980: step 172700, loss = 0.26 (274.8 examples/sec; 0.466 sec/batch)
2016-02-04 08:34:27.621252: step 172710, loss = 0.29 (306.7 examples/sec; 0.417 sec/batch)
2016-02-04 08:34:32.179453: step 172720, loss = 0.31 (262.5 examples/sec; 0.488 sec/batch)
2016-02-04 08:34:36.561439: step 172730, loss = 0.30 (301.7 examples/sec; 0.424 sec/batch)
2016-02-04 08:34:41.179276: step 172740, loss = 0.30 (300.6 examples/sec; 0.426 sec/batch)
2016-02-04 08:34:45.709910: step 172750, loss = 0.34 (259.1 examples/sec; 0.494 sec/batch)
2016-02-04 08:34:50.218145: step 172760, loss = 0.30 (270.4 examples/sec; 0.473 sec/batch)
2016-02-04 08:34:54.589136: step 172770, loss = 0.24 (317.2 examples/sec; 0.403 sec/batch)
2016-02-04 08:34:59.046920: step 172780, loss = 0.36 (268.5 examples/sec; 0.477 sec/batch)
2016-02-04 08:35:03.611288: step 172790, loss = 0.34 (273.7 examples/sec; 0.468 sec/batch)
2016-02-04 08:35:08.184703: step 172800, loss = 0.29 (273.1 examples/sec; 0.469 sec/batch)
2016-02-04 08:35:13.007622: step 172810, loss = 0.26 (288.2 examples/sec; 0.444 sec/batch)
2016-02-04 08:35:17.500260: step 172820, loss = 0.35 (279.1 examples/sec; 0.459 sec/batch)
2016-02-04 08:35:22.152375: step 172830, loss = 0.23 (277.4 examples/sec; 0.461 sec/batch)
2016-02-04 08:35:26.468584: step 172840, loss = 0.26 (299.2 examples/sec; 0.428 sec/batch)
2016-02-04 08:35:31.068201: step 172850, loss = 0.26 (300.1 examples/sec; 0.427 sec/batch)
2016-02-04 08:35:35.635165: step 172860, loss = 0.34 (260.8 examples/sec; 0.491 sec/batch)
2016-02-04 08:35:40.108285: step 172870, loss = 0.35 (259.9 examples/sec; 0.492 sec/batch)
2016-02-04 08:35:44.627160: step 172880, loss = 0.27 (317.3 examples/sec; 0.403 sec/batch)
2016-02-04 08:35:49.020250: step 172890, loss = 0.24 (285.0 examples/sec; 0.449 sec/batch)
2016-02-04 08:35:53.443006: step 172900, loss = 0.24 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 08:35:58.347808: step 172910, loss = 0.50 (304.6 examples/sec; 0.420 sec/batch)
2016-02-04 08:36:02.915740: step 172920, loss = 0.28 (271.3 examples/sec; 0.472 sec/batch)
2016-02-04 08:36:07.292493: step 172930, loss = 0.35 (261.3 examples/sec; 0.490 sec/batch)
2016-02-04 08:36:11.754718: step 172940, loss = 0.27 (313.2 examples/sec; 0.409 sec/batch)
2016-02-04 08:36:16.383188: step 172950, loss = 0.28 (295.9 examples/sec; 0.433 sec/batch)
2016-02-04 08:36:20.949749: step 172960, loss = 0.46 (288.8 examples/sec; 0.443 sec/batch)
2016-02-04 08:36:25.532355: step 172970, loss = 0.23 (280.8 examples/sec; 0.456 sec/batch)
2016-02-04 08:36:29.993586: step 172980, loss = 0.40 (268.7 examples/sec; 0.476 sec/batch)
2016-02-04 08:36:34.603302: step 172990, loss = 0.25 (311.7 examples/sec; 0.411 sec/batch)
2016-02-04 08:36:39.148218: step 173000, loss = 0.25 (294.6 examples/sec; 0.434 sec/batch)
2016-02-04 08:36:43.900249: step 173010, loss = 0.27 (266.3 examples/sec; 0.481 sec/batch)
2016-02-04 08:36:48.376577: step 173020, loss = 0.20 (282.1 examples/sec; 0.454 sec/batch)
2016-02-04 08:36:52.971606: step 173030, loss = 0.26 (293.1 examples/sec; 0.437 sec/batch)
2016-02-04 08:36:57.526678: step 173040, loss = 0.36 (317.9 examples/sec; 0.403 sec/batch)
2016-02-04 08:37:01.939258: step 173050, loss = 0.32 (303.1 examples/sec; 0.422 sec/batch)
2016-02-04 08:37:06.330223: step 173060, loss = 0.26 (314.4 examples/sec; 0.407 sec/batch)
2016-02-04 08:37:10.626175: step 173070, loss = 0.29 (273.3 examples/sec; 0.468 sec/batch)
2016-02-04 08:37:15.256673: step 173080, loss = 0.21 (297.2 examples/sec; 0.431 sec/batch)
2016-02-04 08:37:19.867894: step 173090, loss = 0.28 (256.3 examples/sec; 0.499 sec/batch)
2016-02-04 08:37:24.583341: step 173100, loss = 0.25 (261.6 examples/sec; 0.489 sec/batch)
2016-02-04 08:37:29.463028: step 173110, loss = 0.27 (309.7 examples/sec; 0.413 sec/batch)
2016-02-04 08:37:33.932747: step 173120, loss = 0.32 (267.4 examples/sec; 0.479 sec/batch)
2016-02-04 08:37:38.622279: step 173130, loss = 0.27 (266.1 examples/sec; 0.481 sec/batch)
2016-02-04 08:37:43.078233: step 173140, loss = 0.37 (279.9 examples/sec; 0.457 sec/batch)
2016-02-04 08:37:47.622384: step 173150, loss = 0.24 (275.2 examples/sec; 0.465 sec/batch)
2016-02-04 08:37:52.129140: step 173160, loss = 0.28 (278.4 examples/sec; 0.460 sec/batch)
2016-02-04 08:37:56.891245: step 173170, loss = 0.22 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 08:38:01.556201: step 173180, loss = 0.26 (285.6 examples/sec; 0.448 sec/batch)
2016-02-04 08:38:06.045122: step 173190, loss = 0.24 (288.6 examples/sec; 0.444 sec/batch)
2016-02-04 08:38:10.589580: step 173200, loss = 0.24 (298.6 examples/sec; 0.429 sec/batch)
2016-02-04 08:38:15.735787: step 173210, loss = 0.24 (252.4 examples/sec; 0.507 sec/batch)
2016-02-04 08:38:20.147415: step 173220, loss = 0.29 (279.2 examples/sec; 0.458 sec/batch)
2016-02-04 08:38:24.587961: step 173230, loss = 0.29 (302.4 examples/sec; 0.423 sec/batch)
2016-02-04 08:38:29.159642: step 173240, loss = 0.31 (295.4 examples/sec; 0.433 sec/batch)
2016-02-04 08:38:33.508873: step 173250, loss = 0.21 (270.2 examples/sec; 0.474 sec/batch)
2016-02-04 08:38:38.140457: step 173260, loss = 0.28 (282.4 examples/sec; 0.453 sec/batch)
2016-02-04 08:38:42.637722: step 173270, loss = 0.27 (288.5 examples/sec; 0.444 sec/batch)
2016-02-04 08:38:47.171156: step 173280, loss = 0.21 (274.9 examples/sec; 0.466 sec/batch)
2016-02-04 08:38:51.775257: step 173290, loss = 0.30 (261.8 examples/sec; 0.489 sec/batch)
2016-02-04 08:38:56.187183: step 173300, loss = 0.22 (286.5 examples/sec; 0.447 sec/batch)
2016-02-04 08:39:01.005841: step 173310, loss = 0.28 (307.7 examples/sec; 0.416 sec/batch)
2016-02-04 08:39:05.603125: step 173320, loss = 0.33 (326.8 examples/sec; 0.392 sec/batch)
2016-02-04 08:39:10.126064: step 173330, loss = 0.32 (274.3 examples/sec; 0.467 sec/batch)
2016-02-04 08:39:14.759145: step 173340, loss = 0.33 (277.5 examples/sec; 0.461 sec/batch)
2016-02-04 08:39:19.399499: step 173350, loss = 0.34 (249.2 examples/sec; 0.514 sec/batch)
2016-02-04 08:39:23.994328: step 173360, loss = 0.30 (288.6 examples/sec; 0.444 sec/batch)
2016-02-04 08:39:28.518232: step 173370, loss = 0.16 (314.3 examples/sec; 0.407 sec/batch)
2016-02-04 08:39:33.022211: step 173380, loss = 0.27 (317.5 examples/sec; 0.403 sec/batch)
2016-02-04 08:39:37.469445: step 173390, loss = 0.25 (275.0 examples/sec; 0.466 sec/batch)
2016-02-04 08:39:41.973520: step 173400, loss = 0.36 (291.7 examples/sec; 0.439 sec/batch)
2016-02-04 08:39:46.941247: step 173410, loss = 0.23 (321.8 examples/sec; 0.398 sec/batch)
2016-02-04 08:39:51.395897: step 173420, loss = 0.26 (261.6 examples/sec; 0.489 sec/batch)
2016-02-04 08:39:56.130782: step 173430, loss = 0.28 (275.9 examples/sec; 0.464 sec/batch)
2016-02-04 08:40:00.758137: step 173440, loss = 0.22 (294.3 examples/sec; 0.435 sec/batch)
2016-02-04 08:40:05.282192: step 173450, loss = 0.24 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 08:40:09.843898: step 173460, loss = 0.23 (261.9 examples/sec; 0.489 sec/batch)
2016-02-04 08:40:14.151117: step 173470, loss = 0.26 (282.1 examples/sec; 0.454 sec/batch)
2016-02-04 08:40:18.505588: step 173480, loss = 0.31 (314.6 examples/sec; 0.407 sec/batch)
2016-02-04 08:40:22.817572: step 173490, loss = 0.25 (325.9 examples/sec; 0.393 sec/batch)
2016-02-04 08:40:27.451026: step 173500, loss = 0.30 (259.5 examples/sec; 0.493 sec/batch)
2016-02-04 08:40:32.452366: step 173510, loss = 0.19 (288.2 examples/sec; 0.444 sec/batch)
2016-02-04 08:40:37.150292: step 173520, loss = 0.25 (294.3 examples/sec; 0.435 sec/batch)
2016-02-04 08:40:41.630845: step 173530, loss = 0.28 (270.4 examples/sec; 0.473 sec/batch)
2016-02-04 08:40:46.339008: step 173540, loss = 0.27 (298.4 examples/sec; 0.429 sec/batch)
2016-02-04 08:40:50.836486: step 173550, loss = 0.33 (289.2 examples/sec; 0.443 sec/batch)
2016-02-04 08:40:55.439478: step 173560, loss = 0.24 (276.5 examples/sec; 0.463 sec/batch)
2016-02-04 08:41:00.024280: step 173570, loss = 0.30 (290.4 examples/sec; 0.441 sec/batch)
2016-02-04 08:41:04.565378: step 173580, loss = 0.33 (285.4 examples/sec; 0.448 sec/batch)
2016-02-04 08:41:09.098782: step 173590, loss = 0.28 (290.5 examples/sec; 0.441 sec/batch)
2016-02-04 08:41:13.460694: step 173600, loss = 0.25 (275.1 examples/sec; 0.465 sec/batch)
2016-02-04 08:41:18.553737: step 173610, loss = 0.25 (297.2 examples/sec; 0.431 sec/batch)
2016-02-04 08:41:23.230038: step 173620, loss = 0.25 (273.8 examples/sec; 0.467 sec/batch)
2016-02-04 08:41:27.798791: step 173630, loss = 0.27 (325.2 examples/sec; 0.394 sec/batch)
2016-02-04 08:41:32.248547: step 173640, loss = 0.22 (288.4 examples/sec; 0.444 sec/batch)
2016-02-04 08:41:36.647682: step 173650, loss = 0.26 (304.4 examples/sec; 0.420 sec/batch)
2016-02-04 08:41:41.168521: step 173660, loss = 0.29 (278.4 examples/sec; 0.460 sec/batch)
2016-02-04 08:41:45.706049: step 173670, loss = 0.25 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 08:41:50.302493: step 173680, loss = 0.22 (282.6 examples/sec; 0.453 sec/batch)
2016-02-04 08:41:54.797750: step 173690, loss = 0.27 (293.2 examples/sec; 0.437 sec/batch)
2016-02-04 08:41:59.223018: step 173700, loss = 0.35 (299.7 examples/sec; 0.427 sec/batch)
2016-02-04 08:42:04.093933: step 173710, loss = 0.23 (303.6 examples/sec; 0.422 sec/batch)
2016-02-04 08:42:08.580476: step 173720, loss = 0.37 (296.1 examples/sec; 0.432 sec/batch)
2016-02-04 08:42:13.036017: step 173730, loss = 0.28 (312.4 examples/sec; 0.410 sec/batch)
2016-02-04 08:42:17.548684: step 173740, loss = 0.21 (263.1 examples/sec; 0.486 sec/batch)
2016-02-04 08:42:21.903922: step 173750, loss = 0.32 (319.3 examples/sec; 0.401 sec/batch)
2016-02-04 08:42:26.356218: step 173760, loss = 0.25 (299.5 examples/sec; 0.427 sec/batch)
2016-02-04 08:42:30.794723: step 173770, loss = 0.24 (296.8 examples/sec; 0.431 sec/batch)
2016-02-04 08:42:35.260047: step 173780, loss = 0.22 (285.2 examples/sec; 0.449 sec/batch)
2016-02-04 08:42:39.990882: step 173790, loss = 0.26 (283.7 examples/sec; 0.451 sec/batch)
2016-02-04 08:42:44.329891: step 173800, loss = 0.24 (291.4 examples/sec; 0.439 sec/batch)
2016-02-04 08:42:49.150433: step 173810, loss = 0.31 (302.0 examples/sec; 0.424 sec/batch)
2016-02-04 08:42:53.656623: step 173820, loss = 0.19 (299.1 examples/sec; 0.428 sec/batch)
2016-02-04 08:42:58.145139: step 173830, loss = 0.26 (282.5 examples/sec; 0.453 sec/batch)
2016-02-04 08:43:02.701449: step 173840, loss = 0.24 (284.9 examples/sec; 0.449 sec/batch)
2016-02-04 08:43:07.277664: step 173850, loss = 0.22 (284.4 examples/sec; 0.450 sec/batch)
2016-02-04 08:43:11.760541: step 173860, loss = 0.31 (292.9 examples/sec; 0.437 sec/batch)
2016-02-04 08:43:16.228870: step 173870, loss = 0.19 (278.3 examples/sec; 0.460 sec/batch)
2016-02-04 08:43:20.682123: step 173880, loss = 0.21 (295.2 examples/sec; 0.434 sec/batch)
2016-02-04 08:43:25.105521: step 173890, loss = 0.31 (339.5 examples/sec; 0.377 sec/batch)
2016-02-04 08:43:29.634436: step 173900, loss = 0.30 (264.6 examples/sec; 0.484 sec/batch)
2016-02-04 08:43:34.471590: step 173910, loss = 0.26 (263.5 examples/sec; 0.486 sec/batch)
2016-02-04 08:43:39.110462: step 173920, loss = 0.30 (287.8 examples/sec; 0.445 sec/batch)
2016-02-04 08:43:43.749069: step 173930, loss = 0.24 (322.7 examples/sec; 0.397 sec/batch)
2016-02-04 08:43:48.326125: step 173940, loss = 0.27 (259.9 examples/sec; 0.493 sec/batch)
2016-02-04 08:43:52.747348: step 173950, loss = 0.32 (288.5 examples/sec; 0.444 sec/batch)
2016-02-04 08:43:57.401610: step 173960, loss = 0.22 (285.2 examples/sec; 0.449 sec/batch)
2016-02-04 08:44:02.052729: step 173970, loss = 0.39 (311.0 examples/sec; 0.412 sec/batch)
2016-02-04 08:44:06.568457: step 173980, loss = 0.26 (324.2 examples/sec; 0.395 sec/batch)
2016-02-04 08:44:10.982326: step 173990, loss = 0.26 (339.3 examples/sec; 0.377 sec/batch)
2016-02-04 08:44:15.517992: step 174000, loss = 0.23 (278.0 examples/sec; 0.460 sec/batch)
2016-02-04 08:44:20.446895: step 174010, loss = 0.20 (300.5 examples/sec; 0.426 sec/batch)
2016-02-04 08:44:24.961517: step 174020, loss = 0.32 (290.0 examples/sec; 0.441 sec/batch)
2016-02-04 08:44:29.524758: step 174030, loss = 0.28 (249.2 examples/sec; 0.514 sec/batch)
2016-02-04 08:44:34.186315: step 174040, loss = 0.35 (281.3 examples/sec; 0.455 sec/batch)
2016-02-04 08:44:38.689120: step 174050, loss = 0.32 (287.0 examples/sec; 0.446 sec/batch)
2016-02-04 08:44:43.252138: step 174060, loss = 0.25 (329.3 examples/sec; 0.389 sec/batch)
2016-02-04 08:44:47.761919: step 174070, loss = 0.38 (314.0 examples/sec; 0.408 sec/batch)
2016-02-04 08:44:52.250351: step 174080, loss = 0.20 (281.2 examples/sec; 0.455 sec/batch)
2016-02-04 08:44:56.833626: step 174090, loss = 0.20 (269.4 examples/sec; 0.475 sec/batch)
2016-02-04 08:45:01.378853: step 174100, loss = 0.22 (291.3 examples/sec; 0.439 sec/batch)
2016-02-04 08:45:06.269853: step 174110, loss = 0.28 (290.9 examples/sec; 0.440 sec/batch)
2016-02-04 08:45:10.721972: step 174120, loss = 0.32 (305.4 examples/sec; 0.419 sec/batch)
2016-02-04 08:45:15.267081: step 174130, loss = 0.19 (321.8 examples/sec; 0.398 sec/batch)
2016-02-04 08:45:19.796872: step 174140, loss = 0.27 (305.8 examples/sec; 0.419 sec/batch)
2016-02-04 08:45:24.272150: step 174150, loss = 0.19 (305.3 examples/sec; 0.419 sec/batch)
2016-02-04 08:45:28.694163: step 174160, loss = 0.21 (310.9 examples/sec; 0.412 sec/batch)
2016-02-04 08:45:33.292129: step 174170, loss = 0.25 (291.3 examples/sec; 0.439 sec/batch)
2016-02-04 08:45:37.916406: step 174180, loss = 0.26 (255.4 examples/sec; 0.501 sec/batch)
2016-02-04 08:45:42.310187: step 174190, loss = 0.19 (288.8 examples/sec; 0.443 sec/batch)
2016-02-04 08:45:46.764970: step 174200, loss = 0.28 (244.7 examples/sec; 0.523 sec/batch)
2016-02-04 08:45:51.671618: step 174210, loss = 0.25 (287.0 examples/sec; 0.446 sec/batch)
2016-02-04 08:45:56.207067: step 174220, loss = 0.27 (281.7 examples/sec; 0.454 sec/batch)
2016-02-04 08:46:00.868723: step 174230, loss = 0.21 (269.8 examples/sec; 0.474 sec/batch)
2016-02-04 08:46:05.342775: step 174240, loss = 0.37 (301.2 examples/sec; 0.425 sec/batch)
2016-02-04 08:46:09.934880: step 174250, loss = 0.25 (251.7 examples/sec; 0.509 sec/batch)
2016-02-04 08:46:14.507679: step 174260, loss = 0.23 (293.1 examples/sec; 0.437 sec/batch)
2016-02-04 08:46:19.130449: step 174270, loss = 0.22 (284.3 examples/sec; 0.450 sec/batch)
2016-02-04 08:46:23.552977: step 174280, loss = 0.36 (296.9 examples/sec; 0.431 sec/batch)
2016-02-04 08:46:28.086567: step 174290, loss = 0.33 (278.9 examples/sec; 0.459 sec/batch)
2016-02-04 08:46:32.523398: step 174300, loss = 0.23 (319.7 examples/sec; 0.400 sec/batch)
2016-02-04 08:46:37.548032: step 174310, loss = 0.18 (283.2 examples/sec; 0.452 sec/batch)
2016-02-04 08:46:42.011118: step 174320, loss = 0.26 (281.3 examples/sec; 0.455 sec/batch)
2016-02-04 08:46:46.420111: step 174330, loss = 0.24 (313.5 examples/sec; 0.408 sec/batch)
2016-02-04 08:46:50.860011: step 174340, loss = 0.28 (325.8 examples/sec; 0.393 sec/batch)
2016-02-04 08:46:55.336025: step 174350, loss = 0.32 (292.7 examples/sec; 0.437 sec/batch)
2016-02-04 08:46:59.887612: step 174360, loss = 0.29 (277.9 examples/sec; 0.461 sec/batch)
2016-02-04 08:47:04.554951: step 174370, loss = 0.23 (265.9 examples/sec; 0.481 sec/batch)
2016-02-04 08:47:09.079768: step 174380, loss = 0.30 (274.9 examples/sec; 0.466 sec/batch)
2016-02-04 08:47:13.706615: step 174390, loss = 0.28 (268.0 examples/sec; 0.478 sec/batch)
2016-02-04 08:47:18.277254: step 174400, loss = 0.27 (279.3 examples/sec; 0.458 sec/batch)
2016-02-04 08:47:23.348362: step 174410, loss = 0.35 (296.7 examples/sec; 0.431 sec/batch)
2016-02-04 08:47:27.936111: step 174420, loss = 0.25 (286.4 examples/sec; 0.447 sec/batch)
2016-02-04 08:47:32.536314: step 174430, loss = 0.24 (281.1 examples/sec; 0.455 sec/batch)
2016-02-04 08:47:37.008267: step 174440, loss = 0.32 (280.5 examples/sec; 0.456 sec/batch)
2016-02-04 08:47:41.497575: step 174450, loss = 0.24 (254.8 examples/sec; 0.502 sec/batch)
2016-02-04 08:47:46.140268: step 174460, loss = 0.29 (274.3 examples/sec; 0.467 sec/batch)
2016-02-04 08:47:50.701551: step 174470, loss = 0.30 (259.0 examples/sec; 0.494 sec/batch)
2016-02-04 08:47:55.300131: step 174480, loss = 0.27 (270.0 examples/sec; 0.474 sec/batch)
2016-02-04 08:47:59.887507: step 174490, loss = 0.33 (282.3 examples/sec; 0.453 sec/batch)
2016-02-04 08:48:04.283529: step 174500, loss = 0.24 (287.7 examples/sec; 0.445 sec/batch)
2016-02-04 08:48:09.314526: step 174510, loss = 0.22 (277.6 examples/sec; 0.461 sec/batch)
2016-02-04 08:48:13.870189: step 174520, loss = 0.29 (265.0 examples/sec; 0.483 sec/batch)
2016-02-04 08:48:18.396304: step 174530, loss = 0.29 (255.5 examples/sec; 0.501 sec/batch)
2016-02-04 08:48:22.791267: step 174540, loss = 0.28 (350.0 examples/sec; 0.366 sec/batch)
2016-02-04 08:48:27.178353: step 174550, loss = 0.25 (308.7 examples/sec; 0.415 sec/batch)
2016-02-04 08:48:31.799435: step 174560, loss = 0.38 (253.0 examples/sec; 0.506 sec/batch)
2016-02-04 08:48:36.336327: step 174570, loss = 0.26 (284.7 examples/sec; 0.450 sec/batch)
2016-02-04 08:48:41.017668: step 174580, loss = 0.24 (241.7 examples/sec; 0.530 sec/batch)
2016-02-04 08:48:45.677533: step 174590, loss = 0.23 (249.5 examples/sec; 0.513 sec/batch)
2016-02-04 08:48:50.179810: step 174600, loss = 0.23 (265.9 examples/sec; 0.481 sec/batch)
2016-02-04 08:48:55.049228: step 174610, loss = 0.35 (296.2 examples/sec; 0.432 sec/batch)
2016-02-04 08:48:59.622387: step 174620, loss = 0.26 (259.3 examples/sec; 0.494 sec/batch)
2016-02-04 08:49:04.298669: step 174630, loss = 0.29 (264.9 examples/sec; 0.483 sec/batch)
2016-02-04 08:49:08.976754: step 174640, loss = 0.33 (258.8 examples/sec; 0.495 sec/batch)
2016-02-04 08:49:13.445921: step 174650, loss = 0.25 (262.8 examples/sec; 0.487 sec/batch)
2016-02-04 08:49:17.925886: step 174660, loss = 0.30 (271.5 examples/sec; 0.472 sec/batch)
2016-02-04 08:49:22.652876: step 174670, loss = 0.29 (257.9 examples/sec; 0.496 sec/batch)
2016-02-04 08:49:27.236536: step 174680, loss = 0.34 (267.6 examples/sec; 0.478 sec/batch)
2016-02-04 08:49:31.865712: step 174690, loss = 0.26 (258.1 examples/sec; 0.496 sec/batch)
2016-02-04 08:49:36.297090: step 174700, loss = 0.30 (284.0 examples/sec; 0.451 sec/batch)
2016-02-04 08:49:41.146219: step 174710, loss = 0.31 (267.6 examples/sec; 0.478 sec/batch)
2016-02-04 08:49:45.634167: step 174720, loss = 0.21 (315.6 examples/sec; 0.406 sec/batch)
2016-02-04 08:49:50.256047: step 174730, loss = 0.26 (276.6 examples/sec; 0.463 sec/batch)
2016-02-04 08:49:54.821120: step 174740, loss = 0.32 (329.5 examples/sec; 0.388 sec/batch)
2016-02-04 08:49:59.441890: step 174750, loss = 0.27 (280.7 examples/sec; 0.456 sec/batch)
2016-02-04 08:50:03.916003: step 174760, loss = 0.37 (296.8 examples/sec; 0.431 sec/batch)
2016-02-04 08:50:08.485071: step 174770, loss = 0.33 (295.2 examples/sec; 0.434 sec/batch)
2016-02-04 08:50:12.879777: step 174780, loss = 0.37 (286.6 examples/sec; 0.447 sec/batch)
2016-02-04 08:50:17.402322: step 174790, loss = 0.22 (303.2 examples/sec; 0.422 sec/batch)
2016-02-04 08:50:21.913513: step 174800, loss = 0.24 (281.4 examples/sec; 0.455 sec/batch)
2016-02-04 08:50:26.865171: step 174810, loss = 0.28 (315.1 examples/sec; 0.406 sec/batch)
2016-02-04 08:50:31.307407: step 174820, loss = 0.27 (297.3 examples/sec; 0.431 sec/batch)
2016-02-04 08:50:35.752168: step 174830, loss = 0.26 (299.7 examples/sec; 0.427 sec/batch)
2016-02-04 08:50:40.118575: step 174840, loss = 0.32 (295.4 examples/sec; 0.433 sec/batch)
2016-02-04 08:50:44.605486: step 174850, loss = 0.29 (305.1 examples/sec; 0.420 sec/batch)
2016-02-04 08:50:49.244985: step 174860, loss = 0.29 (305.5 examples/sec; 0.419 sec/batch)
2016-02-04 08:50:53.835574: step 174870, loss = 0.25 (330.5 examples/sec; 0.387 sec/batch)
2016-02-04 08:50:58.413547: step 174880, loss = 0.31 (296.5 examples/sec; 0.432 sec/batch)
2016-02-04 08:51:02.829500: step 174890, loss = 0.27 (305.2 examples/sec; 0.419 sec/batch)
2016-02-04 08:51:07.384286: step 174900, loss = 0.22 (265.7 examples/sec; 0.482 sec/batch)
2016-02-04 08:51:12.559119: step 174910, loss = 0.23 (289.4 examples/sec; 0.442 sec/batch)
2016-02-04 08:51:17.110991: step 174920, loss = 0.28 (278.0 examples/sec; 0.460 sec/batch)
2016-02-04 08:51:21.742513: step 174930, loss = 0.26 (300.8 examples/sec; 0.425 sec/batch)
2016-02-04 08:51:26.277802: step 174940, loss = 0.26 (306.8 examples/sec; 0.417 sec/batch)
2016-02-04 08:51:30.898699: step 174950, loss = 0.22 (303.4 examples/sec; 0.422 sec/batch)
2016-02-04 08:51:35.499109: step 174960, loss = 0.34 (279.3 examples/sec; 0.458 sec/batch)
2016-02-04 08:51:40.022078: step 174970, loss = 0.28 (268.8 examples/sec; 0.476 sec/batch)
2016-02-04 08:51:44.547861: step 174980, loss = 0.23 (252.9 examples/sec; 0.506 sec/batch)
2016-02-04 08:51:48.937069: step 174990, loss = 0.34 (299.9 examples/sec; 0.427 sec/batch)
2016-02-04 08:51:53.519618: step 175000, loss = 0.21 (288.9 examples/sec; 0.443 sec/batch)
2016-02-04 08:51:58.648641: step 175010, loss = 0.32 (291.6 examples/sec; 0.439 sec/batch)
2016-02-04 08:52:03.351710: step 175020, loss = 0.27 (259.0 examples/sec; 0.494 sec/batch)
2016-02-04 08:52:07.747200: step 175030, loss = 0.41 (314.2 examples/sec; 0.407 sec/batch)
2016-02-04 08:52:12.020986: step 175040, loss = 0.21 (270.6 examples/sec; 0.473 sec/batch)
2016-02-04 08:52:16.348945: step 175050, loss = 0.30 (330.2 examples/sec; 0.388 sec/batch)
2016-02-04 08:52:20.887330: step 175060, loss = 0.23 (279.9 examples/sec; 0.457 sec/batch)
2016-02-04 08:52:25.264988: step 175070, loss = 0.35 (317.0 examples/sec; 0.404 sec/batch)
2016-02-04 08:52:29.786656: step 175080, loss = 0.38 (286.5 examples/sec; 0.447 sec/batch)
2016-02-04 08:52:34.202911: step 175090, loss = 0.29 (300.4 examples/sec; 0.426 sec/batch)
2016-02-04 08:52:38.519494: step 175100, loss = 0.33 (291.4 examples/sec; 0.439 sec/batch)
2016-02-04 08:52:43.457636: step 175110, loss = 0.25 (302.9 examples/sec; 0.423 sec/batch)
2016-02-04 08:52:47.894425: step 175120, loss = 0.25 (303.2 examples/sec; 0.422 sec/batch)
2016-02-04 08:52:52.426052: step 175130, loss = 0.28 (258.3 examples/sec; 0.496 sec/batch)
2016-02-04 08:52:56.919544: step 175140, loss = 0.20 (306.7 examples/sec; 0.417 sec/batch)
2016-02-04 08:53:01.368116: step 175150, loss = 0.27 (379.4 examples/sec; 0.337 sec/batch)
2016-02-04 08:53:05.978330: step 175160, loss = 0.28 (289.6 examples/sec; 0.442 sec/batch)
2016-02-04 08:53:10.622687: step 175170, loss = 0.36 (262.4 examples/sec; 0.488 sec/batch)
2016-02-04 08:53:15.164960: step 175180, loss = 0.29 (261.6 examples/sec; 0.489 sec/batch)
2016-02-04 08:53:19.707385: step 175190, loss = 0.38 (279.2 examples/sec; 0.458 sec/batch)
2016-02-04 08:53:24.419973: step 175200, loss = 0.32 (257.6 examples/sec; 0.497 sec/batch)
2016-02-04 08:53:29.338111: step 175210, loss = 0.25 (269.8 examples/sec; 0.475 sec/batch)
2016-02-04 08:53:33.893605: step 175220, loss = 0.27 (254.5 examples/sec; 0.503 sec/batch)
2016-02-04 08:53:38.269827: step 175230, loss = 0.19 (306.5 examples/sec; 0.418 sec/batch)
2016-02-04 08:53:42.852039: step 175240, loss = 0.23 (301.5 examples/sec; 0.425 sec/batch)
2016-02-04 08:53:47.507402: step 175250, loss = 0.28 (289.2 examples/sec; 0.443 sec/batch)
2016-02-04 08:53:51.958613: step 175260, loss = 0.23 (256.5 examples/sec; 0.499 sec/batch)
2016-02-04 08:53:56.394561: step 175270, loss = 0.40 (297.3 examples/sec; 0.430 sec/batch)
2016-02-04 08:54:00.814782: step 175280, loss = 0.25 (275.9 examples/sec; 0.464 sec/batch)
2016-02-04 08:54:05.337615: step 175290, loss = 0.22 (271.5 examples/sec; 0.471 sec/batch)
2016-02-04 08:54:09.849312: step 175300, loss = 0.28 (260.5 examples/sec; 0.491 sec/batch)
2016-02-04 08:54:14.497635: step 175310, loss = 0.24 (296.9 examples/sec; 0.431 sec/batch)
2016-02-04 08:54:18.983779: step 175320, loss = 0.26 (260.9 examples/sec; 0.491 sec/batch)
2016-02-04 08:54:23.555550: step 175330, loss = 0.24 (269.0 examples/sec; 0.476 sec/batch)
2016-02-04 08:54:28.138790: step 175340, loss = 0.23 (299.8 examples/sec; 0.427 sec/batch)
2016-02-04 08:54:32.623952: step 175350, loss = 0.25 (262.2 examples/sec; 0.488 sec/batch)
2016-02-04 08:54:36.982272: step 175360, loss = 0.28 (299.8 examples/sec; 0.427 sec/batch)
2016-02-04 08:54:41.407871: step 175370, loss = 0.28 (287.8 examples/sec; 0.445 sec/batch)
2016-02-04 08:54:45.967013: step 175380, loss = 0.23 (316.9 examples/sec; 0.404 sec/batch)
2016-02-04 08:54:50.442020: step 175390, loss = 0.37 (263.0 examples/sec; 0.487 sec/batch)
2016-02-04 08:54:54.713394: step 175400, loss = 0.22 (356.1 examples/sec; 0.359 sec/batch)
2016-02-04 08:54:59.431177: step 175410, loss = 0.27 (301.6 examples/sec; 0.424 sec/batch)
2016-02-04 08:55:03.790366: step 175420, loss = 0.25 (287.5 examples/sec; 0.445 sec/batch)
2016-02-04 08:55:08.323956: step 175430, loss = 0.23 (278.8 examples/sec; 0.459 sec/batch)
2016-02-04 08:55:12.963665: step 175440, loss = 0.30 (289.2 examples/sec; 0.443 sec/batch)
2016-02-04 08:55:17.226361: step 175450, loss = 0.33 (303.7 examples/sec; 0.421 sec/batch)
2016-02-04 08:55:21.732327: step 175460, loss = 0.30 (279.8 examples/sec; 0.458 sec/batch)
2016-02-04 08:55:26.330304: step 175470, loss = 0.27 (261.3 examples/sec; 0.490 sec/batch)
2016-02-04 08:55:30.934023: step 175480, loss = 0.23 (246.1 examples/sec; 0.520 sec/batch)
2016-02-04 08:55:35.359753: step 175490, loss = 0.32 (292.6 examples/sec; 0.438 sec/batch)
2016-02-04 08:55:39.576103: step 175500, loss = 0.31 (282.1 examples/sec; 0.454 sec/batch)
2016-02-04 08:55:44.520626: step 175510, loss = 0.24 (278.6 examples/sec; 0.459 sec/batch)
2016-02-04 08:55:48.968924: step 175520, loss = 0.23 (287.9 examples/sec; 0.445 sec/batch)
2016-02-04 08:55:53.619044: step 175530, loss = 0.20 (278.0 examples/sec; 0.460 sec/batch)
2016-02-04 08:55:58.153986: step 175540, loss = 0.19 (269.9 examples/sec; 0.474 sec/batch)
2016-02-04 08:56:02.696755: step 175550, loss = 0.29 (271.6 examples/sec; 0.471 sec/batch)
2016-02-04 08:56:07.286739: step 175560, loss = 0.28 (286.4 examples/sec; 0.447 sec/batch)
2016-02-04 08:56:11.745131: step 175570, loss = 0.28 (284.6 examples/sec; 0.450 sec/batch)
2016-02-04 08:56:16.344738: step 175580, loss = 0.28 (277.2 examples/sec; 0.462 sec/batch)
2016-02-04 08:56:20.882049: step 175590, loss = 0.30 (262.1 examples/sec; 0.488 sec/batch)
2016-02-04 08:56:25.322619: step 175600, loss = 0.27 (293.2 examples/sec; 0.437 sec/batch)
2016-02-04 08:56:30.308364: step 175610, loss = 0.28 (290.2 examples/sec; 0.441 sec/batch)
2016-02-04 08:56:34.664488: step 175620, loss = 0.23 (288.8 examples/sec; 0.443 sec/batch)
2016-02-04 08:56:39.101742: step 175630, loss = 0.23 (315.3 examples/sec; 0.406 sec/batch)
2016-02-04 08:56:43.519695: step 175640, loss = 0.30 (273.8 examples/sec; 0.468 sec/batch)
2016-02-04 08:56:48.030512: step 175650, loss = 0.27 (297.0 examples/sec; 0.431 sec/batch)
2016-02-04 08:56:52.469004: step 175660, loss = 0.23 (260.1 examples/sec; 0.492 sec/batch)
2016-02-04 08:56:56.988190: step 175670, loss = 0.28 (263.6 examples/sec; 0.486 sec/batch)
2016-02-04 08:57:01.570931: step 175680, loss = 0.25 (283.8 examples/sec; 0.451 sec/batch)
2016-02-04 08:57:06.076590: step 175690, loss = 0.29 (266.6 examples/sec; 0.480 sec/batch)
2016-02-04 08:57:10.649482: step 175700, loss = 0.26 (278.7 examples/sec; 0.459 sec/batch)
2016-02-04 08:57:15.589355: step 175710, loss = 0.26 (308.7 examples/sec; 0.415 sec/batch)
2016-02-04 08:57:20.129348: step 175720, loss = 0.27 (211.7 examples/sec; 0.605 sec/batch)
2016-02-04 08:57:24.554539: step 175730, loss = 0.29 (288.4 examples/sec; 0.444 sec/batch)
2016-02-04 08:57:29.058971: step 175740, loss = 0.39 (288.2 examples/sec; 0.444 sec/batch)
2016-02-04 08:57:33.378590: step 175750, loss = 0.27 (270.7 examples/sec; 0.473 sec/batch)
2016-02-04 08:57:37.915199: step 175760, loss = 0.24 (330.3 examples/sec; 0.388 sec/batch)
2016-02-04 08:57:42.524789: step 175770, loss = 0.27 (286.6 examples/sec; 0.447 sec/batch)
2016-02-04 08:57:46.971006: step 175780, loss = 0.30 (276.6 examples/sec; 0.463 sec/batch)
2016-02-04 08:57:51.492008: step 175790, loss = 0.31 (283.4 examples/sec; 0.452 sec/batch)
2016-02-04 08:57:55.973975: step 175800, loss = 0.28 (299.3 examples/sec; 0.428 sec/batch)
2016-02-04 08:58:00.986796: step 175810, loss = 0.30 (283.2 examples/sec; 0.452 sec/batch)
2016-02-04 08:58:05.556219: step 175820, loss = 0.25 (310.4 examples/sec; 0.412 sec/batch)
2016-02-04 08:58:09.876876: step 175830, loss = 0.34 (305.9 examples/sec; 0.418 sec/batch)
2016-02-04 08:58:14.348458: step 175840, loss = 0.28 (279.3 examples/sec; 0.458 sec/batch)
2016-02-04 08:58:18.822076: step 175850, loss = 0.33 (281.1 examples/sec; 0.455 sec/batch)
2016-02-04 08:58:23.220757: step 175860, loss = 0.21 (310.1 examples/sec; 0.413 sec/batch)
2016-02-04 08:58:27.823887: step 175870, loss = 0.32 (257.6 examples/sec; 0.497 sec/batch)
2016-02-04 08:58:32.353330: step 175880, loss = 0.25 (349.2 examples/sec; 0.367 sec/batch)
2016-02-04 08:58:36.675920: step 175890, loss = 0.32 (294.8 examples/sec; 0.434 sec/batch)
2016-02-04 08:58:41.194802: step 175900, loss = 0.30 (282.8 examples/sec; 0.453 sec/batch)
2016-02-04 08:58:46.114104: step 175910, loss = 0.32 (296.6 examples/sec; 0.432 sec/batch)
2016-02-04 08:58:50.635560: step 175920, loss = 0.21 (285.7 examples/sec; 0.448 sec/batch)
2016-02-04 08:58:55.170005: step 175930, loss = 0.31 (295.0 examples/sec; 0.434 sec/batch)
2016-02-04 08:58:59.693997: step 175940, loss = 0.28 (301.5 examples/sec; 0.425 sec/batch)
2016-02-04 08:59:04.182797: step 175950, loss = 0.27 (293.5 examples/sec; 0.436 sec/batch)
2016-02-04 08:59:08.643092: step 175960, loss = 0.28 (264.9 examples/sec; 0.483 sec/batch)
2016-02-04 08:59:13.146979: step 175970, loss = 0.36 (335.0 examples/sec; 0.382 sec/batch)
2016-02-04 08:59:17.708563: step 175980, loss = 0.28 (280.1 examples/sec; 0.457 sec/batch)
2016-02-04 08:59:22.276360: step 175990, loss = 0.28 (281.3 examples/sec; 0.455 sec/batch)
2016-02-04 08:59:26.830594: step 176000, loss = 0.27 (264.1 examples/sec; 0.485 sec/batch)
2016-02-04 08:59:31.674665: step 176010, loss = 0.20 (289.2 examples/sec; 0.443 sec/batch)
2016-02-04 08:59:35.967103: step 176020, loss = 0.27 (257.0 examples/sec; 0.498 sec/batch)
2016-02-04 08:59:40.213084: step 176030, loss = 0.20 (313.7 examples/sec; 0.408 sec/batch)
2016-02-04 08:59:44.689166: step 176040, loss = 0.24 (261.8 examples/sec; 0.489 sec/batch)
2016-02-04 08:59:49.290808: step 176050, loss = 0.23 (286.4 examples/sec; 0.447 sec/batch)
2016-02-04 08:59:53.730357: step 176060, loss = 0.27 (271.4 examples/sec; 0.472 sec/batch)
2016-02-04 08:59:58.252423: step 176070, loss = 0.26 (272.1 examples/sec; 0.470 sec/batch)
2016-02-04 09:00:02.680530: step 176080, loss = 0.22 (290.4 examples/sec; 0.441 sec/batch)
2016-02-04 09:00:06.968265: step 176090, loss = 0.30 (277.3 examples/sec; 0.462 sec/batch)
2016-02-04 09:00:11.322718: step 176100, loss = 0.21 (259.0 examples/sec; 0.494 sec/batch)
2016-02-04 09:00:16.320421: step 176110, loss = 0.26 (298.9 examples/sec; 0.428 sec/batch)
2016-02-04 09:00:20.731993: step 176120, loss = 0.26 (293.4 examples/sec; 0.436 sec/batch)
2016-02-04 09:00:25.308000: step 176130, loss = 0.37 (261.6 examples/sec; 0.489 sec/batch)
2016-02-04 09:00:29.865667: step 176140, loss = 0.29 (285.9 examples/sec; 0.448 sec/batch)
2016-02-04 09:00:34.433243: step 176150, loss = 0.27 (283.8 examples/sec; 0.451 sec/batch)
2016-02-04 09:00:38.966478: step 176160, loss = 0.24 (259.0 examples/sec; 0.494 sec/batch)
2016-02-04 09:00:43.655379: step 176170, loss = 0.30 (241.1 examples/sec; 0.531 sec/batch)
2016-02-04 09:00:48.186422: step 176180, loss = 0.22 (286.1 examples/sec; 0.447 sec/batch)
2016-02-04 09:00:52.797956: step 176190, loss = 0.23 (292.0 examples/sec; 0.438 sec/batch)
2016-02-04 09:00:57.500119: step 176200, loss = 0.28 (242.9 examples/sec; 0.527 sec/batch)
2016-02-04 09:01:02.628606: step 176210, loss = 0.29 (271.8 examples/sec; 0.471 sec/batch)
2016-02-04 09:01:07.110939: step 176220, loss = 0.22 (287.1 examples/sec; 0.446 sec/batch)
2016-02-04 09:01:11.547699: step 176230, loss = 0.27 (280.6 examples/sec; 0.456 sec/batch)
2016-02-04 09:01:16.125174: step 176240, loss = 0.19 (273.8 examples/sec; 0.468 sec/batch)
2016-02-04 09:01:20.720657: step 176250, loss = 0.24 (243.7 examples/sec; 0.525 sec/batch)
2016-02-04 09:01:25.289935: step 176260, loss = 0.21 (275.6 examples/sec; 0.464 sec/batch)
2016-02-04 09:01:29.748424: step 176270, loss = 0.26 (287.3 examples/sec; 0.446 sec/batch)
2016-02-04 09:01:34.419309: step 176280, loss = 0.23 (298.8 examples/sec; 0.428 sec/batch)
2016-02-04 09:01:38.976286: step 176290, loss = 0.26 (276.4 examples/sec; 0.463 sec/batch)
2016-02-04 09:01:43.401372: step 176300, loss = 0.27 (289.3 examples/sec; 0.442 sec/batch)
2016-02-04 09:01:48.331911: step 176310, loss = 0.36 (293.6 examples/sec; 0.436 sec/batch)
2016-02-04 09:01:52.927640: step 176320, loss = 0.31 (297.7 examples/sec; 0.430 sec/batch)
2016-02-04 09:01:57.476198: step 176330, loss = 0.26 (285.1 examples/sec; 0.449 sec/batch)
2016-02-04 09:02:01.813158: step 176340, loss = 0.27 (297.3 examples/sec; 0.431 sec/batch)
2016-02-04 09:02:06.283479: step 176350, loss = 0.27 (271.1 examples/sec; 0.472 sec/batch)
2016-02-04 09:02:10.800360: step 176360, loss = 0.26 (299.6 examples/sec; 0.427 sec/batch)
2016-02-04 09:02:15.348590: step 176370, loss = 0.26 (292.2 examples/sec; 0.438 sec/batch)
2016-02-04 09:02:19.906789: step 176380, loss = 0.26 (291.9 examples/sec; 0.439 sec/batch)
2016-02-04 09:02:24.455618: step 176390, loss = 0.28 (289.9 examples/sec; 0.442 sec/batch)
2016-02-04 09:02:28.980270: step 176400, loss = 0.28 (286.9 examples/sec; 0.446 sec/batch)
2016-02-04 09:02:33.905014: step 176410, loss = 0.31 (288.7 examples/sec; 0.443 sec/batch)
2016-02-04 09:02:38.362869: step 176420, loss = 0.30 (302.0 examples/sec; 0.424 sec/batch)
2016-02-04 09:02:42.896669: step 176430, loss = 0.24 (287.5 examples/sec; 0.445 sec/batch)
2016-02-04 09:02:47.357660: step 176440, loss = 0.35 (265.0 examples/sec; 0.483 sec/batch)
2016-02-04 09:02:51.854716: step 176450, loss = 0.25 (265.9 examples/sec; 0.481 sec/batch)
2016-02-04 09:02:56.467372: step 176460, loss = 0.27 (308.4 examples/sec; 0.415 sec/batch)
2016-02-04 09:03:01.084350: step 176470, loss = 0.27 (273.1 examples/sec; 0.469 sec/batch)
2016-02-04 09:03:05.695980: step 176480, loss = 0.31 (246.4 examples/sec; 0.519 sec/batch)
2016-02-04 09:03:10.324828: step 176490, loss = 0.30 (283.6 examples/sec; 0.451 sec/batch)
2016-02-04 09:03:14.764742: step 176500, loss = 0.29 (294.4 examples/sec; 0.435 sec/batch)
2016-02-04 09:03:19.843771: step 176510, loss = 0.20 (265.4 examples/sec; 0.482 sec/batch)
2016-02-04 09:03:24.553947: step 176520, loss = 0.27 (288.7 examples/sec; 0.443 sec/batch)
2016-02-04 09:03:29.160612: step 176530, loss = 0.36 (288.0 examples/sec; 0.444 sec/batch)
2016-02-04 09:03:33.659777: step 176540, loss = 0.21 (289.5 examples/sec; 0.442 sec/batch)
2016-02-04 09:03:38.216177: step 176550, loss = 0.31 (282.2 examples/sec; 0.454 sec/batch)
2016-02-04 09:03:42.759272: step 176560, loss = 0.26 (300.3 examples/sec; 0.426 sec/batch)
2016-02-04 09:03:47.323470: step 176570, loss = 0.25 (275.2 examples/sec; 0.465 sec/batch)
2016-02-04 09:03:51.779588: step 176580, loss = 0.30 (318.1 examples/sec; 0.402 sec/batch)
2016-02-04 09:03:56.152621: step 176590, loss = 0.25 (269.8 examples/sec; 0.474 sec/batch)
2016-02-04 09:04:00.621839: step 176600, loss = 0.25 (277.0 examples/sec; 0.462 sec/batch)
2016-02-04 09:04:05.630992: step 176610, loss = 0.20 (315.2 examples/sec; 0.406 sec/batch)
2016-02-04 09:04:09.860806: step 176620, loss = 0.26 (315.8 examples/sec; 0.405 sec/batch)
2016-02-04 09:04:14.449700: step 176630, loss = 0.23 (321.9 examples/sec; 0.398 sec/batch)
2016-02-04 09:04:18.931422: step 176640, loss = 0.24 (304.7 examples/sec; 0.420 sec/batch)
2016-02-04 09:04:23.461856: step 176650, loss = 0.31 (264.5 examples/sec; 0.484 sec/batch)
2016-02-04 09:04:27.857529: step 176660, loss = 0.25 (293.6 examples/sec; 0.436 sec/batch)
2016-02-04 09:04:32.384676: step 176670, loss = 0.31 (262.6 examples/sec; 0.488 sec/batch)
2016-02-04 09:04:36.924696: step 176680, loss = 0.27 (293.0 examples/sec; 0.437 sec/batch)
2016-02-04 09:04:41.555017: step 176690, loss = 0.27 (258.9 examples/sec; 0.494 sec/batch)
2016-02-04 09:04:46.105995: step 176700, loss = 0.27 (278.1 examples/sec; 0.460 sec/batch)
2016-02-04 09:04:51.019275: step 176710, loss = 0.32 (253.1 examples/sec; 0.506 sec/batch)
2016-02-04 09:04:55.575987: step 176720, loss = 0.26 (275.3 examples/sec; 0.465 sec/batch)
2016-02-04 09:05:00.014179: step 176730, loss = 0.33 (257.6 examples/sec; 0.497 sec/batch)
2016-02-04 09:05:04.637864: step 176740, loss = 0.37 (298.3 examples/sec; 0.429 sec/batch)
2016-02-04 09:05:09.170237: step 
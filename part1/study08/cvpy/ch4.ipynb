{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4\n",
    "\n",
    "# Camera Models and Augmented Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 The Pin-hole Camera Model \n",
    "\n",
    "\n",
    "### pin-hole camera model 참고\n",
    "\n",
    "핀홀 카메라 모델은 아래 그림과 같이 하나의 바늘구멍(pinhole)을 통해 외부의 상이 이미지로 투영된다는 모델. \n",
    "이 바늘구멍(pinhole)이 렌즈 중심에 해당되며 이곳에서 뒷면의 상이 맺히는 곳까지의 거리가 카메라 초점거리 임.\n",
    "\n",
    "\n",
    "![](pinhole.jpg)\n",
    "<그림 > pinhole 카메라 모델 (그림출처: 위키피디아)\n",
    "\n",
    "\n",
    " \n",
    "### 카메라 초점거리(focal length)는 렌즈에서 이미지 센서까지의 거리\n",
    "![](http://cfile6.uf.tistory.com/image/2535F04A52672EB427032B)\n",
    "<그림 > 렌즈-이미지센서 투영\n",
    "\n",
    "\n",
    "초점거리라 하면 볼록렌즈의 초점을 생각하기 쉬운데, 여기서(카메라 모델) 말하는 초점거리는 렌즈중심과 이미지센서(CCD, CMOS 등)와의 거리를 말함.\n",
    "\n",
    "디지털 카메라 등에서 초점거리는 mm 단위로 표현되지만 카메라 모델에서 말하는 카메라 초점거리는 픽셀(pixel) 단위로 표현됩니다. 즉, fx, fy의 단위가 픽셀임.\n",
    " \n",
    "\n",
    "초점으로부터 거리가 1(unit distance)인 평면을 normalized image plane이라고 부르며 이 평면상의 좌표를 보통 normalized image coordinate라고 부릅니다. 물론 이것은 실제는 존재하지 않는 가상의(상상의) 이미지 평면입니다. 카메라 좌표계 상의 한 점 (Xc, Yc, Zc)를 영상좌표계로 변환할 때 먼저 Xc, Yc를 Zc(카메라 초점에서의 거리)로 나누는 것은 이 normalized image plane 상의 좌표로 변환하는 것이며, 여기에 다시 초점거리 f를 곱하면 우리가 원하는 실제 영상좌표(pixel)가 나옵니다 (아래 그림 참조).\n",
    "\n",
    "![](http://cfile29.uf.tistory.com/image/017ADB3A511435821DF008)\n",
    "\n",
    "\n",
    "\n",
    "[출처 : http://darkpgmr.tistory.com/  다크 프로그래머]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pin-hole camera model\n",
    "![](4-1.jpg)\n",
    "\n",
    "Figure 4.1: The pin-hole camera model. The image point x is at the intersection of the\n",
    "image plane and the line joining the 3D point X and the camera center C. The dashed\n",
    "line is the optical axis of the camera.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting 3D Points\n",
    "### Let’s create a camera class to handle all the operations we need for modeling cameras and projections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "from scipy import linalg \n",
    "\n",
    "class Camera(object):\n",
    "    \"\"\" Class for representing pin-hole cameras. \"\"\"\n",
    "\n",
    "    def __init__(self,P):\n",
    "        \"\"\" Initialize P = K[R|t] camera model. \"\"\"\n",
    "        self.P = P\n",
    "        self.K = None # calibration matrix self.R = None # rotation\n",
    "        self.t = None # translation\n",
    "        self.c = None # camera center\n",
    "\n",
    "    def project(self,X):\n",
    "        \"\"\" Project points in X (4*n array) and normalize coordinates. \"\"\"\n",
    "        x = dot(self.P,X) \n",
    "        for i in range(3):\n",
    "            x[i] /= x[2] \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The example below shows how to project 3D points into an image view. \n",
    "### In this example, we will use one of  the Oxford multi-view datasets, the “Model House” data set, available at  http://www.robots.ox.ac.uk/~vgg/data/data-mview.html. Download the 3D geometry file and copy the house.p3d file to your working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named camera",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2465918a2722>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcamera\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# load points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'house.p3d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named camera"
     ]
    }
   ],
   "source": [
    "\n",
    "import camera\n",
    "\n",
    "# load points\n",
    "points = loadtxt('house.p3d').T\n",
    "points = vstack((points,ones(points.shape[1])))\n",
    "\n",
    "# setup camera\n",
    "P = hstack((eye(3),array([[0],[0],[-10]]))) \n",
    "cam = camera.Camera(P)\n",
    "x = cam.project(points)\n",
    "\n",
    "# plot projection\n",
    "figure() \n",
    "plot(x[0],x[1],'k.') \n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To see how moving the camera changes the projection, try the following piece of code that incrementally rotates the camera around a random 3D axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-d19050abacdf>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-d19050abacdf>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    plot(x[0],x[1],’k.’)\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# create transformation\n",
    "r = 0.05*random.rand(3)\n",
    "rot = camera.rotation_matrix(r)\n",
    "# rotate camera and project\n",
    "figure()\n",
    "for t in range(20):\n",
    "    cam.P = dot(cam.P,rot)\n",
    "    x = cam.project(points)\n",
    "plot(x[0],x[1],’k.’)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we used the helper function rotation_matrix(), which creates a rotation matrix\n",
    "### for 3D rotations around a vector (add this to camera.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rotation_matrix(a):\n",
    "    \"\"\" Creates a 3D rotation matrix for rotation around the axis of the vector a. \"\"\"\n",
    "    R = eye(4)\n",
    "    R[:3,:3] = linalg.expm([[0,-a[2],a[1]],[a[2],0,-a[0]],[-a[1],a[0],0]]) \n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](4-2.jpg)\n",
    "\n",
    "Figure 4.2: An example of projecting 3D points. (left) sample image. (middle) projected\n",
    "points into a view. (right) trajectory of projected points under camera rotation.\n",
    "Data from the Oxford \"Model House\" dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factoring the Camera Matrix\n",
    "### If we are given a camera matrix P of the form in equation (4.2), we need to be able to recover the internal parameters K and the camera position and pose t and R.\n",
    "### Partitioning the matrix is called factorization. In this case, we will use a type of matrix factorization called RQ-factorization.\n",
    "### Add the following method to the Camera class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def factor(self):\n",
    "    \"\"\" Factorize the camera matrix into K,R,t as P = K[R|t]. \"\"\"\n",
    "    # factor first 3*3 part\n",
    "    K,R = linalg.rq(self.P[:,:3])\n",
    "    # make diagonal of K positive\n",
    "    T = diag(sign(diag(K)))\n",
    "    if linalg.det(T) < 0:\n",
    "        T[1,1] *= -1\n",
    "    self.K = dot(K,T)\n",
    "    self.R = dot(T,R) # T is its own inverse\n",
    "    self.t = dot(linalg.inv(self.K),self.P[:,3])\n",
    "    return self.K, self.R, self.t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ-factorization is not unique, there is a sign ambiguity in the factorization. Since we need the rotation matrix R to have positive determinant (otherwise the coordinate axis can get flipped), we can add a transform T to change the sign when needed.\n",
    "### Try this on a sample camera to see that it works:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named camera",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a6a57c7aad02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcamera\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcamera\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotation_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mRt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named camera"
     ]
    }
   ],
   "source": [
    "import camera\n",
    "\n",
    "K = array([[1000,0,500],[0,1000,300],[0,0,1]])\n",
    "tmp = camera.rotation_matrix([0,0,1])[:3,:3]\n",
    "Rt = hstack((tmp,array([[50],[40],[30]])))\n",
    "cam = camera.Camera(dot(K,Rt))\n",
    "\n",
    "print K,Rt\n",
    "print cam.factor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Camera Center\n",
    "### Given a camera projection matrix, P, it is useful to be able to compute the camera’s position in space. The camera center, C, is a 3D point with the property P C = 0. For acamerawith  P =K [R|t], this gives\n",
    "## K[R | t]C = KRC + Kt = 0, \n",
    "### and the camera center can be computed as\n",
    "## C = −RT t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-14-9194a5323978>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-9194a5323978>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    self.factor()\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def center(self):\n",
    "\n",
    "    if self.c is not None:\n",
    "        return self.c\n",
    "    else:\n",
    "    # compute c by factoring\n",
    "    self.factor()\n",
    "    self.c = -dot(self.R.T,self.t)\n",
    "    return self.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Camera Calibration\n",
    "\n",
    "\n",
    "### 카메라 캘리브레이션(camera calibration)은 위와 같은 3D 공간좌표와 2D 영상좌표 사이의 변환관계 또는 이 변환관계를 설명하는 파라미터를 찾는 과정\n",
    "\n",
    "### 핀홀(pinhole) 카메라 모델에서 이러한 변환 관계는 다음과 같이 모델링 됨.\n",
    "\n",
    "![](matrix.jpg)\n",
    "\n",
    "여기서, (X,Y,Z)는 월드 좌표계(world coordinate system) 상의 3D 점의 좌표, [R|t]는 월드 좌표계를 카메라 좌표계로 변환시키기 위한 회전/이동변환 행렬이며 A는 camera matrix\n",
    "\n",
    "![](calibration.jpg)\n",
    " \n",
    " \n",
    " 식(1)에서 [R|t]를 카메라 외부 파라미터(extrinsic parameter), A를 내부 파라미터(intrinsic parameter)라고 함.\n",
    " \n",
    " 카메라 외부 파라미터는 카메라의 설치 높이, 방향(팬, 틸트) 등 카메라와 외부 공간과의 기하학적 관계에 관련된 파라미터이며 내부 파라미터는 카메라의 초점 거리, aspect ratio, 중심점 등 카메라 자체의 내부적인 파라미터를 의미\n",
    " \n",
    " ![](4-3.jpg)\n",
    "Figure 4.3: A simple camera calibration setup. (left) an image of the setup used.\n",
    "(right) the image used for the calibration. Measuring the width and height of the\n",
    "calibration object in the image and the physical dimensions of the setup is enough to\n",
    "determine the focal length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Pose Estimation from Planes and Markers\n",
    "\n",
    "### we saw how to estimate homographies between planes. \n",
    "### Combining this with a calibrated camera makes it possible to compute the camera’s pose\n",
    "### Let’s illustrate with an example. Consider the two top images in Figure 4-4. The follow- ing code will extract SIFT features in both images and robustly estimate a homography using RANSAC:\n",
    "\n",
    "![](4-4.jpg)\n",
    "Figure 4.4: Example of computing the projection matrix for a new view using a planar\n",
    "object as marker. Matching image features to an aligned marker gives a homography\n",
    "that can be used to compute the pose of the camera. (top left) template image with\n",
    "a blue square. (top right) an image taken from an unknown viewpoint with the same\n",
    "square transformed with the estimated homography. (bottom) a cube transformed\n",
    "using the estimated camera matrix.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named homography",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-fd77e9c42693>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhomography\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcamera\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msift\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# compute features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msift\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'book_frontal.JPG'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'im0.sift'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named homography"
     ]
    }
   ],
   "source": [
    "import homography \n",
    "import camera \n",
    "import sift\n",
    "# compute features\n",
    "sift.process_image('book_frontal.JPG','im0.sift') \n",
    "l0,d0 = sift.read_features_from_file('im0.sift')\n",
    "sift.process_image('book_perspective.JPG','im1.sift') \n",
    "l1,d1 = sift.read_features_from_file('im1.sift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sift' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-da14793639a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# match features and estimate homography\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msift\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_twosided\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mndx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhomography\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_homog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mndx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mndx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mndx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sift' is not defined"
     ]
    }
   ],
   "source": [
    "# match features and estimate homography\n",
    "matches = sift.match_twosided(d0,d1)\n",
    "ndx = matches.nonzero()[0]\n",
    "fp = homography.make_homog(l0[ndx,:2].T) \n",
    "ndx2 = [int(matches[i]) for i in ndx]\n",
    "tp = homography.make_homog(l1[ndx2,:2].T)\n",
    "\n",
    "model = homography.RansacModel()\n",
    "H = homography.H_from_ransac(fp,tp,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To check our results, we will need some simple 3D object placed on the marker. Here we will use a cube and generate the cube points using the function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cube_points(c,wid):\n",
    "    p = []\n",
    "    # bottom\n",
    "    p.append([c[0]-wid,c[1]-wid,c[2]-wid]) \n",
    "    p.append([c[0]-wid,c[1]+wid,c[2]-wid]) \n",
    "    p.append([c[0]+wid,c[1]+wid,c[2]-wid]) \n",
    "    p.append([c[0]+wid,c[1]-wid,c[2]-wid]) \n",
    "    p.append([c[0]-wid,c[1]-wid,c[2]-wid])\n",
    "    # top\n",
    "    p.append([c[0]-wid,c[1]-wid,c[2]+wid]) \n",
    "    p.append([c[0]-wid,c[1]+wid,c[2]+wid]) \n",
    "    p.append([c[0]+wid,c[1]+wid,c[2]+wid]) \n",
    "    p.append([c[0]+wid,c[1]-wid,c[2]+wid]) \n",
    "    p.append([c[0]-wid,c[1]-wid,c[2]+wid])\n",
    "    # vertical sides\n",
    "    p.append([c[0]-wid,c[1]-wid,c[2]+wid]) \n",
    "    p.append([c[0]-wid,c[1]+wid,c[2]+wid]) \n",
    "    p.append([c[0]-wid,c[1]+wid,c[2]-wid]) \n",
    "    p.append([c[0]+wid,c[1]+wid,c[2]-wid]) \n",
    "    p.append([c[0]+wid,c[1]+wid,c[2]+wid]) \n",
    "    p.append([c[0]+wid,c[1]-wid,c[2]+wid]) \n",
    "    p.append([c[0]+wid,c[1]-wid,c[2]-wid])\n",
    "    # same as first to close plot\n",
    "    # same as first to close plot\n",
    "    return array(p).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some points are reoccurring so that plot() will generate a nice-looking cube.\n",
    "### With a homography and a camera calibration matrix, we can now determine the relative transformation between the two views:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_calibration' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-8b6050281cae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# camera calibration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_calibration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m747\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 3D points at plane z=0 with sides of length 0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcube_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'my_calibration' is not defined"
     ]
    }
   ],
   "source": [
    "# camera calibration\n",
    "K = my_calibration((747,1000))\n",
    "\n",
    "# 3D points at plane z=0 with sides of length 0.2\n",
    "box = cube_points([0,0,0.1],0.1)\n",
    "\n",
    "# project bottom square in first image\n",
    "cam1 = camera.Camera( hstack((K,dot(K,array([[0],[0],[-1]])) )) )\n",
    "\n",
    "# first points are the bottom square\n",
    "box_cam1 = cam1.project(homography.make_homog(box[:,:5]))\n",
    "\n",
    "# use H to transfer points to the second image\n",
    "box_trans = homography.normalize(dot(H,box_cam1))\n",
    "\n",
    "# compute second camera matrix from cam1 and H\n",
    "cam2 = camera.Camera(dot(H,cam1.P))\n",
    "A = dot(linalg.inv(K),cam2.P[:,:3])\n",
    "A = array([A[:,0],A[:,1],cross(A[:,0],A[:,1])]).T \n",
    "cam2.P[:,:3] = dot(K,A)\n",
    "\n",
    "# project with the second camera\n",
    "box_cam2 = cam2.project(homography.make_homog(box))\n",
    "\n",
    "# test: projecting point on z=0 should give the same\n",
    "point = array([1,1,0,1]).T\n",
    "print homography.normalize(dot(dot(H,cam1.P),point)) \n",
    "print cam2.project(point)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Augmented Reality\n",
    "\n",
    "### Augmented reality (AR) is a collective term for placing objects and information on top of image data. The classic example is placing a 3D computer graphics model so that it looks like it belongs in the scene, and moves naturally with the camera motion in the case of video.\n",
    "\n",
    "## From Camera Matrix to OpenGL Format\n",
    "### OpenGL uses 4 × 4 matrices to represent transforms (both 3D transforms and projec- tions). This is only slightly different from our use of 3 × 4 camera matrices.\n",
    "\n",
    "### Given that we have a camera calibrated so that the calibration matrix K is known, the following function translates the camera properties to an OpenGL projection matrix:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def set_projection_from_camera(K):\n",
    "    \"\"\" Set view from a camera calibration matrix. \"\"\"\n",
    "    glMatrixMode(GL_PROJECTION) \n",
    "    glLoadIdentity()\n",
    "    fx = K[0,0]\n",
    "    fy = K[1,1]\n",
    "    fovy = 2*arctan(0.5*height/fy)*180/pi \n",
    "    aspect = (width*fy)/(height*fx)\n",
    "    \n",
    "    # define the near and far clipping planes\n",
    "    near = 0.1 \n",
    "    far = 100.0\n",
    "    \n",
    "    # set perspective\n",
    "    gluPerspective(fovy,aspect,near,far) \n",
    "    glViewport(0,0,width,height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def set_modelview_from_camera(Rt):\n",
    "    \"\"\" Set the model view matrix from camera pose. \"\"\"\n",
    "    glMatrixMode(GL_MODELVIEW) \n",
    "    glLoadIdentity()\n",
    "    \n",
    "    # rotate teapot 90 deg around x-axis so that z-axis is up\n",
    "    Rx = array([[1,0,0],[0,0,-1],[0,1,0]])\n",
    "    \n",
    "    # set rotation to best approximation\n",
    "    R = Rt[:,:3]\n",
    "    U,S,V = linalg.svd(R)\n",
    "    R = dot(U,V)\n",
    "    R[0,:] = -R[0,:] # change sign of x-axis\n",
    "    \n",
    "    # set translation\n",
    "    t = Rt[:,3]\n",
    "    \n",
    "    # setup 4*4 model view matrix\n",
    "    M = eye(4)\n",
    "    M[:3,:3] = dot(R,Rx) \n",
    "    M[:3,3] = t\n",
    "    \n",
    "    # transpose and flatten to get column order\n",
    "    M = M.T\n",
    "    m = M.flatten()\n",
    "    # replace model view with the new matrix\n",
    "    glLoadMatrixf(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placing Virtual Objects in the Image\n",
    "### The first thing we need to do is to add the image (the one we want to place virtual objects in) as a background. In OpenGL this is done by creating a quadrilateral, a quad, that fills the whole view. \n",
    "\n",
    "### This function loads an image, converts it to an OpenGL texture, and places that texture on the quad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_background(imname):\n",
    "    \"\"\" Draw background image using a quad. \"\"\"\n",
    "    # load background image (should be .bmp) to OpenGL texture\n",
    "    bg_image = pygame.image.load(imname).convert() \n",
    "    bg_data = pygame.image.tostring(bg_image,\"RGBX\",1)\n",
    "    glMatrixMode(GL_MODELVIEW)\n",
    "    glLoadIdentity()\n",
    "    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)\n",
    "    \n",
    "    # bind the texture\n",
    "    glEnable(GL_TEXTURE_2D)\n",
    "    glBindTexture(GL_TEXTURE_2D,glGenTextures(1)) \n",
    "    glTexImage2D(GL_TEXTURE_2D,0,GL_RGBA,width,height,0,GL_RGBA,GL_UNSIGNED_BYTE,bg_data) \n",
    "    glTexParameterf(GL_TEXTURE_2D,GL_TEXTURE_MAG_FILTER,GL_NEAREST) \n",
    "    glTexParameterf(GL_TEXTURE_2D,GL_TEXTURE_MIN_FILTER,GL_NEAREST)\n",
    "    \n",
    "    # create quad to fill the whole window\n",
    "    glBegin(GL_QUADS)\n",
    "    glTexCoord2f(0.0,0.0); glVertex3f(-1.0,-1.0,-1.0) \n",
    "    glTexCoord2f(1.0,0.0); glVertex3f( 1.0,-1.0,-1.0) \n",
    "    glTexCoord2f(1.0,1.0); glVertex3f( 1.0, 1.0,-1.0) \n",
    "    glTexCoord2f(0.0,1.0); glVertex3f(-1.0, 1.0,-1.0) \n",
    "    glEnd()\n",
    "    # clear the texture\n",
    "    glDeleteTextures(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tying It All Together\n",
    "### The full script for generating an image like the one in Figure 4-5 looks like this (assuming that you also have the functions introduced above in the same file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named OpenGL.GL",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-33d74b1e0294>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mOpenGL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mOpenGL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLU\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mOpenGL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLUT\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named OpenGL.GL"
     ]
    }
   ],
   "source": [
    "from OpenGL.GL import * \n",
    "from OpenGL.GLU import * \n",
    "from OpenGL.GLUT import * \n",
    "import pygame, pygame.image \n",
    "from pygame.locals import * \n",
    "import pickle\n",
    "\n",
    "width,height = 1000,747\n",
    "\n",
    "def setup():\n",
    "    \"\"\" Setup window and pygame environment. \"\"\"\n",
    "    pygame.init() \n",
    "    pygame.display.set_mode((width,height),OPENGL | DOUBLEBUF) \n",
    "    pygame.display.set_caption('OpenGL AR demo')\n",
    "    \n",
    "    # load camera data\n",
    "    with open('ar_camera.pkl','r') as f: \n",
    "        K = pickle.load(f)\n",
    "        Rt = pickle.load(f)\n",
    "    setup() \n",
    "    draw_background('book_perspective.bmp') \n",
    "    set_projection_from_camera(K) \n",
    "    set_modelview_from_camera(Rt) \n",
    "    draw_teapot(0.02)\n",
    "    \n",
    "    while True:\n",
    "        event = pygame.event.poll()\n",
    "        if event.type in (QUIT,KEYDOWN):\n",
    "            break \n",
    "    pygame.display.flip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](4-6.jpg)\n",
    "Figure 4.6: Loading a 3D model from an .obj file and placing it on a book in a scene\n",
    "using camera parameters computed from feature matches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
